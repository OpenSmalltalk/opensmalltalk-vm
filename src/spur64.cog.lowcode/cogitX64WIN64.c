/* Automatically generated by
	CCodeGenerator VMMaker.oscog-eem.3455 uuid: 3fb5350e-7e08-4a76-8b1b-7755e8e0811f
   from
	StackToRegisterMappingCogit VMMaker.oscog-eem.3455 uuid: 3fb5350e-7e08-4a76-8b1b-7755e8e0811f
 */
static char __buildInfo[] = "StackToRegisterMappingCogit VMMaker.oscog-eem.3455 uuid: 3fb5350e-7e08-4a76-8b1b-7755e8e0811f " __DATE__ ;
char *__cogitBuildInfo = __buildInfo;



#include "sqConfig.h"
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "sqPlatformSpecific.h"
#include "sqMemoryAccess.h"
#include "sqCogStackAlignment.h"
#include "dispdbg.h"
#include "cogmethod.h"
#if COGMTVM
#include "cointerpmt.h"
#else
#include "cointerp.h"
#endif
#include "cogit.h"


/*** Constants ***/
#define ABICalleeSavedRegisterMask 0xF0C8
#define ABICallerSavedRegisterMask 0xF07
#define ABIResultReg 0
#undef ABIResultRegHigh
#define AddCqR 106
#define AddCwR 114
#define AddcCqR 120
#define AddcRR 119
#define AddRdRd 132
#define AddRR 100
#define AddRsRs 139
#define AlignmentNops 3
#define AltBlockCreationBytecodeSize 3
#define AltFirstSpecialSelector 96
#define AltNumSpecialSelectors 32
#define AndCqR 108
#define AndCwR 116
#define AndRR 102
#define AnnotationShift 5
#define Arg0Reg 1
#define Arg1Reg 2
#define ArithmeticShiftRightCqR 91
#define ArithmeticShiftRightRR 92
#define BadRegisterSet 1
#define BlockCreationBytecodeSize 4
#define BSR 169
#define BytecodeSetHasDirectedSuperSend 1
#define CArg0Reg 1
#define CArg1Reg 2
#define CArg2Reg 8
#define CArg3Reg 9
#define Call 6
#define CallerSavedRegisterMask 0x706
#define CallFull 7
#define CallR 8
#define CDQ 161
#if !defined(CheckRememberedInTrampoline) /* Allow this to be overridden on the compiler command line */
# define CheckRememberedInTrampoline 0
#endif
#define CLD 165
#define ClassArray 7
#define ClassArrayCompactIndex 51
#define ClassBlockClosureCompactIndex 37
#define ClassFloatCompactIndex 34
#define ClassFullBlockClosureCompactIndex 38
#define ClassLargeNegativeInteger 42
#define ClassLargeNegativeIntegerCompactIndex 32
#define ClassLargePositiveInteger 13
#define ClassLargePositiveIntegerCompactIndex 33
#define ClassMethodContextCompactIndex 36
#define ClassPointCompactIndex 54
#define ClassReg 8
#define ClosureFirstCopiedValueIndex 3
#define ClosureIndex 4
#define ClosureNumArgsIndex 2
#define ClosureOuterContextIndex 0
#define ClosureStartPCIndex 1
#define ClzRR 157
#define CMBlock 4
#define CMClosedPIC 2
#define CMFree 1
#define CMMaxUsageCount 7
#define CMMethod 5
#define CMMethodFlaggedForBecome 6
#define CMOpenPIC 3
#define CMPXCHGRMr 174
#define CmpC32R 113
#define CmpCqR 105
#define CmpCwR 112
#define CmpRdRd 131
#define CmpRR 99
#define CmpRsRs 138
#define CompletePrimitive 4
#define ConstZero 1
#define ConvertRdR 146
#define ConvertRdRs 148
#define ConvertRRd 145
#define ConvertRRs 150
#define ConvertRsR 149
#define ConvertRsRd 147
#define CPUID 160
#if !defined(Debug) /* Allow this to be overridden on the compiler command line */
# define Debug DEBUGVM
#endif
#define DisplacementMask 0x1F
#define DisplacementX2N 0
#define DivRdRd 135
#define DivRsRs 142
#define DPFPReg0 0
#define DPFPReg1 1
#define DPFPReg2 2
#define DPFPReg3 3
#define DPFPReg4 4
#define DPFPReg5 5
#define DPFPReg6 6
#define DPFPReg7 7
#define EncounteredUnknownBytecode -6
#define Extra0Reg 7
#define Extra1Reg 6
#define Extra2Reg 12
#define Extra3Reg 13
#define Extra4Reg 14
#define Extra5Reg 15
#define Fill32 4
#define FirstAnnotation 64
#define FirstJump 12
#define FirstShortJump 16
#define FirstSpecialSelector 176
#define FoxCallerSavedIP 8
#define FoxIFSavedIP -32
#define FoxMethod -8
#define FoxMFReceiver -24
#define FoxSavedFP 0
#define FoxThisContext -16
#define FPReg 5
#define FullClosureCompiledBlockIndex 1
#define FullClosureFirstCopiedValueIndex 4
#define FullClosureReceiverIndex 3
#define GCModeBecome 8
#define GCModeFull 1
#define GCModeNewSpace 2
#define HasBytecodePC 5
#define HashMultiplyConstant 1664525
#define HashMultiplyMask 0xFFFFFFF
#define HeaderIndex 0
#define IDIVR 162
#if !defined(IMMUTABILITY) /* Allow this to be overridden on the compiler command line */
# define IMMUTABILITY 1
#endif
#define IMULRR 163
#define InFullBlock 2
#define InstanceSpecificationIndex 2
#define InstructionPointerIndex 1
#define InsufficientCodeSpace -2
#define InVanillaBlock 1
#define IsAbsPCReference 3
#define IsAnnotationExtension 1
#define IsDirectedSuperBindingSend 10
#define IsDirectedSuperSend 9
#define IsDisplacementX2N 0
#define IsNSDynamicSuperSend null
#define IsNSSelfSend null
#define IsNSSendCall null
#define IsObjectReference 2
#define IsRelativeCall 4
#define IsSendCall 7
#define IsSuperSend 8
#define Jump 16
#define JumpAbove 33
#define JumpAboveOrEqual 32
#define JumpBelow 31
#define JumpBelowOrEqual 34
#define JumpCarry 25
#define JumpFPEqual 35
#define JumpFPGreater 39
#define JumpFPGreaterOrEqual 40
#define JumpFPLess 37
#define JumpFPLessOrEqual 38
#define JumpFPNotEqual 36
#define JumpFPOrdered 41
#define JumpFPUnordered 42
#define JumpFull 12
#define JumpGreater 29
#define JumpGreaterOrEqual 28
#define JumpLess 27
#define JumpLessOrEqual 30
#define JumpLong 13
#define JumpLongNonZero 15
#define JumpLongZero 14
#define JumpNegative 19
#define JumpNoCarry 26
#define JumpNonNegative 20
#define JumpNonZero 18
#define JumpNoOverflow 22
#define JumpOverflow 21
#define JumpR 10
#define JumpZero 17
#define Label 1
#define LargeContextSlots 62
#define LastJump 42
#define LFENCE 170
#undef LinkReg
#define Literal 2
#define LiteralStart 1
#define LOCK 173
#define LoadEffectiveAddressMwrR 88
#define LogicalShiftLeftCqR 95
#define LogicalShiftLeftRR 96
#define LogicalShiftRightCqR 93
#define LogicalShiftRightRR 94
#define LowcodeContextMark 60
#define MapEnd 0
#define MaxCompiledPrimitiveIndex 582
#define MaxCPICCases 6
#define MaxMethodSize 65535
#define MaxNegativeErrorCode -8
#define MaxStackAllocSize 1572864
#define MaxStackCheckOffset 0xFFF
#define MaxX2NDisplacement 992
#define MethodCacheClass 2
#define MethodCacheMask 0xFFC
#define MethodCacheMethod 3
#define MethodCacheSelector 1
#define MethodIndex 3
#define MethodTooBig -4
#define MFENCE 171
#define MFMethodFlagHasContextFlag 1
#define MFMethodFlagIsBlockFlag 2
#define MOVSB 167
#define MOVSQ 168
#define ModReg 3
#define ModRegInd 0
#define ModRegRegDisp32 2
#define ModRegRegDisp8 1
#define MoveA32R 45
#define MoveAbR 48
#define MoveAwR 44
#define MoveC32R 71
#define MoveCqR 69
#define MoveCwR 70
#define MoveM16rR 57
#define MoveM32rR 61
#define MoveM32rRs 78
#define MoveM64rRd 75
#define MoveM8rR 54
#define MoveMbrR 65
#define MoveMs8rR 55
#define MoveMwrR 50
#define MovePerfCnt64RL 159
#define MovePerfCnt64RRL 158
#define MoveRA32 47
#define MoveRAb 49
#define MoveRAw 46
#define MoveRAwNoVBR 176
#define MoveRdM64r 76
#define MoveRdR 73
#define MoveRdRd 74
#define MoveRM16r 58
#define MoveRM32r 62
#define MoveRM8r 56
#define MoveRMbr 66
#define MoveRMwr 51
#define MoveRR 43
#define MoveRRd 72
#define MoveRsM32r 79
#define MoveRsRs 77
#define MoveRX32rR 64
#define MoveRXbrR 68
#define MoveRXwrR 53
#define MoveX32rRR 63
#define MoveXbrRR 67
#define MoveXwrRR 52
#define MulRdRd 134
#define MulRsRs 141
#define NativePopR 84
#define NativePushR 85
#define NativeRetN 86
#define NativeSPReg 4
#define NeedsMergeFixupFlag 2
#define NeedsNonMergeFixupFlag 1
#define NegateR 89
#define Nop 5
#define NoReg -1
#define NotFullyInitialized -1
#define NotR 90
#define NumObjRefsInRuntime 0
#define NumSendTrampolines 4
#define NumSpecialSelectors 32
#define NumStoreTrampolines 5
#define NumTrampolines (80 + (IMMUTABILITY ? 5 : 0))
#define OrCqR 109
#define OrCwR 117
#define OrRR 103
#define PopR 80
#define PrefetchAw 87
#define PrimCallCollectsProfileSamples 16
#define PrimCallIsExternalCall 32
#define PrimCallMayEndureCodeCompaction 8
#define PrimCallNeedsNewMethod 4
#define PrimCallOnSmalltalkStack 1
#define PrimCallOnSmalltalkStackAlign2x 2
#define PrimNumberExternalCall 117
#define PrimNumberFFICall 120
#define PushCq 82
#define PushCw 83
#define PushR 81
#define R12 12
#define R13 13
#define R15 15
#define R8 8
#define RAX 0
#define RBP 5
#define RBX 3
#define RCX 1
#define RDI 7
#define RDX 2
#define REP 166
#define ReceiverIndex 5
#define ReceiverResultReg 9
#define RetN 9
#define RISCTempReg 11
#define RotateLeftCqR 97
#define RotateRightCqR 98
#define RSI 6
#define RSP 4
#define SETE 175
#define SelectorCannotInterpret 34
#define SelectorDoesNotUnderstand 20
#define SenderIndex 0
#define SendNumArgsReg 10
#define SFENCE 172
#define ShouldNotJIT -8
#define SIB1 0
#define SIB4 2
#define SIB8 3
#define SignExtend16RR 152
#define SignExtend32RR 153
#define SignExtend8RR 151
#define SmallContextSlots 22
#define SPReg 4
#define SpecialSelectors 23
#define SqrtRd 136
#define SqrtRs 143
#define SSBaseOffset 1
#define SSConstant 2
#define SSConstantFloat32 15
#define SSConstantFloat64 16
#define SSConstantInt32 13
#define SSConstantInt64 14
#define SSConstantNativePointer 17
#define SSNativeRegister 5
#define SSRegister 3
#define SSRegisterDoubleFloat 7
#define SSRegisterPair 6
#define SSRegisterSingleFloat 8
#define SSSpill 4
#define SSSpillFloat32 11
#define SSSpillFloat64 12
#define SSSpillInt64 10
#define SSSpillNative 9
#define StackPointerIndex 2
#define Stop 11
#define SubbCqR 122
#define SubbRR 121
#define SubCqR 107
#define SubCwR 115
#define SubRdRd 133
#define SubRR 101
#define SubRsRs 140
#define TempReg 0
#define TstCqR 110
#define UnfailingPrimitive 3
#define UnimplementedPrimitive -7
#define ValueIndex 1
#define VarBaseReg 3
#define XCHGRR 164
#define XMM0L 0
#define XMM1L 1
#define XMM2L 2
#define XMM3L 3
#define XorCwR 118
#define XorRdRd 137
#define XorRR 104
#define XorRsRs 144
#define YoungSelectorInPIC -5
#define ZeroExtend16RR 155
#define ZeroExtend32RR 156
#define ZeroExtend8RR 154

typedef struct _AbstractInstruction {
	unsigned char	opcode;
	unsigned char	machineCodeSize;
	unsigned char	maxSize;
	unsigned char	annotation;
	usqInt		operands[3];
	usqInt	address;
	struct _AbstractInstruction *dependent;
	unsigned char		machineCode[14];
 } AbstractInstruction;

#define CogInLineLiteralsX64Compiler AbstractInstruction
#define CogX64Compiler AbstractInstruction
#define CogAbstractInstruction AbstractInstruction


typedef struct {
	AbstractInstruction *fakeHeader;
	AbstractInstruction *fillInstruction;
	sqInt	numArgs;
	sqInt	numCopied;
	sqInt	numInitialNils;
	sqInt	startpc;
	AbstractInstruction *entryLabel;
	AbstractInstruction *stackCheckLabel;
	sqInt	span;
	sqInt	hasInstVarRef;
 } BlockStart;

#define CogBlockStart BlockStart


typedef struct _BytecodeDescriptor {
	sqInt (*generator)(void);
	sqInt NoDbgRegParms (*spanFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt);
	sqInt NoDbgRegParms (*needsFrameFunction)(sqInt);
	signed char	stackDelta;
	unsigned char	opcode;
	unsigned char	numBytes;
	unsigned		isBranchTrue : 1;
	unsigned		isBranchFalse : 1;
	unsigned		isReturn : 1;
	unsigned		isBlockCreation : 1;
	unsigned		isMapped : 1;
	unsigned		isMappedInBlock : 1;
	unsigned		isExtension : 1;
	unsigned		isInstVarRef : 1;
	unsigned		is1ByteInstVarStore : 1;
	unsigned		hasUnsafeJump : 1;
 } BytecodeDescriptor;

#define CogBytecodeDescriptor BytecodeDescriptor


typedef struct {
	sqInt (*primitiveGenerator)(void);
	sqInt	primNumArgs;
 } PrimitiveDescriptor;

#define CogPrimitiveDescriptor PrimitiveDescriptor


typedef struct {
	char	type;
	char	spilled;
	signed char	liveRegister;
	signed char	registerr;
	sqInt	offset;
	sqInt	constant;
	sqInt	bcptr;
 } SimStackEntry;

#define CogSimStackEntry SimStackEntry


typedef struct {
	AbstractInstruction *targetInstruction;
	unsigned char	simStackPtr;
	char	isTargetOfBackwardBranch;
	unsigned short	instructionIndex;
	short	simNativeStackPtr;
	unsigned short	simNativeStackSize;
 } BytecodeFixup;

#define CogSSBytecodeFixup BytecodeFixup
#define CogBytecodeFixup BytecodeFixup


typedef struct {
	sqInt	isReceiverResultRegLive;
	CogSimStackEntry *ssEntry;
 } CogSSOptStatus;


typedef struct {
	char	type;
	char	spilled;
	sqInt	registerr;
	sqInt	registerSecond;
	sqInt	offset;
	sqInt	constant;
	sqInt	constantInt32;
	sqLong	constantInt64;
	float	constantFloat32;
	double	constantFloat64;
	sqInt	constantNativePointer;
	sqInt	bcptr;
 } CogSimStackNativeEntry;



/*** Function Prototypes ***/


#if !PRODUCTION && defined(PlatformNoDbgRegParms)
# define NoDbgRegParms PlatformNoDbgRegParms
#endif

#if !defined(NoDbgRegParms)
# define NoDbgRegParms /*empty*/
#endif



#if !defined(NeverInline)
# define NeverInline /*empty*/
#endif

static NoDbgRegParms AbstractInstruction * addDependent(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anInstruction);
static NoDbgRegParms sqInt availableFloatRegisterOrNoneFor(AbstractInstruction *self_in_CogAbstractInstruction, sqInt liveRegsMask);
static NoDbgRegParms AbstractInstruction * cloneLiteralFrom(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *existingLiteral);
static NoDbgRegParms sqInt concretizeAt(AbstractInstruction *self_in_CogAbstractInstruction, sqInt actualAddress);
static NoDbgRegParms sqInt genLoadCStackPointer(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadCStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadStackPointerForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg);
static NoDbgRegParms sqInt genLoadStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadStackPointersForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg);
static NoDbgRegParms sqInt genSaveStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms AbstractInstruction * genWriteCResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister);
static NoDbgRegParms AbstractInstruction * genWriteCSecondResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister);
static NoDbgRegParms sqInt has64BitPerformanceCounter(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms AbstractInstruction * initializeSharableLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal);
static NoDbgRegParms AbstractInstruction * initializeUniqueLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal);
static NoDbgRegParms int isJump(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms AbstractInstruction * relocateJumpLongBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta);
static NoDbgRegParms AbstractInstruction * relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta);
static NoDbgRegParms AbstractInstruction * resolveJumpTarget(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt rewriteConditionalJumpLongAttarget(AbstractInstruction *self_in_CogAbstractInstruction, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms CogMethod * cmHomeMethod(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMBlock(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMClosedPIC(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMFree(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMMethodEtAl(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMOpenPIC(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms sqInt isBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor);
static NoDbgRegParms sqInt isConditionalBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor);
static NoDbgRegParms int notAFixup(BytecodeFixup *self_in_CogBytecodeFixup);
static NoDbgRegParms int computeSizeOfArithCqR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler);
static NoDbgRegParms int computeSizeOfArithCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler);
static NoDbgRegParms sqInt concretizeArithCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt x64opcode);
static NoDbgRegParms sqInt concretizeMoveCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler);
static NoDbgRegParms unsigned int inlineCacheTagAt(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt isPCDependent(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler);
static NoDbgRegParms unsigned int literal32BeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt followingAddress);
static NoDbgRegParms sqInt literalBeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt followingAddress);
static NoDbgRegParms sqInt loadLiteralByteSize(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler);
static NoDbgRegParms unsigned char sizePCDependentInstructionAt(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt eventualAbsoluteAddress);
static NoDbgRegParms AbstractInstruction * storeLiteralbeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt literal, sqInt followingAddress);
static NoDbgRegParms AbstractInstruction * gAddCqR(sqInt quickConstant, sqInt reg);
static NoDbgRegParms AbstractInstruction * gAddCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * gAddRRR(sqInt addendReg, sqInt badendReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * gAndCqR(sqInt quickConstant, sqInt reg);
static NoDbgRegParms AbstractInstruction * gAndCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
extern sqInt abortOffset(void);
static NoDbgRegParms int abstractInstructionfollows(AbstractInstruction *theAbstractInstruction, AbstractInstruction *anAbstractInstruction);
static void addCleanBlockStarts(void);
extern void addCogMethodsToHeapMap(void);
static NoDbgRegParms sqInt addressIsInFixups(BytecodeFixup *address);
static NoDbgRegParms sqInt addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC);
static void alignMethodZoneBase(void);
static NoDbgRegParms sqInt alignUptoRoutineBoundary(sqInt anAddress);
static sqInt allMachineCodeObjectReferencesValid(void);
static sqInt allMethodsHaveCorrectHeader(void);
static NoDbgRegParms AbstractInstruction * annotateAbsolutePCRef(AbstractInstruction *abstractInstruction);
static NoDbgRegParms AbstractInstruction * annotateBytecode(AbstractInstruction *abstractInstruction);
static NoDbgRegParms AbstractInstruction * annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop);
static NoDbgRegParms void assertSaneJumpTarget(AbstractInstruction *jumpTarget);
static NoDbgRegParms sqInt availableRegisterOrNoneIn(sqInt liveRegsMask);
static NoDbgRegParms sqInt blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg);
extern sqInt bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static NoDbgRegParms AbstractInstruction * CallRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved);
static NoDbgRegParms AbstractInstruction * gCall(sqInt callTarget);
static NoDbgRegParms AbstractInstruction * gCmpCqR(sqInt quickConstant, sqInt reg);
static void callCogCodePopReceiver(void);
static void callCogCodePopReceiverAndClassRegs(void);
static NoDbgRegParms sqInt ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver);
static NoDbgRegParms void ceFree(void *pointer);
static NoDbgRegParms void* ceMalloc(size_t size);
static NoDbgRegParms sqInt ceSICMiss(sqInt receiver);
static NoDbgRegParms sqInt checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
static NoDbgRegParms sqInt checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern sqInt checkIntegrityOfObjectReferencesInCode(sqInt gcModes);
static NoDbgRegParms sqInt checkMaybeObjRefInClosedPIC(sqInt maybeObject);
static NoDbgRegParms sqInt checkValidObjectReferencesInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms NeverInline sqInt cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg);
static NoDbgRegParms sqInt closedPICRefersToUnmarkedObject(CogMethod *cPIC);
extern char * codeEntryFor(char *address);
extern char * codeEntryNameFor(char *address);
extern sqInt cogCodeBase(void);
extern sqInt cogCodeConstituents(sqInt withDetails);
static NoDbgRegParms void cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase);
extern CogMethod * cogFullBlockMethodnumCopied(sqInt aMethodObj, sqInt numCopied);
extern void cogitPostGCAction(sqInt gcMode);
static NoDbgRegParms sqInt cogMethodDoesntLookKosher(CogMethod *cogMethod);
extern CogMethod * cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs);
static NoDbgRegParms CogMethod * cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs);
static NoDbgRegParms CogMethod * cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase);
extern CogMethod * cogselector(sqInt aMethodObj, sqInt aSelectorOop);
static NoDbgRegParms sqInt collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static NoDbgRegParms sqInt collectCogMethodConstituent(CogMethod *cogMethod);
extern void compactCogCompiledCode(void);
static void compactPICsWithFreedTargets(void);
static AbstractInstruction * compileAbort(void);
static NoDbgRegParms sqInt compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex);
static NoDbgRegParms void compileBlockEntry(BlockStart *blockStart);
static NoDbgRegParms void compileCallFornumArgsargargargargfloatResultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask);
static NoDbgRegParms void compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask);
static NoDbgRegParms void compileCallFornumArgsargargargargresultRegresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt resultReg2OrNone, sqInt regMask);
static NoDbgRegParms void compileCallFornumArgsfloatArgfloatArgfloatArgfloatArgresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask);
static AbstractInstruction * compileCPICEntry(void);
static NoDbgRegParms sqInt compileEntireFullBlockMethod(sqInt numCopied);
static void compileEntry(void);
static sqInt compileFullBlockEntry(void);
static sqInt compileMethodBody(void);
static NoDbgRegParms sqInt compilePICAbort(sqInt numArgs);
static NoDbgRegParms AbstractInstruction * compileStackOverflowCheck(sqInt canContextSwitch);
static NoDbgRegParms void compileTrampolineFornumArgsargargargargregsToSavepushLinkRegfloatResultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone);
static NoDbgRegParms void compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone);
static NoDbgRegParms void compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt resultReg2OrNone);
static NoDbgRegParms void compileTrampolineFornumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone);
static void computeEntryOffsets(void);
static void computeFullBlockEntryOffsets(void);
static usqInt computeGoodVarBaseAddress(void);
static void computeMaximumSizes(void);
static NoDbgRegParms sqInt configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta);
static NoDbgRegParms sqInt configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta);
static NoDbgRegParms sqInt cPICCompactAndIsNowEmpty(CogMethod *cPIC);
static NoDbgRegParms sqInt cPICHasForwardedClass(CogMethod *cPIC);
static NoDbgRegParms sqInt cPICHasFreedTargets(CogMethod *cPIC);
static usqInt cPICPrototypeCaseOffset(void);
static NoDbgRegParms sqInt cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod);
static NoDbgRegParms sqInt createCPICData(CogMethod *cPIC);
static NoDbgRegParms AbstractInstruction * gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder);
extern int defaultCogCodeSize(void);
static NoDbgRegParms sqInt deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader);
static NoDbgRegParms sqInt endPCOf(sqInt aMethod);
static void enterCogCodePopReceiver(void);
static NoDbgRegParms sqInt entryPointTagIsSelector(sqInt entryPoint);
static NoDbgRegParms sqInt expectedClosedPICPrototype(CogMethod *cPIC);
static sqInt extABytecode(void);
static sqInt extBBytecode(void);
static NoDbgRegParms sqInt fillInBlockHeadersAt(sqInt startAddress);
static NoDbgRegParms void fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector);
static NoDbgRegParms sqInt findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc);
static NoDbgRegParms usqInt findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc);
static NoDbgRegParms usqInt findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod);
extern CogBlockMethod * findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod);
static NoDbgRegParms sqInt findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc);
static NoDbgRegParms sqInt firstMappedPCFor(CogMethod *cogMethod);
static sqInt firstPrototypeMethodOop(void);
static NoDbgRegParms BytecodeFixup * fixupAt(sqInt fixupPC);
extern void flagCogMethodForBecome(CogMethod *cogMethod);
static NoDbgRegParms void followForwardedLiteralsImplementationIn(CogMethod *cogMethod);
extern void followForwardedLiteralsIn(CogMethod *cogMethod);
extern void followMovableLiteralsAndUpdateYoungReferrers(void);
extern void freeBecomeFlaggedMethods(void);
extern void freeCogMethod(CogMethod *cogMethod);
extern void freeUnmarkedMachineCode(void);
static NoDbgRegParms AbstractInstruction * genCallMustBeBooleanFor(sqInt boolean);
static NoDbgRegParms AbstractInstruction * genConditionalBranchoperand(sqInt opcode, sqInt operandOne);
static NoDbgRegParms void (*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void);
static NoDbgRegParms void genEnilopmartReturn(sqInt forCall);
static NoDbgRegParms NeverInline void generateCaptureCStackPointers(sqInt captureFramePointer);
static void generateClosedPICPrototype(void);
static CogMethod * generateCogFullBlock(void);
static NoDbgRegParms CogMethod * generateCogMethod(sqInt selector);
static NoDbgRegParms sqInt generateMapAtstart(usqInt addressOrNull, usqInt startAddress);
static void generateOpenPICPrototype(void);
static void generateRunTimeTrampolines(void);
static void generateStackPointerCapture(void);
static void generateTrampolines(void);
static NoDbgRegParms BytecodeDescriptor * generatorForPC(sqInt pc);
static usqInt genFFICalloutTrampoline(void);
static void genGetLeafCallStackPointers(void);
static NoDbgRegParms usqInt genInnerPICAbortTrampoline(char *name);
static void (*genInvokeInterpretTrampoline(void))(void);
static NoDbgRegParms void genLoadInlineCacheWithSelector(sqInt selectorIndex);
static usqInt genReturnToInterpreterTrampoline(void);
static NoDbgRegParms sqInt genSmalltalkToCStackSwitch(sqInt pushLinkReg);
static NoDbgRegParms usqInt genTrampolineForcalledargfloatResult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg);
static NoDbgRegParms usqInt genTrampolineForcalledargresultresult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg, sqInt resultReg2);
static NoDbgRegParms usqInt genTrampolineForcalledfloatArgresult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg);
static NoDbgRegParms usqInt genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegfloatResultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean);
static NoDbgRegParms usqInt genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean);
static NoDbgRegParms usqInt genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt resultReg2OrNone, sqInt appendBoolean);
static NoDbgRegParms usqInt genTrampolineForcallednumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean);
static NoDbgRegParms void genTrampolineReturn(sqInt lnkRegWasPushed);
static NoDbgRegParms AbstractInstruction * gen(sqInt opcode);
static NoDbgRegParms AbstractInstruction * genoperand(sqInt opcode, sqInt operand);
static NoDbgRegParms AbstractInstruction * genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo);
static NoDbgRegParms AbstractInstruction * genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree);
static NoDbgRegParms sqInt getLiteral(sqInt litIndex);
static NoDbgRegParms sqInt incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt indexForSelectorin(sqInt selector, CogMethod *cogMethod);
extern void initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress);
static sqInt initialMethodUsageCount(void);
static int initialOpenPICUsageCount(void);
static NoDbgRegParms sqInt inverseBranchFor(sqInt opcode);
static NoDbgRegParms int isPCWithinMethodZone(void *address);
extern sqInt isSendReturnPC(sqInt retpc);
static NoDbgRegParms AbstractInstruction * gJumpFPEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPGreaterOrEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPGreater(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPLessOrEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPLess(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPNotEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gLogicalShiftLeftCqR(sqInt quickConstant, sqInt reg);
static NoDbgRegParms AbstractInstruction * gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * gLogicalShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * lastOpcode(void);
extern void linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver);
static BytecodeDescriptor * loadBytesAndGetDescriptor(void);
static NoDbgRegParms void loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc);
static NoDbgRegParms AbstractInstruction * gMoveCwR(sqInt wordConstant, sqInt reg);
static NoDbgRegParms AbstractInstruction * gMovePerfCnt64RL(sqInt destReg, sqInt liveRegisterMask);
static NoDbgRegParms AbstractInstruction * gMovePerfCnt64RRL(sqInt destRegLo, sqInt destRegHi, sqInt liveRegisterMask);
static NoDbgRegParms usqInt mapEndFor(CogMethod *cogMethod);
static NoDbgRegParms sqInt mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg);
static NoDbgRegParms sqInt mapObjectReferencesInClosedPIC(CogMethod *cPIC);
static void mapObjectReferencesInGeneratedRuntime(void);
static void mapObjectReferencesInMachineCodeForBecome(void);
static void mapObjectReferencesInMachineCodeForFullGC(void);
static void mapObjectReferencesInMachineCodeForYoungGC(void);
extern void mapObjectReferencesInMachineCode(sqInt gcMode);
extern void markAndTraceMachineCodeOfMarkedMethods(void);
static void markAndTraceObjectReferencesInGeneratedRuntime(void);
static NoDbgRegParms sqInt markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit);
static NoDbgRegParms sqInt markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC);
static NoDbgRegParms sqInt markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
static NoDbgRegParms sqInt markLiteralspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern void markMethodAndReferents(CogBlockMethod *aCogMethod);
extern usqInt maxCogMethodAddress(void);
static NoDbgRegParms sqInt maximumDistanceFromCodeZone(sqInt anAddress);
static sqInt maybeAllocAndInitIRCs(void);
static NoDbgRegParms sqInt maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod);
static int mclassCouldBeContext(void);
static int mclassIsSmallInteger(void);
extern usqInt mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static NoDbgRegParms sqInt methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral);
extern sqInt mnuOffset(void);
static NoDbgRegParms AbstractInstruction * gNativePopR(sqInt reg);
static NoDbgRegParms AbstractInstruction * gNativePushR(sqInt reg);
static NoDbgRegParms AbstractInstruction * gNativeRetN(sqInt offset);
static NoDbgRegParms sqInt needsFrameIfImmutability(sqInt stackDelta);
static NoDbgRegParms int needsFrameIfInBlock(sqInt stackDelta);
static NoDbgRegParms sqInt needsFrameNever(sqInt stackDelta);
static NoDbgRegParms sqInt noAssertMethodClassAssociationOf(sqInt methodPointer);
static sqInt noCogMethodsMaximallyMarked(void);
static NoDbgRegParms int noTargetsFreeInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms AbstractInstruction * gOrCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt outputInstructionsAt(sqInt startAddress);
static NoDbgRegParms sqInt outputInstructionsForGeneratedRuntimeAt(sqInt startAddress);
extern sqInt patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver);
static sqInt picAbortDiscriminatorValue(void);
static sqInt picInterpretAbortOffset(void);
extern void printCogMethodFor(void *address);
extern void printTrampolineTable(void);
static sqInt processorHasDivQuoRemAndMClassIsSmallInteger(void);
static sqInt processorHasMultiplyAndMClassIsSmallInteger(void);
static NoDbgRegParms void recordGeneratedRunTimeaddress(char *aString, sqInt address);
extern int recordPrimTraceFunc(void);
static void recordRunTimeObjectReferences(void);
static NoDbgRegParms sqInt registerMaskFor(sqInt reg);
static NoDbgRegParms sqInt registerMaskForand(sqInt reg1, sqInt reg2);
static NoDbgRegParms sqInt registerMaskForandandand(sqInt reg1, sqInt reg2, sqInt reg3, sqInt reg4);
static NoDbgRegParms void relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod);
static NoDbgRegParms void relocateCallsInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms sqInt relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg);
static NoDbgRegParms sqInt remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr);
static NoDbgRegParms sqInt remapMaybeObjRefInClosedPICAt(sqInt mcpc);
static NoDbgRegParms void rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget);
static NoDbgRegParms AbstractInstruction * gSubRRR(sqInt subReg, sqInt fromReg, sqInt destReg);
static sqInt scanForCleanBlocks(void);
extern void setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop);
static NoDbgRegParms sqInt spanForCleanBlockStartingAt(sqInt startPC);
static NoDbgRegParms usqInt stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc);
static sqInt subsequentPrototypeMethodOop(void);
static NoDbgRegParms AbstractInstruction * gTstCqR(sqInt quickConstant, sqInt reg);
extern sqInt traceLinkedSendOffset(void);
static NoDbgRegParms char * trampolineNamenumArgs(char *routinePrefix, sqInt numArgs);
static NoDbgRegParms char * trampolineNamenumArgslimit(char *routinePrefix, int numArgs, sqInt argsLimit);
static NoDbgRegParms char * trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs);
extern void unflagBecomeFlaggedMethods(void);
static int unknownBytecode(void);
extern void unlinkAllSends(void);
static NoDbgRegParms sqInt unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector);
static NoDbgRegParms sqInt unlinkIfInvalidClassSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod);
extern void unlinkSendsLinkedForInvalidClasses(void);
extern void unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector);
static void unlinkSendsToFree(void);
extern void unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue);
extern void unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue);
extern void voidCogCompiledCode(void);
static void zeroOpcodeIndex(void);
static NoDbgRegParms sqInt counters(CogMethod *self_in_CogMethod);
static NoDbgRegParms void addToOpenPICList(CogMethod *anOpenPIC);
static NoDbgRegParms void addToYoungReferrers(CogMethod *writableCogMethod);
static NoDbgRegParms usqInt allocate(sqInt numBytes);
extern CogMethod * cogMethodContaining(usqInt mcpc);
static void compactCompiledCode(void);
static NoDbgRegParms void ensureInYoungReferrers(CogMethod *cogMethod);
static void followForwardedLiteralsInOpenPICList(void);
static NoDbgRegParms void freeMethod(CogMethod *cogMethod);
static void freeOlderMethodsForCompaction(void);
extern sqInt kosherYoungReferrers(void);
static NoDbgRegParms sqInt mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod);
extern CogMethod * methodFor(void *address);
extern sqInt methodsCompiledToMachineCodeInto(sqInt arrayObj);
extern sqInt numMethods(void);
extern sqInt numMethodsOfType(sqInt cogMethodType);
static NoDbgRegParms sqInt occurrencesInYoungReferrers(CogMethod *cogMethod);
static NoDbgRegParms CogMethod * openPICWithSelector(sqInt aSelector);
static void planCompaction(void);
extern void printCogMethods(void);
extern void printCogMethodsOfType(sqInt cmType);
extern void printCogMethodsWithMethod(sqInt methodOop);
extern void printCogMethodsWithPrimitive(sqInt primIdx);
extern void printCogMethodsWithSelector(sqInt selectorOop);
extern void printCogYoungReferrers(void);
extern sqInt printOpenPICList(void);
static sqInt pruneYoungReferrers(void);
static sqInt relocateAndPruneYoungReferrers(void);
static sqInt relocateMethodsPreCompaction(void);
static NoDbgRegParms sqInt removeFromOpenPICList(CogMethod *anOpenPIC);
static NoDbgRegParms void restorePICUsageCount(CogMethod *cogMethod);
static NoDbgRegParms sqInt roundUpLength(sqInt numBytes);
static NoDbgRegParms void savePICUsageCount(CogMethod *cogMethod);
static void voidOpenPICList(void);
static void voidUnpairedMethodList(void);
static void voidYoungReferrersPostTenureAll(void);
extern char * whereIsMaybeCodeThing(sqInt anOop);
static sqInt zoneAlignment(void);
static NoDbgRegParms sqInt checkValidObjectReference(sqInt anOop);
static NoDbgRegParms AbstractInstruction * genCmpClassFloatCompactIndexR(sqInt reg);
static NoDbgRegParms AbstractInstruction * genCmpClassMethodContextCompactIndexR(sqInt reg);
static void generateLowcodeObjectTrampolines(void);
static NoDbgRegParms sqInt genGetMethodHeaderOfintoscratch(sqInt methodReg, sqInt headerReg, sqInt scratchReg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallIntegersInandscratch(sqInt aRegister, sqInt bRegister, sqInt scratchRegister);
static NoDbgRegParms void genLcByteSizeOfto(sqInt oop, sqInt resultRegister);
static NoDbgRegParms void genLcFloat32toOop(sqInt value, sqInt object);
static NoDbgRegParms void genLcFloat64toOop(sqInt value, sqInt object);
static NoDbgRegParms void genLcInstantiateOop(sqInt classOop);
static NoDbgRegParms void genLcInstantiateOopconstantIndexableSize(sqInt classOop, sqInt indexableSize);
static NoDbgRegParms void genLcInstantiateOopindexableSize(sqInt classOop, sqInt indexableSize);
static NoDbgRegParms void genLcInt64ToOop(sqInt value);
static NoDbgRegParms void genLcInt64ToOophighPart(sqInt valueLow, sqInt valueHigh);
static NoDbgRegParms void genLcOopToInt64(sqInt value);
static NoDbgRegParms void genLcOopToPointer(sqInt object);
static NoDbgRegParms void genLcOopToUInt64(sqInt value);
static NoDbgRegParms void genLcOoptoFloat32(sqInt object, sqInt value);
static NoDbgRegParms void genLcOoptoFloat64(sqInt object, sqInt value);
static NoDbgRegParms void genLcOoptoInt64highPart(sqInt object, sqInt valueLow, sqInt valueHigh);
static NoDbgRegParms void genLcOoptoUInt64highPart(sqInt object, sqInt valueLow, sqInt valueHigh);
static NoDbgRegParms void genLcPointerToOopclass(sqInt pointer, sqInt pointerClass);
static NoDbgRegParms void genLcUInt64ToOop(sqInt value);
static NoDbgRegParms void genLcUInt64ToOophighPart(sqInt valueLow, sqInt valueHigh);
static NoDbgRegParms sqInt genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg);
static int genPrimitiveAdd(void);
static int genPrimitiveAsFloat(void);
static int genPrimitiveBitAnd(void);
static int genPrimitiveBitOr(void);
static int genPrimitiveBitShift(void);
static int genPrimitiveBitXor(void);
static int genPrimitiveClass(void);
static int genPrimitiveDiv(void);
static int genPrimitiveDivide(void);
static int genPrimitiveEqual(void);
static sqInt genPrimitiveFloatAdd(void);
static sqInt genPrimitiveFloatDivide(void);
static sqInt genPrimitiveFloatMultiply(void);
static int genPrimitiveFloatSquareRoot(void);
static sqInt genPrimitiveFloatSubtract(void);
static int genPrimitiveGreaterOrEqual(void);
static int genPrimitiveGreaterThan(void);
static int genPrimitiveHighBit(void);
static sqInt genPrimitiveIdentical(void);
static int genPrimitiveLessOrEqual(void);
static int genPrimitiveLessThan(void);
static int genPrimitiveMod(void);
static int genPrimitiveMultiply(void);
static int genPrimitiveNewMethod(void);
static int genPrimitiveNotEqual(void);
static sqInt genPrimitiveNotIdentical(void);
static int genPrimitiveQuo(void);
static sqInt genPrimitiveSmallFloatAdd(void);
static sqInt genPrimitiveSmallFloatDivide(void);
static sqInt genPrimitiveSmallFloatEqual(void);
static sqInt genPrimitiveSmallFloatGreaterOrEqual(void);
static sqInt genPrimitiveSmallFloatGreaterThan(void);
static sqInt genPrimitiveSmallFloatLessOrEqual(void);
static sqInt genPrimitiveSmallFloatLessThan(void);
static sqInt genPrimitiveSmallFloatMultiply(void);
static sqInt genPrimitiveSmallFloatNotEqual(void);
static sqInt genPrimitiveSmallFloatSquareRoot(void);
static sqInt genPrimitiveSmallFloatSubtract(void);
static int genPrimitiveSubtract(void);
static NoDbgRegParms int genSmallIntegerComparison(sqInt jumpOpcode);
static NoDbgRegParms sqInt isUnannotatableConstant(CogSimStackEntry *simStackEntry);
static NoDbgRegParms sqInt classForInlineCacheTag(sqInt classIndex);
static NoDbgRegParms sqInt genAddSmallIntegerTagsTo(sqInt aRegister);
static NoDbgRegParms AbstractInstruction * genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static NoDbgRegParms AbstractInstruction * genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static NoDbgRegParms AbstractInstruction * genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static NoDbgRegParms sqInt genClearAndSetSmallIntegerTagsIn(sqInt scratchReg);
static NoDbgRegParms sqInt genConvertBitsToSmallFloatInscratch(sqInt reg, sqInt scratch);
static NoDbgRegParms void genConvertCharacterToSmallIntegerInReg(sqInt reg);
static NoDbgRegParms sqInt genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genConvertIntegerToSmallIntegerInReg(sqInt reg);
static NoDbgRegParms sqInt genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(sqInt reg, sqInt scratch);
static NoDbgRegParms void genConvertSmallIntegerToCharacterInReg(sqInt reg);
static NoDbgRegParms sqInt genConvertSmallIntegerToIntegerInReg(sqInt reg);
static NoDbgRegParms sqInt genFetchRegArgsForPerformWithArguments(sqInt sizeReg);
static NoDbgRegParms sqInt genFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed);
static NoDbgRegParms sqInt genFloatComparisonorIntegerComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt jumpOpcode, sqInt invertComparison, sqInt rcvrBoxed);
static NoDbgRegParms sqInt genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg);
static NoDbgRegParms sqInt genGetHashFieldNonImmOfinto(sqInt instReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry);
static NoDbgRegParms sqInt genGetNumBytesOfinto(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genGetOverflowSlotsOfinto(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genGetSmallFloatValueOfscratchinto(sqInt oopReg, sqInt scratch, sqInt dpReg);
static NoDbgRegParms AbstractInstruction * genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static NoDbgRegParms AbstractInstruction * genJumpNotCharacter(sqInt reg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallFloatValueBitsscratch(sqInt reg, sqInt exponent);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallFloat(sqInt reg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallInteger(sqInt reg);
static NoDbgRegParms AbstractInstruction * genJumpSmallInteger(sqInt aRegister);
static NoDbgRegParms sqInt genLcInt32ToOop(sqInt value);
static NoDbgRegParms sqInt genLcOopToInt32(sqInt value);
static NoDbgRegParms sqInt genLcOopToUInt32(sqInt value);
static NoDbgRegParms sqInt genLcUInt32ToOop(sqInt value);
static NoDbgRegParms sqInt genPrimitiveAtPutSigned(sqInt signedVersion);
static NoDbgRegParms sqInt genPrimitiveAtSigned(sqInt signedVersion);
static sqInt genPrimitiveFloatEqual(void);
static sqInt genPrimitiveFloatGreaterOrEqual(void);
static sqInt genPrimitiveFloatGreaterThan(void);
static sqInt genPrimitiveFloatLessOrEqual(void);
static sqInt genPrimitiveFloatLessThan(void);
static sqInt genPrimitiveFloatNotEqual(void);
static sqInt genPrimitiveIdentityHash(void);
static sqInt genPrimitiveImmediateAsInteger(void);
static int genPrimitiveNew(void);
static int genPrimitiveNewWithArg(void);
static sqInt genPrimitiveShallowCopy(void);
static sqInt genPrimitiveSlotAt(void);
static sqInt genPrimitiveSlotAtPut(void);
static sqInt genPrimitiveStringAt(void);
static sqInt genPrimitiveStringAtPut(void);
static int genPrimitiveUninitializedNewWithArg(void);
static NoDbgRegParms sqInt genPureFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed);
static NoDbgRegParms sqInt genPureFloatComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt invertComparison, sqInt rcvrBoxed);
static NoDbgRegParms sqInt genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg);
static NoDbgRegParms sqInt genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg);
static NoDbgRegParms int genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison);
static NoDbgRegParms sqInt getLiteralCountOfplusOneinBytesintoscratch(sqInt methodReg, sqInt plusOne, sqInt inBytes, sqInt litCountReg, sqInt scratchReg);
static NoDbgRegParms sqInt inlineCacheTagForInstance(sqInt oop);
static sqInt log2BytesPerWord(void);
static void maybeGenerateSelectorIndexDereferenceRoutine(void);
static sqInt numSmallIntegerBits(void);
static sqInt numSmallIntegerTagBits(void);
static NoDbgRegParms sqInt validInlineCacheTag(sqInt classIndexOrTagPattern);
static void callStoreCheckTrampoline(void);
static NoDbgRegParms int checkValidDerivedObjectReference(sqInt bodyAddress);
static NoDbgRegParms sqInt checkValidOopReference(sqInt anOop);
static NoDbgRegParms int couldBeDerivedObject(sqInt bodyAddress);
static NoDbgRegParms sqInt couldBeObject(sqInt literal);
static NoDbgRegParms usqInt genActiveContextTrampolineLargeinBlockcalled(sqInt isLarge, sqInt isInBlock, char *aString);
static NoDbgRegParms AbstractInstruction * genCheckRememberedBitOfscratch(sqInt objReg, sqInt scratchReg);
static NoDbgRegParms sqInt genConvertCharacterToCodeInReg(sqInt reg);
static NoDbgRegParms sqInt genConvertIntegerToCharacterInReg(sqInt reg);
static NoDbgRegParms sqInt genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(sqInt compiledBlock, sqInt numArgs, sqInt numCopied, sqInt ignoreContext, sqInt contextNumArgs, sqInt contextIsLarge, sqInt contextIsBlock);
static NoDbgRegParms sqInt genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(sqInt reg, sqInt scratch, void *fwdJumpTarget, void *nonFwdJumpTargetOrZero);
static NoDbgRegParms sqInt genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(sqInt reg, sqInt scratch, sqInt index, sqInt objReg);
static void generateObjectRepresentationTrampolines(void);
static NoDbgRegParms sqInt genGetActiveContextLargeinBlock(sqInt isLarge, sqInt isInBlock);
static NoDbgRegParms sqInt genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock);
static NoDbgRegParms sqInt genGetBitsofFormatByteOfinto(sqInt mask, sqInt sourceReg, sqInt destReg);
static NoDbgRegParms sqInt genGetClassIndexOfNonImminto(sqInt sourceReg, sqInt destReg);
static NoDbgRegParms sqInt genGetClassObjectOfClassIndexintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static NoDbgRegParms sqInt genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeForwarder);
static NoDbgRegParms AbstractInstruction * genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static NoDbgRegParms sqInt genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg);
static NoDbgRegParms sqInt genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg);
static NoDbgRegParms sqInt genGetFormatOfinto(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(sqInt sourceReg, sqInt destReg, sqInt scratchRegOrNone);
static NoDbgRegParms sqInt genGetNumSlotsOfinto(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genGetRawSlotSizeOfNonImminto(sqInt sourceReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * genJumpImmediate(sqInt aRegister);
#if IMMUTABILITY
static NoDbgRegParms AbstractInstruction * genJumpImmutablescratchReg(sqInt sourceReg, sqInt scratchReg);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static NoDbgRegParms AbstractInstruction * genJumpMutablescratchReg(sqInt sourceReg, sqInt scratchReg);
#endif /* IMMUTABILITY */
static NoDbgRegParms void genLcFirstFieldPointer(sqInt objectReg);
static NoDbgRegParms void genLcFirstIndexableFieldPointer(sqInt objectReg);
static NoDbgRegParms void genLcIsBytesto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsFloatObjectto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsIndexableto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsIntegerObjectto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsPointersto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsWordsOrBytesto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcIsWordsto(sqInt objectReg, sqInt valueReg);
static NoDbgRegParms void genLcLoadObjectat(sqInt object, sqInt fieldIndex);
static NoDbgRegParms void genLcLoadObjectfield(sqInt object, sqInt fieldIndex);
static NoDbgRegParms void genLcStoreobjectat(sqInt value, sqInt object, sqInt fieldIndex);
static NoDbgRegParms void genLcStoreobjectfield(sqInt value, sqInt object, sqInt fieldIndex);
static NoDbgRegParms sqInt genNewArrayOfSizeinitialized(sqInt size, sqInt initialize);
static NoDbgRegParms sqInt genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock);
static int genPrimitiveAsCharacter(void);
static sqInt genPrimitiveAt(void);
static sqInt genPrimitiveAtPut(void);
static NoDbgRegParms sqInt genPrimitiveIdenticalOrNotIf(sqInt orNot);
static sqInt genPrimitiveIntegerAt(void);
static sqInt genPrimitiveIntegerAtPut(void);
static sqInt genPrimitiveMakePoint(void);
static sqInt genPrimitiveObjectAt(void);
static sqInt genPrimitiveSize(void);
static sqInt genPrimitiveStringCompareWith(void);
static sqInt genPrimitiveStringReplace(void);
static NoDbgRegParms sqInt genSetSmallIntegerTagsIn(sqInt scratchReg);
static usqInt genStoreCheckContextReceiverTrampoline(void);
static NoDbgRegParms sqInt genStoreCheckReceiverRegvalueRegscratchReginFrame(sqInt destReg, sqInt valueReg, sqInt scratchReg, sqInt inFrame);
static NoDbgRegParms sqInt genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck);
static NoDbgRegParms sqInt genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg);
#if IMMUTABILITY
static NoDbgRegParms usqInt genStoreTrampolineCalledinstVarIndex(char *trampolineName, sqInt instVarIndex);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static NoDbgRegParms sqInt genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static NoDbgRegParms sqInt genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static NoDbgRegParms sqInt genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needsStoreCheck, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
static sqInt getActiveContextAllocatesInMachineCode(void);
static NoDbgRegParms sqInt inlineCacheTagIsYoung(sqInt cacheTag);
static NoDbgRegParms AbstractInstruction * jumpNotCharacterUnsignedValueInRegister(sqInt reg);
static NoDbgRegParms sqInt markAndTraceLiteralinatpc(sqInt literal, CogMethod *cogMethodOrNil, usqInt address);
static NoDbgRegParms void markAndTraceLiteralinat(sqInt literal, CogMethod *cogMethod, sqInt *address);
static NoDbgRegParms void markAndTraceUpdatedLiteralin(sqInt objOop, CogMethod *cogMethodOrNil);
static NoDbgRegParms sqInt maybeCompileRetryOfonPrimitiveFailflags(void (*primitiveRoutine)(void), sqInt primIndex, sqInt flags);
static NoDbgRegParms sqInt maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg);
static sqInt numCharacterBits(void);
static NoDbgRegParms sqInt remapObject(sqInt objOop);
static NoDbgRegParms sqInt remapOop(sqInt objOop);
static NoDbgRegParms sqInt shouldAnnotateObjectReference(sqInt anOop);
static NoDbgRegParms sqInt slotOffsetOfInstVarIndex(sqInt index);
static NoDbgRegParms SimStackEntry * ensureSpilledAtfrom(SimStackEntry *self_in_CogSimStackEntry, sqInt baseOffset, sqInt baseRegister);
static NoDbgRegParms sqInt floatRegisterMask(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms sqInt isSameEntryAs(SimStackEntry *self_in_CogSimStackEntry, CogSimStackEntry *ssEntry);
static NoDbgRegParms sqInt mayBeAForwarder(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms SimStackEntry * popToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg);
static NoDbgRegParms sqInt registerMask(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms sqInt registerMaskOrNone(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms sqInt registerOrNone(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms SimStackEntry * storeToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg);
static NoDbgRegParms CogSimStackNativeEntry * ensureIsMarkedAsSpilled(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms CogSimStackNativeEntry * ensureSpilledSPscratchRegister(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt spRegister, sqInt scratchRegister);
static NoDbgRegParms sqInt nativeFloatRegisterMask(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms sqInt nativeFloatRegisterOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms CogSimStackNativeEntry * nativePopToReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg);
static NoDbgRegParms CogSimStackNativeEntry * nativePopToRegsecondReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg, sqInt secondReg);
static NoDbgRegParms sqInt nativeRegisterMask(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms sqInt nativeRegisterOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms sqInt nativeRegisterSecondOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms CogSimStackNativeEntry * nativeStackPopToReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg);
static NoDbgRegParms CogSimStackNativeEntry * nativeStackPopToRegsecondReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg, sqInt secondReg);
static NoDbgRegParms sqInt spillingNeedsScratchRegister(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms sqInt stackSpillSize(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry);
static NoDbgRegParms int isMergeFixup(BytecodeFixup *self_in_CogSSBytecodeFixup);
static NoDbgRegParms sqInt availableRegisterOrNoneFor(AbstractInstruction *self_in_CogX64Compiler, sqInt liveRegsMask);
static NoDbgRegParms sqInt callFullInstructionByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt callFullTargetFromReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt callInstructionByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt callTargetFromReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress);
static NoDbgRegParms AbstractInstruction * cFloatResultToRd(AbstractInstruction *self_in_CogX64Compiler, sqInt reg);
static NoDbgRegParms AbstractInstruction * cFloatResultToRs(AbstractInstruction *self_in_CogX64Compiler, sqInt reg);
static NoDbgRegParms sqInt cmpC32RTempByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt computeMaximumSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms int computeShiftRRSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt computeSizeOfPushCw(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeArithCqRWithROraxOpcode(AbstractInstruction *self_in_CogX64Compiler, sqInt regOpcode, sqInt raxOpcode);
static NoDbgRegParms sqInt concretizeFill32(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeMovePerfCnt64RL(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeMoveRX32rR(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeMoveX32rRR(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeOpRR(AbstractInstruction *self_in_CogX64Compiler, sqInt x64opcode);
static NoDbgRegParms sqInt concretizePrefetchAw(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt concretizeReverseOpRR(AbstractInstruction *self_in_CogX64Compiler, sqInt x64opcode);
static NoDbgRegParms sqInt concretizeSet(AbstractInstruction *self_in_CogX64Compiler, sqInt conditionCode);
static NoDbgRegParms sqInt concretizeXCHGRR(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms AbstractInstruction * detectFeatures(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt dispatchConcretize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt dispatchConcretizeProcessorSpecific(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt fullCallsAreRelative(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms AbstractInstruction * genDivRRQuoRem(AbstractInstruction *self_in_CogX64Compiler, sqInt abstractRegDivisor, sqInt abstractRegDividend, sqInt abstractRegQuotient, sqInt abstractRegRemainder);
static NoDbgRegParms AbstractInstruction * genMemCopytoconstantSize(AbstractInstruction *self_in_CogX64Compiler, sqInt originalSourceReg, sqInt originalDestReg, sqInt size);
static NoDbgRegParms AbstractInstruction * genMemCopytosize(AbstractInstruction *self_in_CogX64Compiler, sqInt originalSourceReg, sqInt originalDestReg, sqInt originalSize);
static NoDbgRegParms AbstractInstruction * genMulRR(AbstractInstruction *self_in_CogX64Compiler, sqInt regSource, sqInt regDest);
static NoDbgRegParms AbstractInstruction * genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction *self_in_CogX64Compiler, sqInt numArgs);
static NoDbgRegParms AbstractInstruction * genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction *self_in_CogX64Compiler, sqInt numArgs, sqInt scratchReg);
static NoDbgRegParms sqInt genRemoveNArgsFromStack(AbstractInstruction *self_in_CogX64Compiler, sqInt n);
static NoDbgRegParms sqInt genRestoreRegs(AbstractInstruction *self_in_CogX64Compiler, sqInt regMask);
static NoDbgRegParms sqInt genSaveRegs(AbstractInstruction *self_in_CogX64Compiler, sqInt regMask);
static NoDbgRegParms AbstractInstruction * genSubstituteReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt retpc);
static NoDbgRegParms AbstractInstruction * genSwapRRScratch(AbstractInstruction *self_in_CogX64Compiler, sqInt regA, sqInt regB, sqInt regTmp);
static NoDbgRegParms sqInt hasVarBaseRegister(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt instructionSizeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc);
static NoDbgRegParms int is32BitSignedImmediate(AbstractInstruction *self_in_CogX64Compiler, sqInt a64BitUnsignedOperand);
static NoDbgRegParms int isCallPrecedingReturnPC(AbstractInstruction *self_in_CogX64Compiler, sqInt mcpc);
static NoDbgRegParms sqInt isJumpAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc);
static NoDbgRegParms int isQuick(AbstractInstruction *self_in_CogX64Compiler, usqIntptr_t operand);
static NoDbgRegParms sqInt isWithinMwOffsetRange(AbstractInstruction *self_in_CogX64Compiler, sqInt anAddress);
static NoDbgRegParms AbstractInstruction * jmpTarget(AbstractInstruction *self_in_CogX64Compiler, AbstractInstruction *anAbstractInstruction);
static NoDbgRegParms sqInt jumpLongByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt jumpLongConditionalByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt jumpLongTargetBeforeFollowingAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt mcpc);
static NoDbgRegParms usqInt jumpTargetPCAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc);
static NoDbgRegParms sqInt leafCallStackPointerDelta(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt literalBeforeInlineCacheTagAt(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt loadPICLiteralByteSize(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms unsigned char machineCodeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt anOffset);
static NoDbgRegParms sqInt machineCodeBytes(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms sqInt modRMRO(AbstractInstruction *self_in_CogX64Compiler, sqInt mod, sqInt regMode, sqInt regOpcode);
static NoDbgRegParms sqInt numIntRegArgs(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms AbstractInstruction * padIfPossibleWithStopsFromto(AbstractInstruction *self_in_CogX64Compiler, sqInt startAddr, sqInt endAddr);
static NoDbgRegParms sqInt registerToSaveIP(AbstractInstruction *self_in_CogX64Compiler);
static NoDbgRegParms AbstractInstruction * relocateCallBeforeReturnPCby(AbstractInstruction *self_in_CogX64Compiler, sqInt retpc, sqInt delta);
static NoDbgRegParms AbstractInstruction * relocateMethodReferenceBeforeAddressby(AbstractInstruction *self_in_CogX64Compiler, sqInt pc, sqInt delta);
static NoDbgRegParms sqInt rewriteCallAttarget(AbstractInstruction *self_in_CogX64Compiler, usqInt callSiteReturnAddress, usqInt callTargetAddress);
static NoDbgRegParms sqInt rewriteCallFullAttarget(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms AbstractInstruction * rewriteCPICJumpAttarget(AbstractInstruction *self_in_CogX64Compiler, usqInt addressFollowingJump, usqInt jumpTargetAddr);
static NoDbgRegParms sqInt rewriteInlineCacheAttagtarget(AbstractInstruction *self_in_CogX64Compiler, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress);
static NoDbgRegParms AbstractInstruction * rewriteInlineCacheTagat(AbstractInstruction *self_in_CogX64Compiler, sqInt cacheTag, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt rewriteJumpFullAttarget(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms sqInt rexRxb(AbstractInstruction *self_in_CogX64Compiler, sqInt reg, sqInt sibReg, sqInt fieldReg);
static NoDbgRegParms sqInt rexwrxb(AbstractInstruction *self_in_CogX64Compiler, sqInt width64, sqInt reg, sqInt sibReg, sqInt fieldReg);
static NoDbgRegParms sqInt setsConditionCodesFor(AbstractInstruction *self_in_CogX64Compiler, sqInt aConditionalJumpOpcode);
static NoDbgRegParms int sizeHasModrmat(AbstractInstruction *self_in_CogX64Compiler, sqInt op, sqInt pc);
static NoDbgRegParms int sizeImmediateGroup1at(AbstractInstruction *self_in_CogX64Compiler, sqInt op, sqInt pc);
static NoDbgRegParms AbstractInstruction * stopsFromto(AbstractInstruction *self_in_CogX64Compiler, sqInt startAddr, sqInt endAddr);
static NoDbgRegParms AbstractInstruction * storeLiteral32beforeFollowingAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt literal, sqInt followingAddress);
static NoDbgRegParms sqInt sib(AbstractInstruction *self_in_CogX64Compiler, sqInt scale, sqInt indexReg, sqInt baseReg);
static NoDbgRegParms int twoByteInstructionSizeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc);
static NoDbgRegParms sqInt zoneCallsAreRelative(AbstractInstruction *self_in_CogX64Compiler);
extern sqInt cogMethodHasExternalPrim(CogMethod *aCogMethod);
extern sqInt cogMethodHasMachineCodePrim(CogMethod *aCogMethod);
static sqInt compileBlockDispatch(void);
static void compileGetErrorCode(void);
static sqInt compileInterpreterPrimitive(void);
static NoDbgRegParms sqInt compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags);
static NoDbgRegParms sqInt compileOnStackExternalPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags);
static NoDbgRegParms AbstractInstruction * compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone);
static NoDbgRegParms void compileOpenPICnumArgs(sqInt selector, sqInt numArgs);
static NoDbgRegParms AbstractInstruction * compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone);
static sqInt compilePrimitive(void);
static sqInt extendedPushBytecode(void);
static sqInt extendedStoreAndPopBytecode(void);
static sqInt extendedStoreBytecode(void);
static int frameOffsetOfNativeFrameMark(void);
static int frameOffsetOfNativeFramePointer(void);
static int frameOffsetOfNativeStackPointer(void);
static int frameOffsetOfPreviousNativeStackPointer(void);
static NoDbgRegParms sqInt frameOffsetOfTemporary(sqInt index);
static int genCallMappedInlinedPrimitive(void);
static NoDbgRegParms AbstractInstruction * genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg);
static sqInt genExtendedSendBytecode(void);
static sqInt genExtendedSuperBytecode(void);
static sqInt genExtJumpIfFalse(void);
static sqInt genExtJumpIfTrue(void);
static sqInt genExtNopBytecode(void);
static sqInt genExtPushCharacterBytecode(void);
static sqInt genExtPushIntegerBytecode(void);
static sqInt genExtPushLiteralBytecode(void);
static sqInt genExtPushLiteralVariableBytecode(void);
static sqInt genExtPushPseudoVariable(void);
static sqInt genExtPushReceiverVariableBytecode(void);
static sqInt genExtSendBytecode(void);
static sqInt genExtSendSuperBytecode(void);
static sqInt genExtStoreAndPopLiteralVariableBytecode(void);
static sqInt genExtStoreAndPopReceiverVariableBytecode(void);
static sqInt genExtStoreLiteralVariableBytecode(void);
static sqInt genExtStoreReceiverVariableBytecode(void);
static sqInt genExtUnconditionalJump(void);
static sqInt genFastPrimFail(void);
static NoDbgRegParms void genFastPrimTraceUsingand(sqInt r1, sqInt r2);
static void genLoadNewMethod(void);
static sqInt genLongJumpIfFalse(void);
static sqInt genLongJumpIfTrue(void);
static sqInt genLongPushTemporaryVariableBytecode(void);
static sqInt genLongStoreAndPopTemporaryVariableBytecode(void);
static sqInt genLongStoreTemporaryVariableBytecode(void);
static sqInt genLongUnconditionalBackwardJump(void);
static sqInt genLongUnconditionalForwardJump(void);
static NoDbgRegParms sqInt genLookupForPerformNumArgs(sqInt numArgs);
static NoDbgRegParms usqInt genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName);
static int genPrimitiveHashMultiply(void);
static NoDbgRegParms void genPrimReturnEnterCogCodeEnilopmart(sqInt profiling);
static sqInt genPushConstantFalseBytecode(void);
static sqInt genPushConstantNilBytecode(void);
static sqInt genPushConstantOneBytecode(void);
static sqInt genPushConstantTrueBytecode(void);
static sqInt genPushConstantZeroBytecode(void);
static sqInt genPushLiteralConstantBytecode(void);
static sqInt genPushLiteralVariable16CasesBytecode(void);
static sqInt genPushLiteralVariableBytecode(void);
static sqInt genPushQuickIntegerConstantBytecode(void);
static sqInt genPushReceiverVariableBytecode(void);
static sqInt genPushTemporaryVariableBytecode(void);
extern sqInt genQuickReturnConst(void);
extern sqInt genQuickReturnInstVar(void);
extern sqInt genQuickReturnSelf(void);
static sqInt genReturnFalse(void);
static sqInt genReturnNil(void);
static sqInt genReturnNilFromBlock(void);
static sqInt genReturnTrue(void);
static sqInt genSecondExtendedSendBytecode(void);
static sqInt genSendLiteralSelector0ArgsBytecode(void);
static sqInt genSendLiteralSelector1ArgBytecode(void);
static sqInt genSendLiteralSelector2ArgsBytecode(void);
static sqInt genShortJumpIfFalse(void);
static sqInt genShortJumpIfTrue(void);
static sqInt genShortUnconditionalJump(void);
static sqInt genSpecialSelectorEqualsEquals(void);
static sqInt genSpecialSelectorNotEqualsEquals(void);
static sqInt genSpecialSelectorSend(void);
static sqInt genStoreAndPopReceiverVariableBytecode(void);
static sqInt genStoreAndPopRemoteTempLongBytecode(void);
static sqInt genStoreAndPopTemporaryVariableBytecode(void);
static sqInt genStoreRemoteTempLongBytecode(void);
static void genTakeProfileSample(void);
static int genUnconditionalTrapBytecode(void);
static NoDbgRegParms void loadNativeArgumentAddressto(sqInt baseOffset, sqInt reg);
static NoDbgRegParms void loadNativeFramePointerInto(sqInt reg);
static NoDbgRegParms void loadNativeLocalAddressto(sqInt baseOffset, sqInt reg);
extern sqInt mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj);
static sqInt numSpecialSelectors(void);
static NoDbgRegParms usqInt pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod);
static NoDbgRegParms sqInt pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static PrimitiveDescriptor * primitiveGeneratorOrNil(void);
static NoDbgRegParms int registerisInMask(sqInt reg, sqInt mask);
static NoDbgRegParms int registerisNotInMask(sqInt reg, sqInt mask);
static NoDbgRegParms sqInt v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v4BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v4LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v4LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
extern double getCogCodeZoneThreshold(void);
extern sqInt setCogCodeZoneThreshold(double ratio);
static NoDbgRegParms BlockStart * addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span);
static NoDbgRegParms void adjustArgumentsForPerform(sqInt numArgs);
static NoDbgRegParms sqInt allocateFloatRegNotConflictingWith(sqInt regMask);
static NoDbgRegParms sqInt allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask);
static NoDbgRegParms sqInt allocateRegNotConflictingWith(sqInt regMask);
static NoDbgRegParms sqInt anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n);
static NoDbgRegParms void beginHighLevelCall(sqInt alignment);
extern void callCogCodePopReceiverArg0Regs(void);
extern void callCogCodePopReceiverArg1Arg0Regs(void);
static sqInt callSwitchToCStack(void);
static void callSwitchToSmalltalkStack(void);
static NoDbgRegParms sqInt compileAbstractInstructionsFromthrough(sqInt start, sqInt end);
static sqInt compileBlockBodies(void);
static NoDbgRegParms void compileBlockFrameBuild(BlockStart *blockStart);
static NoDbgRegParms void compileBlockFramelessEntry(BlockStart *blockStart);
static NoDbgRegParms CogMethod * compileCogFullBlockMethod(sqInt numCopied);
static NoDbgRegParms CogMethod * compileCogMethod(sqInt selector);
static sqInt compileEntireMethod(void);
static void compileFrameBuild(void);
static NoDbgRegParms void compileFullBlockFramelessEntry(sqInt numCopied);
static NoDbgRegParms void compileFullBlockMethodFrameBuild(sqInt numCopied);
#if IMMUTABILITY
static void compileTwoPathFrameBuild(void);
#endif /* IMMUTABILITY */
static void compileTwoPathFramelessInit(void);
static NoDbgRegParms sqInt cPICMissTrampolineFor(sqInt numArgs);
static sqInt doubleExtendedDoAnythingBytecode(void);
static sqInt duplicateTopBytecode(void);
static void endHighLevelCallWithCleanup(void);
static void endHighLevelCallWithoutCleanup(void);
static NoDbgRegParms BytecodeFixup * ensureFixupAt(sqInt targetPC);
static NoDbgRegParms BytecodeFixup * ensureNonMergeFixupAt(sqInt targetPC);
static void ensureReceiverResultRegContainsSelf(void);
static NoDbgRegParms void evaluateat(BytecodeDescriptor *descriptor, sqInt pc);
static NoDbgRegParms sqInt eventualTargetOf(sqInt targetBytecodePC);
static NoDbgRegParms sqInt freeAnyFloatRegNotConflictingWith(sqInt regMask);
static NoDbgRegParms sqInt freeAnyRegNotConflictingWith(sqInt regMask);
static sqInt genBlockReturn(void);
static NoDbgRegParms void (*genCallPICEnilopmartNumArgs(sqInt numArgs))(void);
static sqInt genCallPrimitiveBytecode(void);
static sqInt genExternalizePointersForPrimitiveCall(void);
static AbstractInstruction * genExternalizeStackPointerForFastPrimitiveCall(void);
static sqInt genExtPushClosureBytecode(void);
static sqInt genExtPushFullClosureBytecode(void);
static void generateEnilopmarts(void);
static NoDbgRegParms sqInt generateInstructionsAt(sqInt eventualAbsoluteAddress);
static void generateMissAbortTrampolines(void);
static void generateSendTrampolines(void);
static void generateTracingTrampolines(void);
static NoDbgRegParms sqInt genForwardersInlinedIdenticalOrNotIf(sqInt orNot);
static NoDbgRegParms sqInt genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot);
static NoDbgRegParms sqInt genInlinedIdenticalOrNotIf(sqInt orNot);
static NoDbgRegParms sqInt genJumpBackTo(sqInt targetBytecodePC);
static NoDbgRegParms sqInt genJumpIfto(sqInt boolean, sqInt targetBytecodePC);
static NoDbgRegParms sqInt genJumpTo(sqInt targetBytecodePC);
static NoDbgRegParms sqInt genLowcodeBinaryInlinePrimitive(sqInt prim);
static NoDbgRegParms sqInt genLowcodeNullaryInlinePrimitive(sqInt prim);
static NoDbgRegParms sqInt genLowcodeTrinaryInlinePrimitive(sqInt prim);
static NoDbgRegParms sqInt genLowcodeUnaryInlinePrimitive2(sqInt prim);
static NoDbgRegParms sqInt genLowcodeUnaryInlinePrimitive3(sqInt prim);
static NoDbgRegParms sqInt genLowcodeUnaryInlinePrimitive4(sqInt prim);
static NoDbgRegParms sqInt genLowcodeUnaryInlinePrimitive5(sqInt prim);
static NoDbgRegParms sqInt genLowcodeUnaryInlinePrimitive(sqInt prim);
static NoDbgRegParms sqInt genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable);
static NoDbgRegParms usqInt genMethodAbortTrampolineFor(sqInt numArgs);
static NoDbgRegParms usqInt genPICAbortTrampolineFor(sqInt numArgs);
static NoDbgRegParms usqInt genPICMissTrampolineFor(sqInt numArgs);
static sqInt genPopStackBytecode(void);
static sqInt genPrimitiveClosureValue(void);
static sqInt genPrimitiveFullClosureValue(void);
static sqInt genPrimitivePerform(void);
static sqInt genPrimitivePerformWithArguments(void);
static sqInt genPushActiveContextBytecode(void);
static sqInt genPushClosureCopyCopiedValuesBytecode(void);
static NoDbgRegParms sqInt genPushLiteralIndex(sqInt literalIndex);
static NoDbgRegParms sqInt genPushLiteralVariable(sqInt literalIndex);
static NoDbgRegParms sqInt genPushMaybeContextReceiverVariable(sqInt slotIndex);
static sqInt genPushNewArrayBytecode(void);
static sqInt genPushReceiverBytecode(void);
static NoDbgRegParms sqInt genPushReceiverVariable(sqInt index);
static void genPushRegisterArgs(void);
static sqInt genPushRemoteTempLongBytecode(void);
static NoDbgRegParms sqInt genPushTemporaryVariable(sqInt index);
static sqInt genReturnReceiver(void);
static sqInt genReturnTopFromBlock(void);
static sqInt genReturnTopFromMethod(void);
static NoDbgRegParms sqInt genSendDirectedSupernumArgs(sqInt selectorIndex, sqInt numArgs);
static NoDbgRegParms sqInt genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs);
static NoDbgRegParms usqInt genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3);
static NoDbgRegParms sqInt genSendnumArgs(sqInt selectorIndex, sqInt numArgs);
static sqInt genSpecialSelectorArithmetic(void);
static sqInt genSpecialSelectorClass(void);
static sqInt genSpecialSelectorComparison(void);
static sqInt genStaticallyResolvedSpecialSelectorComparison(void);
static NoDbgRegParms sqInt genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck);
static NoDbgRegParms sqInt genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex);
static sqInt genUpArrowReturn(void);
static NoDbgRegParms sqInt genVanillaInlinedIdenticalOrNotIf(sqInt orNot);
static NoDbgRegParms void initSimStackForFramefulMethod(sqInt startpc);
static NoDbgRegParms void initSimStackForFramelessBlock(sqInt startpc);
static NoDbgRegParms void initSimStackForFramelessMethod(sqInt startpc);
static NoDbgRegParms sqInt isNonForwarderReceiver(sqInt reg);
static void leaveNativeFrame(void);
static sqInt liveFloatRegisters(void);
static sqInt liveRegisters(void);
static NoDbgRegParms sqInt mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor);
static NoDbgRegParms void marshallSendArguments(sqInt numArgs);
static sqInt maybeCompilingFirstPassOfBlockWithInitialPushNil(void);
static NoDbgRegParms sqInt mergeWithFixupIfRequired(BytecodeFixup *fixup);
static NoDbgRegParms sqInt methodAbortTrampolineFor(sqInt numArgs);
static sqInt methodFoundInvalidPostScan(void);
static NoDbgRegParms int needsFrameIfMod16GENumArgs(sqInt stackDelta);
static NoDbgRegParms int needsFrameIfStackGreaterThanOne(sqInt stackDelta);
static NoDbgRegParms sqInt numberOfSpillsInTopNItems(sqInt n);
static NoDbgRegParms sqInt picAbortTrampolineFor(sqInt numArgs);
static sqInt prevInstIsPCAnnotated(void);
static int receiverIsInReceiverResultReg(void);
static NoDbgRegParms void reinitializeFixupsFromthrough(sqInt start, sqInt end);
static NoDbgRegParms sqInt scanBlock(BlockStart *blockStart);
static sqInt scanMethod(void);
static NoDbgRegParms sqInt squeakV3orSistaV1PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils);
static NoDbgRegParms sqInt squeakV3orSistaV1NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms void ssAllocateRequiredFloatRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr);
static NoDbgRegParms void ssAllocateRequiredFloatReg(sqInt requiredReg);
static NoDbgRegParms void ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr);
static NoDbgRegParms void ssFlushUpThroughReceiverVariable(sqInt slotIndex);
static NoDbgRegParms void ssFlushUpThroughTemporaryVariable(sqInt tempIndex);
static NoDbgRegParms void ssNativeFlushTo(sqInt index);
static NoDbgRegParms void ssNativePop(sqInt n);
static NoDbgRegParms void ssNativePush(sqInt n);
static CogSimStackNativeEntry * ssNativeTop(void);
static NoDbgRegParms CogSimStackNativeEntry * ssNativeValue(sqInt n);
static NoDbgRegParms void ssPopNativeSize(sqInt popSize);
static NoDbgRegParms void ssPop(sqInt n);
static NoDbgRegParms sqInt ssPushAnnotatedConstant(sqInt literal);
static NoDbgRegParms sqInt ssPushBaseoffset(sqInt reg, sqInt offset);
static NoDbgRegParms sqInt ssPushConstant(sqInt literal);
static NoDbgRegParms sqInt ssPushDesc(SimStackEntry simStackEntry);
static NoDbgRegParms sqInt ssPushNativeConstantFloat32(float aFloat32);
static NoDbgRegParms sqInt ssPushNativeConstantFloat64(double aFloat64);
static NoDbgRegParms sqInt ssPushNativeConstantInt32(sqInt anInt32);
static NoDbgRegParms sqInt ssPushNativeConstantInt64(sqLong anInt64);
static NoDbgRegParms sqInt ssPushNativeConstantPointer(sqInt aNativePointer);
static NoDbgRegParms sqInt ssPushNativeRegisterDoubleFloat(sqInt reg);
static NoDbgRegParms sqInt ssPushNativeRegisterSingleFloat(sqInt reg);
static NoDbgRegParms sqInt ssPushNativeRegister(sqInt reg);
static NoDbgRegParms sqInt ssPushNativeRegistersecondRegister(sqInt reg, sqInt secondReg);
static NoDbgRegParms sqInt ssPushRegister(sqInt reg);
static NoDbgRegParms void ssPush(sqInt n);
static SimStackEntry ssSelfDescriptor(void);
static NoDbgRegParms void ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg);
static NoDbgRegParms sqInt ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg);
static NoDbgRegParms void ssStorePoptoReg(sqInt popBoolean, sqInt reg);
static CogSimStackEntry * ssTop(void);
static NoDbgRegParms CogSimStackEntry * ssValue(sqInt n);
static NoDbgRegParms sqInt stackEntryIsBoolean(CogSimStackEntry *simStackEntry);
static sqInt tempsValidAndVolatileEntriesSpilled(void);
static NoDbgRegParms sqInt tryCollapseTempVectorInitializationOfSize(sqInt slots);
static sqInt violatesEnsureSpilledSpillAssert(void);
static void voidReceiverResultRegContainsSelf(void);


/*** Variables ***/
static AbstractInstruction * abstractOpcodes;
static usqInt allocationThreshold;
static usqInt baseAddress;
static sqInt blockCount;
static AbstractInstruction * blockEntryLabel;
static AbstractInstruction * blockEntryNoContextSwitch;
static BlockStart * blockStarts;
static sqInt breakBlock;
static sqInt breakMethod;
static sqInt byte0;
static sqInt byte1;
static sqInt byte2;
static sqInt byte3;
static sqInt bytecodePC;
static sqInt bytecodeSetOffset;
static sqInt ceByteSizeOfTrampoline;
static sqInt ceCPICMissTrampoline;
static sqInt ceDereferenceSelectorIndex;
static sqInt ceFetchContextInstVarTrampoline;
static sqInt ceFFICalloutTrampoline;
static sqInt ceFloatObjectOfTrampoline;
static sqInt ceFloatValueOfTrampoline;
static sqInt ceFlushICache;
static sqInt ceFreeTrampoline;
static sqInt ceInlineNewHashTrampoline;
static sqInt ceInstantiateClassIndexableSizeTrampoline;
static sqInt ceInstantiateClassTrampoline;
static sqInt ceLargeActiveContextInBlockTrampoline;
static sqInt ceLargeActiveContextInFullBlockTrampoline;
static sqInt ceLargeActiveContextInMethodTrampoline;
static sqInt ceMallocTrampoline;
static sqInt ceMethodAbortTrampoline;
static sqInt ceNewHashTrampoline;
static sqInt ceNonLocalReturnTrampoline;
static sqInt cePICAbortTrampoline;
static sqInt cePositive64BitIntegerTrampoline;
static sqInt cePositive64BitValueOfTrampoline;
static sqInt cePrimReturnEnterCogCode;
static sqInt cePrimReturnEnterCogCodeProfiling;
static sqInt ceReapAndResetErrorCodeTrampoline;
static sqInt ceScheduleScavengeTrampoline;
static sqInt ceSendMustBeBooleanAddFalseTrampoline;
static sqInt ceSendMustBeBooleanAddTrueTrampoline;
static sqInt ceSigned64BitIntegerTrampoline;
static sqInt ceSigned64BitValueOfTrampoline;
static sqInt ceSmallActiveContextInBlockTrampoline;
static sqInt ceSmallActiveContextInFullBlockTrampoline;
static sqInt ceSmallActiveContextInMethodTrampoline;
static sqInt ceStoreCheckContextReceiverTrampoline;
static sqInt ceStoreCheckTrampoline;
static sqInt ceStoreContextInstVarTrampoline;
static sqInt ceTraceBlockActivationTrampoline;
static sqInt ceTraceLinkedSendTrampoline;
static sqInt ceTraceStoreTrampoline;
static sqInt checkedEntryAlignment;
static sqInt closedPICSize;
static sqInt codeBase;
static sqInt codeModified;
#if DUAL_MAPPED_CODE_ZONE
static sqInt codeToDataDelta;
#else
# define codeToDataDelta 0
#endif
static sqInt cogConstituentIndex;
static sqInt compactionInProgress;
static sqInt compilationPass;
static sqInt compilationTrace;
static sqInt cPICCaseSize;
static sqInt cPICEndOfCodeOffset;
static sqInt cPICEndSize;
static CogMethod * cPICPrototype;
static usqIntptr_t cpuidWord1;
static sqInt currentCallCleanUpSize;
static sqInt deadCode;
static sqInt debugBytecodePointers;
static sqInt debugFixupBreaks;
static sqInt debugOpcodeIndices;
static sqInt debugStackPointers;
static sqInt directedSendUsesBinding;
static sqInt directedSuperBindingSendTrampolines[NumSendTrampolines];
static sqInt directedSuperSendTrampolines[NumSendTrampolines];
static sqInt disassemblingMethod;
static AbstractInstruction * endCPICCase0;
static sqInt endPC;
static AbstractInstruction * entry;
static sqInt entryPointMask;
static CogMethod * enumeratingCogMethod;
static sqInt expectedFPAlignment;
static sqInt expectedSPAlignment;
static sqInt extA;
static sqInt extB;
static sqInt firstCPICCaseOffset;
static sqInt firstSend;
static BytecodeFixup * fixups;
static AbstractInstruction * fullBlockEntry;
static AbstractInstruction * fullBlockNoContextSwitchEntry;
static BytecodeDescriptor generatorTable[512] = {
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushReceiverBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantTrueBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantFalseBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantNilBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genReturnReceiver, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTrue, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnFalse, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNil, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromMethod, 0, needsFrameIfInBlock, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extendedPushBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ extendedStoreBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ extendedStoreAndPopBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ doubleExtendedDoAnythingBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtendedSuperBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0 },
	{ genSecondExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genPopStackBytecode, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ duplicateTopBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushActiveContextBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushNewArrayBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genCallPrimitiveBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushClosureCopyCopiedValuesBytecode, v3BlockCodeSize, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AddRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, SubRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLess, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreater, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLessOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreaterOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpNonZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AndRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, OrRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorClass, 0, needsFrameIfStackGreaterThanOne, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorNotEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushReceiverBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantTrueBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantFalseBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantNilBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantZeroBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantOneBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushPseudoVariable, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ duplicateTopBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genReturnReceiver, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTrue, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnFalse, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNil, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromMethod, 0, needsFrameIfInBlock, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNilFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ genReturnTopFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtNopBytecode, 0, needsFrameNever, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AddRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, SubRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLess, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreater, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLessOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreaterOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpNonZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AndRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, OrRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorClass, 0, needsFrameIfStackGreaterThanOne, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorNotEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPopStackBytecode, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genUnconditionalTrapBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extABytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extBBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genExtPushReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genExtPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushLiteralBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongPushTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genPushNewArrayBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushIntegerBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushCharacterBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtSendSuperBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genCallMappedInlinedPrimitive, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1 },
	{ genExtUnconditionalJump, v4LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtJumpIfTrue, v4LongBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtJumpIfFalse, v4LongBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtStoreAndPopReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtStoreAndPopLiteralVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 0, 0, 0 },
	{ genLongStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtStoreReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtStoreLiteralVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 0, 0, 0 },
	{ genLongStoreTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genCallPrimitiveBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 },
	{ genExtPushFullClosureBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushClosureBytecode, v4BlockCodeSize, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 },
	{ genPushRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 }
};
static sqInt guardPageSize;
static sqInt hasMovableLiteral;
static sqInt hasNativeFrame;
static sqInt hasYoungReferent;
static sqInt inBlock;
static sqInt initialPC;
static sqInt introspectionData;
static sqInt introspectionDataIndex;
static sqInt lastSend;
static usqInt limitAddress;
static sqInt maxLitIndex;
static sqInt methodAbortTrampolines[4];
static sqInt methodBytesFreedSinceLastCompaction;
static sqInt methodCount;
static sqInt methodHeader;
static sqInt methodObj;
static sqInt methodOrBlockNumArgs;
static sqInt methodOrBlockNumTemps;
static usqIntptr_t minValidCallAddress;
static usqInt mzFreeStart;
static sqInt needsFrame;
static AbstractInstruction * noCheckEntry;
static sqInt numAbstractOpcodes;
static sqInt numExtB;
static usqInt objectReferencesInRuntime[NumObjRefsInRuntime+1];
static sqInt opcodeIndex;
static CogMethod *openPICList = 0;
static sqInt openPICSize;
static sqInt ordinarySendTrampolines[NumSendTrampolines];
static sqInt picAbortTrampolines[4];
static AbstractInstruction * picInterpretAbort;
static sqInt picMissTrampolines[4];
static AbstractInstruction * picMNUAbort;
static BytecodeDescriptor * prevBCDescriptor;
static PrimitiveDescriptor primitiveGeneratorTable[MaxCompiledPrimitiveIndex+1] = {
	{ 0, -1 },
	{ genPrimitiveAdd, 1 },
	{ genPrimitiveSubtract, 1 },
	{ genPrimitiveLessThan, 1 },
	{ genPrimitiveGreaterThan, 1 },
	{ genPrimitiveLessOrEqual, 1 },
	{ genPrimitiveGreaterOrEqual, 1 },
	{ genPrimitiveEqual, 1 },
	{ genPrimitiveNotEqual, 1 },
	{ genPrimitiveMultiply, 1 },
	{ genPrimitiveDivide, 1 },
	{ genPrimitiveMod, 1 },
	{ genPrimitiveDiv, 1 },
	{ genPrimitiveQuo, 1 },
	{ genPrimitiveBitAnd, 1 },
	{ genPrimitiveBitOr, 1 },
	{ genPrimitiveBitXor, 1 },
	{ genPrimitiveBitShift, 1 },
	{ genPrimitiveMakePoint, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAsFloat, 0 },
	{ genPrimitiveFloatAdd, 1 },
	{ genPrimitiveFloatSubtract, 1 },
	{ genPrimitiveFloatLessThan, 1 },
	{ genPrimitiveFloatGreaterThan, 1 },
	{ genPrimitiveFloatLessOrEqual, 1 },
	{ genPrimitiveFloatGreaterOrEqual, 1 },
	{ genPrimitiveFloatEqual, 1 },
	{ genPrimitiveFloatNotEqual, 1 },
	{ genPrimitiveFloatMultiply, 1 },
	{ genPrimitiveFloatDivide, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveFloatSquareRoot, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAt, 1 },
	{ genPrimitiveAtPut, 2 },
	{ genPrimitiveSize, 0 },
	{ genPrimitiveStringAt, 1 },
	{ genPrimitiveStringAtPut, 2 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genPrimitiveObjectAt, 1 },
	{ 0, -1 },
	{ genPrimitiveNew, 0 },
	{ genPrimitiveNewWithArg, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNewMethod, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitivePerform, -1 },
	{ genPrimitivePerformWithArguments, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringReplace, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentical, 1 },
	{ genPrimitiveClass, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveShallowCopy, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringCompareWith, 1 },
	{ genPrimitiveHashMultiply, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIntegerAt, 1 },
	{ genPrimitiveIntegerAtPut, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNotIdentical, 1 },
	{ genPrimitiveAsCharacter, -1 },
	{ genPrimitiveImmediateAsInteger, 0 },
	{ 0, -1 },
	{ genPrimitiveSlotAt, 1 },
	{ genPrimitiveSlotAtPut, 2 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ genPrimitiveClosureValue, 2 },
	{ genPrimitiveClosureValue, 3 },
	{ genPrimitiveClosureValue, 4 },
	{ 0, -1 },
	{ genPrimitiveFullClosureValue, -1 },
	{ 0, -1 },
	{ genPrimitiveFullClosureValue, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveSmallFloatAdd, 1 },
	{ genPrimitiveSmallFloatSubtract, 1 },
	{ genPrimitiveSmallFloatLessThan, 1 },
	{ genPrimitiveSmallFloatGreaterThan, 1 },
	{ genPrimitiveSmallFloatLessOrEqual, 1 },
	{ genPrimitiveSmallFloatGreaterOrEqual, 1 },
	{ genPrimitiveSmallFloatEqual, 1 },
	{ genPrimitiveSmallFloatNotEqual, 1 },
	{ genPrimitiveSmallFloatMultiply, 1 },
	{ genPrimitiveSmallFloatDivide, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveSmallFloatSquareRoot, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveHighBit, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveUninitializedNewWithArg, 1 }
};
static sqInt primitiveIndex;
static sqInt processorLock;
static sqInt receiverTags;
static sqInt regArgsHaveBeenPushed;
static sqInt runtimeObjectRefIndex;
static AbstractInstruction * sendMiss;
static sqInt simNativeSpillBase;
static CogSimStackNativeEntry simNativeStack[70];
static sqInt simNativeStackPtr;
static sqInt simNativeStackSize;
static sqInt simSpillBase;
static SimStackEntry simStack[70];
static sqInt simStackPtr;
static AbstractInstruction * stackCheckLabel;
static AbstractInstruction * stackOverflowCall;
static sqInt superSendTrampolines[NumSendTrampolines];
static sqInt tempOop;
static char *trampolineAddresses[NumTrampolines*2];
static sqInt trampolineTableIndex;
static sqInt uncheckedEntryAlignment;
static usqInt unpairedMethodList;
static sqInt useTwoPaths;
static sqInt varBaseAddress;
static usqInt youngReferrers;
static const int cStackAlignment = STACK_ALIGN_BYTES;
static int labelCounter;
static AbstractInstruction aMethodLabel;
static AbstractInstruction * const backEnd = &aMethodLabel;
#if DUAL_MAPPED_CODE_ZONE
static void (*ceFlushDCache)(usqIntptr_t from, usqIntptr_t to);
#endif
#if IMMUTABILITY
static sqInt ceStoreTrampolines[5];;
#endif
static AbstractInstruction * const methodLabel = &aMethodLabel;
static double thresholdRatio = 0.5;
sqInt blockNoContextSwitchOffset;
sqInt breakPC;
sqInt cbEntryOffset;
sqInt cbNoSwitchEntryOffset;
sqInt ceBaseFrameReturnTrampoline;
sqInt ceCannotResumeTrampoline;
sqInt ceCheckForInterruptTrampoline;
sqInt ceReturnToInterpreterTrampoline;
#if !defined(cFramePointerInUse)
sqInt cFramePointerInUse;
#endif
sqInt cmEntryOffset;
sqInt cmNoCheckEntryOffset;
usqInt methodZoneBase;
sqInt missOffset;
const char * traceFlagsMeanings[] = {
		"1: print trace", "2: trace sends", "4: trace block activations", "8: trace interpreter primitives",
		"16: trace events (context switches, GCs, etc)", "32: trace stack overflow (poll for events hook)",
		"64: trace linked sends", "128: trace fast C call interpreter primitives", null
	};
sqInt traceStores;
int traceFlags = 8 /* prim trace log on by default */;
void (*ceCall0ArgsPIC)(void);
void (*ceCall1ArgsPIC)(void);
void (*ceCall2ArgsPIC)(void);
void (*ceCallCogCodePopReceiverAndClassRegs)(void);
void (*ceCallCogCodePopReceiverArg0Regs)(void);
void (*ceCallCogCodePopReceiverArg1Arg0Regs)(void);
void (*ceCallCogCodePopReceiverReg)(void);
void (*ceCaptureCStackPointers)(void);
void (*ceEnterCogCodePopReceiverReg)(void);
usqIntptr_t (*ceGetFP)(void);
usqIntptr_t (*ceGetSP)(void);
void (*ceInvokeInterpret)(void);
void (*realCECallCogCodePopReceiverAndClassRegs)(void);
void (*realCECallCogCodePopReceiverArg0Regs)(void);
void (*realCECallCogCodePopReceiverArg1Arg0Regs)(void);
void (*realCECallCogCodePopReceiverReg)(void);
void (*realCEEnterCogCodePopReceiverReg)(void);


/*** Macros ***/
#define roundUpToMethodAlignment(ignored,numBytes) ((numBytes) + 7 & -8)
#define cPICNumCases stackCheckOffset
#define cPICNumCasesHack hack hack hack i.e. the getter macro does all the work
#define abstractInstructionAt(index) (&abstractOpcodes[index])
#define addressIsInInstructions(address) (!((usqInt)(address) & (BytesPerWord-1)) \
							&& (address) >= &abstractOpcodes[0] \
							&& (address) < &abstractOpcodes[opcodeIndex])
#define allocateBlockStarts(numBlocks) do { \
		blockStarts = (numBlocks) ? alloca(sizeof(BlockStart) * (numBlocks)) : 0; \
} while (0)
#define assertValidDualZone() true
#define assertValidDualZoneReadAddress(address) 0
#define assertValidDualZoneWriteAddress(address) 0
#define backEnd() backEnd
#define blockAlignment() 8
#define blockStartAt(index) (&blockStarts[index])
#define ceBaseFrameReturnPC() ceBaseFrameReturnTrampoline
#define ceCannotResumePC() ((usqInt)ceCannotResumeTrampoline)
#define ceCheckForInterruptTrampoline() ceCheckForInterruptTrampoline
#define ceReturnToInterpreterPC() ((usqInt)ceReturnToInterpreterTrampoline)
#define codeByteAtput(address,value) byteAtput((address) + codeToDataDelta, value)
#define codeLong32Atput(address,value) long32Atput((address) + codeToDataDelta, value)
#define codeLong64Atput(address,value) long64Atput((address) + codeToDataDelta, value)
#define codeLongAtput(address,value) longAtput((address) + codeToDataDelta, value)
#define codeMemcpy(dest,src,bytes) memcpy(dest,src,bytes)
#define codeMemmove(dest,src,bytes) memmove((char *)(dest)+codeToDataDelta,src,bytes)
#define cr() putchar('\n')
#define entryOffset() cmEntryOffset
#define generatorAt(index) (&generatorTable[index])
#define getCodeToDataDelta() codeToDataDelta
#define getIsObjectReference() 2
#define halt() warning("halt")
#define haltmsg(msg) warning("halt: " msg)
#define interpretOffset() missOffset
#define mapPerMethodProfile() 0
#define maxCogCodeSize() (16*1024*1024)
#define maybeBreakGeneratingFromto(address,end) 0
#define maybeBreakGeneratingInstructionWithIndex(i) 0
#define maybeHaltIfDebugPC() 0
#define methodLabel() methodLabel
#define methodZoneBase() methodZoneBase
#define minCogMethodAddress() methodZoneBase
#define moveProfileToMethods() 0
#define noCheckEntryOffset() cmNoCheckEntryOffset
#define noContextSwitchBlockEntryOffset() blockNoContextSwitchOffset
#define notYetImplemented() warning("not yet implemented")
#define null 0
#define printNum(n) printf("%" PRIdSQINT, (sqInt) (n))
#define printOnTrace() (traceFlags & 1)
#define recordBlockTrace() (traceFlags & 4)
#define recordEventTrace() (traceFlags & 16)
#define recordFastCCallPrimTrace() (traceFlags & 128)
#define recordOverflowTrace() (traceFlags & 32)
#define recordPrimTrace() (traceFlags & 8)
#define recordSendTrace() (traceFlags & 2)
#define reportError(n) warning("compilation error")
#define setHasMovableLiteral(b) (hasMovableLiteral = (b))
#define setHasYoungReferent(b) (hasYoungReferent = (b))
#define varBaseAddress() varBaseAddress
#define nextOpenPIC methodObject
#define nextOpenPICHack hack hack hack i.e. the getter macro does all the work
#define freeStart() mzFreeStart
#define limitZony() ((CogMethod *)mzFreeStart)
#define methodBytesFreedSinceLastCompaction() methodBytesFreedSinceLastCompaction
#define youngReferrers() youngReferrers
#define numRegArgs() 2
#define maybeConstant(sse) ((sse)->constant)
#define cpuidWord1(ign) cpuidWord1
#define flushDCacheFromto(me,startAddress,endAddress) 0
#define flushICacheFromto(me,startAddress,endAddress) 0
#define inlineCacheValueForSelectorin(backEnd,selector,aCogMethod) indexForSelectorin(selector,aCogMethod)
#define setCpuidWord1(ign,word) cpuidWord1 = word
#define unalignedLong32At(inst,byteAddress) long32At(byteAddress)
#define unalignedLong32Atput(inst,byteAddress,aWord) long32Atput(byteAddress,aWord)
#define unalignedLongAt(byteAddress) longAt(byteAddress)
#define unalignedLongAtput(byteAddress,aWord) longAtput(byteAddress,aWord)
#define fullBlockEntryOffset() cbEntryOffset
#define fullBlockNoContextSwitchEntryOffset() cbNoSwitchEntryOffset
#define needsFrame() needsFrame
#define fixupAtIndex(index) (&fixups[index])
#define simNativeStackAt(index) (simNativeStack + (index))
#define simSelf() simStack
#define simStackAt(index) (simStack + (index))
#define traceDescriptor(ign) 0
#define traceFixupmerge(igu,ana) 0
#define traceMerge(ign) 0
#define traceSimStack() 0
#define traceSpill(ign) 0
#define allocatype(numElements, elementType) alloca((numElements)*sizeof(elementType))
#define numElementsIn(anArray) (sizeof(anArray)/sizeof(anArray[0]))
#define oopisGreaterThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) >= (usqInt)(otherOop))
#define oopisGreaterThanOrEqualToandLessThanOrEqualTo(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) <= (usqInt)(limitOop))
#define oopisGreaterThanOrEqualToandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisGreaterThan(anOop,otherOop) ((usqInt)(anOop) > (usqInt)(otherOop))
#define oopisGreaterThanandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) > (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisLessThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) <= (usqInt)(otherOop))
#define oopisLessThan(anOop,otherOop) ((usqInt)(anOop) < (usqInt)(otherOop))


/*** Methods ***/

	/* CogAbstractInstruction>>#addDependent: */
static NoDbgRegParms AbstractInstruction *
addDependent(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anInstruction)
{
	if (!(((self_in_CogAbstractInstruction->dependent)) == null)) {
		(anInstruction->dependent = (self_in_CogAbstractInstruction->dependent));
	}
	return ((self_in_CogAbstractInstruction->dependent) = anInstruction);
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them. */

	/* CogAbstractInstruction>>#availableFloatRegisterOrNoneFor: */
static NoDbgRegParms sqInt
availableFloatRegisterOrNoneFor(AbstractInstruction *self_in_CogAbstractInstruction, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << DPFPReg0))) != 0))) {
		return DPFPReg0;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg1))) != 0))) {
		return DPFPReg1;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg2))) != 0))) {
		return DPFPReg2;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg3))) != 0))) {
		return DPFPReg3;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg4))) != 0))) {
		return DPFPReg4;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg5))) != 0))) {
		return DPFPReg5;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg6))) != 0))) {
		return DPFPReg6;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg7))) != 0))) {
		return DPFPReg7;
	}
	return NoReg;
}


/*	For out-of-line literal support, clone a literal from a literal. */

	/* CogAbstractInstruction>>#cloneLiteralFrom: */
static NoDbgRegParms AbstractInstruction *
cloneLiteralFrom(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *existingLiteral)
{
	assert((((existingLiteral->opcode)) == Literal)
	 && ((!((self_in_CogAbstractInstruction->dependent)))
	 && (!((self_in_CogAbstractInstruction->address)))));
	(self_in_CogAbstractInstruction->opcode) = Literal;
	(self_in_CogAbstractInstruction->annotation) = (existingLiteral->annotation);
	((self_in_CogAbstractInstruction->operands))[0] = (((existingLiteral->operands))[0]);
	((self_in_CogAbstractInstruction->operands))[1] = (((existingLiteral->operands))[1]);
	((self_in_CogAbstractInstruction->operands))[2] = (((existingLiteral->operands))[2]);
	return self_in_CogAbstractInstruction;
}


/*	Generate concrete machine code for the instruction at actualAddress,
	setting machineCodeSize, and answer the following address. */

	/* CogAbstractInstruction>>#concretizeAt: */
static NoDbgRegParms sqInt
concretizeAt(AbstractInstruction *self_in_CogAbstractInstruction, sqInt actualAddress)
{
	(self_in_CogAbstractInstruction->address) = actualAddress;
	(self_in_CogAbstractInstruction->machineCodeSize) = dispatchConcretize(self_in_CogAbstractInstruction);
	assert((((self_in_CogAbstractInstruction->maxSize)) == null)
	 || (((self_in_CogAbstractInstruction->maxSize)) >= ((self_in_CogAbstractInstruction->machineCodeSize))));
	return actualAddress + ((self_in_CogAbstractInstruction->machineCodeSize));
}


/*	Load the stack pointer register with that of the C stack, effecting
	a switch to the C stack. Used when machine code calls into the
	CoInterpreter run-time (e.g. to invoke interpreter primitives). */

	/* CogAbstractInstruction>>#genLoadCStackPointer */
static NoDbgRegParms sqInt
genLoadCStackPointer(AbstractInstruction *self_in_CogAbstractInstruction)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cStackPointerAddress(), NativeSPReg);
	return 0;
}


/*	Load the frame and stack pointer registers with those of the C stack,
	effecting a switch to the C stack. Used when machine code calls into
	the CoInterpreter run-time (e.g. to invoke interpreter primitives).
	N.B. CoInterpreter stack layout dictates that the stack pointer should be
	loaded first.
	The stack zone is allocated on the C stack before the interpreter runs and
	hence before CStackPointer and CFramePointer are captured. So when running
	in machine
	code the native stack pointer and frame pointer appear to be on a colder
	part of the
	stack to CStackPointer and CFramePointer. When CStackPointerhas been set
	and the frame pointer is still in machine code the current frame looks
	like it has lots of
	stack. If the frame pointer was set to CFramePointer before hand then it
	would be beyond the stack pointer for that one instruction. */

	/* CogAbstractInstruction>>#genLoadCStackPointers */
static NoDbgRegParms sqInt
genLoadCStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cStackPointerAddress(), NativeSPReg);
	genoperandoperand(MoveAwR, cFramePointerAddress(), FPReg);
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointerForPrimCall: */
static NoDbgRegParms sqInt
genLoadStackPointerForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, stackPointerAddress(), spareReg);
	genoperandoperand(SubCqR, BytesPerWord, spareReg);
	genoperandoperand(MoveRR, spareReg, SPReg);
	return 0;
}


/*	Switch back to the Smalltalk stack. Assign SPReg first
	because typically it is used immediately afterwards. */

	/* CogAbstractInstruction>>#genLoadStackPointers */
static NoDbgRegParms sqInt
genLoadStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, stackPointerAddress(), SPReg);
	genoperandoperand(MoveAwR, framePointerAddress(), FPReg);
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointersForPrimCall: */
static NoDbgRegParms sqInt
genLoadStackPointersForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, stackPointerAddress(), spareReg);
	genoperandoperand(SubCqR, BytesPerWord, spareReg);
	genoperandoperand(MoveRR, spareReg, SPReg);
	genoperandoperand(MoveAwR, framePointerAddress(), FPReg);
	return 0;
}


/*	Save the frame and stack pointer registers to the framePointer
	and stackPointer variables. Used to save the machine code frame
	for use by the run-time when calling into the CoInterpreter run-time. */

	/* CogAbstractInstruction>>#genSaveStackPointers */
static NoDbgRegParms sqInt
genSaveStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveRAw, FPReg, framePointerAddress());
	genoperandoperand(MoveRAw, SPReg, stackPointerAddress());
	return 0;
}

	/* CogAbstractInstruction>>#genWriteCResultIntoReg: */
static NoDbgRegParms AbstractInstruction *
genWriteCResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister)
{
	if ((abstractRegister != NoReg)
	 && (abstractRegister != ABIResultReg)) {
		genoperandoperand(MoveRR, ABIResultReg, abstractRegister);
	}
	return self_in_CogAbstractInstruction;
}

	/* CogAbstractInstruction>>#genWriteCSecondResultIntoReg: */
static NoDbgRegParms AbstractInstruction *
genWriteCSecondResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister)
{
	if ((abstractRegister != NoReg)
	 && (abstractRegister != RDX)) {
		genoperandoperand(MoveRR, RDX, abstractRegister);
	}
	return self_in_CogAbstractInstruction;
}


/*	At least x86, x86_64, AArch32 and AArch64 have a 64-bit performance
	counter. Subclasses can turn this off if rquired. */

	/* CogAbstractInstruction>>#has64BitPerformanceCounter */
static NoDbgRegParms sqInt
has64BitPerformanceCounter(AbstractInstruction *self_in_CogAbstractInstruction)
{
	return 1;
}


/*	For out-of-line literal support, initialize a sharable literal. */

	/* CogAbstractInstruction>>#initializeSharableLiteral: */
static NoDbgRegParms AbstractInstruction *
initializeSharableLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal)
{
	(self_in_CogAbstractInstruction->opcode) = Literal;
	/* separate := nil for Slang */
	(self_in_CogAbstractInstruction->annotation) = null;
	(self_in_CogAbstractInstruction->address) = null;
	(self_in_CogAbstractInstruction->dependent) = null;
	((self_in_CogAbstractInstruction->operands))[0] = literal;
	((self_in_CogAbstractInstruction->operands))[1] = (1 + (((sqInt)((usqInt)(BytesPerOop) << 1))));
	((self_in_CogAbstractInstruction->operands))[2] = -1;
	return self_in_CogAbstractInstruction;
}


/*	For out-of-line literal support, initialize an unsharable literal. */

	/* CogAbstractInstruction>>#initializeUniqueLiteral: */
static NoDbgRegParms AbstractInstruction *
initializeUniqueLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal)
{
	(self_in_CogAbstractInstruction->opcode) = Literal;
	/* separate := nil for Slang */
	(self_in_CogAbstractInstruction->annotation) = null;
	(self_in_CogAbstractInstruction->address) = null;
	(self_in_CogAbstractInstruction->dependent) = null;
	((self_in_CogAbstractInstruction->operands))[0] = literal;
	((self_in_CogAbstractInstruction->operands))[1] = (0 + (((sqInt)((usqInt)(BytesPerOop) << 1))));
	((self_in_CogAbstractInstruction->operands))[2] = -1;
	return self_in_CogAbstractInstruction;
}

	/* CogAbstractInstruction>>#isJump */
static NoDbgRegParms int
isJump(AbstractInstruction *self_in_CogAbstractInstruction)
{
	return ((((self_in_CogAbstractInstruction->opcode)) >= FirstJump) && (((self_in_CogAbstractInstruction->opcode)) <= LastJump));
}


/*	We assume here that calls and jumps look the same as regards their
	displacement. This works on at least x86, ARM and x86_64. Processors on
	which that isn't the
	case can override as necessary. */

	/* CogAbstractInstruction>>#relocateJumpLongBeforeFollowingAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateJumpLongBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta)
{
	relocateCallBeforeReturnPCby(self_in_CogAbstractInstruction, pc, delta);
	return self_in_CogAbstractInstruction;
}


/*	Relocate a long conditional jump before pc. Default to relocating a
	non-conditional jump.
	Processors that have different formats for conditional and unconditional
	jumps override. */

	/* CogAbstractInstruction>>#relocateJumpLongConditionalBeforeFollowingAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta)
{
	relocateJumpLongBeforeFollowingAddressby(self_in_CogAbstractInstruction, pc, delta);
	return self_in_CogAbstractInstruction;
}

	/* CogAbstractInstruction>>#resolveJumpTarget */
static NoDbgRegParms AbstractInstruction *
resolveJumpTarget(AbstractInstruction *self_in_CogAbstractInstruction)
{
    BytecodeFixup *fixup;

	assert(isJump(self_in_CogAbstractInstruction));
	fixup = ((BytecodeFixup *) (((self_in_CogAbstractInstruction->operands))[0]));
	if (addressIsInFixups(fixup)) {
		assert(addressIsInInstructions((fixup->targetInstruction)));
		jmpTarget(self_in_CogAbstractInstruction, (fixup->targetInstruction));
	}
	return self_in_CogAbstractInstruction;
}


/*	Rewrite a conditional jump long to jump to target. This version defaults
	to using
	rewriteJumpLongAt:, which works for many ISAs. Subclasses override if
	necessary.  */

	/* CogAbstractInstruction>>#rewriteConditionalJumpLongAt:target: */
static NoDbgRegParms sqInt
rewriteConditionalJumpLongAttarget(AbstractInstruction *self_in_CogAbstractInstruction, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteCallAttarget(self_in_CogAbstractInstruction, callSiteReturnAddress, callTargetAddress);
}

	/* CogBlockMethod>>#cmHomeMethod */
static NoDbgRegParms CogMethod *
cmHomeMethod(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cpicHasMNUCaseOrCMIsFullBlock)
		? ((CogMethod *) self_in_CogBlockMethod)
		: ((CogMethod *) ((((usqInt)self_in_CogBlockMethod)) - ((self_in_CogBlockMethod->homeOffset)))));
}

	/* CogBlockMethod>>#isCMBlock */
static NoDbgRegParms int
isCMBlock(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMBlock;
}

	/* CogBlockMethod>>#isCMClosedPIC */
static NoDbgRegParms int
isCMClosedPIC(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMClosedPIC;
}

	/* CogBlockMethod>>#isCMFree */
static NoDbgRegParms int
isCMFree(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMFree;
}

	/* CogBlockMethod>>#isCMMethodEtAl */
static NoDbgRegParms int
isCMMethodEtAl(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) >= CMMethod;
}

	/* CogBlockMethod>>#isCMOpenPIC */
static NoDbgRegParms int
isCMOpenPIC(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMOpenPIC;
}

	/* CogBytecodeDescriptor>>#isBranch */
static NoDbgRegParms sqInt
isBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor)
{
	return (((self_in_CogBytecodeDescriptor->spanFunction)))
	 && (!((self_in_CogBytecodeDescriptor->isBlockCreation)));
}

	/* CogBytecodeDescriptor>>#isConditionalBranch */
static NoDbgRegParms sqInt
isConditionalBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor)
{
	return ((self_in_CogBytecodeDescriptor->isBranchTrue))
	 || ((self_in_CogBytecodeDescriptor->isBranchFalse));
}

	/* CogBytecodeFixup>>#notAFixup */
static NoDbgRegParms int
notAFixup(BytecodeFixup *self_in_CogBytecodeFixup)
{
	return ((self_in_CogBytecodeFixup->targetInstruction)) == 0;
}


/*	With CqR we assume constants are 32-bits or less. */

	/* CogInLineLiteralsX64Compiler>>#computeSizeOfArithCqR */
static NoDbgRegParms int
computeSizeOfArithCqR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler)
{
	if (isQuick(self_in_CogInLineLiteralsX64Compiler, ((self_in_CogInLineLiteralsX64Compiler->operands))[0])) {
		return 4;
	}
	if (is32BitSignedImmediate(self_in_CogInLineLiteralsX64Compiler, ((self_in_CogInLineLiteralsX64Compiler->operands))[0])) {
		return ((((self_in_CogInLineLiteralsX64Compiler->operands))[1]) == RAX
			? 6
			: 7);
	}
	return 13;
}

	/* CogInLineLiteralsX64Compiler>>#computeSizeOfArithCwR */
static NoDbgRegParms int
computeSizeOfArithCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler)
{
	return 13;
}

	/* CogInLineLiteralsX64Compiler>>#concretizeArithCwR: */
static NoDbgRegParms sqInt
concretizeArithCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt x64opcode)
{
    usqInt reg;
    sqInt reverse;
    usqInt value;

	value = ((self_in_CogInLineLiteralsX64Compiler->operands))[0];
	reg = ((self_in_CogInLineLiteralsX64Compiler->operands))[1];
	/* Tst & Cmp; backwards */
	reverse = (x64opcode == 133)
	 || (x64opcode == 57);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[0] = (rexRxb(self_in_CogInLineLiteralsX64Compiler, RISCTempReg, 0, RISCTempReg));
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[1] = (184 + (RISCTempReg & 7));
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[2] = (value & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[3] = (((value) >> 8) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[4] = (((value) >> 16) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[5] = (((value) >> 24) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[6] = (((value) >> 32) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[7] = (((value) >> 40) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[8] = (((value) >> 48) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[9] = (((value) >> 56) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[10] = ((reverse
	? rexRxb(self_in_CogInLineLiteralsX64Compiler, RISCTempReg, 0, reg)
	: rexRxb(self_in_CogInLineLiteralsX64Compiler, reg, 0, RISCTempReg)));
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[11] = x64opcode;
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[12] = ((reverse
	? modRMRO(self_in_CogInLineLiteralsX64Compiler, ModReg, reg, RISCTempReg)
	: modRMRO(self_in_CogInLineLiteralsX64Compiler, ModReg, RISCTempReg, reg)));
	assert((((self_in_CogInLineLiteralsX64Compiler->machineCode))[12]) > 144);
	return 13;
}


/*	Will get inlined into concretizeAt: switch. */

	/* CogInLineLiteralsX64Compiler>>#concretizeMoveCwR */
static NoDbgRegParms sqInt
concretizeMoveCwR(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler)
{
    sqInt offset;
    usqInt reg;
    usqInt value;

	value = ((self_in_CogInLineLiteralsX64Compiler->operands))[0];
	reg = ((self_in_CogInLineLiteralsX64Compiler->operands))[1];
	if (	/* begin isAnInstruction: */
		(addressIsInInstructions(((AbstractInstruction *) value)))
	 || ((((AbstractInstruction *) value)) == (methodLabel()))) {
		value = ((((AbstractInstruction *) value))->address);
	}
	if (	/* begin addressIsInCurrentCompilation: */
		((((usqInt)value)) >= ((methodLabel->address)))
	 && ((((usqInt)value)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))) {
		offset = value - (((self_in_CogInLineLiteralsX64Compiler->address)) + 7);
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[0] = (rexRxb(self_in_CogInLineLiteralsX64Compiler, reg, 0, 0));
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[1] = 141;
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[2] = (modRMRO(self_in_CogInLineLiteralsX64Compiler, ModRegInd, 5, reg));
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[3] = (offset & 0xFF);
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[4] = ((((usqInt)(offset)) >> 8) & 0xFF);
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[5] = ((((usqInt)(offset)) >> 16) & 0xFF);
		((self_in_CogInLineLiteralsX64Compiler->machineCode))[6] = ((((usqInt)(offset)) >> 24) & 0xFF);
		return 7;
	}
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[0] = (rexRxb(self_in_CogInLineLiteralsX64Compiler, 0, 0, reg));
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[1] = (184 + (reg & 7));
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[2] = (value & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[3] = (((value) >> 8) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[4] = (((value) >> 16) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[5] = (((value) >> 24) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[6] = (((value) >> 32) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[7] = (((value) >> 40) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[8] = (((value) >> 48) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[9] = (((value) >> 56) & 0xFF);
	((self_in_CogInLineLiteralsX64Compiler->machineCode))[10] = 144;
	return 11;
}


/*	Answer the inline cache tag for the return address of a send. */

	/* CogInLineLiteralsX64Compiler>>#inlineCacheTagAt: */
static NoDbgRegParms unsigned int
inlineCacheTagAt(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt callSiteReturnAddress)
{
	return literal32BeforeFollowingAddress(self_in_CogInLineLiteralsX64Compiler, ((usqInt)(callSiteReturnAddress - 5)));
}


/*	Answer if the receiver is a pc-dependent instruction. */

	/* CogInLineLiteralsX64Compiler>>#isPCDependent */
static NoDbgRegParms sqInt
isPCDependent(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler)
{
	return (isJump(self_in_CogInLineLiteralsX64Compiler))
	 || (((self_in_CogInLineLiteralsX64Compiler->opcode)) == AlignmentNops);
}


/*	Answer the 32-bit literal embedded in the instruction immediately
	preceding followingAddress.
 */

	/* CogInLineLiteralsX64Compiler>>#literal32BeforeFollowingAddress: */
static NoDbgRegParms unsigned int
literal32BeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt followingAddress)
{
	return ((unsigned int) (unalignedLong32At(self_in_CogInLineLiteralsX64Compiler, followingAddress - 4)));
}


/*	Answer the literal embedded in the instruction immediately preceding
	followingAddress. This is used in the MoveCwR, PushCw and ArithCwR cases;
	these are distinguished by a
	nop following the literal load in MoveCwR, a 16r48 + reg ending the PushCw
	sequence, and
	a (self mod: ModReg RM: rX RO: rY) ending the ArithCwR sequence, which is
	at least 16rC0. */

	/* CogInLineLiteralsX64Compiler>>#literalBeforeFollowingAddress: */
static NoDbgRegParms sqInt
literalBeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt followingAddress)
{
    sqInt base;
    sqInt lastByte;

	lastByte = byteAt(followingAddress - 1);
	/* ArithCwR */
	base = followingAddress - ((lastByte == 144
	? 9
	: (lastByte < 144
			? 10
			: 11)));
	return unalignedLongAt(base);
}

	/* CogInLineLiteralsX64Compiler>>#loadLiteralByteSize */
static NoDbgRegParms sqInt
loadLiteralByteSize(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler)
{
	return 11 /* begin moveCwRByteSize */;
}


/*	Size a jump and set its address. The target may be another instruction
	or an absolute address. On entry the address inst var holds our virtual
	address. On exit address is set to eventualAbsoluteAddress, which is
	where this instruction will be output. The span of a jump to a following
	instruction is therefore between that instruction's address and this
	instruction's address ((which are both still their virtual addresses), but
	the span of a jump to a preceding instruction or to an absolute address is
	between that instruction's address (which by now is its eventual absolute
	address) or absolute address and eventualAbsoluteAddress. */

	/* CogInLineLiteralsX64Compiler>>#sizePCDependentInstructionAt: */
static NoDbgRegParms unsigned char
sizePCDependentInstructionAt(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt eventualAbsoluteAddress)
{
    AbstractInstruction *abstractInstruction;
    usqInt alignment;
    sqInt maximumSpan;
    usqInt target;

	if (((self_in_CogInLineLiteralsX64Compiler->opcode)) == AlignmentNops) {
		(self_in_CogInLineLiteralsX64Compiler->address) = eventualAbsoluteAddress;
		alignment = ((self_in_CogInLineLiteralsX64Compiler->operands))[0];
		return ((self_in_CogInLineLiteralsX64Compiler->machineCodeSize) = ((eventualAbsoluteAddress + (alignment - 1)) & (-alignment)) - eventualAbsoluteAddress);
	}
	assert(isJump(self_in_CogInLineLiteralsX64Compiler));
	target = ((self_in_CogInLineLiteralsX64Compiler->operands))[0];
	abstractInstruction = ((AbstractInstruction *) target);
	if (	/* begin isAnInstruction: */
		(addressIsInInstructions(abstractInstruction))
	 || (abstractInstruction == (methodLabel()))) {
		maximumSpan = ((abstractInstruction->address)) - (((abstractInstructionfollows(self_in_CogInLineLiteralsX64Compiler, abstractInstruction)
	? eventualAbsoluteAddress
	: (self_in_CogInLineLiteralsX64Compiler->address))) + 2);
	}
	else {
		maximumSpan = target - (eventualAbsoluteAddress + 2);
	}
	(self_in_CogInLineLiteralsX64Compiler->address) = eventualAbsoluteAddress;
	if (((self_in_CogInLineLiteralsX64Compiler->opcode)) >= FirstShortJump) {
		(self_in_CogInLineLiteralsX64Compiler->machineCodeSize) = (isQuick(self_in_CogInLineLiteralsX64Compiler, maximumSpan)
			? 2
			: (((self_in_CogInLineLiteralsX64Compiler->opcode)) == Jump
					? 5
					: 6));
	}
	else {
		switch ((self_in_CogInLineLiteralsX64Compiler->opcode)) {
		case JumpLong:
			(self_in_CogInLineLiteralsX64Compiler->machineCodeSize) = 5;
			break;
		case JumpFull:
			(self_in_CogInLineLiteralsX64Compiler->machineCodeSize) = 12;
			break;
		case JumpLongZero:
		case JumpLongNonZero:
			(self_in_CogInLineLiteralsX64Compiler->machineCodeSize) = 6;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	return (self_in_CogInLineLiteralsX64Compiler->machineCodeSize);
}


/*	Rewrite the literal in the instruction immediately preceding
	followingAddress. This is used in the MoveCwR, PushCw and CmpCwR cases;
	these are distinguished by a
	nop following the literal load in MoveCwR, a 16r50 + reg ending the PushCw
	sequence, and
	a (self mod: ModReg RM: rX RO: rY) ending the CmpCwR sequence, which is at
	least 16rC0. */

	/* CogInLineLiteralsX64Compiler>>#storeLiteral:beforeFollowingAddress: */
static NoDbgRegParms AbstractInstruction *
storeLiteralbeforeFollowingAddress(AbstractInstruction *self_in_CogInLineLiteralsX64Compiler, sqInt literal, sqInt followingAddress)
{
    sqInt base;
    sqInt lastByte;

	lastByte = byteAt(followingAddress - 1);
	/* ArithCwR */
	base = followingAddress - ((lastByte <= 144
	? (lastByte == 144
			? 9
			: 10)
	: 11));
	unalignedLongAtput(base, literal);
	return self_in_CogInLineLiteralsX64Compiler;
}

	/* Cogit>>#AddCq:R: */
static NoDbgRegParms AbstractInstruction *
gAddCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(AddCqR, quickConstant, reg);
}


/*	N.B. if the condition codes don't require setting and three address
	arithmetic is unavailable, then LoadEffectiveAddressMw:r:R: can be used
	instead. 
 */

	/* Cogit>>#AddCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gAddCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	if (srcReg == destReg) {
		return genoperandoperand(AddCqR, quickConstant, destReg);
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, quickConstant, destReg);
	return first;
}


/*	destReg := addendReg + badendReg */

	/* Cogit>>#AddR:R:R: */
static NoDbgRegParms AbstractInstruction *
gAddRRR(sqInt addendReg, sqInt badendReg, sqInt destReg)
{
    AbstractInstruction *first;

	assert(badendReg != destReg);
	first = genoperandoperand(MoveRR, addendReg, destReg);
	genoperandoperand(AddRR, badendReg, destReg);
	return first;
}

	/* Cogit>>#AndCq:R: */
static NoDbgRegParms AbstractInstruction *
gAndCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(AndCqR, quickConstant, reg);
}

	/* Cogit>>#AndCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gAndCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	if (srcReg == destReg) {
		return genoperandoperand(AndCqR, quickConstant, destReg);
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, destReg);
	return first;
}


/*	destReg := (signed)srcReg >> quickConstant */

	/* Cogit>>#ArithmeticShiftRightCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#abortOffset */
sqInt
abortOffset(void)
{
	return missOffset;
}

	/* Cogit>>#abstractInstruction:follows: */
static NoDbgRegParms int
abstractInstructionfollows(AbstractInstruction *theAbstractInstruction, AbstractInstruction *anAbstractInstruction)
{
	return theAbstractInstruction > anAbstractInstruction;
}

	/* Cogit>>#addCleanBlockStarts */
static void
addCleanBlockStarts(void)
{
    sqInt i;
    sqInt iLimiT;
    sqInt lit;
    sqInt startPCOrNil;

	for (i = 1, iLimiT = (literalCountOf(methodObj)); i <= iLimiT; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (!(startPCOrNil == null)) {
			maxLitIndex = ((maxLitIndex < i) ? i : maxLitIndex);
			addBlockStartAtnumArgsnumCopiedspan(startPCOrNil - 1, argumentCountOfClosure(lit), copiedValueCountOfClosure(lit), spanForCleanBlockStartingAt(startPCOrNil - 1));
		}
	}
}


/*	Perform an integrity/leak check using the heapMap.
	Set a bit at each cog method's header. */

	/* Cogit>>#addCogMethodsToHeapMap */
void
addCogMethodsToHeapMap(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			heapMapAtWordPut(cogMethod, 1);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#addressIsInFixups: */
static NoDbgRegParms sqInt
addressIsInFixups(BytecodeFixup *address)
{
	return (BytecodeFixup *)address >= fixups && (BytecodeFixup *)address < (fixups + numAbstractOpcodes);
}


/*	calculate the end of the n'th case statement - which is complicated
	because we have case 1 right at the top of our CPIC and then build up from
	the last one. Yes I know this sounds strange, but trust me - I'm an
	Engineer, we do things backwards all the emit
 */

	/* Cogit>>#addressOfEndOfCase:inCPIC: */
static NoDbgRegParms sqInt
addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC)
{
	assert((n >= 1)
	 && (n <= MaxCPICCases));
	return (n == 1
		? (((sqInt)cPIC)) + firstCPICCaseOffset
		: ((((sqInt)cPIC)) + firstCPICCaseOffset) + (((MaxCPICCases + 1) - n) * cPICCaseSize));
}


/*	Align methodZoneBase to that for the start of a method. */

	/* Cogit>>#alignMethodZoneBase */
static void
alignMethodZoneBase(void)
{
    usqInt oldBase;

	oldBase = methodZoneBase;
	methodZoneBase = roundUpToMethodAlignment(backEnd(), methodZoneBase);
	stopsFromto(backEnd, oldBase, methodZoneBase - 1);
}

	/* Cogit>>#alignUptoRoutineBoundary: */
static NoDbgRegParms sqInt
alignUptoRoutineBoundary(sqInt anAddress)
{
	return (((anAddress + 7) | 7) - 7);
}


/*	Check that all methods have valid selectors, and that all linked sends are
	to valid targets and have valid cache tags
 */

	/* Cogit>>#allMachineCodeObjectReferencesValid */
static sqInt
allMachineCodeObjectReferencesValid(void)
{
    CogMethod *cogMethod;
    sqInt ok;

	ok = 1;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!(asserta(checkValidOopReference((cogMethod->selector))))) {
				ok = 0;
			}
			if (!(asserta((cogMethodDoesntLookKosher(cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if ((((cogMethod->cmType)) >= CMMethod)
		 || (((cogMethod->cmType)) == CMOpenPIC)) {
			if (!(asserta((mapForperformUntilarg(cogMethod, checkIfValidOopRefAndTargetpccogMethod, cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if ((((cogMethod->cmType)) >= CMMethod)) {
		}
		if (((cogMethod->cmType)) == CMClosedPIC) {
			if (!(asserta(noTargetsFreeInClosedPIC(cogMethod)))) {
				ok = 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#allMethodsHaveCorrectHeader */
static sqInt
allMethodsHaveCorrectHeader(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (!(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}

	/* Cogit>>#annotateAbsolutePCRef: */
static NoDbgRegParms AbstractInstruction *
annotateAbsolutePCRef(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = IsAbsPCReference);
	return abstractInstruction;
}

	/* Cogit>>#annotateBytecode: */
static NoDbgRegParms AbstractInstruction *
annotateBytecode(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = HasBytecodePC);
	return abstractInstruction;
}

	/* Cogit>>#annotate:objRef: */
static NoDbgRegParms AbstractInstruction *
annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop)
{
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(anOop))
	 && ((oopisGreaterThan(anOop, classTableRootObj()))
	 || (oopisLessThan(anOop, nilObject())))) {
		setHasMovableLiteral(1);
		if (isYoungObject(anOop)) {
			setHasYoungReferent(1);
		}
		(abstractInstruction->annotation = IsObjectReference);
	}
	return abstractInstruction;
}

	/* Cogit>>#assertSaneJumpTarget: */
static NoDbgRegParms void
assertSaneJumpTarget(AbstractInstruction *jumpTarget)
{
	assert((!closedPICSize)
	 || ((!openPICSize)
	 || ((addressIsInInstructions(jumpTarget))
	 || ((((((usqInt)jumpTarget)) >= codeBase) && ((((usqInt)jumpTarget)) <= ((((sqInt)(limitZony()))) + (((closedPICSize < openPICSize) ? openPICSize : closedPICSize)))))))));
}


/*	Answer an unused abstract register in the registerMask, or NoReg if none. */

	/* Cogit>>#availableRegisterOrNoneIn: */
static NoDbgRegParms sqInt
availableRegisterOrNoneIn(sqInt liveRegsMask)
{
    sqInt reg;

	if (liveRegsMask != 0) {
		for (reg = 0; reg <= 0x1F; reg += 1) {
			if (((liveRegsMask & (1ULL << reg)) != 0)) {
				return reg;
			}
		}
	}
	return NoReg;
}


/*	Evaluate binaryFunction with the block start mcpc and supplied arg for
	each entry in the block dispatch. If the function answers non-zero answer
	the value
	it answered. Used to update back-references to the home method in
	compaction.  */

	/* Cogit>>#blockDispatchTargetsFor:perform:arg: */
static NoDbgRegParms sqInt
blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg)
{
    sqInt blockEntry;
    usqInt end;
    sqInt pc;
    sqInt result;
    usqInt targetpc;

	if (((cogMethod->blockEntryOffset)) == 0) {
		return null;
	}
	blockEntry = ((cogMethod->blockEntryOffset)) + (((sqInt)cogMethod));
	pc = blockEntry;
	end = (mapEndFor(cogMethod)) - 1;
	while (pc < end) {
		if (isJumpAt(backEnd, pc)) {
			targetpc = jumpTargetPCAt(backEnd, pc);
			if (targetpc < blockEntry) {
				result = binaryFunction(targetpc, arg);
				if (result != 0) {
					return result;
				}
			}
		}
		pc += instructionSizeAt(backEnd, pc);
	}
	return 0;
}


/*	Answer the zero-relative bytecode pc matching the machine code pc argument
	in cogMethod, given the start of the bytecodes for cogMethod's block or
	method object. */

	/* Cogit>>#bytecodePCFor:startBcpc:in: */
sqInt
bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    usqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc1;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt targetPC;


	/* begin mapFor:bcpc:performUntil:arg: */
	latestContinuation = 0;
	assert(((cogMethod->stackCheckOffset)) > 0);
	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc1 = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = findIsBackwardBranchMcpcBcpcMatchingMcpc(null, 0 + (((sqInt)((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc1), startbcpc, ((void *)mcpc));
	if (result != 0) {
		return result;
	}
	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		isInBlock = (cogMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}
		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - (/* begin blockCreationBytecodeSizeForHeader: */
	(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
	? AltBlockCreationBytecodeSize
	: BlockCreationBytecodeSize));
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc1 += (mapByte & DisplacementMask);
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((				/* begin isBackwardBranch:at:exts:in: */
					assert(((descriptor->spanFunction))),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = findIsBackwardBranchMcpcBcpcMatchingMcpc(descriptor, (isBackwardBranch
					? (((sqInt)((usqInt)(annotation) << 1))) + 1
					: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc1), (isBackwardBranch
					? bcpc - (2 * nExts)
					: bcpc), ((void *)mcpc));
				if (result != 0) {
					return result;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc1 += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	return 0;
}

	/* Cogit>>#CallRT:registersToBeSavedMask: */
static NoDbgRegParms AbstractInstruction *
CallRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved)
{
    AbstractInstruction *abstractInstruction;
    sqInt callerSavedRegsToBeSaved;
    AbstractInstruction *lastInst;
    sqInt reg;
    sqInt registersToBePushed;

	callerSavedRegsToBeSaved = CallerSavedRegisterMask & registersToBeSaved;
	registersToBePushed = callerSavedRegsToBeSaved;
	reg = 0;
	while (registersToBePushed != 0) {
		if (((registersToBePushed & 1) != 0)) {
			/* begin PushR: */
			genoperand(PushR, reg);
		}
		reg += 1;
		registersToBePushed = (registersToBePushed) >> 1;
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, callTarget);
	(abstractInstruction->annotation = IsRelativeCall);
	lastInst = abstractInstruction;
	while (reg > 0) {
		reg -= 1;
		if (((callerSavedRegsToBeSaved & (1ULL << reg)) != 0)) {
			lastInst = genoperand(PopR, reg);
		}
	}
	return lastInst;
}

	/* Cogit>>#Call: */
static NoDbgRegParms AbstractInstruction *
gCall(sqInt callTarget)
{
	return genoperand(Call, callTarget);
}

	/* Cogit>>#CmpCq:R: */
static NoDbgRegParms AbstractInstruction *
gCmpCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(CmpCqR, quickConstant, reg);
}


/*	This is a static version of ceCallCogCodePopReceiverReg for break-pointing
	when debugging in C. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#callCogCodePopReceiver */
static void
callCogCodePopReceiver(void)
{
	realCECallCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	This is a static version of ceCallCogCodePopReceiverAndClassRegs for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#callCogCodePopReceiverAndClassRegs */
static void
callCogCodePopReceiverAndClassRegs(void)
{
	realCECallCogCodePopReceiverAndClassRegs();
}


/*	Code entry closed PIC miss. A send has fallen
	through a closed (finite) polymorphic inline cache.
	Either extend it or patch the send site to an open PIC.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */
/*	Marked <api> so the code generator won't delete it. */

	/* Cogit>>#ceCPICMiss:receiver: */
static NoDbgRegParms sqInt
ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver)
{
    sqInt cacheTag;
    sqInt errorSelectorOrNil;
    sqInt errsel;
    sqInt method;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    sqInt outerReturn;
    sqInt result;
    sqInt selector;

	errsel = 0;
	method = 0;
	if (isOopForwarded(receiver)) {
		return ceSendFromInLineCacheMiss(cPIC);
	}
	outerReturn = stackTop();
	assert(!(((inlineCacheTagAt(backEnd, outerReturn)) == (picAbortDiscriminatorValue()))));
	if (((cPIC->cPICNumCases)) < MaxCPICCases) {
		selector = (cPIC->selector);
		/* begin lookup:for:methodAndErrorSelectorInto: */
		methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorCannotInterpret;
				goto l1;
			}
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {
				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, selector);
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = null;
			goto l1;
		}
		if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
			methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
			if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
				assert(isOopCompiledMethod(methodOrSelectorIndex));
				if ((!(methodHasCogMethod(methodOrSelectorIndex)))
				 && (methodShouldBeCogged(methodOrSelectorIndex))) {
					/* We assume cog:selector: will *not* reclaim the method zone */
					cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
				}
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorDoesNotUnderstand;
				goto l1;
			}
			newTargetMethodOrNil = null;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = methodOrSelectorIndex;
	l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	}
	else {
		newTargetMethodOrNil = (errorSelectorOrNil = null);
	}
	assert(outerReturn == (stackTop()));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cacheTag = inlineCacheTagForInstance(receiver);
	if ((((cPIC->cPICNumCases)) >= MaxCPICCases)
	 || (	/* begin closedPICInappropriateForCacheTag:targetMethod:orErrorSelector: */
		((errorSelectorOrNil)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((!newTargetMethodOrNil)
	 || (isYoung(newTargetMethodOrNil))))) {
		result = patchToOpenPICFornumArgsreceiver((cPIC->selector), (cPIC->cmNumArgs), receiver);
		assert(!result);
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		return ceSendFromInLineCacheMiss(cPIC);
	}
	cogExtendPICCaseNMethodtagisMNUCase(cPIC, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	executeCogPICfromLinkedSendWithReceiverandCacheTag(cPIC, receiver, literal32BeforeFollowingAddress(backEnd, ((usqInt)(outerReturn - 5))));
	return null;
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceFree: */
static NoDbgRegParms void
ceFree(void *pointer)
{
	free(pointer);
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceMalloc: */
static NoDbgRegParms void*
ceMalloc(size_t size)
{
	return malloc(size);
}


/*	An in-line cache check in a method has failed. The failing entry check has
	jumped to the ceMethodAbort abort call at the start of the method which
	has called this routine.
	If possible allocate a closed PIC for the current and existing classes.
	The stack looks like:
	receiver
	args
	sender return address
	sp=>	ceMethodAbort call return address
	So we can find the method that did the failing entry check at
	ceMethodAbort call return address - missOffset
	and we can find the send site from the outer return address.
	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it.  */

	/* Cogit>>#ceSICMiss: */
static NoDbgRegParms sqInt
ceSICMiss(sqInt receiver)
{
    sqInt cacheTag;
    sqInt entryPoint;
    sqInt errorSelectorOrNil;
    sqInt errsel;
    sqInt extent;
    usqInt innerReturn;
    sqInt method;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    usqInt outerReturn;
    CogMethod *pic;
    sqInt result;
    sqInt selector;
    CogMethod *targetMethod;


	/* Whether we can relink to a PIC or not we need to pop off the inner return and identify the target method. */
	errsel = 0;
	method = 0;
	innerReturn = ((usqInt)(popStack()));
	targetMethod = ((CogMethod *) (innerReturn - missOffset));
	if (isOopForwarded(receiver)) {
		return ceSendFromInLineCacheMiss(targetMethod);
	}
	outerReturn = ((usqInt)(stackTop()));
	assert(((outerReturn >= methodZoneBase) && (outerReturn <= (freeStart()))));
	entryPoint = callTargetFromReturnAddress(backEnd, outerReturn);
	assert(((targetMethod->selector)) != (nilObject()));
	assert(((((sqInt)targetMethod)) + cmEntryOffset) == entryPoint);
	selector = (targetMethod->selector);
	/* begin lookup:for:methodAndErrorSelectorInto: */
	methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
	if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
		if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorCannotInterpret;
			goto l1;
		}
		if ((!(methodHasCogMethod(methodOrSelectorIndex)))
		 && (methodShouldBeCogged(methodOrSelectorIndex))) {
			/* We assume cog:selector: will *not* reclaim the method zone */
			cogselector(methodOrSelectorIndex, selector);
		}
		newTargetMethodOrNil = methodOrSelectorIndex;
		errorSelectorOrNil = null;
		goto l1;
	}
	if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
		methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			assert(isOopCompiledMethod(methodOrSelectorIndex));
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {
				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = SelectorDoesNotUnderstand;
		goto l1;
	}
	newTargetMethodOrNil = null;
	errorSelectorOrNil = methodOrSelectorIndex;
	l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	assert(outerReturn == (stackTop()));
	cacheTag = inlineCacheTagForInstance(receiver);
	if ((	/* begin closedPICInappropriateForCacheTag:targetMethod:orErrorSelector: */
		((errorSelectorOrNil)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((!newTargetMethodOrNil)
	 || (isYoung(newTargetMethodOrNil))))
	 || ((literal32BeforeFollowingAddress(backEnd, ((usqInt)(outerReturn - 5)))) == 0 /* begin picAbortDiscriminatorValue */)) {
		result = patchToOpenPICFornumArgsreceiver((targetMethod->selector), (targetMethod->cmNumArgs), receiver);
		assert(!result);
		return ceSendFromInLineCacheMiss(targetMethod);
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	pic = openPICWithSelector((targetMethod->selector));
	if ((!pic)) {
		/* otherwise attempt to create a closed PIC for the two cases. */
		pic = cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase((targetMethod->selector), (targetMethod->cmNumArgs), targetMethod, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);
		if ((((((sqInt)pic)) >= MaxNegativeErrorCode) && ((((sqInt)pic)) <= -1))) {
			/* For some reason the PIC couldn't be generated, most likely a lack of code memory.
			   Continue as if this is an unlinked send. */
			if ((((sqInt)pic)) == InsufficientCodeSpace) {
				callForCogCompiledCodeCompaction();
			}
			/* begin ensureExecutableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
#      endif

			return ceSendFromInLineCacheMiss(targetMethod);
		}
	}
	if (((pic->cmType)) == CMOpenPIC) {
		extent = rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), mframeHomeMethodExport()), (((sqInt)pic)) + cmEntryOffset);
	}
	else {
		extent = rewriteCallAttarget(backEnd, outerReturn, (((sqInt)pic)) + cmEntryOffset);
	}
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, (((usqInt)pic)) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	flushICacheFromto(backEnd, ((usqInt)pic), (((usqInt)pic)) + closedPICSize);
	flushICacheFromto(backEnd, (((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
	executeCogPICfromLinkedSendWithReceiverandCacheTag(pic, receiver, literal32BeforeFollowingAddress(backEnd, ((usqInt)(outerReturn - 5))));
	return null;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRefAndTarget:pc:cogMethod: */
static NoDbgRegParms sqInt
checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt cacheTag;
    unsigned int cacheTag1;
    sqInt entryPoint;
    sqInt entryPoint1;
    sqInt entryPt;
    sqInt literal;
    sqInt offset;
    sqInt offset1;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt tagCouldBeObj;
    sqInt tagCouldBeObject;
    sqInt targetMethod;
    CogMethod *targetMethod1;

	cacheTag = 0;
	entryPt = 0;
	tagCouldBeObject = 0;
	targetMethod = 0;
	if (annotation == IsObjectReference) {
		literal = literalBeforeFollowingAddress(backEnd, ((usqInt)mcpc));
		if (!(asserta(checkValidOopReference(literal)))) {
			return 1;
		}
		if ((couldBeObject(literal))
		 && (isReallyYoungObject(literal))) {
			if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
				return 2;
			}
		}
	}
	if (annotation >= IsSendCall) {
		if (!(asserta(isCMMethodEtAl(((CogBlockMethod *) (((CogMethod *) cogMethod))))))) {
			return 3;
		}
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTag1 = literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)));
		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj = 0;
		entryPoint = entryPoint1;
		if (tagCouldBeObj) {
			if (couldBeObject(cacheTag1)) {
				if (!(asserta(checkValidOopReference(cacheTag1)))) {
					return 4;
				}
			}
			else {
				if (!(asserta(validInlineCacheTag(cacheTag1)))) {
					return 5;
				}
			}
			if ((couldBeObject(cacheTag1))
			 && (isReallyYoungObject(cacheTag1))) {
				if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
					return 6;
				}
			}
		}
		else {
			if (entryPointTagIsSelector(entryPoint)) {
				if ((((int) cacheTag1)) < 0) {
					if ((-(((int) cacheTag1))) > NumSpecialSelectors) {
						return 7;
					}
				}
				else {
					if (cacheTag1 >= (literalCountOf((enumeratingCogMethod->methodObject)))) {
						return 8;
					}
				}
			}
			else {
				if (!(asserta(validInlineCacheTag(cacheTag1)))) {
					return 9;
				}
			}
		}
		if (entryPoint > methodZoneBase) {
			/* It's a linked send; find which kind. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (!(asserta((isCMMethodEtAl(((CogBlockMethod *) targetMethod1)))
				 || ((isCMClosedPIC(((CogBlockMethod *) targetMethod1)))
				 || (isCMOpenPIC(((CogBlockMethod *) targetMethod1))))))) {
				return 10;
			}
		}
	}
	return 0;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRef:pc:cogMethod: */
static NoDbgRegParms sqInt
checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt entryPoint;
    sqInt literal;
    sqInt off;
    sqInt offset;
    sqInt offset1;
    unsigned int selectorOrCacheTag;
    sqInt *sendTable;

	off = 0;
	if (annotation == IsObjectReference) {
		literal = literalBeforeFollowingAddress(backEnd, ((usqInt)mcpc));
		if (!(checkValidOopReference(literal))) {
			print("object ref leak in CM ");
			printHex(((sqInt)cogMethod));
			print(" @ ");
			printHex(((sqInt)mcpc));
			eekcr();
			return 1;
		}
	}
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {
			offset = entryPoint;
		}
		else {
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable = superSendTrampolines;
;
					}
				}
			}
			offset = offset1;
		}
		selectorOrCacheTag = literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)));
		if ((entryPoint > methodZoneBase)
		 && ((offset != cmNoCheckEntryOffset)
		 && (!((((((CogMethod *) (entryPoint - offset)))->cmType)) == CMOpenPIC)))) {
			/* linked non-super send, cacheTag is a cacheTag */
			if (!(validInlineCacheTag(selectorOrCacheTag))) {
				print("cache tag leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" @ ");
				printHex(((sqInt)mcpc));
				eekcr();
				return 1;
			}
		}
		else {
			/* unlinked send or super send; cacheTag is a selector unless 64-bit, in which case it is an index. */
		}
	}
	return 0;
}


/*	Answer if all references to objects in machine-code are valid. */

	/* Cogit>>#checkIntegrityOfObjectReferencesInCode: */
sqInt
checkIntegrityOfObjectReferencesInCode(sqInt gcModes)
{
    CogMethod *cogMethod;
    sqInt count;
    sqInt ok;

	cogMethod = ((CogMethod *) methodZoneBase);
	ok = 1;
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((cogMethod->cmRefersToYoung)) {
				if (((count = occurrencesInYoungReferrers(cogMethod))) != 1) {
					print("young referrer CM ");
					printHex(((sqInt)cogMethod));
					if (count == 0) {
						print(" is not in youngReferrers");
						eekcr();
					}
					else {
						print(" is in youngReferrers ");
						printNum(count);
						print(" times!");
						eekcr();
					}
					ok = 0;
				}
			}
			if (!(checkValidOopReference((cogMethod->selector)))) {
				print("object leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" selector");
				eekcr();
				ok = 0;
			}
			if (((cogMethod->cmType)) >= CMMethod) {
				assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
				if (!(checkValidObjectReference((cogMethod->methodObject)))) {
					print("object leak in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if (!(isOopCompiledMethod((cogMethod->methodObject)))) {
					print("non-method in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if ((mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) != 0) {
					ok = 0;
				}
				if (((isYoungObject((cogMethod->methodObject)))
				 || (isYoung((cogMethod->selector))))
				 && (!((cogMethod->cmRefersToYoung)))) {
					print("CM ");
					printHex(((sqInt)cogMethod));
					print(" refers to young but not marked as such");
					eekcr();
					ok = 0;
				}
			}
			else {
				if (((cogMethod->cmType)) == CMClosedPIC) {
					if (!(checkValidObjectReferencesInClosedPIC(cogMethod))) {
						ok = 0;
					}
				}
				else {
					if (((cogMethod->cmType)) == CMOpenPIC) {
						if ((mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) != 0) {
							ok = 0;
						}
					}
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#checkMaybeObjRefInClosedPIC: */
static NoDbgRegParms sqInt
checkMaybeObjRefInClosedPIC(sqInt maybeObject)
{
	if (maybeObject == 0) {
		return 1;
	}
	if (!(couldBeObject(maybeObject))) {
		return 1;
	}
	return checkValidObjectReference(maybeObject);
}

	/* Cogit>>#checkValidObjectReferencesInClosedPIC: */
static NoDbgRegParms sqInt
checkValidObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt ok;
    sqInt pc;

	ok = 1;
	/* first we check the obj ref at the beginning of the CPIC */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		print("object leak in CPIC ");
		printHex(((sqInt)cPIC));
		print(" @ ");
		printHex(pc - (jumpLongByteSize(backEnd)));
		cr();
		ok = 0;
	}
	/* For each case we check any object reference at the end address - sizeof(conditional instruction) and then increment the end address by case size */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			print("object leak in CPIC ");
			printHex(((sqInt)cPIC));
			print(" @ ");
			printHex(pc - (jumpLongConditionalByteSize(backEnd)));
			cr();
			ok = 0;
		}
		pc += cPICCaseSize;
	}
	return ok;
}


/*	i.e. this should never be called, so keep it out of the main path. */

	/* Cogit>>#cleanUpFailingCogCodeConstituents: */
static NoDbgRegParms NeverInline sqInt
cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg)
{
    CogMethod *cogMethod;

	cogMethod = cogMethodArg;
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMClosedPIC) {
			(cogMethod->methodObject = 0);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	popRemappableOop();
	return null;
}


/*	Answer if the ClosedPIC refers to any unmarked objects or freed/freeable
	target methods,
	applying markAndTraceOrFreeCogMethod:firstVisit: to those targets to
	determine if freed/freeable.
 */

	/* Cogit>>#closedPICRefersToUnmarkedObject: */
static NoDbgRegParms sqInt
closedPICRefersToUnmarkedObject(CogMethod *cPIC)
{
    sqInt i;
    sqInt object;
    sqInt pc;

	if (!((isImmediate((cPIC->selector)))
		 || (isMarked((cPIC->selector))))) {
		return 1;
	}
	pc = addressOfEndOfCaseinCPIC(1, cPIC);
	if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		if (!(isMarked(object))) {
			return 1;
		}
	}
	if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
		return 1;
	}
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			if (!(isMarked(object))) {
				return 1;
			}
		}
		if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
			return 1;
		}
	}
	return 0;
}

	/* Cogit>>#codeEntryFor: */
char *
codeEntryFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i + 1];
		}
	}
	return null;
}

	/* Cogit>>#codeEntryNameFor: */
char *
codeEntryNameFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i];
		}
	}
	return null;
}


/*	used e.g. in the platform's backtrace generators. Declared api to place it
	in cogit.h
 */

	/* Cogit>>#cogCodeBase */
sqInt
cogCodeBase(void)
{
	return codeBase;
}


/*	Answer the contents of the code zone as an array of pair-wise element,
	address in ascending address order.
	Answer a string for a runtime routine or abstract label (beginning, end,
	etc), a CompiledMethod for a CMMethod,
	or a selector (presumably a Symbol) for a PIC.
	If withDetails is true
	- answer machine-code to bytecode pc mapping information for methods
	- answer class, target pair information for closed PIC
	N.B. Since the class tag for the first case of a closed PIC is stored at
	the send site, it must be collected
	by scanning methods (see
	collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method:). Since closed PICs
	are never shared they always come after the method that references them,
	so we don't need an extra pass
	to collect the first case class tags, which are (temporarily) assigned to
	each closed PIC's methodObject field.
	But we do need to reset the methodObject fields to zero. This is done in
	createPICData:, unless memory
	runs out, in which case it is done by cleanUpFailingCogCodeConstituents:. */

	/* Cogit>>#cogCodeConstituents: */
sqInt
cogCodeConstituents(sqInt withDetails)
{
    CogMethod *cogMethod;
    sqInt constituents;
    sqInt count;
    sqInt i;
    sqInt label;
    sqInt profileData;
    sqInt value;


	/* + 3 for start, freeStart and end */
	count = (trampolineTableIndex / 2) + 3;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			count += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	constituents = instantiateClassindexableSize(classArray(), count * 2);
	if (!constituents) {
		return constituents;
	}
	pushRemappableOop(constituents);
	if ((!((label = stringForCString("CogCode"))))
	 || (!((value = 
/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(codeBase))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, constituents, label);
	storePointerUncheckedofObjectwithValue(1, constituents, value);
	for (i = 0; i < trampolineTableIndex; i += 2) {
		if ((!((label = stringForCString(trampolineAddresses[i]))))
		 || (!((value = 
/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)(trampolineAddresses[i + 1]))))))) {
			popRemappableOop();
			return null;
		}
		storePointerUncheckedofObjectwithValue(2 + i, constituents, label);
		storePointerUncheckedofObjectwithValue(3 + i, constituents, value);
	}
	count = trampolineTableIndex + 2;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			profileData = 
			/* begin profileDataFor:withDetails: */
(((cogMethod->cmType)) >= CMMethod
				? (cogMethod->methodObject)
				: (withDetails
					 && (((cogMethod->cmType)) == CMClosedPIC)
						? createCPICData(cogMethod)
						: (cogMethod->selector)));
			if (!profileData) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count, constituents, profileData);
			value = (withDetails
				? collectCogMethodConstituent(cogMethod)
				: 
					/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)cogMethod)));
			if (!value) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count + 1, constituents, value);
			count += 2;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if ((!((label = stringForCString("CCFree"))))
	 || (!((value = 
/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(mzFreeStart))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count, constituents, label);
	storePointerUncheckedofObjectwithValue(count + 1, constituents, value);
	if ((!((label = stringForCString("CCEnd"))))
	 || (!((value = 
/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(limitAddress))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count + 2, constituents, label);
	storePointerUncheckedofObjectwithValue(count + 3, constituents, value);
	constituents = popRemappableOop();
	beRootIfOld(constituents);
	return constituents;
}


/*	Extend the cPIC with the supplied case. If caseNMethod is cogged dispatch
	direct to
	its unchecked entry-point. If caseNMethod is not cogged, jump to the fast
	interpreter dispatch, and if isMNUCase then dispatch to fast MNU
	invocation and mark the cPIC as
	having the MNU case for cache flushing. */

	/* Cogit>>#cogExtendPIC:CaseNMethod:tag:isMNUCase: */
static NoDbgRegParms void
cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase)
{
    sqInt address;
    sqInt operand;
    sqInt target;

	compilationBreakpointclassTagisMNUCase((cPIC->selector), caseNTag, isMNUCase);
	assert(!(inlineCacheTagIsYoung(caseNTag)));
	assert((caseNMethod)
	 && (!(isYoung(caseNMethod))));
	if ((!isMNUCase)
	 && (methodHasCogMethod(caseNMethod))) {
		/* this isn't an MNU and we have an already cogged method to jump to */
		operand = 0;
		target = (((sqInt)(cogMethodOf(caseNMethod)))) + cmNoCheckEntryOffset;
	}
	else {
		operand = caseNMethod;
		if (isMNUCase) {
			/* this is an MNU so tag the CPIC header and setup a jump to the MNUAbort */
			/* begin cpicHasMNUCase: */
			((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cpicHasMNUCaseOrCMIsFullBlock) = 1;
			target = (((sqInt)cPIC)) + (sizeof(CogMethod));
		}
		else {
			/* setup a jump to the interpretAborth so we can cog the target method */
			target = (((sqInt)cPIC)) + (picInterpretAbortOffset());
		}
	}
	address = addressOfEndOfCaseinCPIC(((cPIC->cPICNumCases)) + 1, cPIC);
	rewriteCPICCaseAttagobjReftarget(address, caseNTag, operand, target);
	/* begin rewriteCPIC:caseJumpTo: */
	rewriteCPICJumpAttarget(backEnd, (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - 11 /* begin moveCwRByteSize */, address - cPICCaseSize);
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = ((cPIC->cPICNumCases)) + 1);
	flushICacheFromto(backEnd, ((usqInt)cPIC), (((usqInt)cPIC)) + closedPICSize);
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, (((usqInt)cPIC)) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif
}


/*	Attempt to produce a machine code method for the bytecode method
	object aMethodObj. N.B. If there is no code memory available do *NOT*
	attempt to reclaim the method zone. Certain clients (e.g. ceSICMiss:)
	depend on the zone remaining constant across method generation. */

	/* Cogit>>#cogFullBlockMethod:numCopied: */
CogMethod *
cogFullBlockMethodnumCopied(sqInt aMethodObj, sqInt numCopied)
{
    CogMethod *cogMethod;

	assert(!((methodHasCogMethod(aMethodObj))));
	if (aMethodObj == breakMethod) {
		haltmsg("Compilation of breakMethod");
	}
	ensureNoForwardedLiteralsIn(aMethodObj);
	if (methodUsesAlternateBytecodeSet(aMethodObj)) {
		if ((numElementsIn(generatorTable)) <= 0x100) {
			return null;
		}
		bytecodeSetOffset = 0x100;
	}
	else {
		bytecodeSetOffset = 0;
	}
	assert(isFullBlockMethod(aMethodObj));
	methodObj = aMethodObj;
	methodHeader = methodHeaderOf(aMethodObj);
	/* lazy initialization */
	receiverTags = -1;
	cogMethod = compileCogFullBlockMethod(numCopied);
	if ((((((sqInt)cogMethod)) >= MaxNegativeErrorCode) && ((((sqInt)cogMethod)) <= -1))) {
		if ((((sqInt)cogMethod)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		return null;
	}
	return cogMethod;
}

	/* Cogit>>#cogitPostGCAction: */
void
cogitPostGCAction(sqInt gcMode)
{
	
	if (gcMode == GCModeBecome) {
		followForwardedLiteralsInOpenPICList();
	}
	assert(allMethodsHaveCorrectHeader());
	assert(((!(gcMode & (GCModeFull + GCModeNewSpace))))
	 || (kosherYoungReferrers()));
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Check that the header fields onf a non-free method are consistent with
	the type. Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#cogMethodDoesntLookKosher: */
static NoDbgRegParms sqInt
cogMethodDoesntLookKosher(CogMethod *cogMethod)
{
	if (((((cogMethod->blockSize)) & (BytesPerWord - 1)) != 0)
	 || ((((cogMethod->blockSize)) < (sizeof(CogMethod)))
	 || (((cogMethod->blockSize)) >= 0x8000))) {
		return 1;
	}
	if (((cogMethod->cmType)) == CMFree) {
		return 2;
	}
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!((((((cogMethod->methodHeader))) & 7) == 1))) {
			return 11;
		}
		if (!(couldBeObject((cogMethod->methodObject)))) {
			return 12;
		}
		if ((((cogMethod->stackCheckOffset)) > 0)
		 && (((cogMethod->stackCheckOffset)) < cmNoCheckEntryOffset)) {
			return 13;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (((cogMethod->blockSize)) != openPICSize) {
			return 21;
		}
		if (((cogMethod->methodHeader)) != 0) {
			return 22;
		}
		if (((cogMethod->objectHeader)) >= 0) {
			if (!((((cogMethod->methodObject)) == 0)
				 || (compactionInProgress
				 || (((cogMethod->methodObject)) == (((usqInt)(methodFor(((void *)((cogMethod->methodObject))))))))))) {
				return 23;
			}
		}
		if (((cogMethod->stackCheckOffset)) != 0) {
			return 24;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (((cogMethod->blockSize)) != closedPICSize) {
			return 0x1F;
		}
		if (!(((((cogMethod->cPICNumCases)) >= 1) && (((cogMethod->cPICNumCases)) <= MaxCPICCases)))) {
			return 32;
		}
		if (((cogMethod->methodHeader)) != 0) {
			return 33;
		}
		if (((cogMethod->methodObject)) != 0) {
			return 34;
		}
		return 0;
	}
	return 9;
}


/*	Attempt to create a one-case PIC for an MNU.
	The tag for the case is at the send site and so doesn't need to be
	generated. 
 */

	/* Cogit>>#cogMNUPICSelector:receiver:methodOperand:numArgs: */
CogMethod *
cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs)
{
    CogMethod *actualPIC;
    usqInt startAddress;
    CogMethod *writablePIC;

	if ((isYoung(selector))
	 || ((inlineCacheTagForInstance(rcvr)) == 0 /* begin picAbortDiscriminatorValue */)) {
		return 0;
	}
	compilationBreakpointclassTagisMNUCase(selector, fetchClassTagOf(rcvr), 1);
	assert(endCPICCase0);
	startAddress = allocate(closedPICSize);
	if (startAddress == 0) {
		callForCogCompiledCodeCompaction();
		return 0;
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);
	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = CMMaxUsageCount / 2);
	/* begin cpicHasMNUCase: */
	(writablePIC->cpicHasMNUCaseOrCMIsFullBlock) = 1;
	(writablePIC->cPICNumCases = 1);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 1);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureMNUCPICmethodOperandnumArgsdelta((actualPIC = ((CogMethod *) startAddress)), methodOperand, numArgs, startAddress - (((usqInt)cPICPrototype)));
	flushICacheFromto(backEnd, startAddress, startAddress + closedPICSize);
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, startAddress + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return actualPIC;
}


/*	Create an Open PIC. Temporarily create a direct call of
	ceSendFromOpenPIC:. Should become a probe of the first-level method lookup
	cache followed by a
	call of ceSendFromOpenPIC: if the probe fails. */

	/* Cogit>>#cogOpenPICSelector:numArgs: */
static NoDbgRegParms CogMethod *
cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs)
{
    sqInt codeSize;
    sqInt end;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;
    CogMethod *pic;
    usqInt startAddress;

	compilationBreakpointisMNUCase(selector, 0);
	startAddress = allocate(openPICSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	(methodLabel->address = startAddress);
	(methodLabel->dependent = null);
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	compileOpenPICnumArgs(selector, numArgs);
	computeMaximumSizes();
	concretizeAt(methodLabel, startAddress);
	codeSize = generateInstructionsAt(startAddress + (sizeof(CogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	mapSize = generateMapAtstart((startAddress + openPICSize) - 1, startAddress + cmNoCheckEntryOffset);
	assert((((entry->address)) - startAddress) == cmEntryOffset);
	assert(((roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpLength(mapSize))) <= openPICSize);
	end = outputInstructionsAt(startAddress + (sizeof(CogMethod)));
	pic = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	/* begin fillInOPICHeader:numArgs:selector: */
	(pic->cmType = CMOpenPIC);
	(pic->objectHeader = 0);
	(pic->blockSize = openPICSize);
	addToOpenPICList(pic);
	(pic->methodHeader = 0);
	(pic->selector = selector);
	(pic->cmNumArgs = numArgs);
	(pic->cmHasMovableLiteral = isNonImmediate(selector));
	if ((pic->cmRefersToYoung = isYoung(selector))) {
		addToYoungReferrers(pic);
	}
	(pic->cmUsageCount = initialOpenPICUsageCount());
	/* begin cpicHasMNUCase: */
	(pic->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(pic->cPICNumCases = 0);
	(pic->blockEntryOffset = 0);
	flushICacheFromto(backEnd, (((usqInt)pic)) - codeToDataDelta, ((((usqInt)pic)) - codeToDataDelta) + openPICSize);
	assert(isCMOpenPIC(((CogBlockMethod *) pic)));
	assert(((pic->selector)) == selector);
	assert(((pic->cmNumArgs)) == numArgs);
	assert((callTargetFromReturnAddress(backEnd, ((((sqInt)pic)) - codeToDataDelta) + missOffset)) == (picAbortTrampolineFor(numArgs)));
	assert(openPICSize == (roundUpLength(openPICSize)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, ((((usqInt)pic)) - codeToDataDelta) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return ((CogMethod *) startAddress);
}


/*	Attempt to create a two-case PIC for case0CogMethod and
	case1Method,case1Tag. The tag for case0CogMethod is at the send site and
	so doesn't need to be generated.
	case1Method may be any of
	- a Cog method; link to its unchecked entry-point
	- a CompiledMethod; link to ceInterpretMethodFromPIC:
	- a CompiledMethod; link to ceMNUFromPICMNUMethod:receiver: */

	/* Cogit>>#cogPICSelector:numArgs:Case0Method:Case1Method:tag:isMNUCase: */
static NoDbgRegParms CogMethod *
cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase)
{
    CogMethod *actualPIC;
    usqInt startAddress;
    CogMethod *writablePIC;

	if (isYoung(selector)) {
		return ((CogMethod *) YoungSelectorInPIC);
	}
	compilationBreakpointclassTagisMNUCase(selector, case1Tag, isMNUCase);
	startAddress = allocate(closedPICSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);
	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);
	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = CMMaxUsageCount / 2);
	/* begin cpicHasMNUCase: */
	(writablePIC->cpicHasMNUCaseOrCMIsFullBlock) = isMNUCase;
	(writablePIC->cPICNumCases = 2);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 2);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta((actualPIC = ((CogMethod *) startAddress)), case0CogMethod, case1MethodOrNil, case1Tag, isMNUCase, numArgs, startAddress - (((usqInt)cPICPrototype)));
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));
	return actualPIC;
}


/*	Attempt to produce a machine code method for the bytecode method
	object aMethodObj. N.B. If there is no code memory available do *NOT*
	attempt to reclaim the method zone. Certain clients (e.g. ceSICMiss:)
	depend on the zone remaining constant across method generation. */

	/* Cogit>>#cog:selector: */
CogMethod *
cogselector(sqInt aMethodObj, sqInt aSelectorOop)
{
    CogMethod *cogMethod;
    sqInt selector;

	assert(!((methodHasCogMethod(aMethodObj))));
	/* coInterpreter stringOf: selector */
	selector = (aSelectorOop == (nilObject())
		? maybeSelectorOfMethod(aMethodObj)
		: aSelectorOop);
	if (!(selector == null)) {
		compilationBreakpointisMNUCase(selector, 0);
	}
	if (aMethodObj == breakMethod) {
		haltmsg("Compilation of breakMethod");
	}
	ensureNoForwardedLiteralsIn(aMethodObj);
	if (methodUsesAlternateBytecodeSet(aMethodObj)) {
		if ((numElementsIn(generatorTable)) <= 0x100) {
			return null;
		}
		bytecodeSetOffset = 0x100;
	}
	else {
		bytecodeSetOffset = 0;
	}
	assert(!((isFullBlockMethod(aMethodObj))));
	methodObj = aMethodObj;
	methodHeader = methodHeaderOf(aMethodObj);
	/* lazy initialization */
	receiverTags = -1;
	cogMethod = compileCogMethod(aSelectorOop);
	if ((((((sqInt)cogMethod)) >= MaxNegativeErrorCode) && ((((sqInt)cogMethod)) <= -1))) {
		if ((((sqInt)cogMethod)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		return null;
	}
	return cogMethod;
}

	/* Cogit>>#collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
static NoDbgRegParms sqInt
collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt address;
    sqInt annotation;
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt *sendTable1;
    sqInt sendTable2;
    CogMethod *targetMethod;
    CogMethod *targetMethod1;

	targetMethod = ((CogMethod *) 0);
	if (!descriptor) {
		return 0;
	}
	if (!((descriptor->isMapped))) {
		return 0;
	}
	address = 
	/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)mcpc));
	if (!address) {
		return PrimErrNoMemory;
	}
	storePointerUncheckedofObjectwithValue(cogConstituentIndex, topRemappableOop(), address);
	storePointerUncheckedofObjectwithValue(cogConstituentIndex + 1, topRemappableOop(), (((usqInt)bcpc << 3) | 1));
	/* Collect any first case classTags for closed PICs. */
	cogConstituentIndex += 2;
	if (((!(isBackwardBranchAndAnnotation & 1)))
	 && (	/* begin isSendAnnotation: */
		((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= IsSendCall))) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* send is linked */
			annotation = ((usqInt)(isBackwardBranchAndAnnotation)) >> 1;
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmType)) == CMClosedPIC) {
				(targetMethod1->methodObject = classForInlineCacheTag(literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)))));
			}
		}
	}
	return 0;
}


/*	Answer a description of the mapping between machine code pointers and
	bytecode pointers for the Cog Method.
	First value is the address of the cog method.
	Following values are pairs of machine code pc and bytecode pc */

	/* Cogit>>#collectCogMethodConstituent: */
static NoDbgRegParms sqInt
collectCogMethodConstituent(CogMethod *cogMethod)
{
    sqInt address;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    CogBlockMethod *cogBlockMethod;
    sqInt data;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    usqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt nSlots;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;

	if (!(((cogMethod->cmType)) >= CMMethod)) {
		return 
		/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)cogMethod));
	}
	cogBlockMethod = ((CogBlockMethod *) cogMethod);
	if (((cogBlockMethod->stackCheckOffset)) == 0) {
		/* isFrameless ? */
		return 
		/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)cogMethod));
	}
	/* +1 for first address */
	nSlots = ((((byteSizeOf((cogMethod->methodObject))) - (startPCOfMethodHeader((cogMethod->methodHeader)))) * 2) + (minSlotsForShortening())) + 1;
	data = instantiateClassindexableSize(splObj(ClassArray), nSlots);
	if (!data) {
		return null;
	}
	pushRemappableOop(data);
	address = 
	/* begin positiveMachineIntegerFor: */
positive64BitIntegerFor(((usqInt)cogMethod));
	if (!address) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, topRemappableOop(), address);
	cogConstituentIndex = 1;
	startbcpc = startPCOfMethod((cogMethod->methodObject));
	/* begin mapFor:bcpc:performUntil:arg: */
	latestContinuation = 0;
	assert(((cogBlockMethod->stackCheckOffset)) > 0);
	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset));
	result = collectCogConstituentForAnnotationMcpcBcpcMethod(null, 0 + (((sqInt)((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)cogMethod));
	if (result != 0) {
		errCode = result;
		goto l2;
	}
	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogBlockMethod->cmType)) >= CMMethod) {
		isInBlock = (cogBlockMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogBlockMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogBlockMethod->startpc)));
		homeMethod = cmHomeMethod(cogBlockMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogBlockMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}
		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - (/* begin blockCreationBytecodeSizeForHeader: */
	(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
	? AltBlockCreationBytecodeSize
	: BlockCreationBytecodeSize));
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask);
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l2;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l2;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((				/* begin isBackwardBranch:at:exts:in: */
					assert(((descriptor->spanFunction))),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = collectCogConstituentForAnnotationMcpcBcpcMethod(descriptor, (isBackwardBranch
					? (((sqInt)((usqInt)(annotation) << 1))) + 1
					: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
					? bcpc - (2 * nExts)
					: bcpc), ((void *)cogMethod));
				if (result != 0) {
					errCode = result;
					goto l2;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	errCode = 0;
	l2:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode != 0) {
		popRemappableOop();
		return null;
	}
	if (cogConstituentIndex < nSlots) {
		shortentoIndexableSize(topRemappableOop(), cogConstituentIndex);
	}
	return popRemappableOop();
}

	/* Cogit>>#compactCogCompiledCode */
void
compactCogCompiledCode(void)
{
	assertValidDualZone();
	assert(noCogMethodsMaximallyMarked());
	moveProfileToMethods();
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	markActiveMethodsAndReferents();
	freeOlderMethodsForCompaction();
	compactPICsWithFreedTargets();
	planCompaction();
	updateStackZoneReferencesToCompiledCodePreCompaction();
	relocateMethodsPreCompaction();
	assertValidDualZone();
	compactCompiledCode();
	stopsFromto(backEnd, freeStart(), (youngReferrers()) - 1);
	flushICacheFromto(backEnd, ((usqInt)methodZoneBase), ((usqInt)(youngReferrers())));
	assert(allMethodsHaveCorrectHeader());
	assert(kosherYoungReferrers());
	assertValidDualZone();
}

	/* Cogit>>#compactPICsWithFreedTargets */
static void
compactPICsWithFreedTargets(void)
{
    CogMethod *cogMethod;
    sqInt count;

	cogMethod = ((CogMethod *) methodZoneBase);
	count = 0;
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) == CMClosedPIC)
		 && (cPICCompactAndIsNowEmpty(cogMethod))) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMFree);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		count += 1;
	}
	assert(count == (numMethods()));
}


/*	The start of a CogMethod has a call to a run-time abort routine that
	either handles an in-line cache failure or a stack overflow. The routine
	selects the
	path depending on ReceiverResultReg; if zero it takes the stack overflow
	path; if nonzero the in-line cache miss path. Neither of these paths
	returns. The abort routine must be called; In the callee the method is
	located by
	adding the relevant offset to the return address of the call.
	
	N.B. This code must match that in compilePICAbort: so that the offset of
	the return address of the call is the same in methods and closed PICs. */

	/* Cogit>>#compileAbort */
static AbstractInstruction *
compileAbort(void)
{
	stackOverflowCall = genoperandoperand(MoveCqR, 0, ReceiverResultReg);
	return (sendMiss = gCall(methodAbortTrampolineFor(methodOrBlockNumArgs)));
}

	/* Cogit>>#compileBlockDispatchFrom:to: */
static NoDbgRegParms sqInt
compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex)
{
    BlockStart *blockStart;
    sqInt halfWay;
    AbstractInstruction *jmp;

	if (lowBlockStartIndex == highBlockStartIndex) {
		blockStart = blockStartAt(lowBlockStartIndex);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)((blockStart->entryLabel))));
		return null;
	}
	halfWay = (highBlockStartIndex + lowBlockStartIndex) / 2;
	assert(((halfWay >= lowBlockStartIndex) && (halfWay <= highBlockStartIndex)));
	/* N.B. FLAGS := TempReg - startpc */
	blockStart = blockStartAt(halfWay);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)(((blockStart->startpc)) + 1) << 3) | 1), TempReg);
	if (lowBlockStartIndex == halfWay) {
		/* begin JumpLessOrEqual: */
		genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)((blockStart->entryLabel))));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
		return null;
	}
	if ((halfWay + 1) == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);
		/* begin JumpGreater: */
		genConditionalBranchoperand(JumpGreater, ((sqInt)((blockStart->entryLabel))));
		return compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	}
	jmp = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	if (halfWay == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);
		jmpTarget(jmp, (blockStart->entryLabel));
	}
	else {
		jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
	}
	return 0;
}


/*	Compile a block's entry. This looks like a dummy CogBlockMethod header
	(for frame parsing)
	followed by either a frame build, if a frame is required, or nothing. The
	CogMethodHeader's objectHeader field is a back pointer to the method, but
	this can't be filled in until code generation. */

	/* Cogit>>#compileBlockEntry: */
static NoDbgRegParms void
compileBlockEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    sqInt alignment;

	alignment = blockAlignment();
	/* begin AlignmentNops: */
	genoperand(AlignmentNops, alignment);
	(blockStart->fakeHeader = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	switch (sizeof(CogBlockMethod)) {
	case 8:
		/* begin Fill32: */
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	case 12:
		/* begin Fill32: */
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	case 16:
		/* begin Fill32: */
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	(blockStart->entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (needsFrame) {
		compileBlockFrameBuild(blockStart);
		if (recordBlockTrace()) {
			/* begin CallRT: */
			abstractInstruction = genoperand(Call, ceTraceBlockActivationTrampoline);
			(abstractInstruction->annotation = IsRelativeCall);
		}
	}
	else {
		compileBlockFramelessEntry(blockStart);
	}
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. The encoding for constants is defined by
	trampolineArgConstant: & trampolineArgValue:. Pass a constant as the
	result of trampolineArgConstant:. */

	/* Cogit>>#compileCallFor:numArgs:arg:arg:arg:arg:floatResultReg:regsToSave: */
static NoDbgRegParms void
compileCallFornumArgsargargargargfloatResultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask)
{
    sqInt delta;
    sqInt numRegsPushed;
    usqInt regMaskCopy;
    sqInt regsToSave;
    sqInt wordsPushedModAlignment;

	regsToSave = (resultRegOrNone == NoReg
		? regMask
		: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
		regMaskCopy = ((usqInt)regsToSave);
		numRegsPushed = 0;
		while (regMaskCopy != 0) {
			numRegsPushed += regMaskCopy & 1;
			regMaskCopy = ((regMaskCopy) >> 1);
		}
		if ((numRegsPushed == 0)
		 && ((numIntRegArgs(((AbstractInstruction *) backEnd))) >= numArgs)) {
			goto l1;
		}
		wordsPushedModAlignment = (numRegsPushed + ((((numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd)))) < 0) ? 0 : (numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd))))))) % (cStackAlignment / BytesPerWord);
		if (wordsPushedModAlignment != 0) {
			delta = (cStackAlignment / BytesPerWord) - wordsPushedModAlignment;
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, delta * BytesPerWord, SPReg);
		}
	l1:	/* end genAlignCStackSavingRegisters:numArgs:wordAlignment: */;
	}
	genSaveRegs(backEnd, regsToSave);
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(numArgs <= 4);
	if (numArgs > 0) {
		if (regOrConst0 < NoReg) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		}
		else {
			if (regOrConst0 != CArg0Reg) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
			}
		}
		if (numArgs > 1) {
			if (regOrConst1 < NoReg) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(MoveCqR, -2 - regOrConst1, CArg1Reg);
			}
			else {
				if (regOrConst1 != CArg1Reg) {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, regOrConst1, CArg1Reg);
				}
			}
			if (numArgs > 2) {
				if (regOrConst2 < NoReg) {
					/* begin checkQuickConstant:forInstruction: */
					genoperandoperand(MoveCqR, -2 - regOrConst2, CArg2Reg);
				}
				else {
					if (regOrConst2 != CArg2Reg) {
						/* begin MoveR:R: */
						genoperandoperand(MoveRR, regOrConst2, CArg2Reg);
					}
				}
				if (numArgs > 3) {
					if (regOrConst3 < NoReg) {
						/* begin checkQuickConstant:forInstruction: */
						genoperandoperand(MoveCqR, -2 - regOrConst3, CArg3Reg);
					}
					else {
						if (regOrConst3 != CArg3Reg) {
							/* begin MoveR:R: */
							genoperandoperand(MoveRR, regOrConst3, CArg3Reg);
						}
					}
				}
			}
		}
	}
	/* begin checkLiteral:forInstruction: */
	genoperand(CallFull, ((usqInt)aRoutine));
	if (resultRegOrNone != NoReg) {
		cFloatResultToRd(backEnd, resultRegOrNone);
	}
	/* begin genRemoveNArgsFromStack: */
	assert(numArgs <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	genRestoreRegs(backEnd, regsToSave);
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. The encoding for constants is defined by
	trampolineArgConstant: & trampolineArgValue:. Pass a constant as the
	result of trampolineArgConstant:. */

	/* Cogit>>#compileCallFor:numArgs:arg:arg:arg:arg:resultReg:regsToSave: */
static NoDbgRegParms void
compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask)
{
    sqInt delta;
    sqInt numRegsPushed;
    usqInt regMaskCopy;
    sqInt regsToSave;
    sqInt wordsPushedModAlignment;

	regsToSave = (resultRegOrNone == NoReg
		? regMask
		: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
		regMaskCopy = ((usqInt)regsToSave);
		numRegsPushed = 0;
		while (regMaskCopy != 0) {
			numRegsPushed += regMaskCopy & 1;
			regMaskCopy = ((regMaskCopy) >> 1);
		}
		if ((numRegsPushed == 0)
		 && ((numIntRegArgs(((AbstractInstruction *) backEnd))) >= numArgs)) {
			goto l1;
		}
		wordsPushedModAlignment = (numRegsPushed + ((((numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd)))) < 0) ? 0 : (numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd))))))) % (cStackAlignment / BytesPerWord);
		if (wordsPushedModAlignment != 0) {
			delta = (cStackAlignment / BytesPerWord) - wordsPushedModAlignment;
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, delta * BytesPerWord, SPReg);
		}
	l1:	/* end genAlignCStackSavingRegisters:numArgs:wordAlignment: */;
	}
	genSaveRegs(backEnd, regsToSave);
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(numArgs <= 4);
	if (numArgs > 0) {
		if (regOrConst0 < NoReg) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		}
		else {
			if (regOrConst0 != CArg0Reg) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
			}
		}
		if (numArgs > 1) {
			if (regOrConst1 < NoReg) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(MoveCqR, -2 - regOrConst1, CArg1Reg);
			}
			else {
				if (regOrConst1 != CArg1Reg) {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, regOrConst1, CArg1Reg);
				}
			}
			if (numArgs > 2) {
				if (regOrConst2 < NoReg) {
					/* begin checkQuickConstant:forInstruction: */
					genoperandoperand(MoveCqR, -2 - regOrConst2, CArg2Reg);
				}
				else {
					if (regOrConst2 != CArg2Reg) {
						/* begin MoveR:R: */
						genoperandoperand(MoveRR, regOrConst2, CArg2Reg);
					}
				}
				if (numArgs > 3) {
					if (regOrConst3 < NoReg) {
						/* begin checkQuickConstant:forInstruction: */
						genoperandoperand(MoveCqR, -2 - regOrConst3, CArg3Reg);
					}
					else {
						if (regOrConst3 != CArg3Reg) {
							/* begin MoveR:R: */
							genoperandoperand(MoveRR, regOrConst3, CArg3Reg);
						}
					}
				}
			}
		}
	}
	/* begin checkLiteral:forInstruction: */
	genoperand(CallFull, ((usqInt)aRoutine));
	genWriteCResultIntoReg(backEnd, resultRegOrNone);
	/* begin genRemoveNArgsFromStack: */
	assert(numArgs <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	genRestoreRegs(backEnd, regsToSave);
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. The encoding for constants is defined by
	trampolineArgConstant: & trampolineArgValue:. Pass a constant as the
	result of trampolineArgConstant:. */

	/* Cogit>>#compileCallFor:numArgs:arg:arg:arg:arg:resultReg:resultReg:regsToSave: */
static NoDbgRegParms void
compileCallFornumArgsargargargargresultRegresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt resultReg2OrNone, sqInt regMask)
{
    sqInt delta;
    sqInt numRegsPushed;
    usqInt regMaskCopy;
    sqInt regsToSave;
    sqInt wordsPushedModAlignment;

	regsToSave = (resultRegOrNone == NoReg
		? regMask
		: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
		regMaskCopy = ((usqInt)regsToSave);
		numRegsPushed = 0;
		while (regMaskCopy != 0) {
			numRegsPushed += regMaskCopy & 1;
			regMaskCopy = ((regMaskCopy) >> 1);
		}
		if ((numRegsPushed == 0)
		 && ((numIntRegArgs(((AbstractInstruction *) backEnd))) >= numArgs)) {
			goto l1;
		}
		wordsPushedModAlignment = (numRegsPushed + ((((numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd)))) < 0) ? 0 : (numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd))))))) % (cStackAlignment / BytesPerWord);
		if (wordsPushedModAlignment != 0) {
			delta = (cStackAlignment / BytesPerWord) - wordsPushedModAlignment;
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, delta * BytesPerWord, SPReg);
		}
	l1:	/* end genAlignCStackSavingRegisters:numArgs:wordAlignment: */;
	}
	genSaveRegs(backEnd, regsToSave);
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(numArgs <= 4);
	if (numArgs > 0) {
		if (regOrConst0 < NoReg) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		}
		else {
			if (regOrConst0 != CArg0Reg) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
			}
		}
		if (numArgs > 1) {
			if (regOrConst1 < NoReg) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(MoveCqR, -2 - regOrConst1, CArg1Reg);
			}
			else {
				if (regOrConst1 != CArg1Reg) {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, regOrConst1, CArg1Reg);
				}
			}
			if (numArgs > 2) {
				if (regOrConst2 < NoReg) {
					/* begin checkQuickConstant:forInstruction: */
					genoperandoperand(MoveCqR, -2 - regOrConst2, CArg2Reg);
				}
				else {
					if (regOrConst2 != CArg2Reg) {
						/* begin MoveR:R: */
						genoperandoperand(MoveRR, regOrConst2, CArg2Reg);
					}
				}
				if (numArgs > 3) {
					if (regOrConst3 < NoReg) {
						/* begin checkQuickConstant:forInstruction: */
						genoperandoperand(MoveCqR, -2 - regOrConst3, CArg3Reg);
					}
					else {
						if (regOrConst3 != CArg3Reg) {
							/* begin MoveR:R: */
							genoperandoperand(MoveRR, regOrConst3, CArg3Reg);
						}
					}
				}
			}
		}
	}
	/* begin checkLiteral:forInstruction: */
	genoperand(CallFull, ((usqInt)aRoutine));
	genWriteCResultIntoReg(backEnd, resultRegOrNone);
	genWriteCSecondResultIntoReg(backEnd, resultReg2OrNone);
	/* begin genRemoveNArgsFromStack: */
	assert(numArgs <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	genRestoreRegs(backEnd, regsToSave);
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. */

	/* Cogit>>#compileCallFor:numArgs:floatArg:floatArg:floatArg:floatArg:resultReg:regsToSave: */
static NoDbgRegParms void
compileCallFornumArgsfloatArgfloatArgfloatArgfloatArgresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask)
{
    double constantFloat64;
    double constantFloat641;
    double constantFloat642;
    double constantFloat643;
    sqInt delta;
    AbstractInstruction *inst;
    AbstractInstruction *inst1;
    AbstractInstruction *inst11;
    AbstractInstruction *inst12;
    AbstractInstruction *inst13;
    AbstractInstruction *inst2;
    AbstractInstruction *inst3;
    AbstractInstruction *inst4;
    sqInt numRegsPushed;
    usqInt regMaskCopy;
    sqInt regsToSave;
    sqInt wordsPushedModAlignment;

	regsToSave = (resultRegOrNone == NoReg
		? regMask
		: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
		regMaskCopy = ((usqInt)regsToSave);
		numRegsPushed = 0;
		while (regMaskCopy != 0) {
			numRegsPushed += regMaskCopy & 1;
			regMaskCopy = ((regMaskCopy) >> 1);
		}
		if ((numRegsPushed == 0)
		 && ((numIntRegArgs(((AbstractInstruction *) backEnd))) >= numArgs)) {
			goto l1;
		}
		wordsPushedModAlignment = (numRegsPushed + ((((numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd)))) < 0) ? 0 : (numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd))))))) % (cStackAlignment / BytesPerWord);
		if (wordsPushedModAlignment != 0) {
			delta = (cStackAlignment / BytesPerWord) - wordsPushedModAlignment;
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, delta * BytesPerWord, SPReg);
		}
	l1:	/* end genAlignCStackSavingRegisters:numArgs:wordAlignment: */;
	}
	genSaveRegs(backEnd, regsToSave);
	/* begin genMarshallNArgs:floatArg:floatArg:floatArg:floatArg: */
	genoperandoperand(SubCqR, 32, RSP);
	if (numArgs == 0) {
		goto l11;
	}
	if (regOrConst0 < NoReg) {
		/* begin MoveCf64:Rd: */
		constantFloat64 = -2 - regOrConst0;
		/* begin genMoveCf64:Rd: */
		inst = genoperand(PushCw, asIEEE64BitWord(constantFloat64));
		inst1 = genoperandoperandoperand(MoveM64rRd, 0, SPReg, XMM0L);
		genoperandoperand(AddCqR, 8, SPReg);
	}
	else {
		if (regOrConst0 != XMM0L) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, regOrConst0, XMM0L);
		}
	}
	if (numArgs == 1) {
		goto l11;
	}
	if (regOrConst1 < NoReg) {
		/* begin MoveCf64:Rd: */
		constantFloat641 = -2 - regOrConst1;
		/* begin genMoveCf64:Rd: */
		inst2 = genoperand(PushCw, asIEEE64BitWord(constantFloat641));
		inst11 = genoperandoperandoperand(MoveM64rRd, 0, SPReg, XMM1L);
		genoperandoperand(AddCqR, 8, SPReg);
	}
	else {
		if (regOrConst1 != XMM1L) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, regOrConst1, XMM1L);
		}
	}
	if (numArgs == 2) {
		goto l11;
	}
	if (regOrConst2 < NoReg) {
		/* begin MoveCf64:Rd: */
		constantFloat642 = -2 - regOrConst2;
		/* begin genMoveCf64:Rd: */
		inst3 = genoperand(PushCw, asIEEE64BitWord(constantFloat642));
		inst12 = genoperandoperandoperand(MoveM64rRd, 0, SPReg, XMM2L);
		genoperandoperand(AddCqR, 8, SPReg);
	}
	else {
		if (regOrConst2 != XMM2L) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, regOrConst2, XMM2L);
		}
	}
	if (numArgs == 3) {
		goto l11;
	}
	if (regOrConst3 < NoReg) {
		/* begin MoveCf64:Rd: */
		constantFloat643 = -2 - regOrConst3;
		/* begin genMoveCf64:Rd: */
		inst4 = genoperand(PushCw, asIEEE64BitWord(constantFloat643));
		inst13 = genoperandoperandoperand(MoveM64rRd, 0, SPReg, XMM3L);
		genoperandoperand(AddCqR, 8, SPReg);
	}
	else {
		if (regOrConst3 != XMM3L) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, regOrConst3, XMM3L);
		}
	}
	assert(numArgs <= 4);
	l11:	/* end genMarshallNArgs:floatArg:floatArg:floatArg:floatArg: */;
	/* begin checkLiteral:forInstruction: */
	genoperand(CallFull, ((usqInt)aRoutine));
	genWriteCResultIntoReg(backEnd, resultRegOrNone);
	/* begin genRemoveNFloatArgsFromStack: */
	assert(numArgs <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	genRestoreRegs(backEnd, regsToSave);
}


/*	Compile the cache tag computation and the first comparison. Answer the
	address of that comparison. */

	/* Cogit>>#compileCPICEntry */
static AbstractInstruction *
compileCPICEntry(void)
{
	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, TempReg);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}


/*	Compile the abstract instructions for the entire full block method. */

	/* Cogit>>#compileEntireFullBlockMethod: */
static NoDbgRegParms sqInt
compileEntireFullBlockMethod(sqInt numCopied)
{
    sqInt result;

	/* begin preenMethodLabel */
	((methodLabel->operands))[1] = 0;
	compileFullBlockEntry();
	compileFullBlockMethodFrameBuild(numCopied);
	if (((result = compileMethodBody())) < 0) {
		return result;
	}
	assert(blockCount == 0);
	return 0;
}


/*	The entry code to a method checks that the class of the current receiver
	matches that in the inline cache. Other non-obvious elements are that its
	alignment must be
	different from the alignment of the noCheckEntry so that the method map
	machinery can distinguish normal and super sends (super sends bind to the
	noCheckEntry).  */

	/* Cogit>>#compileEntry */
static void
compileEntry(void)
{
	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, TempReg);
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)sendMiss));
	noCheckEntry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (((traceFlags & 64) == 64)) {
		/* begin checkLiteral:forInstruction: */
		genoperand(CallFull, ceTraceLinkedSendTrampoline);
	}
}


/*	Compile the abstract instructions for the entire method, including blocks. */
/*	Abort for stack overflow on full block activation (no inline cache miss
	possible). The flag is SendNumArgsReg. */

	/* Cogit>>#compileFullBlockEntry */
static sqInt
compileFullBlockEntry(void)
{
    sqInt callTarget;
    AbstractInstruction *jumpNoContextSwitch;

	stackOverflowCall = genoperandoperand(MoveCqR, 0, ReceiverResultReg);
	callTarget = methodAbortTrampolineFor(methodOrBlockNumArgs);
	/* begin Call: */
	genoperand(Call, callTarget);
	fullBlockNoContextSwitchEntry = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	jumpNoContextSwitch = genoperand(Jump, ((sqInt)0));
	/* begin AlignmentNops: */
	genoperand(AlignmentNops, ((BytesPerWord < 8) ? 8 : BytesPerWord));
	fullBlockEntry = genoperandoperand(MoveRR, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jumpNoContextSwitch, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Compile the top-level method body. */

	/* Cogit>>#compileMethodBody */
static sqInt
compileMethodBody(void)
{
	if (endPC < initialPC) {
		return 0;
	}
	return compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
}


/*	The start of a PIC has a call to a run-time abort routine that either
	handles a dispatch to an
	interpreted method or a dispatch of an MNU case. The routine selects the
	path by testing
	ClassReg, which holds the inline cache tag; if equal to the
	picAbortDiscriminatorValue (zero)
	it takes the MNU path; if nonzero the dispatch to interpreter path.
	Neither of these paths
	returns. The abort routine must be called; In the callee the PIC is
	located by adding the
	relevant offset to the return address of the call.
	
	N.B. This code must match that in compileAbort so that the offset of the
	return address of
	the call is the same in methods and closed PICs. */

	/* Cogit>>#compilePICAbort: */
static NoDbgRegParms sqInt
compilePICAbort(sqInt numArgs)
{
    sqInt callTarget;

	picMNUAbort = genoperandoperand(MoveCqR, 0 /* begin picAbortDiscriminatorValue */, ClassReg);
	callTarget = picAbortTrampolineFor(numArgs);
	/* begin Call: */
	picInterpretAbort = genoperand(Call, callTarget);
	return 0;
}


/*	Compile the compare of stackLimit against the stack pointer, jumping to
	the stackOverflowCall if
	the stack pointer is below the limit. Answer a bytecode annotated label
	that follows the sequence.
	
	The stack check functions both as a genuine stack limit check to prevent
	calls overflowing stack pages,
	and as an event/context-switch break out. To cause an event check
	(including a check for a required
	context switch), stackLimit is set to the highest possible value, and
	hence all stack limit checks will
	fail. A path in the stack overflow abort then arranges to call event
	checking if it has been requested.
	
	Certain block activations (e.g. valueNoContextSwitch:) must not context
	switch, and in that
	case, SendNumArgs is set to zero to communicate to the stack overflow
	abort that it should
	not perform event/context-switch (yet). */

	/* Cogit>>#compileStackOverflowCheck: */
static NoDbgRegParms AbstractInstruction *
compileStackOverflowCheck(sqInt canContextSwitch)
{
    AbstractInstruction *jumpSkip;
    AbstractInstruction *label;


	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, stackLimitAddress(), TempReg);
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SPReg);
	if (canContextSwitch) {
		/* begin JumpBelow: */
		genConditionalBranchoperand(JumpBelow, ((sqInt)stackOverflowCall));
		label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	else {
		jumpSkip = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, SendNumArgsReg);
		genoperand(Jump, ((sqInt)stackOverflowCall));
		jmpTarget(jumpSkip, (label = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	}
	/* begin annotateBytecode: */
	(label->annotation = HasBytecodePC);
	return label;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:floatResultReg: */
static NoDbgRegParms void
compileTrampolineFornumArgsargargargargregsToSavepushLinkRegfloatResultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsargargargargfloatResultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg: */
static NoDbgRegParms void
compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsargargargargresultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg:resultReg: */
static NoDbgRegParms void
compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt resultReg2OrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsargargargargresultRegresultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, resultReg2OrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:floatArg:floatArg:floatArg:floatArg:regsToSave:pushLinkReg:resultReg: */
static NoDbgRegParms void
compileTrampolineFornumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsfloatArgfloatArgfloatArgfloatArgresultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeEntryOffsets */
static void
computeEntryOffsets(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    AbstractInstruction *sendMissCall;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 24;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	methodOrBlockNumArgs = 0;
	sendMissCall = compileAbort();
	compileEntry();
	computeMaximumSizes();
	generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	cmEntryOffset = ((entry->address)) - methodZoneBase;
	cmNoCheckEntryOffset = ((noCheckEntry->address)) - methodZoneBase;
	missOffset = (((sendMissCall->address)) + ((sendMissCall->machineCodeSize))) - methodZoneBase;
	entryPointMask = BytesPerWord - 1;
	while ((cmEntryOffset & entryPointMask) == (cmNoCheckEntryOffset & entryPointMask)) {
		entryPointMask = (entryPointMask + entryPointMask) + 1;
	}
	if (entryPointMask >= (roundUpToMethodAlignment(backEnd(), 1))) {
		error("cannot differentiate checked and unchecked entry-points with current cog method alignment");
	}
	checkedEntryAlignment = cmEntryOffset & entryPointMask;
	uncheckedEntryAlignment = cmNoCheckEntryOffset & entryPointMask;
	assert(checkedEntryAlignment != uncheckedEntryAlignment);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeFullBlockEntryOffsets */
static void
computeFullBlockEntryOffsets(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 24;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	methodOrBlockNumArgs = 0;
	compileFullBlockEntry();
	computeMaximumSizes();
	generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	cbEntryOffset = ((fullBlockEntry->address)) - methodZoneBase;
	cbNoSwitchEntryOffset = ((fullBlockNoContextSwitchEntry->address)) - methodZoneBase;
}


/*	While we order variables in the CoInterpreter in order of dynamic
	frequency, and hence
	expect that stackPointer will be output first, C optimizers and linkers
	may get their own
	ideas and ``improve upon'' this ordering. So we cannot depend on
	stackPointer being
	at the lowest address of the variables we want to access through
	VarBaseReg. Here we
	choose the minimum amongst a set to try to choose a varBaseAddress that is
	just less
	than but within range of all variables we want to access through it. */

	/* Cogit>>#computeGoodVarBaseAddress */
static usqInt
computeGoodVarBaseAddress(void)
{
    usqInt minAddress;


	/* stackLimit was e.g. lowest using the clang toolchain on MacOS X (prior to the use of variable_order) */
	minAddress = stackLimitAddress();
	if ((stackPointerAddress()) < minAddress) {
		minAddress = stackPointerAddress();
	}
	if ((framePointerAddress()) < minAddress) {
		minAddress = framePointerAddress();
	}
	if ((instructionPointerAddress()) < minAddress) {
		minAddress = instructionPointerAddress();
	}
	if ((argumentCountAddress()) < minAddress) {
		minAddress = argumentCountAddress();
	}
	if ((primFailCodeAddress()) < minAddress) {
		minAddress = primFailCodeAddress();
	}
	return minAddress;
}


/*	This pass assigns maximum sizes to all abstract instructions and
	eliminates jump fixups.
	It hence assigns the maximum address an instruction will occur at which
	allows the next
	pass to conservatively size jumps. */

	/* Cogit>>#computeMaximumSizes */
static void
computeMaximumSizes(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt relativeAddress;

	relativeAddress = 0;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		(abstractInstruction->address = relativeAddress);
		(abstractInstruction->maxSize = computeMaximumSize(abstractInstruction));
		relativeAddress += (abstractInstruction->maxSize);
	}
}


/*	Configure a copy of the prototype CPIC for a two-case PIC for 
	case0CogMethod and
	case1Method
	case1Tag.
	The tag for case0CogMethod is at the send site and so doesn't need to be
	generated. case1Method may be any of
	- a Cog method; jump to its unchecked entry-point
	- a CompiledMethod; jump to the ceInterpretFromPIC trampoline
	- nil; call ceMNUFromPIC
	addDelta is the address change from the prototype to the new CPIC
	location, needed
	because the loading of the CPIC label at the end may use a literal instead
	of a pc relative load. */
/*	self disassembleFrom: cPIC asInteger + (self sizeof: CogMethod) to: cPIC
	asInteger + closedPICSize
 */

	/* Cogit>>#configureCPIC:Case0:Case1Method:tag:isMNUCase:numArgs:delta: */
static NoDbgRegParms sqInt
configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta)
{
    sqInt caseEndAddress;
    sqInt jumpTargetAddr;
    sqInt operand;
    sqInt pc;
    sqInt targetEntry;

	assert(case1Method);
	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));
	assert(!(inlineCacheTagIsYoung(case1Tag)));
	if ((!isMNUCase)
	 && (methodHasCogMethod(case1Method))) {
		operand = 0;
		targetEntry = (((sqInt)(cogMethodOf(case1Method)))) + cmNoCheckEntryOffset;
	}
	else {
		/* We do not scavenge PICs, hence we cannot cache the MNU method if it is in new space. */
		operand = ((!case1Method)
		 || (isYoungObject(case1Method))
			? 0
			: case1Method);
		targetEntry = (case1Method == null
			? (((sqInt)cPIC)) + (sizeof(CogMethod))
			: (((sqInt)cPIC)) + (picInterpretAbortOffset()));
	}
	/* begin rewriteJumpLongAt:target: */
	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + firstCPICCaseOffset, (((sqInt)case0CogMethod)) + cmNoCheckEntryOffset);
	/* update the cpic case */
	caseEndAddress = addressOfEndOfCaseinCPIC(2, cPIC);
	rewriteCPICCaseAttagobjReftarget(caseEndAddress, case1Tag, operand, ((sqInt)((isMNUCase
	? (((sqInt)cPIC)) + (sizeof(CogMethod))
	: targetEntry))));
	pc = ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd));
	/* begin relocateMethodReferenceBeforeAddress:by: */
	assert((((byteAt(pc - 6)) == 141)
	 && (((byteAt(pc - 5)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7))))
	 || (((byteAt(pc - 8)) == 141)
	 && (((byteAt(pc - 7)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7)))));
	jumpTargetAddr = cPICMissTrampolineFor(numArgs);
	/* begin rewriteJumpLongAt:target: */
	rewriteCallAttarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + cPICEndOfCodeOffset, jumpTargetAddr);
	return 0;
}


/*	Configure a copy of the prototype CPIC for a one-case MNU CPIC that calls
	ceMNUFromPIC for
	case0Tag The tag for case0 is at the send site and so doesn't need to be
	generated. addDelta is the address change from the prototype to the new
	CPIC location, needed
	because the loading of the CPIC label at the end may be a literal instead
	of a pc-relative load. */
/*	adjust the jump at missOffset, the ceAbortXArgs */

	/* Cogit>>#configureMNUCPIC:methodOperand:numArgs:delta: */
static NoDbgRegParms sqInt
configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta)
{
    sqInt jumpTargetAddr;
    sqInt jumpTargetAddr1;
    sqInt operand;
    sqInt pc;
    sqInt target;

	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));
	/* set the jump to the case0 method */
	operand = ((!methodOperand)
	 || (isYoungObject(methodOperand))
		? 0
		: methodOperand);
	jumpTargetAddr = (((sqInt)cPIC)) + (sizeof(CogMethod));
	/* begin rewriteJumpLongAt:target: */
	rewriteCallAttarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + firstCPICCaseOffset, jumpTargetAddr);
	storeLiteralbeforeFollowingAddress(backEnd, operand, ((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd)));
	jumpTargetAddr1 = cPICMissTrampolineFor(numArgs);
	/* begin rewriteJumpLongAt:target: */
	rewriteCallAttarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + cPICEndOfCodeOffset, jumpTargetAddr1);
	pc = ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd));
	/* begin relocateMethodReferenceBeforeAddress:by: */
	assert((((byteAt(pc - 6)) == 141)
	 && (((byteAt(pc - 5)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7))))
	 || (((byteAt(pc - 8)) == 141)
	 && (((byteAt(pc - 7)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7)))));
	target = addressOfEndOfCaseinCPIC(2, cPIC);
	/* begin rewriteCPIC:caseJumpTo: */
	rewriteCPICJumpAttarget(backEnd, (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - 11 /* begin moveCwRByteSize */, target);
	return 0;
}


/*	Scan the CPIC for target methods that have been freed and eliminate them.
	Since the first entry cannot be eliminated, answer that the PIC should be
	freed if the first entry is to a free target. Answer if the PIC is now
	empty or should be freed. */

	/* Cogit>>#cPICCompactAndIsNowEmpty: */
static NoDbgRegParms sqInt
cPICCompactAndIsNowEmpty(CogMethod *cPIC)
{
    sqInt entryPoint;
    sqInt i;
    sqInt methods[MaxCPICCases];
    sqInt pc;
    int tags[MaxCPICCases];
    CogMethod *targetMethod;
    sqInt targets[MaxCPICCases];
    sqInt used;
    sqInt valid;

	used = 0;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));
		/* Collect all target triples except for triples whose entry-point is a freed method */
		valid = 1;
		if (!(			/* begin containsAddress: */
				((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				if (i == 1) {
					return 1;
				}
				valid = 0;
			}
		}
		if (valid) {
			tags[used] = ((i > 1
	? literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd)))
	: 0));
			targets[used] = entryPoint;
			methods[used] = (literalBeforeFollowingAddress(backEnd, pc - ((i == 1
	? jumpLongByteSize(backEnd)
	: (jumpLongConditionalByteSize(backEnd)) + (cmpC32RTempByteSize(backEnd))))));
			used += 1;
		}
	}
	if (used == ((cPIC->cPICNumCases))) {
		return 0;
	}
	if (used == 0) {
		return 1;
	}
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = used);
	if (used == 1) {
		pc = addressOfEndOfCaseinCPIC(2, cPIC);
		/* begin rewriteCPIC:caseJumpTo: */
		rewriteCPICJumpAttarget(backEnd, (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - 11 /* begin moveCwRByteSize */, pc);
		return 0;
	}
	for (i = 1; i < used; i += 1) {
		pc = addressOfEndOfCaseinCPIC(i + 1, cPIC);
		rewriteCPICCaseAttagobjReftarget(pc, tags[i], methods[i], targets[i]);
	}
	/* begin rewriteCPIC:caseJumpTo: */
	rewriteCPICJumpAttarget(backEnd, (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - 11 /* begin moveCwRByteSize */, pc - cPICCaseSize);
	return 0;
}


/*	The first case in a CPIC doesn't have a class reference so we need only
	step over actually usd subsequent cases.
 */

	/* Cogit>>#cPICHasForwardedClass: */
static NoDbgRegParms sqInt
cPICHasForwardedClass(CogMethod *cPIC)
{
    unsigned int classIndex;
    sqInt i;
    sqInt pc;


	/* start by finding the address of the topmost case, the cPICNumCases'th one */
	pc = (addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC)) - (jumpLongConditionalByteSize(backEnd));
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		classIndex = literal32BeforeFollowingAddress(backEnd, pc);
		if (isForwardedClassIndex(classIndex)) {
			return 1;
		}
		pc += cPICCaseSize;
	}
	return 0;
}


/*	scan the CPIC for target methods that have been freed. */

	/* Cogit>>#cPICHasFreedTargets: */
static NoDbgRegParms sqInt
cPICHasFreedTargets(CogMethod *cPIC)
{
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    CogMethod *targetMethod;

	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		/* Find target from jump.  Ignore jumps to the interpret and MNU calls within this PIC */
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));
		if (!(			/* begin containsAddress: */
				((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				return 1;
			}
		}
	}
	return 0;
}


/*	Whimsey; we want 16rCA5E10 + cPICPrototypeCaseOffset to be somewhere in
	the middle of the zone.
 */

	/* Cogit>>#cPICPrototypeCaseOffset */
static usqInt
cPICPrototypeCaseOffset(void)
{
	return ((methodZoneBase + (youngReferrers())) / 2) - 13262352;
}


/*	Are any of the jumps from this CPIC to targetMethod? */

	/* Cogit>>#cPIC:HasTarget: */
static NoDbgRegParms sqInt
cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod)
{
    sqInt i;
    sqInt pc;
    sqInt target;

	target = (((usqInt)targetMethod)) + cmNoCheckEntryOffset;
	/* Since this is a fast test doing simple compares we don't need to care that some
	   cases have nonsense addresses in there. Just zip on through. */
	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
		return 1;
	}
	for (i = 2; i <= MaxCPICCases; i += 1) {
		pc += cPICCaseSize;
		if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
			return 1;
		}
	}
	return 0;
}


/*	Answer an Array of the PIC's selector, followed by class and
	targetMethod/doesNotUnderstand: for each entry in the PIC.
 */

	/* Cogit>>#createCPICData: */
static NoDbgRegParms sqInt
createCPICData(CogMethod *cPIC)
{
    sqInt class;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt picData;
    sqInt target;
    CogMethod *targetMethod;

	assert((((cPIC->methodObject)) == 0)
	 || (addressCouldBeOop((cPIC->methodObject))));
	picData = instantiateClassindexableSize(classArray(), (((cPIC->cPICNumCases)) * 2) + 1);
	if (!picData) {
		return picData;
	}
	storePointerUncheckedofObjectwithValue(0, picData, (cPIC->selector));
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {
			/* first case may have been collected and stored here by collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
			class = (cPIC->methodObject);
			if (class == 0) {
				class = nilObject();
			}
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			class = classForInlineCacheTag(literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd))));
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		if (		/* begin containsAddress: */
			((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
			target = splObj(SelectorDoesNotUnderstand);
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			target = (targetMethod->methodObject);
		}
		storePointerUncheckedofObjectwithValue((i * 2) - 1, picData, class);
		storePointerUncheckedofObjectwithValue(i * 2, picData, target);
	}
	beRootIfOld(picData);
	(cPIC->methodObject = 0);
	return picData;
}


/*	Division is a little weird on some processors. Defer to the backEnd
	to allow it to generate any special code it may need to. */

	/* Cogit>>#DivR:R:Quo:Rem: */
static NoDbgRegParms AbstractInstruction *
gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder)
{
	genDivRRQuoRem(backEnd, rDivisor, rDividend, rQuotient, rRemainder);
	return abstractInstructionAt(opcodeIndex - 1);
}


/*	Return the default number of bytes to allocate for native code at startup.
	The actual value can be set via vmParameterAt: and/or a preference in the
	ini file. */

	/* Cogit>>#defaultCogCodeSize */
int
defaultCogCodeSize(void)
{
	return 1433600;
}


/*	Answer the number of bytecodes to skip to get to the first bytecode
	past the primitive call and any store of the error code. */

	/* Cogit>>#deltaToSkipPrimAndErrorStoreIn:header: */
static NoDbgRegParms sqInt
deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader)
{
	return (	/* begin methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(aMethodObj, aMethodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(aMethodHeader)) == (fetchByteofObject((startPCOfMethodHeader(aMethodHeader)) + (sizeOfCallPrimitiveBytecode(aMethodHeader)), aMethodObj)))
		? (sizeOfCallPrimitiveBytecode(aMethodHeader)) + (sizeOfLongStoreTempBytecode(aMethodHeader))
		: 0);
}

	/* Cogit>>#endPCOf: */
static NoDbgRegParms sqInt
endPCOf(sqInt aMethod)
{
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt end;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt pc;
    sqInt prim;
    sqInt targetPC;

	pc = (latestContinuation = startPCOfMethod(aMethod));
	if (((prim = primitiveIndexOf(aMethod))) > 0) {
		if (isQuickPrimitiveIndex(prim)) {
			return pc - 1;
		}
	}
	bsOffset = 
	/* begin bytecodeSetOffsetFor: */
(methodUsesAlternateBytecodeSet(aMethod)
		? 0x100
		: 0);
	nExts = 0;
	end = numBytesOf(aMethod);
	while (pc <= end) {
		byte = fetchByteofObject(pc, aMethod);
		descriptor = generatorAt(byte + bsOffset);
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			end = pc;
		}
		if ((isBranch(descriptor))
		 || ((descriptor->isBlockCreation))) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, aMethod);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
			if ((descriptor->isBlockCreation)) {
				pc += distance;
			}
		}
		else {
		}
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
		pc += (descriptor->numBytes);
	}
	return end;
}


/*	This is a static version of ceEnterCogCodePopReceiverReg for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#enterCogCodePopReceiver */
static void
enterCogCodePopReceiver(void)
{
	realCEEnterCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	Answer if the entryPoint's tag is expected to be a selector reference, as
	opposed to a class tag.
 */

	/* Cogit>>#entryPointTagIsSelector: */
static NoDbgRegParms sqInt
entryPointTagIsSelector(sqInt entryPoint)
{
	return (entryPoint < methodZoneBase)
	 || (((entryPoint & entryPointMask) == uncheckedEntryAlignment)
	 || (((entryPoint & entryPointMask) == checkedEntryAlignment)
	 && ((((((CogMethod *) (entryPoint - cmEntryOffset)))->cmType)) == CMOpenPIC)));
}


/*	Use asserts to check if the ClosedPICPrototype is as expected from
	compileClosedPICPrototype, and can be updated as required via
	rewriteCPICCaseAt:tag:objRef:target:. If all asserts pass, answer
	0, otherwise answer a bit mask identifying all the errors. */
/*	self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to:
	methodZoneBase + closedPICSize
 */

	/* Cogit>>#expectedClosedPICPrototype: */
static NoDbgRegParms sqInt
expectedClosedPICPrototype(CogMethod *cPIC)
{
    unsigned int classTag;
    sqInt classTagPC;
    sqInt entryPoint;
    sqInt errors;
    sqInt i;
    sqInt methodObjPC;
    sqInt object;
    sqInt pc;

	errors = 0;
	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((usqInt)cPIC)) + firstCPICCaseOffset;
	object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)));
	if (!(asserta(object == (firstPrototypeMethodOop())))) {
		errors = 1;
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
	if (!(asserta(entryPoint == ((cPICPrototypeCaseOffset()) + 13262352)))) {
		errors += 2;
	}
	for (i = 1; i < MaxCPICCases; i += 1) {
		/* verify information in case is as expected. */
		pc += cPICCaseSize;
		methodObjPC = (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == ((subsequentPrototypeMethodOop()) + i)))) {
			errors = errors | 4;
		}
		classTagPC = pc - (jumpLongConditionalByteSize(backEnd));
		classTag = literal32BeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == (0xBABE1F15U + i)))) {
			errors = errors | 8;
		}
		entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		if (!(asserta(entryPoint == (((cPICPrototypeCaseOffset()) + 13262352) + (i * 16))))) {
			errors = errors | 16;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == (((subsequentPrototypeMethodOop()) + i) ^ 0xA5A5A5A5U)))) {
			errors = errors | 32;
		}
		classTag = literal32BeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == ((0xBABE1F15U + i) ^ 0x5A5A5A5A)))) {
			errors = errors | 64;
		}
		entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		if (!(asserta(entryPoint == ((((cPICPrototypeCaseOffset()) + 13262352) + (i * 16)) ^ 0x55AA50)))) {
			errors = errors | 128;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, (((usqInt)cPIC)) + cPICEndOfCodeOffset);
	if (!(asserta(entryPoint == (cPICMissTrampolineFor(0))))) {
		errors += 0x100;
	}
	return errors;
}


/*	224		11100000	aaaaaaaa	Extend A (Ext A = Ext A prev * 256 + Ext A) */

	/* Cogit>>#extABytecode */
static sqInt
extABytecode(void)
{
	extA = ((((sqInt)((usqInt)(extA) << 8)))) + byte1;
	return 0;
}


/*	225		11100001	sbbbbbbb	Extend B (Ext B = Ext B prev * 256 + Ext B) */

	/* Cogit>>#extBBytecode */
static sqInt
extBBytecode(void)
{
	extB = ((numExtB == 0)
	 && (byte1 > 0x7F)
		? byte1 - 0x100
		: ((((sqInt)((usqInt)(extB) << 8)))) + byte1);
	numExtB += 1;
	return 0;
}


/*	Fill in the block headers now we know the exact layout of the code. */

	/* Cogit>>#fillInBlockHeadersAt: */
static NoDbgRegParms sqInt
fillInBlockHeadersAt(sqInt startAddress)
{
    sqInt aCogMethodOrInteger;
    CogBlockMethod *blockHeader;
    BlockStart *blockStart;
    sqInt i;

	if (!(needsFrame
		 && (blockCount > 0))) {
		return null;
	}
	if (blockNoContextSwitchOffset == null) {
		blockNoContextSwitchOffset = ((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address));
	}
	else {
		assert(blockNoContextSwitchOffset == (((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address))));
	}
	for (i = 0; i < blockCount; i += 1) {
		blockStart = blockStartAt(i);
		aCogMethodOrInteger = (((blockStart->fakeHeader))->address);
		/* begin writableBlockMethodFor: */
		blockHeader = ((CogBlockMethod *) ((((usqInt)aCogMethodOrInteger)) + codeToDataDelta));
		(blockHeader->homeOffset = ((((blockStart->fakeHeader))->address)) - startAddress);
		(blockHeader->startpc = (blockStart->startpc));
		(blockHeader->cmType = CMBlock);
		(blockHeader->cmNumArgs = (blockStart->numArgs));
		(blockHeader->cbUsesInstVars = (blockStart->hasInstVarRef));
		(blockHeader->stackCheckOffset = (((blockStart->stackCheckLabel)) == null
			? 0
			: ((((blockStart->stackCheckLabel))->address)) - ((((blockStart->fakeHeader))->address))));
	}
	return 0;
}


/*	Fill in the header for theCogMethod method. This may be located at the
	writable mapping. */

	/* Cogit>>#fillInMethodHeader:size:selector: */
static NoDbgRegParms void
fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector)
{
    sqInt actualMethodLocation;
    CogMethod *originalMethod;
    sqInt rawHeader;

	actualMethodLocation = (((usqInt)method)) - codeToDataDelta;
	(method->cmType = CMMethod);
	(method->objectHeader = nullHeaderForMachineCodeMethod());
	(method->blockSize = size);
	(method->methodObject = methodObj);
	/* If the method has already been cogged (e.g. Newspeak accessors) then
	   leave the original method attached to its cog method, but get the right header. */
	rawHeader = rawHeaderOf(methodObj);
	if (isCogMethodReference(rawHeader)) {
		originalMethod = ((CogMethod *) rawHeader);
		assert(((originalMethod->blockSize)) == size);
		assert(methodHeader == ((originalMethod->methodHeader)));
	}
	else {
		rawHeaderOfput(methodObj, actualMethodLocation);
	}
	(method->methodHeader = methodHeader);
	(method->selector = selector);
	(method->cmNumArgs = argumentCountOfMethodHeader(methodHeader));
	(method->cmHasMovableLiteral = hasMovableLiteral);
	if ((method->cmRefersToYoung = hasYoungReferent)) {
		addToYoungReferrers(method);
	}
	(method->cmUsageCount = initialMethodUsageCount());
	/* begin cpicHasMNUCase: */
	(method->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(method->cmUsesPenultimateLit = maxLitIndex >= ((literalCountOfMethodHeader(methodHeader)) - 2));
	(method->blockEntryOffset = (blockEntryLabel
		? ((blockEntryLabel->address)) - actualMethodLocation
		: 0));
	if (needsFrame) {
		if (!((((stackCheckLabel->address)) - actualMethodLocation) <= MaxStackCheckOffset)) {
			error("too much code for stack check offset");
		}
	}
	(method->stackCheckOffset = (needsFrame
		? ((stackCheckLabel->address)) - actualMethodLocation
		: 0));
	assert((callTargetFromReturnAddress(backEnd, actualMethodLocation + missOffset)) == (methodAbortTrampolineFor((method->cmNumArgs))));
	assert(size == (roundUpLength(size)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, actualMethodLocation + cmNoCheckEntryOffset, codeToDataDelta);
#  endif
}

	/* Cogit>>#findBackwardBranch:IsBackwardBranch:Mcpc:Bcpc:MatchingBcpc: */
static NoDbgRegParms sqInt
findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc)
{
	return ((((isBackwardBranchAndAnnotation & 1) != 0))
	 && ((((sqInt)targetBcpc)) == bcpc)
		? ((sqInt)mcpc)
		: 0);
}

	/* Cogit>>#findBlockMethodWithEntry:startBcpc: */
static NoDbgRegParms usqInt
findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((cogBlockMethod->startpc)) == startBcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}

	/* Cogit>>#findMapLocationForMcpc:inMethod: */
static NoDbgRegParms usqInt
findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;

	mcpc = 
	/* begin firstMappedPCFor: */
((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	if (mcpc == targetMcpc) {
		return map;
	}
	while (((mapByte = byteAt(map))) != MapEnd) {
		annotation = ((usqInt)(mapByte)) >> AnnotationShift;
		if (annotation != IsAnnotationExtension) {
			mcpc += 1 /* begin codeGranularity */ * ((annotation == IsDisplacementX2N
	? ((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))
	: mapByte & DisplacementMask));
		}
		if (mcpc >= targetMcpc) {
			assert(mcpc == targetMcpc);
			if (annotation == IsDisplacementX2N) {
				map -= 1;
				mapByte = byteAt(map);
				annotation = ((usqInt)(mapByte)) >> AnnotationShift;
				assert(annotation > IsAnnotationExtension);
			}
			return map;
		}
		map -= 1;
	}
	return 0;
}


/*	Find the CMMethod or CMBlock that has zero-relative startbcpc as its first
	bytecode pc.
	As this is for cannot resume processing and/or conversion to machine-code
	on backward
	branch, it doesn't have to be fast. Enumerate block returns and map to
	bytecode pcs. */

	/* Cogit>>#findMethodForStartBcpc:inHomeMethod: */
CogBlockMethod *
findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	if (startbcpc == (startPCOfMethodHeader((cogMethod->methodHeader)))) {
		return ((CogBlockMethod *) cogMethod);
	}
	assert(((cogMethod->blockEntryOffset)) != 0);
	return ((CogBlockMethod *) (blockDispatchTargetsForperformarg(cogMethod, findBlockMethodWithEntrystartBcpc, startbcpc)));
}


/*	Machine code addresses map to the following bytecode for all bytecodes
	except backward branches, where they map to the backward branch itself.
	This is so that loops continue, rather than terminate prematurely. */

	/* Cogit>>#find:IsBackwardBranch:Mcpc:Bcpc:MatchingMcpc: */
static NoDbgRegParms sqInt
findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc)
{
	return (targetMcpc == mcpc
		? ((!descriptor)
			 || (((isBackwardBranchAndAnnotation & 1) != 0))
				? bcpc
				: bcpc + ((descriptor->numBytes)))
		: 0);
}

	/* Cogit>>#firstMappedPCFor: */
static NoDbgRegParms sqInt
firstMappedPCFor(CogMethod *cogMethod)
{
	return ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
}


/*	Answer a fake value for the first method oop in the PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address. */

	/* Cogit>>#firstPrototypeMethodOop */
static sqInt
firstPrototypeMethodOop(void)
{
	return (	/* begin addressIsInCurrentCompilation: */
		((((usqInt)0x5EAF00D)) >= ((methodLabel->address)))
	 && ((((usqInt)0x5EAF00D)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))
		? 0xCA7F00D
		: 0x5EAF00D);
}

	/* Cogit>>#fixupAt: */
static NoDbgRegParms BytecodeFixup *
fixupAt(sqInt fixupPC)
{
	return fixupAtIndex(fixupPC - initialPC);
}

	/* Cogit>>#flagCogMethodForBecome: */
void
flagCogMethodForBecome(CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethodFlaggedForBecome);
}

	/* Cogit>>#followForwardedLiteralsImplementationIn: */
static NoDbgRegParms void
followForwardedLiteralsImplementationIn(CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableCogMethod;

	assert((!(isCMMethodEtAl(((CogBlockMethod *) cogMethod))))
	 || (!(isForwarded((cogMethod->methodObject)))));
	writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	hasYoungObj = isYoung((cogMethod->methodObject));
	if (shouldRemapOop((cogMethod->selector))) {
		(writableCogMethod->selector = remapObj((cogMethod->selector)));
		if (isYoung((cogMethod->selector))) {
			hasYoungObj = 1;
		}
	}
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask);
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), ((void *)hasYoungObjPtr));
			if (result != 0) {
				goto l1;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	l1:	/* end mapFor:performUntil:arg: */;
	if (hasYoungObj) {
		ensureInYoungReferrers(cogMethod);
	}
	else {
		(writableCogMethod->cmRefersToYoung = 0);
	}
}

	/* Cogit>>#followForwardedLiteralsIn: */
void
followForwardedLiteralsIn(CogMethod *cogMethod)
{
    usqInt wasInYoungReferrers;


	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	wasInYoungReferrers = (cogMethod->cmRefersToYoung);
	followForwardedLiteralsImplementationIn(cogMethod);
	if (wasInYoungReferrers
	 && (!((cogMethod->cmRefersToYoung)))) {
		pruneYoungReferrers();
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	To avoid runtime checks on literal variable and literal accesses in == and
	~~, 
	we follow literals in methods having movable literals in the postBecome
	action. To avoid scanning every method, we annotate cogMethods with the 
	cmHasMovableLiteral flag. */

	/* Cogit>>#followMovableLiteralsAndUpdateYoungReferrers */
void
followMovableLiteralsAndUpdateYoungReferrers(void)
{
    CogMethod *cogMethod;

	assert(kosherYoungReferrers());
	codeModified = 0;
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!((((cogMethod->cmType)) == CMFree)
			 || (((cogMethod->cmType)) == CMMethodFlaggedForBecome))) {
			if ((cogMethod->cmHasMovableLiteral)) {
				followForwardedLiteralsImplementationIn(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (codeModified) {
		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)codeBase), freeStart());
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	N.B. because becomeEffectFlags indicates whether jitted methods were
	becommed or not, if this method is called flagged methods exist, will be
	freed, and so on. So there is no need to check. Just do it. */

	/* Cogit>>#freeBecomeFlaggedMethods */
void
freeBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;


	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	unlinkSendsToFree();
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#freeCogMethod: */
void
freeCogMethod(CogMethod *cogMethod)
{
	moveProfileToMethods();
	freeMethod(cogMethod);
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Free machine-code methods whose compiled methods are unmarked
	and open PICs whose selectors are not marked, and closed PICs that
	refer to unmarked objects. */

	/* Cogit>>#freeUnmarkedMachineCode */
void
freeUnmarkedMachineCode(void)
{
    CogMethod *cogMethod;
    sqInt freedMethod;

	moveProfileToMethods();
	freedMethod = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (!(isMarked((cogMethod->methodObject))))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		if ((((cogMethod->cmType)) == CMOpenPIC)
		 && ((!(isImmediate((cogMethod->selector))))
		 && (!(isMarked((cogMethod->selector)))))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		if ((((cogMethod->cmType)) == CMClosedPIC)
		 && (closedPICRefersToUnmarkedObject(cogMethod))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedMethod) {
		unlinkSendsToFree();
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Call ceSendMustBeBooleanTo: via the relevant trampoline. */

	/* Cogit>>#genCallMustBeBooleanFor: */
static NoDbgRegParms AbstractInstruction *
genCallMustBeBooleanFor(sqInt boolean)
{
    AbstractInstruction *abstractInstruction;
    sqInt callTarget;

	callTarget = (boolean == (falseObject())
		? ceSendMustBeBooleanAddFalseTrampoline
		: ceSendMustBeBooleanAddTrueTrampoline);
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, callTarget);
	(abstractInstruction->annotation = IsRelativeCall);
	return abstractInstruction;
}

	/* Cogit>>#genConditionalBranch:operand: */
static NoDbgRegParms AbstractInstruction *
genConditionalBranchoperand(sqInt opcode, sqInt operandOne)
{
	return genoperand(opcode, operandOne);
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. The desired arguments and entry-point are pushed on a stackPage's
	stack. The enilopmart pops off the values to be loaded into registers and
	then executes a return instruction to pop off the entry-point and jump to
	it. 
	BEFORE				AFTER			(stacks grow down)
	whatever			stackPointer ->	whatever
	target address =>	reg1 = reg1val, etc
	reg1val				pc = target address
	reg2val
	stackPointer ->	reg3val */

	/* Cogit>>#genEnilopmartFor:and:and:forCall:called: */
static NoDbgRegParms void
(*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void)
{
    sqInt endAddress;
    usqInt enilopmart;
    sqInt size;

	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	genLoadStackPointers(backEnd);
	if (regArg3OrNone != NoReg) {
		/* begin PopR: */
		genoperand(PopR, regArg3OrNone);
	}
	if (regArg2OrNone != NoReg) {
		/* begin PopR: */
		genoperand(PopR, regArg2OrNone);
	}
	genoperand(PopR, regArg1);
	genEnilopmartReturn(forCall);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineName, enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. At the point the enilopmart enters machine code via a return
	instruction, any argument registers have been loaded with their values and
	the stack, if
	for call, looks like
	ret pc
	stackPointer ->	target address
	
	and if not for call, looks like
	whatever
	stackPointer ->	target address
	
	If forCall and running on a CISC, ret pc must be left on the stack. If
	forCall and
	running on a RISC, ret pc must be popped into LinkReg. In either case,
	target address must be removed from the stack and jumped/returned to. */

	/* Cogit>>#genEnilopmartReturn: */
static NoDbgRegParms void
genEnilopmartReturn(sqInt forCall)
{

	/* begin RetN: */
	genoperand(RetN, 0);
}


/*	Generate the routine that writes the current values of the C frame and
	stack pointers into
	variables. These are used to establish the C stack in trampolines back
	into the C run-time.
	This routine assumes the system's frame pointer is the same as that used
	in generated code. */

	/* Cogit>>#generateCaptureCStackPointers: */
static NoDbgRegParms NeverInline void
generateCaptureCStackPointers(sqInt captureFramePointer)
{
    sqInt callerSavedReg;
    sqInt fixupSize;
    sqInt offset;
    sqInt opcodeSize;
    sqInt pushedVarBaseReg;
    usqInt startAddress;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 32;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	/* Must happen first; value may be used in accessing any of the following addresses */
	startAddress = methodZoneBase;
	callerSavedReg = 0;
	pushedVarBaseReg = 0;
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {
		/* VarBaseReg is not caller-saved; must save and restore it, either by using an available caller-saved reg or push/pop. */
		/* TempReg used below */
		callerSavedReg = availableRegisterOrNoneIn(((ABICallerSavedRegisterMask | (1U << TempReg)) - (1U << TempReg)));
		if (callerSavedReg == NoReg) {
			gNativePushR(VarBaseReg);
			pushedVarBaseReg = 1;
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, VarBaseReg, callerSavedReg);
		}
	}
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	if (captureFramePointer) {
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveRAw, FPReg, cFramePointerAddress());
	}
		/* begin LoadEffectiveAddressMw:r:R: */
	offset = (pushedVarBaseReg
		? (leafCallStackPointerDelta(backEnd)) + BytesPerWord
		: leafCallStackPointerDelta(backEnd));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, NativeSPReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, cStackPointerAddress());
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {
		if (pushedVarBaseReg) {
			gNativePopR(VarBaseReg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, callerSavedReg, VarBaseReg);
		}
	}
	gNativeRetN(0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	flushICacheFromto(backEnd, ((usqInt)startAddress), ((usqInt)methodZoneBase));
	recordGeneratedRunTimeaddress("ceCaptureCStackPointers", startAddress);
	ceCaptureCStackPointers = ((void (*)(void)) startAddress);
}


/*	Generate the prototype ClosedPIC to determine how much space a full closed
	PIC takes.
	When we first allocate a closed PIC it only has one or two cases and we
	want to grow it.
	So we have to determine how big a full one is before hand. */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateClosedPICPrototype */
static void
generateClosedPICPrototype(void)
{
    CogMethod *cPIC;
    AbstractInstruction * cPICEndOfCodeLabel;
    sqInt endAddress;
    AbstractInstruction * endCPICCase1;
    sqInt fixupSize;
    sqInt h;
    AbstractInstruction *jumpNext;
    sqInt jumpTarget;
    sqInt jumpTarget1;
    sqInt jumpTarget2;
    sqInt numArgs;
    sqInt opcodeSize;
    sqInt wordConstant;
    sqInt wordConstant1;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = MaxCPICCases * 9;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);
	/* begin compileClosedPICPrototype */
	compilePICAbort((numArgs = 0));
	/* At the end of the entry code we need to jump to the first case code, which is actually the last chunk.
	   On each entension we must update this jump to move back one case. */
	jumpNext = compileCPICEntry();
	wordConstant = firstPrototypeMethodOop();
	/* begin uniqueLiteral:forInstruction: */
	genoperandoperand(MoveCwR, wordConstant, SendNumArgsReg);
	jumpTarget = (((methodZoneBase + (youngReferrers())) / 2) - 13262352) + 13262352;
	/* begin JumpLong: */
	genoperand(JumpLong, jumpTarget);
	endCPICCase0 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (h = 1; h < MaxCPICCases; h += 1) {
		if (h == (MaxCPICCases - 1)) {
			jmpTarget(jumpNext, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		/* begin MoveUniqueCw:R: */
		wordConstant1 = (subsequentPrototypeMethodOop()) + h;
		/* begin uniqueLiteral:forInstruction: */
		genoperandoperand(MoveCwR, wordConstant1, SendNumArgsReg);
		genoperandoperand(CmpC32R, 0xBABE1F15U + h, TempReg);
		jumpTarget1 = ((((methodZoneBase + (youngReferrers())) / 2) - 13262352) + 13262352) + (h * 16);
		/* begin JumpLongZero: */
		genConditionalBranchoperand(JumpLongZero, ((sqInt)jumpTarget1));
		if (h == 1) {
			endCPICCase1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
	}
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveCwR, (methodLabel->address), ClassReg);
	jumpTarget2 = cPICMissTrampolineFor(numArgs);
	/* begin JumpLong: */
	genoperand(JumpLong, jumpTarget2);
	cPICEndOfCodeLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	computeMaximumSizes();
	cPIC = ((CogMethod *) methodZoneBase);
	closedPICSize = (sizeof(CogMethod)) + (generateInstructionsAt(methodZoneBase + (sizeof(CogMethod))));
	endAddress = outputInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	assert((methodZoneBase + closedPICSize) == endAddress);
	firstCPICCaseOffset = ((endCPICCase0->address)) - methodZoneBase;
	cPICEndOfCodeOffset = ((cPICEndOfCodeLabel->address)) - methodZoneBase;
	cPICCaseSize = ((endCPICCase1->address)) - ((endCPICCase0->address));
	cPICEndSize = closedPICSize - (((MaxCPICCases - 1) * cPICCaseSize) + firstCPICCaseOffset);
	closedPICSize = roundUpToMethodAlignment(backEnd(), closedPICSize);
	assert(((picInterpretAbort->address)) == (((methodLabel->address)) + (picInterpretAbortOffset())));
	assert((expectedClosedPICPrototype(cPIC)) == 0);
	storeLiteralbeforeFollowingAddress(backEnd, 0, ((endCPICCase0->address)) - (jumpLongByteSize(backEnd)));
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	/* self cCode: ''
	   inSmalltalk:
	   [self disassembleFrom: cPIC + (self sizeof: CogMethod) to: cPIC + closedPICSize - 1.
	   self halt] */
	cPICPrototype = cPIC;
}


/*	We handle jump sizing simply. First we make a pass that asks each
	instruction to compute its maximum size. Then we make a pass that
	sizes jumps based on the maxmimum sizes. Then we make a pass
	that fixes up jumps. When fixing up a jump the jump is not allowed to
	choose a smaller offset but must stick to the size set in the second pass. */

	/* Cogit>>#generateCogFullBlock */
static CogMethod *
generateCogFullBlock(void)
{
    sqInt codeSize;
    usqIntptr_t headerSize;
    sqInt mapSize;
    CogMethod *method;
    sqInt result;
    usqInt startAddress;
    sqInt totalSize;

	headerSize = sizeof(CogMethod);
	(methodLabel->address = freeStart());
	computeMaximumSizes();
	concretizeAt(methodLabel, freeStart());
	codeSize = generateInstructionsAt(((methodLabel->address)) + headerSize);
	mapSize = generateMapAtstart(null, ((methodLabel->address)) + cbNoSwitchEntryOffset);
	totalSize = roundUpToMethodAlignment(backEnd(), (headerSize + codeSize) + mapSize);
	if (totalSize > MaxMethodSize) {
		return ((CogMethod *) MethodTooBig);
	}
	startAddress = allocate(totalSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	assert((startAddress + cbEntryOffset) == ((fullBlockEntry->address)));
	assert((startAddress + cbNoSwitchEntryOffset) == ((fullBlockNoContextSwitchEntry->address)));
	result = outputInstructionsAt(startAddress + headerSize);
	assert(((startAddress + headerSize) + codeSize) == result);
	padIfPossibleWithStopsFromto(backEnd, result, ((startAddress + totalSize) - mapSize) - 1);
	generateMapAtstart((startAddress + totalSize) - 1, startAddress + cbNoSwitchEntryOffset);
	flag("TOCHECK");
	method = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	fillInMethodHeadersizeselector(method, totalSize, nilObject());
	(method->cpicHasMNUCaseOrCMIsFullBlock = 1);
	flushICacheFromto(backEnd, startAddress, startAddress + totalSize);
	return ((CogMethod *) startAddress);
}


/*	We handle jump sizing simply. First we make a pass that asks each
	instruction to compute its maximum size. Then we make a pass that
	sizes jumps based on the maxmimum sizes. Then we make a pass
	that fixes up jumps. When fixing up a jump the jump is not allowed to
	choose a smaller offset but must stick to the size set in the second pass. */

	/* Cogit>>#generateCogMethod: */
static NoDbgRegParms CogMethod *
generateCogMethod(sqInt selector)
{
    sqInt codeSize;
    usqIntptr_t headerSize;
    sqInt mapSize;
    sqInt result;
    usqInt startAddress;
    sqInt totalSize;

	headerSize = sizeof(CogMethod);
	(methodLabel->address = freeStart());
	computeMaximumSizes();
	concretizeAt(methodLabel, freeStart());
	codeSize = generateInstructionsAt(((methodLabel->address)) + headerSize);
	mapSize = generateMapAtstart(null, ((methodLabel->address)) + cmNoCheckEntryOffset);
	totalSize = roundUpToMethodAlignment(backEnd(), (headerSize + codeSize) + mapSize);
	if (totalSize > MaxMethodSize) {
		return ((CogMethod *) MethodTooBig);
	}
	startAddress = allocate(totalSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	assert((startAddress + cmEntryOffset) == ((entry->address)));
	assert((startAddress + cmNoCheckEntryOffset) == ((noCheckEntry->address)));
	result = outputInstructionsAt(startAddress + headerSize);
	assert(((startAddress + headerSize) + codeSize) == result);
	padIfPossibleWithStopsFromto(backEnd, result, ((startAddress + totalSize) - mapSize) - 1);
	generateMapAtstart((startAddress + totalSize) - 1, startAddress + cmNoCheckEntryOffset);
	fillInBlockHeadersAt(startAddress);
	fillInMethodHeadersizeselector(((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta)), totalSize, selector);
	flushICacheFromto(backEnd, startAddress, startAddress + totalSize);
	return ((CogMethod *) startAddress);
}


/*	Generate the method map at addressrNull (or compute it if addressOrNull is
	null). Answer the length of the map in byes. Each entry in the map is in
	two parts. In the
	least signficant bits are a displacement of how far from the start or
	previous entry,
	unless it is an IsAnnotationExtension byte, in which case those bits are
	the extension.
	In the most signficant bits are the type of annotation at the point
	reached. A null
	byte ends the map. */

	/* Cogit>>#generateMapAt:start: */
static NoDbgRegParms sqInt
generateMapAtstart(usqInt addressOrNull, usqInt startAddress)
{
    unsigned char annotation;
    sqInt delta;
    sqInt i;
    AbstractInstruction *instruction;
    sqInt length;
    usqInt location;
    sqInt mapEntry;
    sqInt maxDelta;
    usqInt mcpc;

	length = 0;
	location = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		annotation = (instruction->annotation);
		if (!(annotation == null)) {
			mcpc = ((instruction->address)) + ((instruction->machineCodeSize));
			while (((delta = (mcpc - location) / 1 /* begin codeGranularity */)) > DisplacementMask) {
				maxDelta = (((((delta < MaxX2NDisplacement) ? delta : MaxX2NDisplacement)) | DisplacementMask) - DisplacementMask);
				assert((((usqInt)(maxDelta)) >> AnnotationShift) <= DisplacementMask);
				if (!(addressOrNull == null)) {
					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, (((usqInt)(maxDelta)) >> AnnotationShift) + DisplacementX2N);
				}
				location += maxDelta;
				length += 1;
			}
			if (!(addressOrNull == null)) {
				mapEntry = delta + (((sqInt)((usqInt)((((annotation < IsSendCall) ? annotation : IsSendCall))) << AnnotationShift)));
				/* begin addToMap:instruction:byte:at:for: */
				codeByteAtput(addressOrNull - length, mapEntry);
			}
			location += delta;
			length += 1;
			if (annotation > IsSendCall) {
				/* Add the necessary IsAnnotationExtension */
				if (!(addressOrNull == null)) {
					mapEntry = (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift))) + (annotation - IsSendCall);
					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, mapEntry);
				}
				length += 1;
			}
		}
	}
	if (!(addressOrNull == null)) {
		/* begin addToMap:instruction:byte:at:for: */
		codeByteAtput(addressOrNull - length, MapEnd);
	}
	return length + 1;
}


/*	Generate the prototype OpenPIC to determine how much space an open PIC
	takes. 
 */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateOpenPICPrototype */
static void
generateOpenPICPrototype(void)
{
    sqInt codeSize;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);
	compileOpenPICnumArgs(specialSelector(0), numRegArgs());
	computeMaximumSizes();
	concretizeAt(methodLabel, methodZoneBase);
	codeSize = generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	mapSize = generateMapAtstart(null, methodZoneBase + cmNoCheckEntryOffset);
	/* self cCode: ''
	   inSmalltalk:
	   [| end |
	   end := self outputInstructionsAt: methodZoneBase + headerSize.
	   self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to: end - 1.
	   self halt] */
	openPICSize = (roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpToMethodAlignment(backEnd(), mapSize));
}


/*	Generate the run-time entries at the base of the native code zone and
	update the base.
 */

	/* Cogit>>#generateRunTimeTrampolines */
static void
generateRunTimeTrampolines(void)
{
	ceSendMustBeBooleanAddFalseTrampoline = genMustBeBooleanTrampolineForcalled(falseObject(), "ceSendMustBeBooleanAddFalseTrampoline");
	ceSendMustBeBooleanAddTrueTrampoline = genMustBeBooleanTrampolineForcalled(trueObject(), "ceSendMustBeBooleanAddTrueTrampoline");
	/* begin genNonLocalReturnTrampoline */
	zeroOpcodeIndex();
	genoperand(PopR, TempReg);
	genoperandoperand(MoveRAw, TempReg, instructionPointerAddress());
	ceNonLocalReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNonLocalReturn, "ceNonLocalReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg, 1);
	/* begin genCheckForInterruptsTrampoline */
	zeroOpcodeIndex();
	genoperand(PopR, TempReg);
	genoperandoperand(MoveRAw, TempReg, instructionPointerAddress());
	ceCheckForInterruptTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCheckForInterrupt, "ceCheckForInterruptTrampoline", 0, null, null, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg, 1);
	ceFetchContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVar, "ceFetchContextInstVarTrampoline", 2, ReceiverResultReg, SendNumArgsReg, null, null, 0 /* begin emptyRegisterMask */, 1, SendNumArgsReg, 0);
	/* to keep ReceiverResultReg live. */
	ceStoreContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVarvalue, "ceStoreContextInstVarTrampoline", 3, ReceiverResultReg, SendNumArgsReg, ClassReg, null, 0 /* begin emptyRegisterMask */, 1, ReceiverResultReg, 0);
	/* ceInvokeInterpreter is an optimization and a work-around. Historically we used setjmp/longjmp to reenter the
	   interpreter at the current C stack base.  The C stack base is set at start-up and on each callback enter and
	   callback return. The interpreter must be invoked whenever a non-machine-code method must be run.  That might
	   be when invoking an interpreter method from one of the send linking routines (ceSend:...), or on continuing from
	   an evaluation primitive such as primitiveExecuteMethod.  The problem here is that such primitives could have
	   been invoked by the interpreter or by machine code.  So some form of non-local jump is required. But at least as
	   early as MSVC Community 2017, the Microshaft longjmp performs stack unwinding which gets hoplessly confused
	   (bless its little heart) by any stack switch between machine code and C stack, and raises a spurious
	   Stack cookie instrumentation code detected a stack-based buffer overrun
	   error from the bowels of gs_report.c _GSHandlerCheck.
	   Since the CoInterpreter maintains the base of the C stack in CFramePointer & CStackPointer, it is straight-forward
	   for us to simply call interpret after doing the switch to the C stack, avoiding the stack unwind issue altogether. */
	ceCannotResumeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCannotResume, "ceCannotResumeTrampoline", 0, null, null, null, null, 0 /* begin emptyRegisterMask */, 1, NoReg, 0);
	/* These two are unusual; they are reached by return instructions. */
	ceInvokeInterpret = genInvokeInterpretTrampoline();
	ceReturnToInterpreterTrampoline = genReturnToInterpreterTrampoline();
	ceBaseFrameReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceBaseFrameReturn, "ceBaseFrameReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg, 0);
	ceFFICalloutTrampoline = genFFICalloutTrampoline();
	ceMallocTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceMalloc, "ceMallocTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	ceFreeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceFree, "ceFreeTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, NoReg, 0);
}


/*	Generate a routine ceCaptureCStackPointers that will capture the C stack
	pointer, and, if it is in use, the C frame pointer. These are used in
	trampolines to call
	run-time routines in the interpreter from machine-code. */

	/* Cogit>>#generateStackPointerCapture */
static void
generateStackPointerCapture(void)
{
    usqInt oldMethodZoneBase;
    sqInt oldTrampolineTableIndex;


#  if defined(cFramePointerInUse)
	assertCStackWellAligned();
	generateCaptureCStackPointers(cFramePointerInUse);
#  else
	/* For the benefit of the following assert, assume the minimum at first. */
	cFramePointerInUse = 0;
	assertCStackWellAligned();
	oldMethodZoneBase = methodZoneBase;
	oldTrampolineTableIndex = trampolineTableIndex;
	generateCaptureCStackPointers(1);
	ceCaptureCStackPointers();
	if (!((cFramePointerInUse = checkIfCFramePointerInUse()))) {
		methodZoneBase = oldMethodZoneBase;
		trampolineTableIndex = oldTrampolineTableIndex;
		generateCaptureCStackPointers(0);
	}
#  endif // defined(cFramePointerInUse)

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	assertCStackWellAligned();
}


/*	Generate the run-time entries and exits at the base of the native code
	zone and update the base.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* Cogit>>#generateTrampolines */
static void
generateTrampolines(void)
{
    sqInt fixupSize;
    usqInt methodZoneStart;
    sqInt opcodeSize;

	methodZoneStart = methodZoneBase;
	(methodLabel->address = methodZoneStart);
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 80;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	setHasYoungReferent(0);
	maybeGenerateSelectorIndexDereferenceRoutine();
	generateSendTrampolines();
	generateMissAbortTrampolines();
	generateObjectRepresentationTrampolines();
	generateRunTimeTrampolines();
	generateEnilopmarts();
	generateTracingTrampolines();
	recordGeneratedRunTimeaddress("methodZoneBase", methodZoneBase);
}

	/* Cogit>>#generatorForPC: */
static NoDbgRegParms BytecodeDescriptor *
generatorForPC(sqInt pc)
{
	return generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
}

	/* Cogit>>#genFFICalloutTrampoline */
static usqInt
genFFICalloutTrampoline(void)
{
    usqInt startAddress;


	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	genoperand(PopR, R15);
	genoperandoperand(MoveRAw, R15, instructionPointerAddress());
	genoperand(CallR, TempReg);
	genoperandoperand(MoveAwR, instructionPointerAddress(), R15);
	genoperand(PushR, R15);
	genoperand(RetN, 0);
	startAddress = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceFFICalloutTrampoline", startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Generate a pair of routines that answer the frame pointer, and the stack
	pointer immediately
	after a leaf call, used for checking stack pointer alignment, frame
	pointer usage, etc. N.B.
	these are exported to the CoInterpreter et al via Cogit
	class>>mustBeGlobal:. 
 */

	/* Cogit>>#genGetLeafCallStackPointers */
static void
genGetLeafCallStackPointers(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    usqInt startAddress;


	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 4;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	startAddress = methodZoneBase;
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, FPReg, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetFP", startAddress);
	ceGetFP = ((usqIntptr_t (*)(void)) startAddress);
	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, NativeSPReg, ABIResultReg);
	genoperandoperand(AddCqR, leafCallStackPointerDelta(backEnd), ABIResultReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetSP", startAddress);
	ceGetSP = ((usqIntptr_t (*)(void)) startAddress);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged target
	or a call of ceMNUFromPICMNUMethod:receiver: to handle an MNU dispatch
	in a closed PIC. It distinguishes the two by testing ClassReg. If the
	register is zero then this is an MNU.
	
	This poses a problem in 32-bit Spur, where zero is the cache tag for
	immediate characters (tag pattern 2r10) because SmallIntegers have tag
	patterns 2r11
	and 2r01, so anding with 1 reduces these to 0 & 1. We solve the ambiguity
	by patching send sites with a 0 cache tag to open PICs instead of closed
	PICs.  */

	/* Cogit>>#genInnerPICAbortTrampoline: */
static NoDbgRegParms usqInt
genInnerPICAbortTrampoline(char *name)
{
    AbstractInstruction *jumpMNUCase;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0 /* begin picAbortDiscriminatorValue */, ClassReg);
	jumpMNUCase = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceInterpretMethodFromPICreceiver, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpMNUCase, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceMNUFromPICMNUMethodreceiver, name, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg, 1);
}


/*	Switch to the C stack (do *not* save the Smalltalk stack pointers;
	this is the caller's responsibility), and invoke interpret PDQ. */

	/* Cogit>>#genInvokeInterpretTrampoline */
static void
(*genInvokeInterpretTrampoline(void))(void)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(0 <= 4);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cReturnAddressAddress(), ABIResultReg);
	genoperand(PushR, ABIResultReg);
	genoperand(JumpFull, ((sqInt)(((usqInt)interpret))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceInvokeInterpret", startAddress);
	return ((void (*)(void)) startAddress);
}


/*	The in-line cache for a send is implemented as a constant load into
	ClassReg. We always use a 32-bit load, even in 64-bits.
	
	In the initial (unlinked) state the in-line cache is notionally loaded
	with the selector.
	But since in 64-bits an arbitrary selector oop won't fit in a 32-bit
	constant load, we
	instead load the cache with the selector's index, either into the literal
	frame of the
	current method, or into the special selector array. Negative values are
	1-relative indices into the special selector array.
	
	When a send is linked, the load of the selector, or selector index, is
	overwritten with a
	load of the receiver's class, or class tag. Hence, the 64-bit VM is
	currently constrained
	to use class indices as cache tags. If out-of-line literals are used,
	distinct caches /must
	not/ share acche locations, for if they do, send cacheing will be confused
	by the sharing.
	Hence we use the MoveUniqueC32:R: instruction that will not share literal
	locations.  */

	/* Cogit>>#genLoadInlineCacheWithSelector: */
static NoDbgRegParms void
genLoadInlineCacheWithSelector(sqInt selectorIndex)
{
    sqInt cacheValue;

	assert((selectorIndex < 0
		? (((-selectorIndex) >= 1) && ((-selectorIndex) <= (numSpecialSelectors())))
		: ((selectorIndex >= 0) && (selectorIndex <= ((literalCountOf(methodObj)) - 1)))));
	cacheValue = selectorIndex;
	/* begin uniqueLiteral32:forInstruction: */
	genoperandoperand(MoveC32R, cacheValue, ClassReg);
}

	/* Cogit>>#genReturnToInterpreterTrampoline */
static usqInt
genReturnToInterpreterTrampoline(void)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	genoperandoperandoperand(MoveMwrR, FoxIFSavedIP, FPReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, instructionPointerAddress());
	genSmalltalkToCStackSwitch(0);
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(0 <= 4);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cReturnAddressAddress(), ABIResultReg);
	genoperand(PushR, ABIResultReg);
	genoperand(JumpFull, ((sqInt)(((usqInt)interpret))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceReturnToInterpreterTrampoline", startAddress);
	return startAddress;
}


/*	If the client requires, then on an ARM-like RISC processor, the return
	address needs to
	be pushed to the stack so that the interpreter sees the same stack layout
	as on CISC.
 */

	/* Cogit>>#genSmalltalkToCStackSwitch: */
static NoDbgRegParms sqInt
genSmalltalkToCStackSwitch(sqInt pushLinkReg)
{
	genSaveStackPointers(backEnd);
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	return 0;
}


/*	Generate a trampoline with one argument that answers a result.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:arg:floatResult: */
static NoDbgRegParms usqInt
genTrampolineForcalledargfloatResult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg)
{
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegfloatResultRegappendOpcodes(aRoutine, aString, 1, regOrConst0, null, null, null, 0 /* begin emptyRegisterMask */, 1, resultReg, 0);
}


/*	Generate a trampoline with one argument that answers a result.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:arg:result:result: */
static NoDbgRegParms usqInt
genTrampolineForcalledargresultresult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg, sqInt resultReg2)
{
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegresultRegappendOpcodes(aRoutine, aString, 1, regOrConst0, null, null, null, 0 /* begin emptyRegisterMask */, 1, resultReg, resultReg2, 0);
}


/*	Generate a trampoline with one argument that answers a result.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:floatArg:result: */
static NoDbgRegParms usqInt
genTrampolineForcalledfloatArgresult(void *aRoutine, char *aString, sqInt regOrConst0, sqInt resultReg)
{
	return genTrampolineForcallednumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultRegappendOpcodes(aRoutine, aString, 1, regOrConst0, null, null, null, 0 /* begin emptyRegisterMask */, 1, resultReg, 0);
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:floatResultReg:appendOpcodes: */
static NoDbgRegParms usqInt
genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegfloatResultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegfloatResultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg:appendOpcodes: */
static NoDbgRegParms usqInt
genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, codeBase + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return startAddress;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg:resultReg:appendOpcodes: */
static NoDbgRegParms usqInt
genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt resultReg2OrNone, sqInt appendBoolean)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultRegresultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone, resultReg2OrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:floatArg:floatArg:floatArg:floatArg:regsToSave:pushLinkReg:resultReg:appendOpcodes: */
static NoDbgRegParms usqInt
genTrampolineForcallednumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsfloatArgfloatArgfloatArgfloatArgregsToSavepushLinkRegresultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	To return from a trampoline call we have to take the return address off
	the stack,
	iof it has been saved */

	/* Cogit>>#genTrampolineReturn: */
static NoDbgRegParms void
genTrampolineReturn(sqInt lnkRegWasPushed)
{
	genoperand(RetN, 0);
}


/*	<Integer> */

	/* Cogit>>#gen: */
static NoDbgRegParms AbstractInstruction *
gen(sqInt opcode)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand: */
static NoDbgRegParms AbstractInstruction *
genoperand(sqInt opcode, sqInt operand)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operand;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand: */
static NoDbgRegParms AbstractInstruction *
genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand:operand: */
static NoDbgRegParms AbstractInstruction *
genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	((abstractInstruction->operands))[2] = operandThree;
	return abstractInstruction;
}

	/* Cogit>>#getLiteral: */
static NoDbgRegParms sqInt
getLiteral(sqInt litIndex)
{
	if (maxLitIndex < litIndex) {
		maxLitIndex = litIndex;
	}
	return literalofMethod(litIndex, methodObj);
}

	/* Cogit>>#incrementUsageOfTargetIfLinkedSend:mcpc:ignored: */
static NoDbgRegParms sqInt
incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;

	targetMethod = 0;
	if (annotation >= IsSendCall) {
		assert(annotation != IsNSSendCall);
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmUsageCount)) < (CMMaxUsageCount / 2)) {
				((((CogMethod *) ((((usqInt)targetMethod1)) + codeToDataDelta)))->cmUsageCount = ((targetMethod1->cmUsageCount)) + 1);
			}
		}
	}
	return 0;
}


/*	Answer the value to put in an inline-cache that is being loaded with the
	selector. Usually this is simply the selector, but in 64-bits the cache is
	only 32-bits wide
	and so the cache is loaded with the index of the selector. */
/*	First search the special selectors; there are only 32 of them so this
	shouldn't take too long.
	We could short-circuit this by keeping a hint bit in the target method, or
	by maintaining the
	maximum range of selector oops in specialSelectors since they're likely to
	cluster. 
 */

	/* Cogit>>#indexForSelector:in: */
static NoDbgRegParms sqInt
indexForSelectorin(sqInt selector, CogMethod *cogMethod)
{
    sqInt i;
    sqInt iLimiT;
    sqInt methodOop;

	for (i = 0; i < NumSpecialSelectors; i += 1) {
		if (selector == (specialSelector(i))) {
			return -1 - i;
		}
	}
	/* Then search the method's literal frame... open code fetchPointer:ofObject: for speed... */
	methodOop = (cogMethod->methodObject);
	for (i = LiteralStart, iLimiT = (literalCountOfMethodHeader((cogMethod->methodHeader))); i <= iLimiT; i += 1) {
		if ((longAt(((i * BytesPerOop) + BaseHeaderSize) + methodOop)) == selector) {
			assert(selector == (literalofMethod(i - 1, methodOop)));
			return i - 1;
		}
	}
	error("could not find selector in method when unlinking send site");
	return 0;
}

	/* Cogit>>#initializeCodeZoneFrom:upTo: */
void
initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress)
{

	/* begin initializeBackend */
	(methodLabel->machineCodeSize = 0);
	(methodLabel->opcode = Label);
	((methodLabel->operands))[0] = 0;
	((methodLabel->operands))[1] = 0;
	assert((!((registerMaskFor(VarBaseReg)) & CallerSavedRegisterMask)));
	varBaseAddress = computeGoodVarBaseAddress();
	assert((stackLimitAddress()) >= varBaseAddress);
	assert((cStackPointerAddress()) >= varBaseAddress);
	assert((cFramePointerAddress()) >= varBaseAddress);
	assert((cReturnAddressAddress()) >= varBaseAddress);
	assert((nextProfileTickAddress()) >= varBaseAddress);
	sqMakeMemoryExecutableFromToCodeToDataDelta(startAddress, endAddress, 
#  if DUAL_MAPPED_CODE_ZONE
		(&codeToDataDelta)
#  else
		null
#  endif
		);
	codeBase = (methodZoneBase = startAddress);
	stopsFromto(backEnd, startAddress, endAddress - 1);
	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	assertValidDualZone();
	detectFeatures(backEnd);
	/* begin maybeGenerateCacheFlush */
	genGetLeafCallStackPointers();
	generateStackPointerCapture();
	generateTrampolines();
	computeEntryOffsets();
	computeFullBlockEntryOffsets();
	generateClosedPICPrototype();
	alignMethodZoneBase();
	flushICacheFromto(backEnd, startAddress, ((usqInt)methodZoneBase));
	/* begin maybeFlushWritableZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	if (codeToDataDelta > 0) {
		flushDCacheFromto(backEnd, startAddress, ((usqInt)methodZoneBase));
	}
#  endif

	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	generateOpenPICPrototype();
}


/*	Answer a usage count that reflects likely long-term usage.
	Answer 1 for non-primitives or quick primitives (inst var accessors),
	2 for methods with interpreter primitives, and 3 for compiled primitives. */

	/* Cogit>>#initialMethodUsageCount */
static sqInt
initialMethodUsageCount(void)
{
	if ((primitiveIndex == 1)
	 || (isQuickPrimitiveIndex(primitiveIndex))) {
		return 1;
	}
	if (!(primitiveGeneratorOrNil())) {
		return 2;
	}
	return 3;
}


/*	Answer a usage count that reflects likely long-term usage. */

	/* Cogit>>#initialOpenPICUsageCount */
static int
initialOpenPICUsageCount(void)
{
	return CMMaxUsageCount - 1;
}

	/* Cogit>>#inverseBranchFor: */
static NoDbgRegParms sqInt
inverseBranchFor(sqInt opcode)
{
	switch (opcode) {
	case JumpLongZero:
		return JumpLongNonZero;

	case JumpLongNonZero:
		return JumpLongZero;

	case JumpZero:
		return JumpNonZero;

	case JumpNonZero:
		return JumpZero;

	case JumpNegative:
		return JumpNonNegative;

	case JumpNonNegative:
		return JumpNegative;

	case JumpOverflow:
		return JumpNoOverflow;

	case JumpNoOverflow:
		return JumpOverflow;

	case JumpCarry:
		return JumpNoCarry;

	case JumpNoCarry:
		return JumpCarry;

	case JumpLess:
		return JumpGreaterOrEqual;

	case JumpGreaterOrEqual:
		return JumpLess;

	case JumpGreater:
		return JumpLessOrEqual;

	case JumpLessOrEqual:
		return JumpGreater;

	case JumpBelow:
		return JumpAboveOrEqual;

	case JumpAboveOrEqual:
		return JumpBelow;

	case JumpAbove:
		return JumpBelowOrEqual;

	case JumpBelowOrEqual:
		return JumpAbove;

	default:
		error("Case not found and no otherwise clause");
	}
	error("invalid opcode for inverse");
	return 0;
}


/*	Useful for debugging. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#isPCWithinMethodZone: */
static NoDbgRegParms int
isPCWithinMethodZone(void *address)
{
	return (((((usqInt)address)) >= methodZoneBase) && ((((usqInt)address)) <= (freeStart())));
}


/*	Answer if the instruction preceding retpc is a call instruction. */

	/* Cogit>>#isSendReturnPC: */
sqInt
isSendReturnPC(sqInt retpc)
{
    sqInt target;

	if (!(isCallPrecedingReturnPC(backEnd, retpc))) {
		return 0;
	}
	target = callTargetFromReturnAddress(backEnd, retpc);
	return (((target >= firstSend) && (target <= lastSend)))
	 || (((target >= methodZoneBase) && (target <= (freeStart()))));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPEqual(void *jumpTarget)
{
    AbstractInstruction *jumpToTarget;
    AbstractInstruction *jumpUnordered;

	/* begin genJumpFPEqual: */
	jumpUnordered = genoperand(JumpFPUnordered, ((sqInt)jumpTarget));
	jumpToTarget = genoperand(JumpFPEqual, ((sqInt)jumpTarget));
	jmpTarget(jumpUnordered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpToTarget;
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreaterOrEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPGreaterOrEqual(void *jumpTarget)
{
	return genoperand(JumpFPGreaterOrEqual, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreater: */
static NoDbgRegParms AbstractInstruction *
gJumpFPGreater(void *jumpTarget)
{
	return genoperand(JumpFPGreater, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPLessOrEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPLessOrEqual(void *jumpTarget)
{
    AbstractInstruction *jumpToTarget;
    AbstractInstruction *jumpUnordered;

	/* begin genJumpFPLessOrEqual: */
	jumpUnordered = genoperand(JumpFPUnordered, ((sqInt)jumpTarget));
	jumpToTarget = genoperand(JumpFPLessOrEqual, ((sqInt)jumpTarget));
	jmpTarget(jumpUnordered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpToTarget;
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPLess: */
static NoDbgRegParms AbstractInstruction *
gJumpFPLess(void *jumpTarget)
{
    AbstractInstruction *jumpToTarget;
    AbstractInstruction *jumpUnordered;

	/* begin genJumpFPLess: */
	jumpUnordered = genoperand(JumpFPUnordered, ((sqInt)jumpTarget));
	jumpToTarget = genoperand(JumpFPLess, ((sqInt)jumpTarget));
	jmpTarget(jumpUnordered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpToTarget;
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPNotEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPNotEqual(void *jumpTarget)
{
    AbstractInstruction *jumpToTarget;
    AbstractInstruction *jumpUnordered;

	/* begin genJumpFPNotEqual: */
	jumpToTarget = genoperand(JumpFPNotEqual, ((sqInt)jumpTarget));
	jumpUnordered = genoperand(JumpFPUnordered, ((sqInt)jumpTarget));
	addDependent(jumpToTarget, jumpUnordered);
	return jumpToTarget;
}

	/* Cogit>>#LogicalShiftLeftCq:R: */
static NoDbgRegParms AbstractInstruction *
gLogicalShiftLeftCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(LogicalShiftLeftCqR, quickConstant, reg);
}


/*	destReg := srcReg << quickConstant */

	/* Cogit>>#LogicalShiftLeftCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, destReg);
	return first;
}


/*	destReg := (unsigned)srcReg >> quickConstant */

	/* Cogit>>#LogicalShiftRightCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gLogicalShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(LogicalShiftRightCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#lastOpcode */
static AbstractInstruction *
lastOpcode(void)
{
	assert(opcodeIndex > 0);
	return abstractInstructionAt(opcodeIndex - 1);
}

	/* Cogit>>#linkSendAt:in:to:offset:receiver: */
void
linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver)
{
    sqInt extent;
    sqInt inlineCacheTag;

	assert((theEntryOffset == cmEntryOffset)
	 || (theEntryOffset == cmNoCheckEntryOffset));
	assert(((callSiteReturnAddress >= methodZoneBase) && (callSiteReturnAddress <= (freeStart()))));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	if (theEntryOffset == cmNoCheckEntryOffset) {
		/* no need to change selector cache tag */
		extent = rewriteCallAttarget(backEnd, callSiteReturnAddress, (((sqInt)targetMethod)) + cmNoCheckEntryOffset);
	}
	else {
		inlineCacheTag = inlineCacheTagForInstance(receiver);
		extent = rewriteInlineCacheAttagtarget(backEnd, callSiteReturnAddress, inlineCacheTag, (((sqInt)targetMethod)) + theEntryOffset);
	}
	flushICacheFromto(backEnd, (((usqInt)callSiteReturnAddress)) - extent, ((usqInt)callSiteReturnAddress));
}

	/* Cogit>>#loadBytesAndGetDescriptor */
static BytecodeDescriptor *
loadBytesAndGetDescriptor(void)
{
    BytecodeDescriptor *descriptor;

	byte0 = (fetchByteofObject(bytecodePC, methodObj)) + bytecodeSetOffset;
	descriptor = generatorAt(byte0);
	loadSubsequentBytesForDescriptorat(descriptor, bytecodePC);
	return descriptor;
}

	/* Cogit>>#loadSubsequentBytesForDescriptor:at: */
static NoDbgRegParms void
loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc)
{
	if (((descriptor->numBytes)) > 1) {
		byte1 = fetchByteofObject(pc + 1, methodObj);
		if (((descriptor->numBytes)) > 2) {
			byte2 = fetchByteofObject(pc + 2, methodObj);
			if (((descriptor->numBytes)) > 3) {
				byte3 = fetchByteofObject(pc + 3, methodObj);
				if (((descriptor->numBytes)) > 4) {
					notYetImplemented();
				}
			}
		}
	}
}

	/* Cogit>>#MoveCw:R: */
static NoDbgRegParms AbstractInstruction *
gMoveCwR(sqInt wordConstant, sqInt reg)
{
	return genoperandoperand(MoveCwR, wordConstant, reg);
}

	/* Cogit>>#MovePerfCnt64R:L: */
static NoDbgRegParms AbstractInstruction *
gMovePerfCnt64RL(sqInt destReg, sqInt liveRegisterMask)
{
	assert(BytesPerWord == 8);
	return genoperandoperand(MovePerfCnt64RL, destReg, liveRegisterMask);
}

	/* Cogit>>#MovePerfCnt64R:R:L: */
static NoDbgRegParms AbstractInstruction *
gMovePerfCnt64RRL(sqInt destRegLo, sqInt destRegHi, sqInt liveRegisterMask)
{
	assert(BytesPerWord == 4);
	return genoperandoperandoperand(MovePerfCnt64RRL, destRegLo, destRegHi, liveRegisterMask);
}


/*	Answer the address of the null byte at the end of the method map. */

	/* Cogit>>#mapEndFor: */
static NoDbgRegParms usqInt
mapEndFor(CogMethod *cogMethod)
{
    usqInt end;

	end = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while ((byteAt(end)) != MapEnd) {
		end -= 1;
		assert(end > (firstMappedPCFor(cogMethod)));
	}
	return end;
}


/*	Unlinking/GC/Disassembly support */
/*	most of the time arg is a CogMethod... */

	/* Cogit>>#mapFor:performUntil:arg: */
static NoDbgRegParms sqInt
mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	mcpc = 
	/* begin firstMappedPCFor: */
((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask);
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = functionSymbol(annotation, (((char *) mcpc)), arg);
			if (result != 0) {
				return result;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	return 0;
}


/*	Remap all object references in the closed PIC. Answer if any references
	are young.
	Set codeModified if any modifications are made. */

	/* Cogit>>#mapObjectReferencesInClosedPIC: */
static NoDbgRegParms sqInt
mapObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt pc;
    sqInt refersToYoung;


	/* first we check the potential method oop load at the beginning of the CPIC */
	pc = addressOfEndOfCaseinCPIC(1, cPIC);
	/* We find the end address of the cPICNumCases'th case and can then just step forward by the case size thereafter */
	refersToYoung = remapMaybeObjRefInClosedPICAt(pc - (jumpLongByteSize(backEnd)));
	/* Next we check the potential class ref in the compare instruction, and the potential method oop load for each case. */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (remapMaybeObjRefInClosedPICAt((pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))) {
			refersToYoung = 1;
		}
		pc += cPICCaseSize;
	}
	return refersToYoung;
}


/*	Update all references to objects in the generated runtime. */

	/* Cogit>>#mapObjectReferencesInGeneratedRuntime */
static void
mapObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    sqInt mappedLiteral;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = literalBeforeFollowingAddress(backEnd, mcpc);
		mappedLiteral = remapObject(literal);
		if (mappedLiteral != literal) {
			/* begin setCodeModified */
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			codeModified = 1;
#      endif

			/* begin storeLiteral:atAnnotatedAddress:using: */
			storeLiteralbeforeFollowingAddress(backEnd, mappedLiteral, mcpc);
		}
	}
}


/*	Update all references to objects in machine code for a become.
	Unlike incrementalGC or fullGC a method that does not refer to young may
	refer to young as a result of the become operation. Unlike incrementalGC
	or fullGC the reference from a Cog method to its methodObject *must not*
	change since the two are two halves of the same object. */

	/* Cogit>>#mapObjectReferencesInMachineCodeForBecome */
static void
mapObjectReferencesInMachineCodeForBecome(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedPIC;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt remappedMethod;
    sqInt result;
    CogMethod *writableCogMethod;

	hasYoungObj = 0;
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	codeModified = (freedPIC = 0);
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		assert(!hasYoungObj);
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				if ((isYoung((cogMethod->selector)))
				 || (mapObjectReferencesInClosedPIC(cogMethod))) {
					freedPIC = 1;
					freeMethod(cogMethod);
				}
			}
			else {
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					remappedMethod = remapOop((cogMethod->methodObject));
					if (remappedMethod != ((cogMethod->methodObject))) {
						if (methodHasCogMethod(remappedMethod)) {
							error("attempt to become two cogged methods");
						}
						if (!(withoutForwardingOnandwithsendToCogit((cogMethod->methodObject), remappedMethod, (cogMethod->cmUsesPenultimateLit), methodhasSameCodeAscheckPenultimate))) {
							error("attempt to become cogged method into different method");
						}
						if ((rawHeaderOf((cogMethod->methodObject))) == (((sqInt)cogMethod))) {
							rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
							rawHeaderOfput(remappedMethod, ((sqInt)cogMethod));
						}
						else {
							assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
						}
					}
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask);
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), ((CogMethod *) hasYoungObjPtr));
						if (result != 0) {
							goto l1;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
						}
					}
					map -= 1;
				}
	l1:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					ensureInYoungReferrers(cogMethod);
					hasYoungObj = 0;
				}
				else {
					(cogMethod->cmRefersToYoung = 0);
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (freedPIC) {
		unlinkSendsToFree();
	}
	if (codeModified) {
		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)codeBase), freeStart());
	}
}


/*	Update all references to objects in machine code for a full gc. Since
	the current (New)ObjectMemory GC makes everything old in a full GC
	a method not referring to young will not refer to young afterwards */

	/* Cogit>>#mapObjectReferencesInMachineCodeForFullGC */
static void
mapObjectReferencesInMachineCodeForFullGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableCogMethod;

	codeModified = 0;
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(!((cogMethod->cmRefersToYoung)));
				mapObjectReferencesInClosedPIC(cogMethod);
			}
			else {
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask);
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), 0);
						if (result != 0) {
							goto l1;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
						}
					}
					map -= 1;
				}
	l1:	/* end mapFor:performUntil:arg: */;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (codeModified) {
		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)codeBase), freeStart());
	}
}


/*	Update all references to objects in machine code for either a Spur
	scavenging gc
	or a Squeak V3 incremental GC. Avoid scanning all code by using the
	youngReferrers list. In a young gc a method referring to young may no
	longer refer to young, but a
	method not referring to young cannot and will not refer to young
	afterwards.  */

	/* Cogit>>#mapObjectReferencesInMachineCodeForYoungGC */
static void
mapObjectReferencesInMachineCodeForYoungGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    usqInt pointer;
    sqInt result;
    CogMethod *writableCogMethod;
    sqInt zoneIsWritable;

	codeModified = (zoneIsWritable = (hasYoungObj = 0));
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	pointer = youngReferrers();
	while (pointer < limitAddress) {
		assert(!hasYoungObj);
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (((cogMethod->cmType)) == CMFree) {
			assert(!((cogMethod->cmRefersToYoung)));
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			if ((cogMethod->cmRefersToYoung)) {
				assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
				 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
				if (!zoneIsWritable) {
					/* begin ensureWritableCodeZone */
#          if !DUAL_MAPPED_CODE_ZONE
#          endif

					zoneIsWritable = 1;
				}
				writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
				(writableCogMethod->selector = remapOop((cogMethod->selector)));
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask);
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), ((void *)hasYoungObjPtr));
						if (result != 0) {
							goto l1;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
						}
					}
					map -= 1;
				}
	l1:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					hasYoungObj = 0;
				}
				else {
					(writableCogMethod->cmRefersToYoung = 0);
				}
			}
		}
		pointer += BytesPerWord;
	}
	pruneYoungReferrers();
	if (codeModified) {
		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Update all references to objects in machine code. */

	/* Cogit>>#mapObjectReferencesInMachineCode: */
void
mapObjectReferencesInMachineCode(sqInt gcMode)
{
	switch (gcMode) {
	case GCModeNewSpace:
		/* N.B. do *not* ensureWritableCodeZone for every scavenge. */
		mapObjectReferencesInMachineCodeForYoungGC();
		break;
	case GCModeFull:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		mapObjectReferencesInMachineCodeForFullGC();
		break;
	case GCModeBecome:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		mapObjectReferencesInMachineCodeForBecome();
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	mapPerMethodProfile();
	if (!(asserta((freeStart()) <= (youngReferrers())))) {
		error("youngReferrers list overflowed");
	}
}


/*	Mark objects in machine-code of marked methods (or open PICs with marked
	selectors). 
 */

	/* Cogit>>#markAndTraceMachineCodeOfMarkedMethods */
void
markAndTraceMachineCodeOfMarkedMethods(void)
{
    sqInt annotation;
    sqInt annotation1;
    CogMethod *cogMethod;
    usqInt map;
    usqInt map1;
    sqInt mapByte;
    sqInt mapByte1;
    sqInt mcpc;
    sqInt mcpc1;
    sqInt result;
    sqInt result1;

	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	codeModified = 0;
	markAndTraceObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (isMarked((cogMethod->methodObject)))) {
			/* begin markAndTraceLiteralsIn: */
			assert(((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
			 && (isMarked((cogMethod->methodObject))))
			 || ((isCMOpenPIC(((CogBlockMethod *) cogMethod)))
			 && ((isImmediate((cogMethod->selector)))
			 || (isMarked((cogMethod->selector))))));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkIRCsIn: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = markLiteralspcmethod(annotation, ((char *) mcpc), cogMethod);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		if ((((cogMethod->cmType)) == CMOpenPIC)
		 && ((isImmediate((cogMethod->selector)))
		 || (isMarked((cogMethod->selector))))) {
			/* begin markAndTraceLiteralsIn: */
			assert(((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
			 && (isMarked((cogMethod->methodObject))))
			 || ((isCMOpenPIC(((CogBlockMethod *) cogMethod)))
			 && ((isImmediate((cogMethod->selector)))
			 || (isMarked((cogMethod->selector))))));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkIRCsIn: */
			mcpc1 = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map1 = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte1 = byteAt(map1))) != MapEnd) {
				if (mapByte1 >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc1 += (mapByte1 & DisplacementMask);
					if ((((annotation1 = ((usqInt)(mapByte1)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte1 = byteAt(map1 - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation1 += mapByte1 & DisplacementMask;
						map1 -= 1;
					}
					result1 = markLiteralspcmethod(annotation1, ((char *) mcpc1), cogMethod);
					if (result1 != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte1 < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc1 += (((sqInt)((usqInt)((mapByte1 - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map1 -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	if (codeModified) {
		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Mark and trace any object references in the generated run-time. */

	/* Cogit>>#markAndTraceObjectReferencesInGeneratedRuntime */
static void
markAndTraceObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = literalBeforeFollowingAddress(backEnd, mcpc);
		markAndTraceLiteralinatpc(literal, ((CogMethod *) null), ((usqInt)mcpc));
	}
}


/*	Mark and trace objects in the argument and free if it is appropriate.
	Answer if the method has been freed. firstVisit is a hint used to avoid
	scanning methods we've already seen. False positives are fine.
	For a CMMethod this
	frees if the bytecode method isnt marked,
	marks and traces object literals and selectors,
	unlinks sends to targets that should be freed.
	For a CMClosedPIC this
	frees if it refers to anything that should be freed or isn't marked.
	For a CMOpenPIC this
	frees if the selector isn't marked. */
/*	this recurses at most one level down */

	/* Cogit>>#markAndTraceOrFreeCogMethod:firstVisit: */
static NoDbgRegParms sqInt
markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (((cogMethod->cmType)) == CMFree) {
		return 1;
	}
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!(isMarked((cogMethod->methodObject)))) {
			/* begin ensureWritableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
#      endif

			freeMethod(cogMethod);
			return 1;
		}
		if (firstVisit) {
			/* begin markLiteralsAndUnlinkUnmarkedSendsIn: */
			assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
			assert(isMarked((cogMethod->methodObject)));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkIRCsIn: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = markLiteralsAndUnlinkIfUnmarkedSendpcmethod(annotation, ((char *) mcpc), cogMethod);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (!(closedPICRefersToUnmarkedObject(cogMethod))) {
			return 0;
		}
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		freeMethod(cogMethod);
		return 1;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (isMarked((cogMethod->selector))) {
			return 0;
		}
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		freeMethod(cogMethod);
		return 1;
	}
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || ((isCMClosedPIC(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod)))));
	return 0;
}


/*	If entryPoint is that of some method, then mark and trace objects in it
	and free if it is appropriate.
	Answer if the method has been freed. */

	/* Cogit>>#markAndTraceOrFreePICTarget:in: */
static NoDbgRegParms sqInt
markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC)
{
    CogMethod *targetMethod;

	assert((entryPoint > methodZoneBase)
	 && (entryPoint < (freeStart())));
	if (	/* begin containsAddress: */
		((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
	 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
		return 0;
	}
	targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
	assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
	 || (isCMFree(((CogBlockMethod *) targetMethod))));
	return markAndTraceOrFreeCogMethodfirstVisit(targetMethod, (((usqInt)targetMethod)) > (((usqInt)cPIC)));
}


/*	Mark and trace literals. Unlink sends that have unmarked cache tags or
	targets. 
 */

	/* Cogit>>#markLiteralsAndUnlinkIfUnmarkedSend:pc:method: */
static NoDbgRegParms sqInt
markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    unsigned int cacheTag1;
    sqInt cacheTagMarked;
    sqInt entryPoint;
    sqInt entryPoint1;
    sqInt literal;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt tagCouldBeObj;
    sqInt tagCouldBeObj1;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	entryPoint = 0;
	sendTable = 0;
	tagCouldBeObj = 0;
	targetMethod = 0;
	if (annotation == IsObjectReference) {
		literal = literalBeforeFollowingAddress(backEnd, ((usqInt)mcpc));
		if (markAndTraceLiteralinatpc(literal, ((CogMethod *) cogMethod), ((usqInt)mcpc))) {
			codeModified = 1;
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTag1 = literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)));
		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
		cacheTagMarked = tagCouldBeObj1;
		if (entryPoint1 > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint1 - offset1));
			if ((!cacheTagMarked)
			 || (markAndTraceOrFreeCogMethodfirstVisit(targetMethod1, (((usqInt)targetMethod1)) > (((usqInt)mcpc))))) {
				/* Either the cacheTag is unmarked (e.g. new class) or the target
				   has been freed (because it is unmarked), so unlink the send. */
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
				markAndTraceLiteralinat((targetMethod1->selector), targetMethod1, (&((targetMethod1->selector))));
			}
		}
		else {
			/* cacheTag is selector */
		}
	}
	return 0;
}


/*	Mark and trace literals.
	Additionally in Newspeak, void push implicits that have unmarked classes. */

	/* Cogit>>#markLiterals:pc:method: */
static NoDbgRegParms sqInt
markLiteralspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    unsigned int cacheTag1;
    sqInt entryPoint1;
    sqInt literal;
    sqInt tagCouldBeObj;
    sqInt tagCouldBeObj1;

	tagCouldBeObj = 0;
	if (annotation == IsObjectReference) {
		literal = literalBeforeFollowingAddress(backEnd, ((usqInt)mcpc));
		if (markAndTraceLiteralinatpc(literal, cogMethod, ((usqInt)mcpc))) {
			codeModified = 1;
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTag1 = literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)));
		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
	}
	return 0;
}

	/* Cogit>>#markMethodAndReferents: */
void
markMethodAndReferents(CogBlockMethod *aCogMethod)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableMethod;

	assert((isCMMethodEtAl(aCogMethod))
	 || (isCMBlock(aCogMethod)));
	cogMethod = (((aCogMethod->cmType)) >= CMMethod
		? ((CogMethod *) aCogMethod)
		: cmHomeMethod(aCogMethod));
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmUsageCount = CMMaxUsageCount);
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask);
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = incrementUsageOfTargetIfLinkedSendmcpcignored(annotation, ((char *) mcpc), 0);
			if (result != 0) {
				goto l1;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	l1:	/* end mapFor:performUntil:arg: */;
}

	/* Cogit>>#maxCogMethodAddress */
usqInt
maxCogMethodAddress(void)
{
	return ((usqInt)(limitZony()));
}

	/* Cogit>>#maximumDistanceFromCodeZone: */
static NoDbgRegParms sqInt
maximumDistanceFromCodeZone(sqInt anAddress)
{
	return (anAddress > codeBase
		? anAddress - codeBase
		: limitAddress - anAddress);
}


/*	If this is the Newspeak VM and the objectRepresentation supports pinning
	then allocate space for the implicit receiver caches on the heap. */

	/* Cogit>>#maybeAllocAndInitIRCs */
static sqInt
maybeAllocAndInitIRCs(void)
{
	return 1;
}


/*	Check that the header fields are consistent with the type.
	Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#maybeFreeCogMethodDoesntLookKosher: */
static NoDbgRegParms sqInt
maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod)
{
    sqInt result;

	result = cogMethodDoesntLookKosher(cogMethod);
	return (result == 2
		? 0
		: result);
}

	/* Cogit>>#mclassCouldBeContext */
static int
mclassCouldBeContext(void)
{
	if (receiverTags < 0) {
		receiverTags = receiverTagBitsForMethod(methodObj);
	}
	return ((receiverTags & ((1U << (numTagBits())))) != 0);
}

	/* Cogit>>#mclassIsSmallInteger */
static int
mclassIsSmallInteger(void)
{
	if (receiverTags < 0) {
		receiverTags = receiverTagBitsForMethod(methodObj);
	}
	return (((receiverTags) & 7) == 1);
}


/*	Answer the absolute machine code pc matching the zero-relative
	bytecode pc of a backward branch in cogMethod, given the start
	of the bytecodes for cogMethod's block or method object. */

	/* Cogit>>#mcPCForBackwardBranch:startBcpc:in: */
usqInt
mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc1;
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    usqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt targetPC;


	/* begin mapFor:bcpc:performUntil:arg: */
	latestContinuation = 0;
	assert(((cogMethod->stackCheckOffset)) > 0);
	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(null, 0 + (((sqInt)((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)bcpc));
	if (result != 0) {
		return result;
	}
	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc1 = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		isInBlock = (cogMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		bcpc1 += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc1 == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}
		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc1 = startbcpc - (/* begin blockCreationBytecodeSizeForHeader: */
	(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
	? AltBlockCreationBytecodeSize
	: BlockCreationBytecodeSize));
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		byte = (fetchByteofObject(bcpc1, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc1 + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc1, -1, aMethodObj)
	: 0));
		bcpc1 = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask);
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc1, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc1 >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc1 >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj);
							targetPC = (bcpc1 + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc1 + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj)
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc1 = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((				/* begin isBackwardBranch:at:exts:in: */
					assert(((descriptor->spanFunction))),
				(((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj)) < 0));
				result = findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(descriptor, (isBackwardBranch
					? (((sqInt)((usqInt)(annotation) << 1))) + 1
					: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
					? bcpc1 - (2 * nExts)
					: bcpc1), ((void *)bcpc));
				if (result != 0) {
					return result;
				}
				bcpc1 = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	return 0;
}


/*	For the purposes of become: see if the two methods are similar, i.e. can
	be safely becommed.
	This is pretty strict. All literals and bytecodes must be identical. Only
	trailer bytes and header
	flags can differ. */

	/* Cogit>>#method:hasSameCodeAs:checkPenultimate: */
static NoDbgRegParms sqInt
methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral)
{
    sqInt bi;
    sqInt endPCA;
    sqInt headerA;
    sqInt headerB;
    sqInt li;
    sqInt numLitsA;

	headerA = methodHeaderOf(methodA);
	headerB = methodHeaderOf(methodB);
	numLitsA = literalCountOfMethodHeader(headerA);
	endPCA = endPCOf(methodA);
	if (((argumentCountOfMethodHeader(headerA)) != (argumentCountOfMethodHeader(headerB)))
	 || (((temporaryCountOfMethodHeader(headerA)) != (temporaryCountOfMethodHeader(headerB)))
	 || (((primitiveIndexOfMethodheader(methodA, headerA)) != (primitiveIndexOfMethodheader(methodB, headerB)))
	 || ((numLitsA != (literalCountOfMethodHeader(headerB)))
	 || (endPCA > (numBytesOf(methodB))))))) {
		return 0;
	}
	for (li = 1; li < numLitsA; li += 1) {
		if ((fetchPointerofObject(li, methodA)) != (fetchPointerofObject(li, methodB))) {
			if ((li < (numLitsA - 1))
			 || (comparePenultimateLiteral)) {
				return 0;
			}
		}
	}
	for (bi = (startPCOfMethodHeader(headerA)); bi <= endPCA; bi += 1) {
		if ((fetchByteofObject(bi, methodA)) != (fetchByteofObject(bi, methodB))) {
			return 0;
		}
	}
	return 1;
}

	/* Cogit>>#mnuOffset */
sqInt
mnuOffset(void)
{
	return missOffset;
}

	/* Cogit>>#NativePopR: */
static NoDbgRegParms AbstractInstruction *
gNativePopR(sqInt reg)
{
	return genoperand(PopR, reg);
}

	/* Cogit>>#NativePushR: */
static NoDbgRegParms AbstractInstruction *
gNativePushR(sqInt reg)
{
	return genoperand(PushR, reg);
}

	/* Cogit>>#NativeRetN: */
static NoDbgRegParms AbstractInstruction *
gNativeRetN(sqInt offset)
{
	return genoperand(RetN, offset);
}

	/* Cogit>>#needsFrameIfImmutability: */
static NoDbgRegParms sqInt
needsFrameIfImmutability(sqInt stackDelta)
{
	return IMMUTABILITY;
}

	/* Cogit>>#needsFrameIfInBlock: */
static NoDbgRegParms int
needsFrameIfInBlock(sqInt stackDelta)
{
	return inBlock > 0;
}

	/* Cogit>>#needsFrameNever: */
static NoDbgRegParms sqInt
needsFrameNever(sqInt stackDelta)
{
	return 0;
}

	/* Cogit>>#noAssertMethodClassAssociationOf: */
static NoDbgRegParms sqInt
noAssertMethodClassAssociationOf(sqInt methodPointer)
{
	return literalofMethod((literalCountOfMethodHeader(noAssertHeaderOf(methodPointer))) - 1, methodPointer);
}


/*	Check that no method is maximally marked. A maximal mark is an indication
	the method has been scanned to increase the usage count of its referent
	methods.  */

	/* Cogit>>#noCogMethodsMaximallyMarked */
static sqInt
noCogMethodsMaximallyMarked(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->cmUsageCount)) == CMMaxUsageCount)) {
			return 0;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}


/*	Answer if all targets in the PIC are in-use methods. */

	/* Cogit>>#noTargetsFreeInClosedPIC: */
static NoDbgRegParms int
noTargetsFreeInClosedPIC(CogMethod *cPIC)
{
	return !(cPICHasFreedTargets(cPIC));
}

	/* Cogit>>#OrCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gOrCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	if (srcReg == destReg) {
		return genoperandoperand(OrCqR, quickConstant, destReg);
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(OrCqR, quickConstant, destReg);
	return first;
}


/*	Store the generated machine code, answering the last address */

	/* Cogit>>#outputInstructionsAt: */
static NoDbgRegParms sqInt
outputInstructionsAt(sqInt startAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt j;


	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	absoluteAddress = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		assert(((abstractInstruction->address)) == absoluteAddress);
		/* begin outputMachineCodeAt: */
		for (j = 0; j < ((abstractInstruction->machineCodeSize)); j += 1) {
			byteAtput(absoluteAddress + j, ((abstractInstruction->machineCode))[j]);
		}
		absoluteAddress += (abstractInstruction->machineCodeSize);
	}
	return absoluteAddress;
}


/*	Output instructions generated for one of the generated run-time routines,
	a trampoline, etc
 */

	/* Cogit>>#outputInstructionsForGeneratedRuntimeAt: */
static NoDbgRegParms sqInt
outputInstructionsForGeneratedRuntimeAt(sqInt startAddress)
{
    sqInt endAddress;
    sqInt size;

	computeMaximumSizes();
	(methodLabel->address = startAddress);
	size = generateInstructionsAt(startAddress);
	endAddress = outputInstructionsAt(startAddress);
	assert((startAddress + size) == endAddress);
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	return startAddress;
}


/*	Code entry closed PIC full or miss to an instance of a young class or to a
	young target method.
	Attempt to patch the send site to an open PIC. Answer if the attempt
	succeeded; in fact it will
	only return if the attempt failed.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */

	/* Cogit>>#patchToOpenPICFor:numArgs:receiver: */
sqInt
patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver)
{
    sqInt extent;
    CogMethod *oPIC;
    sqInt outerReturn;


	/* See if an Open PIC is already available. */
	outerReturn = stackTop();
	oPIC = openPICWithSelector(selector);
	if (!oPIC) {
		/* otherwise attempt to create an Open PIC. */
		oPIC = cogOpenPICSelectornumArgs(selector, numArgs);
		if ((((((sqInt)oPIC)) >= MaxNegativeErrorCode) && ((((sqInt)oPIC)) <= -1))) {
			/* For some reason the PIC couldn't be generated, most likely a lack of code memory. */
			if ((((sqInt)oPIC)) == InsufficientCodeSpace) {
				callForCogCompiledCodeCompaction();
			}
			return 0;
		}
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	extent = rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, selector, mframeHomeMethodExport()), (((sqInt)oPIC)) + cmEntryOffset);
	flushICacheFromto(backEnd, (((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
	flushICacheFromto(backEnd, ((usqInt)oPIC), (((usqInt)oPIC)) + openPICSize);
	executeCogMethodfromLinkedSendWithReceiver(oPIC, receiver);
	return 1;
}


/*	This value is used to decide between MNU processing
	or interpretation in the closed PIC aborts. */

	/* Cogit>>#picAbortDiscriminatorValue */
static sqInt
picAbortDiscriminatorValue(void)
{
	return 0;
}


/*	Answer the start of the abort sequence for invoking the interpreter in a
	closed PIC.
 */

	/* Cogit>>#picInterpretAbortOffset */
static sqInt
picInterpretAbortOffset(void)
{
	return (interpretOffset()) - (callInstructionByteSize(backEnd));
}


/*	useful for debugging */

	/* Cogit>>#printCogMethodFor: */
void
printCogMethodFor(void *address)
{
    CogMethod *cogMethod;

	cogMethod = methodFor(address);
	if (cogMethod == null) {
		if ((codeEntryFor(address)) == null) {
			print("not a method");
			cr();
		}
		else {
			print("trampoline ");
			print(codeEntryNameFor(address));
			cr();
		}
	}
	else {
		printCogMethod(cogMethod);
	}
}


/*	useful for debugging */

	/* Cogit>>#printTrampolineTable */
void
printTrampolineTable(void)
{
    sqInt i;

	for (i = 0; i < trampolineTableIndex; i += 2) {
		fprintf(getTranscript(),
				"%p: %s\n",
				((void *)(trampolineAddresses[i + 1])),
				((char *)(trampolineAddresses[i])));
	}
}

	/* Cogit>>#processorHasDivQuoRemAndMClassIsSmallInteger */
static sqInt
processorHasDivQuoRemAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#processorHasMultiplyAndMClassIsSmallInteger */
static sqInt
processorHasMultiplyAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#recordGeneratedRunTime:address: */
static NoDbgRegParms void
recordGeneratedRunTimeaddress(char *aString, sqInt address)
{
	assert((trampolineTableIndex + 2) <= (NumTrampolines * 2));
	trampolineAddresses[trampolineTableIndex] = aString;
	trampolineAddresses[trampolineTableIndex + 1] = (((char *) address));
	/* self printTrampolineTable */
	trampolineTableIndex += 2;
}


/*	This one for C support code. */

	/* Cogit>>#recordPrimTraceFunc */
int
recordPrimTraceFunc(void)
{
	return recordPrimTrace();
}

	/* Cogit>>#recordRunTimeObjectReferences */
static void
recordRunTimeObjectReferences(void)
{
    sqInt i;
    AbstractInstruction *instruction;

	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		if (((instruction->annotation)) == IsObjectReference) {
			assert(runtimeObjectRefIndex < NumObjRefsInRuntime);
			assert(!hasYoungReferent);
			if (hasYoungReferent) {
				error("attempt to generate run-time routine containing young object reference.  Cannot initialize Cogit run-time.");
			}
			objectReferencesInRuntime[runtimeObjectRefIndex] = (((usqInt)(((instruction->address)) + ((instruction->machineCodeSize)))));
			runtimeObjectRefIndex += 1;
		}
	}
}


/*	N.B. (self registerMaskFor: NoReg) = 0 */

	/* Cogit>>#registerMaskFor: */
static NoDbgRegParms sqInt
registerMaskFor(sqInt reg)
{
	return ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
}

	/* Cogit>>#registerMaskFor:and: */
static NoDbgRegParms sqInt
registerMaskForand(sqInt reg1, sqInt reg2)
{
	return (1ULL << reg1) | (1ULL << reg2);
}

	/* Cogit>>#registerMaskFor:and:and:and: */
static NoDbgRegParms sqInt
registerMaskForandandand(sqInt reg1, sqInt reg2, sqInt reg3, sqInt reg4)
{
	return (((1ULL << reg1) | (1ULL << reg2)) | (1ULL << reg3)) | (1ULL << reg4);
}

	/* Cogit>>#relocateCallsAndSelfReferencesInMethod: */
static NoDbgRegParms void
relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt callDelta;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqLong refDelta;
    sqInt result;

	refDelta = (cogMethod->objectHeader);
	callDelta = refDelta;
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cogMethod)) + missOffset)) == ((isCMMethodEtAl(((CogBlockMethod *) cogMethod))
		? methodAbortTrampolineFor((cogMethod->cmNumArgs))
		: picAbortTrampolineFor((cogMethod->cmNumArgs)))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cogMethod)) + missOffset, -callDelta);
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask);
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = relocateIfCallOrMethodReferencemcpcdelta(annotation, ((char *) mcpc), ((void *)refDelta));
			if (result != 0) {
				goto l1;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	l1:	/* end mapFor:performUntil:arg: */;
}

	/* Cogit>>#relocateCallsInClosedPIC: */
static NoDbgRegParms void
relocateCallsInClosedPIC(CogMethod *cPIC)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt pc1;
    sqLong refDelta;
    CogMethod *targetMethod;

	refDelta = (cPIC->objectHeader);
	callDelta = refDelta;
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cPIC)) + missOffset)) == (picAbortTrampolineFor((cPIC->cmNumArgs))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cPIC)) + missOffset, -callDelta);
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));
		if (		/* begin containsAddress: */
			((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
			/* Interpret/MNU */
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			if (i == 1) {
				relocateJumpLongBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
			else {
				relocateJumpLongConditionalBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
		}
	}
	assert(((cPIC->cPICNumCases)) > 0);
	pc1 = (addressOfEndOfCaseinCPIC(2, cPIC)) + 7 /* begin loadPICLiteralByteSize */;
	/* begin relocateMethodReferenceBeforeAddress:by: */
	assert((((byteAt(pc1 - 6)) == 141)
	 && (((byteAt(pc1 - 5)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7))))
	 || (((byteAt(pc1 - 8)) == 141)
	 && (((byteAt(pc1 - 7)) | (modRMRO(((AbstractInstruction *) backEnd), 0, 0, 7))) == (modRMRO(((AbstractInstruction *) backEnd), ModRegInd, 5, 7)))));
	relocateJumpLongBeforeFollowingAddressby(backEnd, (((sqInt)cPIC)) + cPICEndOfCodeOffset, -callDelta);
}


/*	To placate the C static type system... */

	/* Cogit>>#relocateIfCallOrMethodReference:mcpc:delta: */
static NoDbgRegParms sqInt
relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt refDelta;
    sqInt sendTable;
    sqInt *sendTable1;
    CogMethod *targetMethod;
    sqInt unlinkedRoutine;

	offset = 0;
	sendTable = 0;
	refDelta = ((sqInt) refDeltaArg);
	callDelta = refDelta;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {
			/* send is not linked; just relocate */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
			return 0;
		}
		/* begin offsetAndSendTableFor:annotation:into: */
		if (annotation == IsSendCall) {
			offset1 = cmEntryOffset;
			sendTable1 = ordinarySendTrampolines;
		}
		else {
			if (annotation == IsDirectedSuperSend) {
				offset1 = cmNoCheckEntryOffset;
				sendTable1 = directedSuperSendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperBindingSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperBindingSendTrampolines;
				}
				else {
										assert(annotation == IsSuperSend);
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = superSendTrampolines;
;
				}
			}
		}
		targetMethod = ((CogMethod *) (entryPoint - offset1));
		if (!(((targetMethod->cmType)) == CMFree)) {
			/* send target not freed; just relocate. */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -(callDelta - ((targetMethod->objectHeader))));
			restorePICUsageCount(targetMethod);
			return 0;
		}
		unlinkedRoutine = sendTable1[((((targetMethod->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod->cmNumArgs)) : (NumSendTrampolines - 1))];
		unlinkedRoutine -= callDelta;
		rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), enumeratingCogMethod), unlinkedRoutine);
		return 0;
	}
	if (annotation == IsRelativeCall) {
		relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
		return 0;
	}
	if (annotation == IsAbsPCReference) {
		/* begin relocateMethodReferenceBeforeAddress:by: */
		assert((((byteAt((((sqInt)mcpc)) - 6)) == 141)
		 && (((byteAt((((sqInt)mcpc)) - 5)) | (modRMRO(backEnd, 0, 0, 7))) == (modRMRO(backEnd, ModRegInd, 5, 7))))
		 || (((byteAt((((sqInt)mcpc)) - 8)) == 141)
		 && (((byteAt((((sqInt)mcpc)) - 7)) | (modRMRO(backEnd, 0, 0, 7))) == (modRMRO(backEnd, ModRegInd, 5, 7)))));
	}
	return 0;
}


/*	to placate the C static type system... */

	/* Cogit>>#remapIfObjectRef:pc:hasYoung: */
static NoDbgRegParms sqInt
remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr)
{
    sqInt cacheTag;
    unsigned int cacheTag1;
    sqInt entryPoint;
    sqInt entryPoint1;
    sqInt literal;
    sqInt mappedCacheTag;
    sqInt mappedLiteral;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt tagCouldBeObj;
    sqInt tagCouldBeObj1;
    CogMethod *targetMethod;
    CogMethod *targetMethod1;

	cacheTag = 0;
	entryPoint = 0;
	tagCouldBeObj = 0;
	targetMethod = ((CogMethod *) 0);
	if (annotation == IsObjectReference) {
		literal = literalBeforeFollowingAddress(backEnd, ((usqInt)mcpc));
		if (couldBeObject(literal)) {
			mappedLiteral = remapObject(literal);
			if (literal != mappedLiteral) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				/* begin storeLiteral:atAnnotatedAddress:using: */
				storeLiteralbeforeFollowingAddress(backEnd, mappedLiteral, ((usqInt)mcpc));
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedLiteral))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTag1 = literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)));
		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
		if (tagCouldBeObj1
		 && (couldBeObject(cacheTag1))) {
			mappedCacheTag = remapObject(cacheTag1);
			if (cacheTag1 != mappedCacheTag) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheTagat(backEnd, mappedCacheTag, ((usqInt)mcpc));
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedCacheTag))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
		if (hasYoungPtr != 0) {
			/* Since the unlinking routines may rewrite the cacheTag to the send's selector, and
			   since they don't have the cogMethod to hand and can't add it to youngReferrers,
			   the method must remain in youngReferrers if the targetMethod's selector is young. */
			if (entryPoint1 > methodZoneBase) {
				/* It's a linked send. */
				/* begin targetMethodAndSendTableFor:annotation:into: */
				offset = 0;
				sendTable = 0;
				/* begin offsetAndSendTableFor:annotation:into: */
				if (annotation == IsSendCall) {
					offset1 = cmEntryOffset;
					sendTable1 = ordinarySendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperSendTrampolines;
					}
					else {
						if (annotation == IsDirectedSuperBindingSend) {
							offset1 = cmNoCheckEntryOffset;
							sendTable1 = directedSuperBindingSendTrampolines;
						}
						else {
														assert(annotation == IsSuperSend);
							offset1 = cmNoCheckEntryOffset;
							sendTable1 = superSendTrampolines;
;
						}
					}
				}
				targetMethod1 = ((CogMethod *) (entryPoint1 - offset1));
				if (isYoung((targetMethod1->selector))) {
					(*((sqInt *) hasYoungPtr) = 1);
				}
			}
		}
	}
	return 0;
}


/*	Remap a potential object reference from a closed PIC.
	This may be an object reference, an inline cache tag or null.
	Answer if the updated literal is young.
	mcpc is the address of the next instruction following either
	the load of the method literal or the compare of the class tag. */

	/* Cogit>>#remapMaybeObjRefInClosedPICAt: */
static NoDbgRegParms sqInt
remapMaybeObjRefInClosedPICAt(sqInt mcpc)
{
    sqInt object;
    sqInt subject;

	object = literalBeforeFollowingAddress(backEnd, mcpc);
	if (!(couldBeObject(object))) {
		return 0;
	}
	subject = remapOop(object);
	if (object != subject) {
		/* begin setCodeModified */
#    if DUAL_MAPPED_CODE_ZONE
		codeModified = 1;
#    else
		codeModified = 1;
#    endif

		storeLiteralbeforeFollowingAddress(backEnd, subject, mcpc);
	}
	return isYoungObject(subject);
}


/*	Rewrite the three values involved in a CPIC case. Used by the initialize &
	extend CPICs.
	c.f. expectedClosedPICPrototype: */
/*	write the obj ref/operand via the second ldr */

	/* Cogit>>#rewriteCPICCaseAt:tag:objRef:target: */
static NoDbgRegParms void
rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget)
{
    sqInt classTagPC;
    sqInt methodObjPC;

	methodObjPC = (followingAddress - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
	storeLiteralbeforeFollowingAddress(backEnd, newObjRef, methodObjPC);
	/* rewite the tag via the first ldr */
	classTagPC = followingAddress - (jumpLongConditionalByteSize(backEnd));
	storeLiteral32beforeFollowingAddress(backEnd, newTag, classTagPC);
	rewriteConditionalJumpLongAttarget(backEnd, followingAddress, newTarget);
}


/*	destReg := fromReg - subReg */

	/* Cogit>>#SubR:R:R: */
static NoDbgRegParms AbstractInstruction *
gSubRRR(sqInt subReg, sqInt fromReg, sqInt destReg)
{
    AbstractInstruction *first;

	assert(subReg != destReg);
	first = genoperandoperand(MoveRR, fromReg, destReg);
	genoperandoperand(SubRR, subReg, destReg);
	return first;
}


/*	Answer the number of clean blocks found in the literal frame */

	/* Cogit>>#scanForCleanBlocks */
static sqInt
scanForCleanBlocks(void)
{
    sqInt i;
    sqInt iLimiT;
    sqInt lit;
    sqInt numCleanBlocks;
    sqInt startPCOrNil;

	numCleanBlocks = 0;
	for (i = 1, iLimiT = (literalCountOf(methodObj)); i <= iLimiT; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (!(startPCOrNil == null)) {
			numCleanBlocks += 1;
		}
	}
	return numCleanBlocks;
}


/*	If a method is compiled to machine code via a block entry it won't have a
	selector. A subsequent send can find the method and hence fill in the
	selector. 
 */
/*	self disassembleMethod: cogMethod */

	/* Cogit>>#setSelectorOf:to: */
void
setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop)
{
	compilationBreakpointisMNUCase(aSelectorOop, 0);
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->selector = aSelectorOop);
	if (isYoung(aSelectorOop)) {
		ensureInYoungReferrers(cogMethod);
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#spanForCleanBlockStartingAt: */
static NoDbgRegParms sqInt
spanForCleanBlockStartingAt(sqInt startPC)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt pc;

	pc = startPC;
	end = numBytesOf(methodObj);
	while (pc <= end) {
		/* begin generatorForPC: */
		descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
		pc += (descriptor->numBytes);
		if ((descriptor->isReturn)) {
			return pc - startPC;
		}
	}
	error("couldn't locate end of clean block");
	return 0;
}

	/* Cogit>>#stackCheckOffsetOfBlockAt:isMcpc: */
static NoDbgRegParms usqInt
stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((((sqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset))) == mcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}


/*	Answer a fake value for the method oop in other than the first case in the
	PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address.
 */

	/* Cogit>>#subsequentPrototypeMethodOop */
static sqInt
subsequentPrototypeMethodOop(void)
{
	return (	/* begin addressIsInCurrentCompilation: */
		((((usqInt)0xBADA550)) >= ((methodLabel->address)))
	 && ((((usqInt)0xBADA550)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))
		? 0xDEADEAD
		: 0xBADA550);
}

	/* Cogit>>#TstCq:R: */
static NoDbgRegParms AbstractInstruction *
gTstCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(TstCqR, quickConstant, reg);
}

	/* Cogit>>#traceLinkedSendOffset */
sqInt
traceLinkedSendOffset(void)
{
	return (cmNoCheckEntryOffset + (callFullInstructionByteSize(backEnd)));
}

	/* Cogit>>#trampolineName:numArgs: */
static NoDbgRegParms char *
trampolineNamenumArgs(char *routinePrefix, sqInt numArgs)
{
    char *theString;


	/* begin trampolineName:numArgs:limit: */
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, ((((int) numArgs)) <= (NumSendTrampolines - 2)
		? '0' + (((int) numArgs))
		: 'N'));
	return theString;
}


/*	Malloc a string with the contents for the trampoline table */

	/* Cogit>>#trampolineName:numArgs:limit: */
static NoDbgRegParms char *
trampolineNamenumArgslimit(char *routinePrefix, int numArgs, sqInt argsLimit)
{
    char *theString;

	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, (numArgs <= argsLimit
		? '0' + numArgs
		: 'N'));
	return theString;
}

	/* Cogit>>#trampolineName:numRegArgs: */
static NoDbgRegParms char *
trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs)
{
    sqInt argsLimit;
    char *theString;

	argsLimit = numRegArgs();
	/* begin trampolineName:numArgs:limit: */
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, ((((int) numArgs)) <= argsLimit
		? '0' + (((int) numArgs))
		: 'N'));
	return theString;
}

	/* Cogit>>#unflagBecomeFlaggedMethods */
void
unflagBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#unknownBytecode */
static int
unknownBytecode(void)
{
	return EncounteredUnknownBytecode;
}


/*	Unlink all sends in cog methods. */

	/* Cogit>>#unlinkAllSends */
void
unlinkAllSends(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cogMethod = ((CogMethod *) methodZoneBase);
	voidOpenPICList();
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcignored(annotation, ((char *) mcpc), 0);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (!(((cogMethod->cmType)) == CMFree)) {
				freeMethod(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfFreeOrLinkedSend:pc:of: */
static NoDbgRegParms sqInt
unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if ((((targetMethod1->cmType)) == CMFree)
			 || (((targetMethod1->selector)) == (((sqInt) theSelector)))) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfInvalidClassSend:pc:ignored: */
static NoDbgRegParms sqInt
unlinkIfInvalidClassSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send, but maybe a super send or linked to an OpenPIC, in which case the cache tag will be a selector.... */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (!((				/* begin annotationIsForUncheckedEntryPoint: */
					(annotation == IsSuperSend)
				 || (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend))))
				 || (((targetMethod1->cmType)) == CMOpenPIC))) {
				if (!(isValidClassTag(literal32BeforeFollowingAddress(backEnd, ((usqInt)((((sqInt)mcpc)) - 5)))))) {
					/* begin unlinkSendAt:targetMethod:sendTable: */
					unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#          if DUAL_MAPPED_CODE_ZONE
					codeModified = 1;
#          else
					codeModified = 1;
#          endif

					rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
				}
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSendToFree:pc:ignored: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmType)) == CMFree) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfLinkedSend:pc:if: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg)
{
    sqInt (*criterion)(CogMethod *);
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	criterion = ((void *)criterionArg);
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (criterion(targetMethod1)) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:ignored: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			/* begin unlinkSendAt:targetMethod:sendTable: */
			unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			codeModified = 1;
#      endif

			rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:to: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod)
{
    sqInt entryPoint;
    sqInt offset;
    sqInt offset1;
    sqInt sendTable;
    sqInt *sendTable1;
    sqInt sendTable2;
    sqInt targetMethod;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	sendTable = 0;
	targetMethod = 0;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {
			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			offset = 0;
			sendTable2 = 0;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
												assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (targetMethod1 == theCogMethod) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	Unlink all sends in cog methods whose class tag is that of a forwarded
	class. 
 */

	/* Cogit>>#unlinkSendsLinkedForInvalidClasses */
void
unlinkSendsLinkedForInvalidClasses(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedPIC;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	cogMethod = ((CogMethod *) methodZoneBase);
	codeModified = (freedPIC = 0);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfInvalidClassSendpcignored(annotation, ((char *) mcpc), 0);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if ((((cogMethod->cmType)) == CMClosedPIC)
			 && (cPICHasForwardedClass(cogMethod))) {
				freeMethod(cogMethod);
				freedPIC = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedPIC) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {
			/* After possibly updating inline caches we need to flush the icache. */
			flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
		}
	}
}


/*	Unlink all sends in cog methods. Free all Closed PICs with the selector,
	or with an MNU case if isMNUSelector. First check if any method actually
	has the selector; if not there can't be any linked send to it. This
	routine (including descendents) is performance critical. It contributes
	perhaps 30% of entire execution time in Compiler recompileAll. */

	/* Cogit>>#unlinkSendsOf:isMNUSelector: */
void
unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt mustScanAndUnlink;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	cogMethod = ((CogMethod *) methodZoneBase);
	mustScanAndUnlink = 0;
	if (isMNUSelector) {
		while (cogMethod < (limitZony())) {
			if (!(((cogMethod->cmType)) == CMFree)) {
				if (				/* begin cpicHasMNUCase */
					((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				 && (((cogMethod->cmType)) == CMClosedPIC)) {
					assert(isCMClosedPIC(((CogBlockMethod *) cogMethod)));
					freeMethod(cogMethod);
					mustScanAndUnlink = 1;
				}
				else {
					if (((cogMethod->selector)) == selector) {
						mustScanAndUnlink = 1;
						if (((cogMethod->cmType)) == CMClosedPIC) {
							freeMethod(cogMethod);
						}
					}
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	else {
		while (cogMethod < (limitZony())) {
			if ((!(((cogMethod->cmType)) == CMFree))
			 && (((cogMethod->selector)) == selector)) {
				mustScanAndUnlink = 1;
				if (((cogMethod->cmType)) == CMClosedPIC) {
					freeMethod(cogMethod);
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	if (!mustScanAndUnlink) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfFreeOrLinkedSendpcof(annotation, ((char *) mcpc), ((CogMethod *) selector));
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {
		/* After possibly updating inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Unlink all sends in cog methods to free methods and/or pics. */

	/* Cogit>>#unlinkSendsToFree */
static void
unlinkSendsToFree(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendToFreepcignored(annotation, ((char *) mcpc), 0);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(noTargetsFreeInClosedPIC(cogMethod));
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {
		/* After possibly updating inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Unlink all sends in cog methods to methods with a machine code
	primitive, and free machine code primitive methods if freeIfTrue.
	To avoid having to scan PICs, free any and all PICs */

	/* Cogit>>#unlinkSendsToMethodsSuchThat:AndFreeIf: */
void
unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedSomething;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = (freedSomething = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (freeIfTrue
			 && (criterion(cogMethod))) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
			else {
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask);
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = unlinkIfLinkedSendpcif(annotation, ((char *) mcpc), ((CogMethod *) criterion));
						if (result != 0) {
							goto l1;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
						}
					}
					map -= 1;
				}
	l1:	/* end mapFor:performUntil:arg: */;
			}
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedSomething) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {
			/* After possibly updating inline caches we need to flush the icache. */
			flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
		}
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Unlink all sends in cog methods to a particular target method.
	If targetMethodObject isn't actually a method (perhaps being
	used via invokeAsMethod) then there's nothing to do. */

	/* Cogit>>#unlinkSendsTo:andFreeIf: */
void
unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedPIC;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *targetMethod;

	if (!((isOopCompiledMethod(targetMethodObject))
		 && (methodHasCogMethod(targetMethodObject)))) {
		return;
	}
	targetMethod = cogMethodOf(targetMethodObject);
	if (!methodZoneBase) {
		return;
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	codeModified = (freedPIC = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask);
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcto(annotation, ((char *) mcpc), targetMethod);
					if (result != 0) {
						goto l1;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
					}
				}
				map -= 1;
			}
	l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if ((((cogMethod->cmType)) == CMClosedPIC)
			 && (cPICHasTarget(cogMethod, targetMethod))) {
				freeMethod(cogMethod);
				freedPIC = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freeIfTrue) {
		freeMethod(targetMethod);
	}
	if (freedPIC) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {
			/* After possibly updating inline caches we need to flush the icache. */
			flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
		}
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#voidCogCompiledCode */
void
voidCogCompiledCode(void)
{
    CogMethod *cogMethod;


	/* begin clearCogCompiledCode */
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) >= CMMethod) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress);
	youngReferrers = (limitAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Access for the object representations when they need to prepend code to
	trampolines. 
 */
/*	Eliminate stale dependent info. */

	/* Cogit>>#zeroOpcodeIndex */
static void
zeroOpcodeIndex(void)
{
    sqInt i;

	for (i = 0; i < opcodeIndex; i += 1) {
		((abstractOpcodes[i]).dependent = null);
	}
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
}

	/* CogMethod>>#counters */
static NoDbgRegParms sqInt
counters(CogMethod *self_in_CogMethod)
{
	return 0;
}

	/* CogMethodZone>>#addToOpenPICList: */
static NoDbgRegParms void
addToOpenPICList(CogMethod *anOpenPIC)
{
	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	assert((openPICList == null)
	 || (isCMOpenPIC(((CogBlockMethod *) openPICList))));
	assertValidDualZoneWriteAddress(anOpenPIC);
	(anOpenPIC->nextOpenPIC = ((usqInt)openPICList));
	openPICList = ((CogMethod *) ((((usqInt)anOpenPIC)) - (getCodeToDataDelta())));
}

	/* CogMethodZone>>#addToYoungReferrers: */
static NoDbgRegParms void
addToYoungReferrers(CogMethod *writableCogMethod)
{
	assertValidDualZoneWriteAddress(writableCogMethod);
	assert((occurrencesInYoungReferrers(writableCogMethod)) == 0);
	assert((writableCogMethod->cmRefersToYoung));
	assert((youngReferrers <= limitAddress)
	 && (youngReferrers >= (limitAddress - (methodCount * BytesPerWord))));
	if (!(asserta((limitAddress - (methodCount * BytesPerWord)) >= mzFreeStart))) {
		error("no room on youngReferrers list");
	}
	youngReferrers -= BytesPerWord;
	codeLongAtput(youngReferrers, (((usqInt)writableCogMethod)) - (getCodeToDataDelta()));
}

	/* CogMethodZone>>#allocate: */
static NoDbgRegParms usqInt
allocate(sqInt numBytes)
{
    usqInt allocation;
    sqInt roundedBytes;

	roundedBytes = (numBytes + 7) & -8;
	if ((mzFreeStart + roundedBytes) >= ((((limitAddress - (methodCount * BytesPerWord)) < allocationThreshold) ? (limitAddress - (methodCount * BytesPerWord)) : allocationThreshold))) {
		return 0;
	}
	allocation = mzFreeStart;
	mzFreeStart += roundedBytes;
	methodCount += 1;
	return allocation;
}


/*	Answer the method containing mcpc for the purposes of code zone
	compaction, where mcpc is actually the value of instructionPointer at the
	time of a compaction. */

	/* CogMethodZone>>#cogMethodContaining: */
CogMethod *
cogMethodContaining(usqInt mcpc)
{
    CogMethod *cogMethod;
    CogMethod *prevMethod;

	if (mcpc > limitAddress) {
		return null;
	}
	if (mcpc < baseAddress) {
		/* begin assertMcpcIsPrimReturn: */
		assert((mcpc == cePrimReturnEnterCogCode)
		 || (mcpc == cePrimReturnEnterCogCodeProfiling));
		return null;
	}
	assert(mcpc < (freeStart()));
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mcpc) {
		prevMethod = cogMethod;
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	assert((prevMethod)
	 && ((mcpc == ((((usqInt)prevMethod)) + ((prevMethod->stackCheckOffset))))
	 || ((mcpcisAtStackCheckOfBlockMethodIn(mcpc, prevMethod))
	 || (((primitiveIndexOfMethodheader((prevMethod->methodObject), (prevMethod->methodHeader))) > 0)
	 || ((isCallPrecedingReturnPC(backEnd(), mcpc))
	 && ((callTargetFromReturnAddress(backEnd(), mcpc)) == (ceCheckForInterruptTrampoline())))))));
	return prevMethod;
}

	/* CogMethodZone>>#compactCompiledCode */
static void
compactCompiledCode(void)
{
    unsigned short bytes;
    CogMethod *dest;
    sqLong objectHeaderValue;
    CogMethod *source;
    CogMethod *writableVersion;

	compactionInProgress = 1;
	methodCount = 0;
	objectHeaderValue = nullHeaderForMachineCodeMethod();
	source = ((CogMethod *) baseAddress);
	voidOpenPICList();
	voidUnpairedMethodList();
	while ((source < (limitZony()))
	 && (!(((source->cmType)) == CMFree))) {
		assert((cogMethodDoesntLookKosher(source)) == 0);
		writableVersion = ((CogMethod *) ((((usqInt)source)) + codeToDataDelta));
		(writableVersion->objectHeader = objectHeaderValue);
		if (((source->cmUsageCount)) > 0) {
			(writableVersion->cmUsageCount = ((source->cmUsageCount)) / 2);
		}
		/* begin maybeLinkOnUnpairedMethodList: */
		if (((writableVersion->cmType)) == CMClosedPIC) {
			(writableVersion->blockEntryOffset = 0);
		}
		if (((source->cmType)) == CMOpenPIC) {
			addToOpenPICList(writableVersion);
		}
		methodCount += 1;
		source = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)source)) + ((source->blockSize)))));
	}
	if (source >= (limitZony())) {
		haltmsg("no free methods; cannot compact.");
		return;
	}
	dest = source;
	while (source < (limitZony())) {
		assert((maybeFreeCogMethodDoesntLookKosher(source)) == 0);
		bytes = (source->blockSize);
		if (!(((source->cmType)) == CMFree)) {
			methodCount += 1;
			codeMemmove(dest, source, bytes);
			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), (((usqInt)dest)) + bytes);
			}
#      endif

			((writableVersion = ((CogMethod *) ((((usqInt)dest)) + codeToDataDelta)))->objectHeader = objectHeaderValue);
			if (((dest->cmType)) >= CMMethod) {
				/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
				   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
				/* Only update the original method's header if it is referring to this CogMethod. */
				if ((((sqInt)(rawHeaderOf((dest->methodObject))))) == (((sqInt)source))) {
					rawHeaderOfput((dest->methodObject), ((sqInt)dest));
				}
				else {
					assert((noAssertMethodClassAssociationOf((dest->methodObject))) == (nilObject()));
					/* begin linkOnUnpairedMethodList: */
				}
			}
			else {
				/* begin clearSavedPICUsageCount: */
				if (((writableVersion->cmType)) == CMClosedPIC) {
					(writableVersion->blockEntryOffset = 0);
				}
				if (((dest->cmType)) == CMOpenPIC) {
					addToOpenPICList(writableVersion);
				}
			}
			if (((dest->cmUsageCount)) > 0) {
				(writableVersion->cmUsageCount = ((dest->cmUsageCount)) / 2);
			}
			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), ((usqInt)(dest + 1)));
			}
#      endif

			dest = ((CogMethod *) ((((usqInt)dest)) + bytes));
		}
		source = ((CogMethod *) ((((usqInt)source)) + bytes));
	}
	mzFreeStart = ((usqInt)dest);
	methodBytesFreedSinceLastCompaction = 0;
	compactionInProgress = 0;
}

	/* CogMethodZone>>#ensureInYoungReferrers: */
static NoDbgRegParms void
ensureInYoungReferrers(CogMethod *cogMethod)
{
    CogMethod *writableMethod;

	assertValidDualZoneReadAddress(cogMethod);
	if (!((cogMethod->cmRefersToYoung))) {
		assert((occurrencesInYoungReferrers(cogMethod)) == 0);
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		((writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmRefersToYoung = 1);
		addToYoungReferrers(writableMethod);
	}
}

	/* CogMethodZone>>#followForwardedLiteralsInOpenPICList */
static void
followForwardedLiteralsInOpenPICList(void)
{
    CogMethod *openPIC;

	openPIC = openPICList;
	while (openPIC) {
		followForwardedLiteralsImplementationIn(openPIC);
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	}
	pruneYoungReferrers();
}

	/* CogMethodZone>>#freeMethod: */
static NoDbgRegParms void
freeMethod(CogMethod *cogMethod)
{
    CogMethod *writableMethod;

	assert(!((isCMFree(((CogBlockMethod *) cogMethod)))));
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	if (((cogMethod->cmType)) >= CMMethod) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
		}
		else {
			/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
			   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
			/* Only reset the original method's header if it is referring to this CogMethod. */
			if ((((sqInt)(rawHeaderOf((cogMethod->methodObject))))) == (((sqInt)cogMethod))) {
				rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
			}
			else {
				assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
			}
		}
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		removeFromOpenPICList(cogMethod);
	}
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmRefersToYoung = 0);
	(writableMethod->cmType = CMFree);
	methodBytesFreedSinceLastCompaction += (cogMethod->blockSize);
}


/*	Free methods, preferring older methods for compaction, up to some
	fraction, currently a quarter.
 */

	/* CogMethodZone>>#freeOlderMethodsForCompaction */
static void
freeOlderMethodsForCompaction(void)
{
    sqInt amountToFree;
    CogMethod *cogMethod;
    sqInt freeableUsage;
    sqInt freedSoFar;
    sqInt initialFreeSpace;
    sqInt zoneSize;

	zoneSize = ((((limitAddress) < allocationThreshold) ? (limitAddress) : allocationThreshold)) - baseAddress;
	initialFreeSpace = (((((limitAddress) < allocationThreshold) ? (limitAddress) : allocationThreshold)) - mzFreeStart) + methodBytesFreedSinceLastCompaction;
	freedSoFar = initialFreeSpace;
	/* 4 needs to be e.g. a start-up parameter */
	amountToFree = zoneSize / 4;
	freeableUsage = 0;
	do {
		cogMethod = ((CogMethod *) baseAddress);
		while (((((usqInt)cogMethod)) < mzFreeStart)
		 && (freedSoFar < amountToFree)) {
			if (			/* begin shouldFreeMethod:given: */
				(((cogMethod->cmType)) >= CMMethod
				? ((cogMethod->cmUsageCount)) <= freeableUsage
				: (!(((cogMethod->cmType)) == CMFree))
					 && (((cogMethod->cmUsageCount)) == 0))) {
				freeMethod(cogMethod);
				freedSoFar += (cogMethod->blockSize);
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	} while((freedSoFar < amountToFree)
		 && (((freeableUsage += 1)) < CMMaxUsageCount));
}


/*	Answer that all entries in youngReferrers are in-use and have the
	cmRefersToYoung flag set.
	Used to check that the youngreferrers pruning routines work correctly. */

	/* CogMethodZone>>#kosherYoungReferrers */
sqInt
kosherYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;
    CogMethod *prevMethod;

	if ((youngReferrers > limitAddress)
	 || (youngReferrers < mzFreeStart)) {
		return 0;
	}
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!((cogMethod->cmRefersToYoung))) {
				return 0;
			}
			if ((occurrencesInYoungReferrers(cogMethod)) != 1) {
				return 0;
			}
		}
		pointer += BytesPerWord;
	}
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		prevMethod = cogMethod;
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((occurrencesInYoungReferrers(cogMethod)) != (((cogMethod->cmRefersToYoung)
				? 1
				: 0))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (cogMethod == prevMethod) {
			return 0;
		}
	}
	return 1;
}


/*	For assert checking... */

	/* CogMethodZone>>#mcpc:isAtStackCheckOfBlockMethodIn: */
static NoDbgRegParms sqInt
mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod)
{
	if (((cogMethod->blockEntryOffset)) == 0) {
		return 0;
	}
	return (blockDispatchTargetsForperformarg(cogMethod, stackCheckOffsetOfBlockAtisMcpc, mcpc)) != 0;
}

	/* CogMethodZone>>#methodFor: */
CogMethod *
methodFor(void *address)
{
    CogMethod *cogMethod;
    CogMethod *nextMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((cogMethod < (limitZony()))
	 && ((((usqInt)cogMethod)) <= (((usqInt)address)))) {
		nextMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (nextMethod == cogMethod) {
			return null;
		}
		if (((((usqInt)address)) >= (((usqInt)cogMethod)))
		 && ((((usqInt)address)) < (((usqInt)nextMethod)))) {
			return cogMethod;
		}
		cogMethod = nextMethod;
	}
	return null;
}

	/* CogMethodZone>>#methodsCompiledToMachineCodeInto: */
sqInt
methodsCompiledToMachineCodeInto(sqInt arrayObj)
{
    CogMethod *cogMethod;
    sqInt methodIndex;

	methodIndex = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			storePointerUncheckedofObjectwithValue(methodIndex, arrayObj, (cogMethod->methodObject));
			methodIndex += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return methodIndex;
}

	/* CogMethodZone>>#numMethods */
sqInt
numMethods(void)
{
	return methodCount;
}

	/* CogMethodZone>>#numMethodsOfType: */
sqInt
numMethodsOfType(sqInt cogMethodType)
{
    CogMethod *cogMethod;
    sqInt n;

	n = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cogMethodType) {
			n += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return n;
}

	/* CogMethodZone>>#occurrencesInYoungReferrers: */
static NoDbgRegParms sqInt
occurrencesInYoungReferrers(CogMethod *cogMethod)
{
    sqInt count;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	count = 0;
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		if ((((sqInt)cogMethod)) == (longAt(pointer))) {
			count += 1;
		}
		pointer += BytesPerWord;
	}
	return count;
}

	/* CogMethodZone>>#openPICWithSelector: */
static NoDbgRegParms CogMethod *
openPICWithSelector(sqInt aSelector)
{
    CogMethod *openPIC;

	openPIC = openPICList;
	do {
		if ((openPIC == null)
		 || (((openPIC->selector)) == aSelector)) {
			return openPIC;
		}
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	Some methods have been freed. Compute how much each survivor needs to
	move during the ensuing compaction and record it in the objectHeader
	field. 
	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#planCompaction */
static void
planCompaction(void)
{
    CogMethod *cogMethod;
    sqInt delta;

	delta = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) == CMFree) {
			delta -= (cogMethod->blockSize);
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->objectHeader = delta);
			savePICUsageCount(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethods */
void
printCogMethods(void)
{
    CogMethod *cogMethod;
    sqInt nb;
    sqInt nc;
    sqInt nf;
    sqInt nm;
    sqInt no;
    sqInt nu;


	/* begin printCogMethodsSummarizing: */
	nm = (nb = (nc = (no = (nf = (nu = 0)))));
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		printCogMethod(cogMethod);
		switch ((cogMethod->cmType)) {
		case CMFree:
			nf += 1;
			break;
		case CMMethod:
			nm += 1;
			break;
		case CMMethodFlaggedForBecome:
			nb += 1;
			break;
		case CMClosedPIC:
			nc += 1;
			break;
		case CMOpenPIC:
			no += 1;
			break;
		default:
			nu += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	print("CMMethod ");
	printNum(nm);
	if (nb > 0) {
		print(" (flagged for become: ");
		printNum(nb);
		print(")");
	}
	print(" CMClosedPIC ");
	printNum(nc);
	print(" CMOpenPIC ");
	printNum(no);
	print(" CMFree ");
	printNum(nf);
	if (nu > 0) {
		print(" UNKNOWN ");
		printNum(nu);
	}
	print(" total ");
	printNum(((((nm + nc) + no) + nf) + nu) + nb);
	cr();
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsOfType: */
void
printCogMethodsOfType(sqInt cmType)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cmType) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithMethod: */
void
printCogMethodsWithMethod(sqInt methodOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->methodObject)) == methodOop)) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithPrimitive: */
void
printCogMethodsWithPrimitive(sqInt primIdx)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (primIdx == (primitiveIndexOfMethodheader((cogMethod->methodObject), (cogMethod->methodHeader))))) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithSelector: */
void
printCogMethodsWithSelector(sqInt selectorOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->selector)) == selectorOop)) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogYoungReferrers */
void
printCogYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!((cogMethod->cmRefersToYoung))) {
			print("*");
		}
		if (((cogMethod->cmType)) == CMFree) {
			print("!");
		}
		if (!(((cogMethod->cmRefersToYoung))
			 && (!(((cogMethod->cmType)) == CMFree)))) {
			print(" ");
		}
		printCogMethod(cogMethod);
		pointer += BytesPerWord;
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printOpenPICList */
sqInt
printOpenPICList(void)
{
    sqInt n;
    CogMethod *openPIC;


	/* begin printOpenPICListSummarizing: */
	n = 0;
	openPIC = openPICList;
	while (!(openPIC == null)) {
		n += 1;
		printCogMethod(openPIC);
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	}
	return n;
}

	/* CogMethodZone>>#pruneYoungReferrers */
static sqInt
pruneYoungReferrers(void)
{
    usqInt dest;
    usqInt next;
    usqInt source;

	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && (((((CogMethod *) (longAt(next))))->cmRefersToYoung)))) break;
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		if (((((CogMethod *) (longAt(source))))->cmRefersToYoung)) {
			assert(source < (dest - BytesPerWord));
			if (!(next == null)) {
				/* convenient first-time flag */
				next = null;
				/* begin ensureWritableCodeZone */
#        if !DUAL_MAPPED_CODE_ZONE
#        endif

			}
			codeLongAtput((dest -= BytesPerWord), longAt(source));
		}
		source -= BytesPerWord;
	}
	youngReferrers = dest;
	assert(kosherYoungReferrers());
	return 0;
}

	/* CogMethodZone>>#relocateAndPruneYoungReferrers */
static sqInt
relocateAndPruneYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt dest;
    usqInt next;
    usqInt source;

	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && ((!(isCMFree(((CogBlockMethod *) ((cogMethod = ((CogMethod *) (longAt(next)))))))))
		 && ((cogMethod->cmRefersToYoung))))) break;
		if (((cogMethod->objectHeader)) != 0) {
			codeLongAtput(next, (((sqInt)cogMethod)) + ((cogMethod->objectHeader)));
		}
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		cogMethod = ((CogMethod *) (longAt(source)));
		if ((!(((cogMethod->cmType)) == CMFree))
		 && ((cogMethod->cmRefersToYoung))) {
			assert(source < (dest - BytesPerWord));
			if (((cogMethod->objectHeader)) != 0) {
				cogMethod = ((CogMethod *) ((((sqInt)cogMethod)) + (((sqInt)((cogMethod->objectHeader))))));
			}
			codeLongAtput((dest -= BytesPerWord), ((sqInt)cogMethod));
		}
		source -= BytesPerWord;
	}
	/* this assert must be deferred until after compaction.  See the end of compactCogCompiledCode */
	/* self assert: self kosherYoungReferrers */
	youngReferrers = dest;
	return 0;
}


/*	All surviving methods have had the amount they are going to relocate by
	stored in their objectHeader fields. Relocate all relative calls so that
	after the compaction of both the method containing each call and the call
	target the calls invoke the same target. */

	/* CogMethodZone>>#relocateMethodsPreCompaction */
static sqInt
relocateMethodsPreCompaction(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				relocateCallsInClosedPIC(cogMethod);
			}
			else {
				relocateCallsAndSelfReferencesInMethod(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	relocateAndPruneYoungReferrers();
	return 1;
}

	/* CogMethodZone>>#removeFromOpenPICList: */
static NoDbgRegParms sqInt
removeFromOpenPICList(CogMethod *anOpenPIC)
{
    CogMethod *prevPIC;

	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	if (!openPICList) {
		return null;
	}
	assert((isCMOpenPIC(((CogBlockMethod *) openPICList)))
	 && ((!((openPICList->nextOpenPIC)))
	 || (isCMOpenPIC(((CogBlockMethod *) (((CogMethod *) ((openPICList->nextOpenPIC)))))))));
	if (anOpenPIC == openPICList) {
		/* N.B. Use self rather than coInterpreter to avoid attempting to cast nil.
		   Conversion to CogMethod done in the nextOpenPIC accessor. */
		openPICList = ((CogMethod *) ((anOpenPIC->nextOpenPIC)));
		return null;
	}
	prevPIC = openPICList;
	do {
		assert((prevPIC != null)
		 && (isCMOpenPIC(((CogBlockMethod *) prevPIC))));
		if (((prevPIC->nextOpenPIC)) == (((usqInt)anOpenPIC))) {
			((((CogMethod *) ((((usqInt)prevPIC)) + codeToDataDelta)))->nextOpenPIC = (anOpenPIC->nextOpenPIC));
			return null;
		}
		prevPIC = ((CogMethod *) ((prevPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#restorePICUsageCount: */
static NoDbgRegParms void
restorePICUsageCount(CogMethod *cogMethod)
{
	if ((((cogMethod->cmType)) == CMClosedPIC)
	 && (((cogMethod->blockEntryOffset)) != 0)) {
		(cogMethod->cmUsageCount = (cogMethod->blockEntryOffset));
		(cogMethod->blockEntryOffset = 0);
	}
}


/*	Determine the default alignment for the start of a CogMethod, which in
	turn determines the size of the mask used to distinguish the checked and
	unchecked entry-points, used to distinguish normal and super sends on
	method unlinking.
	This is passed onto the backEnd to allow processors with coarse
	instructions (ARM) to increase the alignment if required. */

	/* CogMethodZone>>#roundUpLength: */
static NoDbgRegParms sqInt
roundUpLength(sqInt numBytes)
{
	return roundUpToMethodAlignment(backEnd(), numBytes);
}


/*	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#savePICUsageCount: */
static NoDbgRegParms void
savePICUsageCount(CogMethod *cogMethod)
{
	if (((cogMethod->cmType)) == CMClosedPIC) {
		(cogMethod->blockEntryOffset = (cogMethod->cmUsageCount));
		(cogMethod->cmUsageCount = 0);
	}
}

	/* CogMethodZone>>#voidOpenPICList */
static void
voidOpenPICList(void)
{
	openPICList = null;
}

	/* CogMethodZone>>#voidUnpairedMethodList */
static void
voidUnpairedMethodList(void)
{
}

	/* CogMethodZone>>#voidYoungReferrersPostTenureAll */
static void
voidYoungReferrersPostTenureAll(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			(cogMethod->cmRefersToYoung = 0);
		}
		pointer += BytesPerWord;
	}
	youngReferrers = limitAddress;
}

	/* CogMethodZone>>#whereIsMaybeCodeThing: */
char *
whereIsMaybeCodeThing(sqInt anOop)
{
	if (oopisGreaterThanOrEqualToandLessThan(anOop, codeBase, limitAddress)) {
		if (oopisLessThan(anOop, minCogMethodAddress())) {
			return " is in generated runtime";
		}
		if (oopisLessThan(anOop, mzFreeStart)) {
			return " is in generated methods";
		}
		if (oopisLessThan(anOop, youngReferrers)) {
			return " is in code zone";
		}
		return " is in young referrers";
	}
	return null;
}

	/* CogMethodZone>>#zoneAlignment */
static sqInt
zoneAlignment(void)
{
	return 8;
}

	/* CogObjectRepresentation>>#checkValidObjectReference: */
static NoDbgRegParms sqInt
checkValidObjectReference(sqInt anOop)
{
	return (!(isImmediate(anOop)))
	 && ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentation>>#genCmpClassFloatCompactIndexR: */
static NoDbgRegParms AbstractInstruction *
genCmpClassFloatCompactIndexR(sqInt reg)
{
	return genoperandoperand(CmpCqR, ClassFloatCompactIndex, reg);
}

	/* CogObjectRepresentation>>#genCmpClassMethodContextCompactIndexR: */
static NoDbgRegParms AbstractInstruction *
genCmpClassMethodContextCompactIndexR(sqInt reg)
{
	return genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, reg);
}

	/* CogObjectRepresentation>>#generateLowcodeObjectTrampolines */
static void
generateLowcodeObjectTrampolines(void)
{
	ceFloatObjectOfTrampoline = genTrampolineForcalledfloatArgresult(floatObjectOf, "ceFloatObjectOfTrampoline", DPFPReg0, TempReg);
	ceFloatValueOfTrampoline = genTrampolineForcalledargfloatResult(floatValueOf, "ceFloatValueOfTrampoline", ReceiverResultReg, DPFPReg0);
	ceInstantiateClassIndexableSizeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(instantiateClassindexableSize, "ceInstantiateClassIndexableSizeTrampoline", 2, Arg0Reg, Arg1Reg, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	ceInstantiateClassTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(instantiateClassindexableSize, "ceInstantiateClassTrampoline", 2, ReceiverResultReg, 0, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	ceByteSizeOfTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(byteSizeOf, "ceByteSizeOfTrampoline", 1, Arg0Reg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
		cePositive64BitIntegerTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(positive64BitIntegerFor, "cePositive64BitIntegerTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	cePositive64BitValueOfTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(positive64BitValueOf, "cePositive64BitValueOfTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	ceSigned64BitIntegerTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(signed64BitIntegerFor, "ceSigned64BitIntegerTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
	ceSigned64BitValueOfTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(signed64BitValueOf, "ceSigned64BitValueOfTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, TempReg, 0);
}


/*	Get the method header (first word) of a CompiledMethod into headerReg.
	Deal with the method possibly being cogged. */

	/* CogObjectRepresentation>>#genGetMethodHeaderOf:into:scratch: */
static NoDbgRegParms sqInt
genGetMethodHeaderOfintoscratch(sqInt methodReg, sqInt headerReg, sqInt scratchReg)
{
    AbstractInstruction *jumpNotCogged;
    sqInt offset;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize, methodReg, headerReg);
	jumpNotCogged = genJumpSmallInteger(headerReg);
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodHeader);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, headerReg, headerReg);
	jmpTarget(jumpNotCogged, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Generate a compare and branch to test if aRegister and bRegister contains
	other than SmallIntegers,
	i.e. don't branch if both aRegister and bRegister contain SmallIntegers.
	Answer the jump. Destroy scratchRegister if required. */

	/* CogObjectRepresentation>>#genJumpNotSmallIntegersIn:and:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallIntegersInandscratch(sqInt aRegister, sqInt bRegister, sqInt scratchRegister)
{

	/* begin MoveR:R: */
	genoperandoperand(MoveRR, aRegister, scratchRegister);
	genoperandoperand(AndRR, bRegister, scratchRegister);
	return genJumpNotSmallInteger(scratchRegister);
}


/*	TODO: Optimize this one avoiding the trampoline */

	/* CogObjectRepresentation>>#genLcByteSizeOf:to: */
static NoDbgRegParms void
genLcByteSizeOfto(sqInt oop, sqInt resultRegister)
{
    AbstractInstruction *abstractInstruction;

	if (oop != Arg0Reg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, oop, Arg0Reg);
	}
	abstractInstruction = genoperand(Call, ceByteSizeOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, resultRegister);
	ssPushNativeRegister(resultRegister);
}

	/* CogObjectRepresentation>>#genLcFloat32:toOop: */
static NoDbgRegParms void
genLcFloat32toOop(sqInt value, sqInt object)
{
    AbstractInstruction *abstractInstruction;


	/* begin ConvertRs:Rd: */
	genoperandoperand(ConvertRsRd, value, DPFPReg0);
	abstractInstruction = genoperand(Call, ceFloatObjectOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, object);
	ssPushRegister(object);
}

	/* CogObjectRepresentation>>#genLcFloat64:toOop: */
static NoDbgRegParms void
genLcFloat64toOop(sqInt value, sqInt object)
{
    AbstractInstruction *abstractInstruction;

	if (value != DPFPReg0) {
		/* begin MoveRd:Rd: */
		genoperandoperand(MoveRdRd, value, DPFPReg0);
	}
	abstractInstruction = genoperand(Call, ceFloatObjectOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, object);
	ssPushRegister(object);
}

	/* CogObjectRepresentation>>#genLcInstantiateOop: */
static NoDbgRegParms void
genLcInstantiateOop(sqInt classOop)
{
    AbstractInstruction *abstractInstruction;

	if (classOop != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, classOop, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceInstantiateClassTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, classOop);
	ssPushRegister(classOop);
}

	/* CogObjectRepresentation>>#genLcInstantiateOop:constantIndexableSize: */
static NoDbgRegParms void
genLcInstantiateOopconstantIndexableSize(sqInt classOop, sqInt indexableSize)
{
    AbstractInstruction *abstractInstruction;

	if (classOop != Arg0Reg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, classOop, Arg0Reg);
	}
	genoperandoperand(MoveCqR, indexableSize, Arg1Reg);
	abstractInstruction = genoperand(Call, ceInstantiateClassIndexableSizeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, classOop);
	ssPushRegister(classOop);
}

	/* CogObjectRepresentation>>#genLcInstantiateOop:indexableSize: */
static NoDbgRegParms void
genLcInstantiateOopindexableSize(sqInt classOop, sqInt indexableSize)
{
    AbstractInstruction *abstractInstruction;

	if (classOop != Arg0Reg) {
		if (indexableSize == Arg0Reg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, indexableSize, TempReg);
		}
		genoperandoperand(MoveRR, classOop, Arg0Reg);
	}
	if (indexableSize != Arg1Reg) {
		if (indexableSize == Arg0Reg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, TempReg, Arg1Reg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, indexableSize, Arg1Reg);
		}
	}
	abstractInstruction = genoperand(Call, ceInstantiateClassIndexableSizeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, classOop);
	ssPushRegister(classOop);
}

	/* CogObjectRepresentation>>#genLcInt64ToOop: */
static NoDbgRegParms void
genLcInt64ToOop(sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (value != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceSigned64BitIntegerTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
}


/*	Put the arguments in the correct registers */

	/* CogObjectRepresentation>>#genLcInt64ToOop:highPart: */
static NoDbgRegParms void
genLcInt64ToOophighPart(sqInt valueLow, sqInt valueHigh)
{
    AbstractInstruction *abstractInstruction;

	if (valueLow != ReceiverResultReg) {
		if (valueHigh == ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, valueHigh, TempReg);
		}
		genoperandoperand(MoveRR, valueLow, ReceiverResultReg);
	}
	if (valueHigh != Arg0Reg) {
		if (valueHigh == ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, TempReg, Arg0Reg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, valueHigh, Arg0Reg);
		}
	}
	abstractInstruction = genoperand(Call, ceSigned64BitIntegerTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
}

	/* CogObjectRepresentation>>#genLcOopToInt64: */
static NoDbgRegParms void
genLcOopToInt64(sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (value != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceSigned64BitValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushNativeRegister(ReceiverResultReg);
}


/*	Assume this is always correct */

	/* CogObjectRepresentation>>#genLcOopToPointer: */
static NoDbgRegParms void
genLcOopToPointer(sqInt object)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize, object, object);
	ssPushNativeRegister(object);
}

	/* CogObjectRepresentation>>#genLcOopToUInt64: */
static NoDbgRegParms void
genLcOopToUInt64(sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (value != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, cePositive64BitValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushNativeRegister(ReceiverResultReg);
}

	/* CogObjectRepresentation>>#genLcOop:toFloat32: */
static NoDbgRegParms void
genLcOoptoFloat32(sqInt object, sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (object != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, object, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceFloatValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin ConvertRd:Rs: */
	genoperandoperand(ConvertRdRs, DPFPReg0, value);
	ssPushNativeRegisterSingleFloat(value);
}

	/* CogObjectRepresentation>>#genLcOop:toFloat64: */
static NoDbgRegParms void
genLcOoptoFloat64(sqInt object, sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (object != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, object, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceFloatValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	if (DPFPReg0 != value) {
		/* begin MoveRd:Rd: */
		genoperandoperand(MoveRdRd, DPFPReg0, value);
	}
	ssPushNativeRegisterDoubleFloat(value);
}

	/* CogObjectRepresentation>>#genLcOop:toInt64:highPart: */
static NoDbgRegParms void
genLcOoptoInt64highPart(sqInt object, sqInt valueLow, sqInt valueHigh)
{
    AbstractInstruction *abstractInstruction;

	if (object != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, object, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, ceSigned64BitValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	if (Arg0Reg != valueHigh) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, valueHigh);
	}
	genoperandoperand(MoveRR, TempReg, valueLow);
	ssPushNativeRegistersecondRegister(valueLow, valueHigh);
}

	/* CogObjectRepresentation>>#genLcOop:toUInt64:highPart: */
static NoDbgRegParms void
genLcOoptoUInt64highPart(sqInt object, sqInt valueLow, sqInt valueHigh)
{
    AbstractInstruction *abstractInstruction;

	if (object != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, object, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, cePositive64BitValueOfTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	if (Arg0Reg != valueHigh) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, valueHigh);
	}
	genoperandoperand(MoveRR, TempReg, valueLow);
	ssPushNativeRegistersecondRegister(valueLow, valueHigh);
}

	/* CogObjectRepresentation>>#genLcPointerToOop:class: */
static NoDbgRegParms void
genLcPointerToOopclass(sqInt pointer, sqInt pointerClass)
{
    AbstractInstruction *abstractInstruction;


	/* begin PushR: */
	genoperand(PushR, pointer);
	annotateobjRef(genoperandoperand(MoveCwR, pointerClass, Arg0Reg), pointerClass);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, BytesPerOop, Arg1Reg);
	abstractInstruction = genoperand(Call, ceInstantiateClassIndexableSizeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin PopR: */
	genoperand(PopR, pointer);
	genoperandoperandoperand(MoveRMwr, pointer, BaseHeaderSize, TempReg);
	genoperandoperand(MoveRR, TempReg, pointer);
	ssPushRegister(pointer);
}

	/* CogObjectRepresentation>>#genLcUInt64ToOop: */
static NoDbgRegParms void
genLcUInt64ToOop(sqInt value)
{
    AbstractInstruction *abstractInstruction;

	if (value != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value, ReceiverResultReg);
	}
	abstractInstruction = genoperand(Call, cePositive64BitIntegerTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
}


/*	Put the arguments in the correct registers */

	/* CogObjectRepresentation>>#genLcUInt64ToOop:highPart: */
static NoDbgRegParms void
genLcUInt64ToOophighPart(sqInt valueLow, sqInt valueHigh)
{
    AbstractInstruction *abstractInstruction;

	if (valueLow != ReceiverResultReg) {
		if (valueHigh == ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, valueHigh, TempReg);
		}
		genoperandoperand(MoveRR, valueLow, ReceiverResultReg);
	}
	if (valueHigh != Arg0Reg) {
		if (valueHigh == ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, TempReg, Arg0Reg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, valueHigh, Arg0Reg);
		}
	}
	abstractInstruction = genoperand(Call, cePositive64BitIntegerTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
}

	/* CogObjectRepresentation>>#genLoadSlot:sourceReg:destReg: */
static NoDbgRegParms sqInt
genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, (index * BytesPerWord) + BaseHeaderSize, sourceReg, destReg);
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveAdd */
static int
genPrimitiveAdd(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, ReceiverResultReg, ClassReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveAsFloat */
static int
genPrimitiveAsFloat(void)
{
    AbstractInstruction *jumpFailAlloc;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, TempReg, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveBitAnd */
static int
genPrimitiveBitAnd(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* Whether the SmallInteger tags are zero or non-zero, anding them together will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitOr */
static int
genPrimitiveBitOr(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* Whether the SmallInteger tags are zero or non-zero, oring them together will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	rTemp := rArg0
	rClass := tTemp
	rTemp := rTemp & 1
	jz nonInt
	rClass >>= 1
	cmp 0,rClass
	jge neg
	cmp 31,rClass // numSmallIntegerBits, jge for sign
	jge tooBig
	rTemp := rReceiver
	rTemp <<= rClass
	rTemp >>= rClass (arithmetic)
	cmp rTemp,rReceiver
	jnz ovfl
	rReceiver := rReceiver - 1
	rReceiver := rReceiver <<= rClass
	rReceiver := rReceiver + 1
	ret
	neg:
	rClass := 0 - rClass
	cmp 31,rClass // numSmallIntegerBits
	jge inRange
	rClass := 31
	inRange
	rReceiver := rReceiver >>= rClass.
	rReceiver := rReceiver | smallIntegerTags.
	ret
	ovfl
	tooBig
	nonInt:
	fail
 */

	/* CogObjectRepresentation>>#genPrimitiveBitShift */
static int
genPrimitiveBitShift(void)
{
    AbstractInstruction *jumpInRange;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;
    AbstractInstruction *jumpTooBig;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpNegative))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, ClassReg);
	}
	jumpNegative = genConditionalBranchoperand(JumpNegative, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSmallIntegerBits(), ClassReg);
	jumpTooBig = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genoperandoperand(LogicalShiftLeftRR, ClassReg, TempReg);
	genoperandoperand(ArithmeticShiftRightRR, ClassReg, TempReg);
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, ReceiverResultReg);
	jumpOvfl = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);
	/* begin LogicalShiftLeftR:R: */
	genoperandoperand(LogicalShiftLeftRR, ClassReg, ReceiverResultReg);
	genAddSmallIntegerTagsTo(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegative, genoperand(NegateR, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSmallIntegerBits(), ClassReg);
	jumpInRange = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, numSmallIntegerBits(), ClassReg);
	jmpTarget(jumpInRange, genoperandoperand(ArithmeticShiftRightRR, ClassReg, ReceiverResultReg));
	genClearAndSetSmallIntegerTagsIn(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, jmpTarget(jumpTooBig, jmpTarget(jumpOvfl, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitXor */
static int
genPrimitiveBitXor(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* Clear one or the other tag so that xoring will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);
	/* begin XorR:R: */
	genoperandoperand(XorRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveClass */
static int
genPrimitiveClass(void)
{
    sqInt reg;

	reg = ReceiverResultReg;
	if (methodOrBlockNumArgs > 0) {
		if (methodOrBlockNumArgs > 1) {
			return UnimplementedPrimitive;
		}
		reg = Arg0Reg;
		/* begin genLoadArgAtDepth:into: */
		assert(0 < (numRegArgs()));
	}
	if ((genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ReceiverResultReg, TempReg, reg != ReceiverResultReg)) == BadRegisterSet) {
		genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ClassReg, TempReg, reg != ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDiv */
static int
genPrimitiveDiv(void)
{
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, ClassReg);
	}
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ClassReg);
	/* If arg and remainder signs are different we must round down. */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, Arg1Reg);
	}
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, TempReg);
	jmpTarget(jumpSameSign, (convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDivide */
static int
genPrimitiveDivide(void)
{
    AbstractInstruction *jumpInexact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOverflow;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ClassReg);
	/* test for overflow; the only case is SmallInteger minVal / -1 */
	jumpInexact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	jumpOverflow = genJumpNotSmallIntegerValuescratch(TempReg, Arg1Reg);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOverflow, jmpTarget(jumpInexact, jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveEqual */
static int
genPrimitiveEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpZero, gJumpFPEqual, 0)
		: genSmallIntegerComparison(JumpZero));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatAdd */
static sqInt
genPrimitiveFloatAdd(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(AddRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(AddRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatDivide */
static sqInt
genPrimitiveFloatDivide(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 1)
		: genPureFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatMultiply */
static sqInt
genPrimitiveFloatMultiply(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(MulRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(MulRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSquareRoot */
static int
genPrimitiveFloatSquareRoot(void)
{
    AbstractInstruction *jumpFailAlloc;

	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	/* begin SqrtRd: */
	genoperand(SqrtRd, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSubtract */
static sqInt
genPrimitiveFloatSubtract(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(SubRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(SubRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterOrEqual */
static int
genPrimitiveGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreaterOrEqual, gJumpFPGreaterOrEqual, 0)
		: genSmallIntegerComparison(JumpGreaterOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterThan */
static int
genPrimitiveGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreater, gJumpFPGreater, 0)
		: genSmallIntegerComparison(JumpGreater));
}


/*	Implementation notes: there are two reasons to use TempReg
	-1) if primitive fails, ReceiverResultReg must remain unchanged (we
	CompletePrimitive) -2) CLZ/BSR only work on 64bits for registers R0-R7 on
	Intel X64. But Win64 uses R9
	Normally, this should be backEnd dependent, but for now we have a single
	64bits target...
 */

	/* CogObjectRepresentation>>#genPrimitiveHighBit */
static int
genPrimitiveHighBit(void)
{
    AbstractInstruction *jumpNegativeReceiver;
    AbstractInstruction *jumpNegativeReceiver1;
    AbstractInstruction *jumpNegativeReceiver2;
    sqInt quickConstant;
    AbstractInstruction *self_in_CogAbstractInstruction;


	/* remove excess tag bits from the receiver oop */
		gOrCqRR((1U << (numSmallIntegerTagBits())) - 1, ReceiverResultReg, TempReg);
	quickConstant = (numSmallIntegerTagBits()) - 1;
	/* begin ArithmeticShiftRightCq:R: */
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, TempReg);
	self_in_CogAbstractInstruction = ((AbstractInstruction *) (backEnd()));
	/* begin genHighBitIn:ofSmallIntegerOopWithSingleTagBit: */
	if ((((cpuidWord1(self_in_CogAbstractInstruction)) & (32)) != 0)) {
		/* begin genHighBitClzIn:ofSmallIntegerOopWithSingleTagBit: */
		genoperandoperand(ClzRR, TempReg, TempReg);
		if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, 0, TempReg);
		}
		/* Note the nice bit trick below:
		   highBit_1based_of_small_int_value = (BytesPerWord * 8) - leadingZeroCout_of_oop - 1 toAccountForTagBit.
		   This is like 2 complements (- reg - 1) on (BytesPerWord * 8) log2 bits, or exactly a bit invert operation... */
		jumpNegativeReceiver2 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		genoperandoperand(XorCwR, (BytesPerWord * 8) - 1, TempReg);
		jumpNegativeReceiver = jumpNegativeReceiver2;
	}
	else {
		/* begin genHighBitAlternativeIn:ofSmallIntegerOopWithSingleTagBit: */
		if (!(setsConditionCodesFor(lastOpcode(), JumpNegative))) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, 0, TempReg);
		}
		jumpNegativeReceiver1 = genConditionalBranchoperand(JumpNegative, ((sqInt)0));
		genoperandoperand(BSR, TempReg, TempReg);
		jumpNegativeReceiver = jumpNegativeReceiver1;
	}
	if (jumpNegativeReceiver == 0) {
		return UnimplementedPrimitive;
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegativeReceiver, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveIdentical */
static sqInt
genPrimitiveIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(0);
}

	/* CogObjectRepresentation>>#genPrimitiveLessOrEqual */
static int
genPrimitiveLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLessOrEqual, gJumpFPGreaterOrEqual, 1)
		: genSmallIntegerComparison(JumpLessOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveLessThan */
static int
genPrimitiveLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLess, gJumpFPGreater, 1)
		: genSmallIntegerComparison(JumpLess));
}

	/* CogObjectRepresentation>>#genPrimitiveMod */
static int
genPrimitiveMod(void)
{
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, Arg1Reg);
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genRemoveSmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ClassReg);
	/* If arg and remainder signs are different we must reflect around zero. */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, Arg1Reg);
	}
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	genoperandoperand(AddRR, Arg1Reg, ClassReg);
	jmpTarget(jumpSameSign, jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genSetSmallIntegerTagsIn(ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveMultiply */
static int
genPrimitiveMultiply(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(processorHasMultiplyAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	genoperandoperand(MoveRR, ReceiverResultReg, Arg1Reg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	genRemoveSmallIntegerTagsInScratchReg(Arg1Reg);
	/* begin MulOverflowR:R: */
	genMulRR(backEnd, Arg1Reg, ClassReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genSetSmallIntegerTagsIn(ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveNewMethod */
static int
genPrimitiveNewMethod(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveNotEqual */
static int
genPrimitiveNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpNonZero, gJumpFPNotEqual, 0)
		: genSmallIntegerComparison(JumpNonZero));
}

	/* CogObjectRepresentation>>#genPrimitiveNotIdentical */
static sqInt
genPrimitiveNotIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(1);
}

	/* CogObjectRepresentation>>#genPrimitiveQuo */
static int
genPrimitiveQuo(void)
{
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, ClassReg);
	}
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ClassReg);
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatAdd */
static sqInt
genPrimitiveSmallFloatAdd(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(AddRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(AddRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatDivide */
static sqInt
genPrimitiveSmallFloatDivide(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 0)
		: genPureFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatEqual */
static sqInt
genPrimitiveSmallFloatEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPEqual, JumpZero, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatGreaterOrEqual */
static sqInt
genPrimitiveSmallFloatGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpGreaterOrEqual, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatGreaterThan */
static sqInt
genPrimitiveSmallFloatGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpGreater, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatLessOrEqual */
static sqInt
genPrimitiveSmallFloatLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpLessOrEqual, 1, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 1, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatLessThan */
static sqInt
genPrimitiveSmallFloatLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpLess, 1, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 1, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatMultiply */
static sqInt
genPrimitiveSmallFloatMultiply(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(MulRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(MulRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatNotEqual */
static sqInt
genPrimitiveSmallFloatNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPNotEqual, JumpNonZero, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPNotEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatSquareRoot */
static sqInt
genPrimitiveSmallFloatSquareRoot(void)
{
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpNegative;

	genGetSmallFloatValueOfscratchinto(ReceiverResultReg, SendNumArgsReg, DPFPReg0);
	/* begin XorRd:Rd: */
	genoperandoperand(XorRdRd, DPFPReg1, DPFPReg1);
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	jumpNegative = gJumpFPGreater(0);
	/* begin SqrtRd: */
	genoperand(SqrtRd, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegative, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatSubtract */
static sqInt
genPrimitiveSmallFloatSubtract(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(SubRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(SubRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSubtract */
static int
genPrimitiveSubtract(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genoperandoperand(SubRR, Arg0Reg, TempReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genAddSmallIntegerTagsTo(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genSmallIntegerComparison: */
static NoDbgRegParms int
genSmallIntegerComparison(sqInt jumpOpcode)
{
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpTrue;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpFail = genJumpNotSmallInteger(Arg0Reg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, 
	/* begin genMoveConstant:R: */
genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#isUnannotatableConstant: */
static NoDbgRegParms sqInt
isUnannotatableConstant(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((isImmediate((simStackEntry->constant)))
	 || (!(	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate((simStackEntry->constant)))
	 && ((oopisGreaterThan((simStackEntry->constant), classTableRootObj()))
	 || (oopisLessThan((simStackEntry->constant), nilObject()))))));
}

	/* CogObjectRepresentationFor64BitSpur>>#classForInlineCacheTag: */
static NoDbgRegParms sqInt
classForInlineCacheTag(sqInt classIndex)
{
	return classOrNilAtIndex(classIndex);
}

	/* CogObjectRepresentationFor64BitSpur>>#genAddSmallIntegerTagsTo: */
static NoDbgRegParms sqInt
genAddSmallIntegerTagsTo(sqInt aRegister)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, aRegister);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genAlloc64BitPositiveIntegerValue:into:scratchReg:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    sqInt allocSize;
    AbstractInstruction *jumpFail;
    usqLong newLPIHeader;

	allocSize = BaseHeaderSize + BytesPerWord;
	newLPIHeader = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), resultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveRAw, scratch1, freeStartAddress());
	genoperandoperand(MoveCqR, newLPIHeader, scratch1);
	genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg);
	genoperandoperandoperand(MoveRMwr, valueReg, BaseHeaderSize, resultReg);
	return jumpFail;
}

	/* CogObjectRepresentationFor64BitSpur>>#genAlloc64BitSignedIntegerValue:into:scratchReg:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    sqInt allocSize;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpNeg;
    usqLong newLNIHeader;

	allocSize = BaseHeaderSize + BytesPerWord;
	newLNIHeader = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargeNegativeIntegerCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), resultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveRAw, scratch1, freeStartAddress());
	genoperandoperand(MoveCqR, newLNIHeader, scratch1);
	genoperandoperand(CmpCqR, 0, valueReg);
	/* We can avoid duplicating the large constant and a jump choosing between
	   the alternatives by incrementing the single constant if positive.  The assert
	   checks that the hack works. Compact code is to be preferred because it is
	   an uncommon case; usually the value will fit in a SmallInteger. */
	jumpNeg = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	assert((headerForSlotsformatclassIndex(0, 0, 1)) == 1);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, ClassLargePositiveIntegerCompactIndex - ClassLargeNegativeIntegerCompactIndex, scratch1);
	jmpTarget(jumpNeg, genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, valueReg, BaseHeaderSize, resultReg);
	return jumpFail;
}


/*	Override to answer a SmallFloat64 if possible. */

	/* CogObjectRepresentationFor64BitSpur>>#genAllocFloatValue:into:scratchReg:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    usqIntptr_t allocSize;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpFail1;
    AbstractInstruction *jumpMerge;
    AbstractInstruction *jumpNotSF;
    usqLong newFloatHeader;


	/* begin MoveRd:R: */
	assert(BytesPerWord == 8);
	genoperandoperand(MoveRdR, dpreg, resultReg);
	jumpNotSF = genJumpNotSmallFloatValueBitsscratch(resultReg, scratch1);
	genConvertBitsToSmallFloatInscratch(resultReg, scratch1);
	jumpMerge = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpNotSF, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	allocSize = BaseHeaderSize + (sizeof(double));
	newFloatHeader = headerForSlotsformatclassIndex((sizeof(double)) / BytesPerWord, firstLongFormat(), ClassFloatCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), resultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	jumpFail1 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveRAw, scratch1, freeStartAddress());
	genoperandoperand(MoveCqR, newFloatHeader, scratch1);
	genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg);
	genoperandoperandoperand(MoveRdM64r, dpreg, BaseHeaderSize, resultReg);
	jumpFail = jumpFail1;
	jmpTarget(jumpMerge, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpFail;
}


/*	Set the SmallInteger tag bits when the tag bits may be filled with
	garbage. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genClearAndSetSmallIntegerTagsIn: */
static NoDbgRegParms sqInt
genClearAndSetSmallIntegerTagsIn(sqInt scratchReg)
{
    sqInt quickConstant;

	/* begin AndCq:R: */
	quickConstant = -1 - (tagMask());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, scratchReg);
	genoperandoperand(OrCqR, 1, scratchReg);
	return 0;
}


/*	Convert the in-SmallFloat64-range floating point value in integer register
	into a tagged SmallFloat64 oop.
	c.f. Spur64BitMemoryManager>>smallFloatObjectOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertBitsToSmallFloatIn:scratch: */
static NoDbgRegParms sqInt
genConvertBitsToSmallFloatInscratch(sqInt reg, sqInt scratch)
{
    AbstractInstruction *jumpZero;
    sqInt quickConstant;


	/* begin RotateLeftCq:R: */
	genoperandoperand(RotateLeftCqR, 1, reg);
	genoperandoperand(CmpCqR, 1, reg);
	jumpZero = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin SubCq:R: */
	quickConstant = ((sqInt)((usqInt)((smallFloatExponentOffset())) << ((smallFloatMantissaBits()) + 1)));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, quickConstant, reg);
	jmpTarget(jumpZero, genoperandoperand(LogicalShiftLeftCqR, numTagBits(), reg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, smallFloatTag(), reg);
	return 0;
}


/*	Convert the Character in reg to a SmallInteger, assuming
	the Character's value is a valid character. */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertCharacterToSmallIntegerInReg: */
static NoDbgRegParms void
genConvertCharacterToSmallIntegerInReg(sqInt reg)
{
    sqInt quickConstant;

	/* begin SubCq:R: */
	quickConstant = (characterTag()) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, quickConstant, reg);
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertIntegerInReg:toSmallIntegerInReg: */
static NoDbgRegParms sqInt
genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg)
{
	gLogicalShiftLeftCqRR(numTagBits(), srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, destReg);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertIntegerToSmallIntegerInReg: */
static NoDbgRegParms sqInt
genConvertIntegerToSmallIntegerInReg(sqInt reg)
{

	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, numTagBits(), reg);
	genoperandoperand(AddCqR, 1, reg);
	return 0;
}


/*	Convert the SmallFloat in reg to its identityHash as a SmallInteger.
	Rotate the sign bit from bit 3 (zero-relative) to the sign bit. 
	c.f. Spur64BitMemoryManager>>rotatedFloatBitsOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallFloatToSmallFloatHashAsIntegerInReg:scratch: */
static NoDbgRegParms sqInt
genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(sqInt reg, sqInt scratch)
{
    sqInt quickConstant;
    sqInt quickConstant1;

	assert(((((usqInt)((smallFloatTag()))) >> 1) - (smallIntegerTag())) == (smallIntegerTag()));
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 1, reg);
	gAndCqRR(1U << ((numTagBits()) - 1), reg, scratch);
	/* begin SubR:R: */
	genoperandoperand(SubRR, scratch, reg);
	quickConstant = (((usqInt)((smallFloatTag()))) >> 1) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, quickConstant, reg);
	quickConstant1 = 0x3F - ((numTagBits()) - 1);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, quickConstant1, scratch);
	genoperandoperand(OrRR, scratch, reg);
	return 0;
}


/*	Convert the SmallInteger in reg to a Character, assuming
	the SmallInteger's value is a valid character. */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallIntegerToCharacterInReg: */
static NoDbgRegParms void
genConvertSmallIntegerToCharacterInReg(sqInt reg)
{
    sqInt quickConstant;

	/* begin AddCq:R: */
	quickConstant = (characterTag()) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, quickConstant, reg);
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallIntegerToIntegerInReg: */
static NoDbgRegParms sqInt
genConvertSmallIntegerToIntegerInReg(sqInt reg)
{

	/* begin ArithmeticShiftRightCq:R: */
	genoperandoperand(ArithmeticShiftRightCqR, numTagBits(), reg);
	return 0;
}


/*	The arguments are in an array in Arg1Reg. Its size is in sizeReg.
	Load Arg0Reg and Arg1Reg with the first two slots, as appropriate.
	Since objects always have at least one slot it is safe to load arg0
	without checking.
	But the array could be at the end of memory so we must check that it has
	two slots before it is safe to access the second slot. */

	/* CogObjectRepresentationFor64BitSpur>>#genFetchRegArgsForPerformWithArguments: */
static NoDbgRegParms sqInt
genFetchRegArgsForPerformWithArguments(sqInt sizeReg)
{
    AbstractInstruction *skip;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, Arg0Reg);
	genoperandoperand(CmpCqR, 2, sizeReg);
	skip = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize + BytesPerWord, Arg1Reg, Arg1Reg);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genFloatArithmetic:preOpCheck:boxed: */
static NoDbgRegParms sqInt
genFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed)
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpNotSmallInteger;

	jumpFailCheck = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (!(preOpCheckOrNil == null)) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotSmallInteger = genJumpNotSmallInteger(Arg0Reg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, Arg1Reg, DPFPReg1);
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpNotSmallInteger, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	if (!(preOpCheckOrNil == null)) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailAlloc->operands))[0])));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genFloatComparison:orIntegerComparison:invert:boxed: */
static NoDbgRegParms sqInt
genFloatComparisonorIntegerComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt jumpOpcode, sqInt invertComparison, sqInt rcvrBoxed)
{
    AbstractInstruction *compareFloat;
    AbstractInstruction *jumpAmbiguous;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpNotSmallInteger;
    AbstractInstruction *jumpTrue;
    AbstractInstruction *returnTrue;


	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	if (invertComparison) {
		/* May need to invert for NaNs */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}
	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, 
	/* begin genMoveConstant:R: */
(returnTrue = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* Test for ambiguity, that is when floatRcvr == (double) intArg */
	jumpNotSmallInteger = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, Arg0Reg, DPFPReg1);
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	/* Case of non ambiguity, use compareFloat(floatRcvr,(double) intArg) */
	jumpAmbiguous = gJumpFPEqual(0);
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpAmbiguous, genoperandoperand(ConvertRdR, DPFPReg0, ReceiverResultReg));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, returnTrue);
	jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Fetch the instance's identity hash into destReg, encoded as a
	SmallInteger. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genGetHashFieldNonImmOf:asSmallIntegerInto: */
static NoDbgRegParms sqInt
genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg)
{
    sqInt quickConstant;
    sqInt quickConstant1;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	quickConstant = (identityHashFullWordShift()) - (numTagBits());
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, destReg);
	quickConstant1 = ((sqInt)((usqInt)((identityHashHalfWordMask())) << (numTagBits())));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant1, destReg);
	genoperandoperand(AddCqR, smallIntegerTag(), destReg);
	return 0;
}


/*	Fetch the instance's identity hash into destReg, unencoded. */

	/* CogObjectRepresentationFor64BitSpur>>#genGetHashFieldNonImmOf:into: */
static NoDbgRegParms sqInt
genGetHashFieldNonImmOfinto(sqInt instReg, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	genoperandoperand(LogicalShiftRightCqR, 32, destReg);
	genoperandoperand(AndCqR, identityHashHalfWordMask(), destReg);
	return 0;
}


/*	Extract the inline cache tag for the object in sourceReg into destReg. The
	inline cache tag for a given object is the value loaded in inline caches
	to distinguish
	objects of different classes. In Spur this is either the tags for
	immediates, or
	the receiver's classIndex. Answer the label for the start of the sequence. */

	/* CogObjectRepresentationFor64BitSpur>>#genGetInlineCacheClassTagFrom:into:forEntry: */
static NoDbgRegParms AbstractInstruction *
genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry)
{
    AbstractInstruction *entryLabel;
    AbstractInstruction *jumpImm;

	if (forEntry) {
		/* begin AlignmentNops: */
		genoperand(AlignmentNops, BytesPerWord);
	}
	entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	gAndCqRR(tagMask(), sourceReg, destReg);
	/* Get least significant half of header word in destReg */
	jumpImm = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, sourceReg, destReg);
	genoperandoperand(AndCqR, classIndexMask(), destReg);
	jmpTarget(jumpImm, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return entryLabel;
}


/*	Get the size in byte-sized slots of the object in srcReg into destReg.
	srcReg may equal destReg.
	destReg <- numSlots << self shiftForWord - (fmt bitAnd: 7).
	Assumes the object in srcReg has a byte format, i.e. 16 to 23 or 24 to 31 */

	/* CogObjectRepresentationFor64BitSpur>>#genGetNumBytesOf:into: */
static NoDbgRegParms sqInt
genGetNumBytesOfinto(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *jmp;

	genGetRawSlotSizeOfNonImminto(srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSlotsMask(), destReg);
	jmp = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genGetOverflowSlotsOfinto(srcReg, destReg);
	jmpTarget(jmp, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), destReg));
	genGetBitsofFormatByteOfinto(7, srcReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, destReg);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genGetOverflowSlotsOf:into: */
static NoDbgRegParms sqInt
genGetOverflowSlotsOfinto(sqInt srcReg, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, -BaseHeaderSize, srcReg, destReg);
	genoperandoperand(LogicalShiftLeftCqR, 8, destReg);
	genoperandoperand(LogicalShiftRightCqR, 8, destReg);
	return 0;
}


/*	Convert the SmallFloat oop in ooppReg into the corresponding float value
	in dpReg.
	c.f. Spur64BitMemoryManager>>smallFloatBitsOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genGetSmallFloatValueOf:scratch:into: */
static NoDbgRegParms sqInt
genGetSmallFloatValueOfscratchinto(sqInt oopReg, sqInt scratch, sqInt dpReg)
{
    AbstractInstruction *jumpSFZero;
    sqInt quickConstant;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, oopReg, scratch);
	genoperandoperand(LogicalShiftRightCqR, numTagBits(), scratch);
	genoperandoperand(CmpCqR, 1, scratch);
	jumpSFZero = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant = ((sqInt)((usqInt)((smallFloatExponentOffset())) << ((smallFloatMantissaBits()) + 1)));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, quickConstant, scratch);
	jmpTarget(jumpSFZero, genoperandoperand(RotateRightCqR, 1, scratch));
	/* begin MoveR:Rd: */
	assert(BytesPerWord == 8);
	genoperandoperand(MoveRRd, scratch, dpReg);
	return 0;
}


/*	Generate a test for aRegister containing an integer value in the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. Spur64BitMemoryManager>>isIntegerValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpIsSmallIntegerValue:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
    sqInt quickConstant;

	gArithmeticShiftRightCqRR(0x3F - (numTagBits()), aRegister, scratchReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, scratchReg);
	quickConstant = (1U << ((numTagBits()) + 1)) - 1;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, scratchReg);
	genoperandoperand(CmpCqR, 1, scratchReg);
	return genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
}


/*	Generate a compare and branch to test if aRegister contains other than a
	Character. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotCharacter: */
static NoDbgRegParms AbstractInstruction *
genJumpNotCharacter(sqInt reg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(TstCqR, characterTag(), reg);
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}


/*	Generate a test to check that the integer register contains a floating
	point value within the SmallFloat64 range,
	and answer the jump. c.f. Spur64BitMemoryManager>>isSmallFloatValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallFloatValueBits:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallFloatValueBitsscratch(sqInt reg, sqInt exponent)
{
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpMaxExponent;
    AbstractInstruction *jumpMinExponent;
    AbstractInstruction *jumpTest;
    AbstractInstruction *jumpZeroMantissa;
    sqInt quickConstant;
    sqInt quickConstant1;

	flag("if we combine the exponent range test with the conversion to tagged representation we test for a zero exponent only once. further, if we extract tags once into a scratch on the input side we test for immediates, SmallInteger and SmallFloat using the same intermediate result.  so to do is to move fp arithmetic into the object representations");
	gLogicalShiftLeftCqRR(1, reg, exponent);
	quickConstant = (smallFloatMantissaBits()) + 1;
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, exponent);
	genoperandoperand(CmpCqR, smallFloatExponentOffset(), exponent);
	jumpMinExponent = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin CmpCq:R: */
	quickConstant1 = 0xFF + (smallFloatExponentOffset());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant1, exponent);
	jumpMaxExponent = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	jumpFail = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpMinExponent, gTstCqR((1ULL << (smallFloatMantissaBits())) - 1, reg));
	jumpZeroMantissa = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, smallFloatExponentOffset(), exponent);
	jumpTest = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpZeroMantissa, genoperandoperand(CmpCqR, 0, exponent));
	jmpTarget(jumpTest, genConditionalBranchoperand(JumpNonZero, ((sqInt)jumpFail)));
	jmpTarget(jumpMaxExponent, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpFail;
}


/*	Generate a compare and branch to test if aRegister contains other than a
	SmallFloat. Answer the jump. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallFloat: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallFloat(sqInt reg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(TstCqR, smallFloatTag(), reg);
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}


/*	Generate a test for aRegister containing an integer value outside the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. Spur64BitMemoryManager>>isIntegerValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallIntegerValue:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
    sqInt quickConstant;

	gArithmeticShiftRightCqRR(0x3F - (numTagBits()), aRegister, scratchReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, scratchReg);
	quickConstant = (1U << ((numTagBits()) + 1)) - 1;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, scratchReg);
	genoperandoperand(CmpCqR, 1, scratchReg);
	return genConditionalBranchoperand(JumpGreater, ((sqInt)0));
}


/*	Generate a compare and branch to test if aRegister contains other than a
	SmallInteger. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallInteger: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallInteger(sqInt reg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(TstCqR, smallIntegerTag(), reg);
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}


/*	Generate a compare and branch to test if aRegister contains a
	SmallInteger. Answer the jump, or UnimplementedOperation if this cannot be
	done with
	a single register. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpSmallInteger: */
static NoDbgRegParms AbstractInstruction *
genJumpSmallInteger(sqInt aRegister)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(TstCqR, smallIntegerTag(), aRegister);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}

	/* CogObjectRepresentationFor64BitSpur>>#genLcInt32ToOop: */
static NoDbgRegParms sqInt
genLcInt32ToOop(sqInt value)
{

	/* begin SignExtend32R:R: */
	genoperandoperand(SignExtend32RR, value, value);
	genConvertIntegerToSmallIntegerInReg(value);
	ssPushRegister(value);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genLcOopToInt32: */
static NoDbgRegParms sqInt
genLcOopToInt32(sqInt value)
{
	genConvertSmallIntegerToIntegerInReg(value);
	ssPushNativeRegister(value);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genLcOopToUInt32: */
static NoDbgRegParms sqInt
genLcOopToUInt32(sqInt value)
{
	genConvertSmallIntegerToIntegerInReg(value);
	ssPushNativeRegister(value);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genLcUInt32ToOop: */
static NoDbgRegParms sqInt
genLcUInt32ToOop(sqInt value)
{

	/* begin ZeroExtend32R:R: */
	genoperandoperand(ZeroExtend32RR, value, value);
	genConvertIntegerToSmallIntegerInReg(value);
	ssPushRegister(value);
	return 0;
}


/*	Generate the code for primitives 61 & 165, at:put:/basicAt:put: &
	integerAt:put:. If signedVersion is true
	then generate signed accesses to the bits classes (a la 164 & 165). If
	signedVersion is false,
	generate unsigned accesses (a la 60, 61, 63 & 64). */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveAtPutSigned: */
static NoDbgRegParms sqInt
genPrimitiveAtPutSigned(sqInt signedVersion)
{
    sqInt formatReg;
    AbstractInstruction *jump64BitArgIsImmediate;
    AbstractInstruction *jump64BitsOutOfBounds;
    AbstractInstruction *jumpArrayOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpDoubleWordsOutOfRange;
    AbstractInstruction *jumpFixedFieldsOutOfBounds;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNonSmallIntegerValue;
    AbstractInstruction *jumpNot64BitIndexable;
    AbstractInstruction *jumpNot8ByteInteger;
    AbstractInstruction *jumpNotIndexableBits;
    AbstractInstruction *jumpNotIndexablePointers;
    AbstractInstruction *jumpNotPointers;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    AbstractInstruction *methodInBounds;
    sqInt nSlotsOrBytesReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    AbstractInstruction *rejoin;

	jumpDoubleWordsOutOfRange = ((AbstractInstruction *) 0);
	jumpImmutable = ((AbstractInstruction *) 0);
	jumpNegative = ((AbstractInstruction *) 0);
	nSlotsOrBytesReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg0Reg);
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY

	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	/* optimistic store check; assume index in range (almost always is). */
	jumpNotPointers = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg1Reg, TempReg, 0);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, arrayFormat(), formatReg);
	jumpNotIndexablePointers = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	jumpHasFixedFields = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpArrayOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg0Reg);
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpHasFixedFields, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genGetClassIndexOfNonImminto(ReceiverResultReg, formatReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, formatReg);
	/* get # fixed fields in formatReg */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(formatReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, formatReg);
	genConvertSmallIntegerToIntegerInReg(formatReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), formatReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpFixedFieldsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, formatReg, Arg0Reg);
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotPointers, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNonSmallIntegerValue = genJumpNotSmallInteger(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	/* fall through to 64-bit words */
	jumpNotIndexableBits = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genoperandoperand(MoveRR, Arg1Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	if (!signedVersion) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, SendNumArgsReg);
		jumpNegative = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	}
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	rejoin = genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jump64BitsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg0Reg);
	genoperandoperandoperand(MoveRXwrR, SendNumArgsReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonSmallIntegerValue, genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg));
	jumpNot64BitIndexable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	jump64BitArgIsImmediate = genJumpImmediate(Arg1Reg);
	if (signedVersion) {
		/* Test top bit of 64-bit word in large integer for range check. */
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, TempReg);
		genoperandoperand(CmpCqR, 0, TempReg);
		jumpDoubleWordsOutOfRange = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, Arg1Reg, SendNumArgsReg);
	quickConstant = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, SendNumArgsReg);
	quickConstant1 = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant1, SendNumArgsReg);
	jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, SendNumArgsReg);
	genoperand(Jump, ((sqInt)rejoin));
	if (signedVersion) {
		/* Now check if the header is that of an 8 byte LargeNegativeInteger */
		jmpTarget(jumpNot8ByteInteger, gCmpCqR(headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargeNegativeIntegerCompactIndex), SendNumArgsReg));
		jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, TempReg);
		genoperandoperand(MoveCqR, 0, SendNumArgsReg);
		genoperandoperand(SubRR, TempReg, SendNumArgsReg);
		genoperand(Jump, ((sqInt)rejoin));
	}
	if (signedVersion) {
		jmpTarget(jumpIsWords, gArithmeticShiftRightCqRR(0x1F + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, 1, TempReg);
		genoperandoperand(CmpCqR, 1, TempReg);
	}
	else {
		jmpTarget(jumpIsWords, genoperandoperand(CmpCqR, (((usqInt)0xFFFFFFFFU << 3) | 1), Arg1Reg));
	}
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, nSlotsOrBytesReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg0Reg);
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	if (signedVersion) {
		jmpTarget(jumpIsBytes, gArithmeticShiftRightCqRR(7 + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, 1, TempReg);
		genoperandoperand(CmpCqR, 1, TempReg);
	}
	else {
		jmpTarget(jumpIsBytes, genoperandoperand(CmpCqR, (((usqInt)0xFF << 3) | 1), Arg1Reg));
	}
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrBytesReg);
	gAndCqRR(BytesPerWord - 1, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	methodInBounds = genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	if (signedVersion) {
		jmpTarget(jumpIsShorts, gArithmeticShiftRightCqRR(15 + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, 1, TempReg);
		genoperandoperand(CmpCqR, 1, TempReg);
	}
	else {
		jmpTarget(jumpIsShorts, genoperandoperand(CmpCqR, (((usqInt)0xFFFF << 3) | 1), Arg1Reg));
	}
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, nSlotsOrBytesReg);
	genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsCompiledMethod, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	getLiteralCountOfplusOneinBytesintoscratch(ReceiverResultReg, 1, 1, nSlotsOrBytesReg, TempReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelow: */
	genConditionalBranchoperand(JumpBelow, ((sqInt)methodInBounds));
	jmpTarget(jumpNot8ByteInteger, jmpTarget(jump64BitArgIsImmediate, jmpTarget(jumpNot64BitIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpNotIndexableBits, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpArrayOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jump64BitsOutOfBounds, jmpTarget(jumpNotIndexablePointers, jmpTarget(jumpFixedFieldsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))))))))));
	if (signedVersion) {
		jmpTarget(jumpDoubleWordsOutOfRange, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
	}
	else {
		jmpTarget(jumpNegative, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
	}
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
#  endif

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, Arg0Reg);
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	Generate the code for primitives 60 & 164, at:/basicAt: & integerAt:. If
	signedVersion is true
	then generate signed accesses to the bits classes (a la 164 & 165). If
	signedVersion is false,
	generate unsigned accesses (a la 60, 61, 63 & 64). */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveAtSigned: */
static NoDbgRegParms sqInt
genPrimitiveAtSigned(sqInt signedVersion)
{
    AbstractInstruction *convertToIntAndReturn;
    sqInt formatReg;
    AbstractInstruction *jumpArrayOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFixedFieldsOutOfBounds;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsArray;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsLongs;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpLongsOutOfBounds;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotSmallInteger;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *methodInBounds;
    sqInt nSlotsOrElementsReg;
    sqInt quickConstant;

	nSlotsOrElementsReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg1Reg);
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrElementsReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, arrayFormat(), formatReg);
	jumpIsArray = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jumpNotIndexable = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	jumpHasFixedFields = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	jumpIsLongs = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jmpTarget(jumpNotIndexable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotIndexable = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsArray, 
		(		/* begin CmpR:R: */
			assert(!((Arg1Reg == SPReg))),
		genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg)));
	jumpArrayOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg1Reg);
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrElementsReg));
	gAndCqRR(7, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	methodInBounds = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (signedVersion) {
		/* begin MoveXbr:R:R: */
		genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	}
	else {
		/* formatReg already contains a value <= 16r1f, so no need to zero it */
		genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, formatReg);
		genoperandoperand(MoveRR, formatReg, ReceiverResultReg);
	}
	if (signedVersion) {
		/* begin SignExtend8R:R: */
		genoperandoperand(SignExtend8RR, ReceiverResultReg, ReceiverResultReg);
	}
	convertToIntAndReturn = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, 3, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	if (signedVersion) {
		/* begin SignExtend16R:R: */
		genoperandoperand(SignExtend16RR, ReceiverResultReg, ReceiverResultReg);
	}
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpIsWords, gLogicalShiftLeftCqR((shiftForWord()) - 2, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg1Reg);
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (signedVersion) {
		/* begin SignExtend32R:R: */
		genoperandoperand(SignExtend32RR, ReceiverResultReg, ReceiverResultReg);
	}
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpHasFixedFields, genoperandoperand(AndCqR, classIndexMask(), TempReg));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, formatReg);
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(formatReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, formatReg);
	genConvertSmallIntegerToIntegerInReg(formatReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* index is (formatReg (fixed fields) + Arg1Reg (0-rel index)) * wordSize + baseHeaderSize */
	jumpFixedFieldsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, formatReg, Arg1Reg);
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg1Reg);
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsLongs, 
		(		/* begin CmpR:R: */
			assert(!((Arg1Reg == SPReg))),
		genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg)));
	jumpLongsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg1Reg);
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ClassReg);
	genoperandoperand(MoveRR, ClassReg, TempReg);
	if (signedVersion) {
		/* c.f. Spur64BitMemoryManager>>#isIntegerValue: */
		/* begin ArithmeticShiftRightCq:R: */
		genoperandoperand(ArithmeticShiftRightCqR, numSmallIntegerBits(), TempReg);
		genoperandoperand(AndCqR, 15, TempReg);
		genoperandoperand(CmpCqR, 1, TempReg);
		jumpNotSmallInteger = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
		genoperand(Jump, ((sqInt)convertToIntAndReturn));
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
	}
	else {
		quickConstant = (numSmallIntegerBits()) - 1;
		/* begin LogicalShiftRightCq:R: */
		genoperandoperand(LogicalShiftRightCqR, quickConstant, TempReg);
		if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, 0, TempReg);
		}
		jumpNotSmallInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
		genoperand(Jump, ((sqInt)convertToIntAndReturn));
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
	}
	jmpTarget(jumpIsMethod, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	getLiteralCountOfplusOneinBytesintoscratch(ReceiverResultReg, 1, 1, nSlotsOrElementsReg, TempReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelow: */
	genConditionalBranchoperand(JumpBelow, ((sqInt)methodInBounds));
	jmpTarget(jumpFailAlloc, jmpTarget(jumpLongsOutOfBounds, jmpTarget(jumpFixedFieldsOutOfBounds, jmpTarget(jumpArrayOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))))))));
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatEqual */
static sqInt
genPrimitiveFloatEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPEqual, JumpZero, 0, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPEqual, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatGreaterOrEqual */
static sqInt
genPrimitiveFloatGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpGreaterOrEqual, 0, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatGreaterThan */
static sqInt
genPrimitiveFloatGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpGreater, 0, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatLessOrEqual */
static sqInt
genPrimitiveFloatLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpLessOrEqual, 1, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 1, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatLessThan */
static sqInt
genPrimitiveFloatLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpLess, 1, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 1, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatNotEqual */
static sqInt
genPrimitiveFloatNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPNotEqual, JumpNonZero, 0, 1)
		: genPureFloatComparisoninvertboxed(gJumpFPNotEqual, 0, 1));
}


/*	Arguably we should fail for immediates, but so far no one has complained,
	so... 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveIdentityHash */
static sqInt
genPrimitiveIdentityHash(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpNotCharacter;
    AbstractInstruction *jumpNotSet;
    AbstractInstruction *ret;


	/* uses TstCqR */
	jumpImm = genJumpImmediate(ReceiverResultReg);
	genGetHashFieldNonImmOfasSmallIntegerInto(ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ConstZero, TempReg);
	jumpNotSet = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		ret = genoperand(RetN, 0);
	}
	else {
		ret = genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImm, gAndCqRR(tagMask(), ReceiverResultReg, TempReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, characterTag(), TempReg);
	jumpNotCharacter = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertCharacterToSmallIntegerInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		ret = genoperand(RetN, 0);
	}
	else {
		ret = genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotCharacter, genoperandoperand(CmpCqR, smallFloatTag(), TempReg));
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)ret));
	genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(ReceiverResultReg, TempReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSet, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (!(primitiveIndex == 75)) {
		return 0;
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceNewHashTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveImmediateAsInteger */
static sqInt
genPrimitiveImmediateAsInteger(void)
{
    AbstractInstruction *jumpNotCharacter;
    AbstractInstruction *ret;

	gAndCqRR(tagMask(), ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, characterTag(), TempReg);
	jumpNotCharacter = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertCharacterToSmallIntegerInReg(ReceiverResultReg);
	ret = 
	/* begin genPrimReturn */
(methodOrBlockNumArgs <= (numRegArgs())
		? genoperand(RetN, 0)
		: genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord));
	jmpTarget(jumpNotCharacter, genoperandoperand(CmpCqR, smallFloatTag(), TempReg));
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)ret));
	genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(ReceiverResultReg, TempReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}


/*	Implement primitiveNew for convenient cases:
	- the receiver has a hash
	- the receiver is fixed size (excluding ephemerons to save instructions &
	miniscule time)
	- single word header/num slots < numSlotsMask
	- the result fits in eden (actually below scavengeThreshold)
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveNew */
static int
genPrimitiveNew(void)
{
    sqInt byteSizeReg;
    AbstractInstruction *fillLoop;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpTooBig;
    AbstractInstruction *jumpUnhashed;
    AbstractInstruction *jumpVariableOrEphemeron;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 0) {
		return UnimplementedPrimitive;
	}
	/* inst spec will hold class's instance specification, then byte size and finally end of new object. */
	headerReg = (fillReg = SendNumArgsReg);
	/* get freeStart as early as possible so as not to wait later... */
	instSpecReg = (byteSizeReg = ClassReg);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), Arg1Reg);
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* get class's format inst var for both inst spec (format field) and num fixed fields */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, fixedFieldsFieldWidth(), TempReg);
	genoperandoperand(AndCqR, formatMask(), TempReg);
	genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), instSpecReg);
	genoperandoperand(CmpCqR, nonIndexablePointerFormat(), TempReg);
	jumpVariableOrEphemeron = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSlotsMask(), instSpecReg);
	/* Add format to classIndex/format in header; the add in numSlots */
	jumpTooBig = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, numSlotsFullShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(CmpCqR, 0, byteSizeReg);
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	/* round up to allocationUnit */
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	/* write back new freeStart; get result. byteSizeReg holds new freeStart, the limit of the object */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(MoveRAw, byteSizeReg, freeStartAddress());
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveRMwr, headerReg, 0, Arg1Reg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, BaseHeaderSize, ReceiverResultReg, Arg1Reg);
	genoperandoperand(MoveCqR, nilObject(), fillReg);
	fillLoop = genoperandoperandoperand(MoveRMwr, fillReg, 0, Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, Arg1Reg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, byteSizeReg);
	/* begin JumpAbove: */
	genConditionalBranchoperand(JumpAbove, ((sqInt)fillLoop));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpUnhashed, jmpTarget(jumpVariableOrEphemeron, jmpTarget(jumpTooBig, jmpTarget(jumpNoSpace, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return 0;
}


/*	Implement primitiveNewWithArg for convenient cases:
	- the receiver has a hash
	- the receiver is variable and not compiled method
	- single word header/num slots < numSlotsMask
	- the result fits in eden
	See superclass method for dynamic frequencies of formats.
	For the moment we implement only arrayFormat, firstByteFormat &
	firstLongFormat 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveNewWithArg */
static int
genPrimitiveNewWithArg(void)
{
    sqInt byteSizeReg;
    AbstractInstruction *fillLoop;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpArrayFormat;
    AbstractInstruction *jumpArrayTooBig;
    AbstractInstruction *jumpByteFormat;
    AbstractInstruction *jumpBytePrepDone;
    AbstractInstruction *jumpByteTooBig;
    AbstractInstruction *jumpFailCuzFixed;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpLongPrepDone;
    AbstractInstruction *jumpLongTooBig;
    AbstractInstruction *jumpNElementsNonInt;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpUnhashed;
    sqInt maxSlots;
    sqInt quickConstant;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 1) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* Assume there's an available scratch register on 64-bit machines.  This holds the saved numFixedFields and then the value to fill with */
	headerReg = SendNumArgsReg;
	fillReg = Extra0Reg;
	assert(fillReg > 0);
	/* The max slots we'll allocate here are those for a single header */
	instSpecReg = (byteSizeReg = ClassReg);
	/* get freeStart as early as possible so as not to wait later... */
	maxSlots = (numSlotsMask()) - 1;
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), Arg1Reg);
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* get index and fail if not a +ve integer */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* get class's format inst var for inst spec (format field) */
	jumpNElementsNonInt = genJumpNotSmallInteger(Arg0Reg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, instSpecReg);
	quickConstant = (fixedFieldsFieldWidth()) + (numSmallIntegerTagBits());
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, instSpecReg);
	genoperandoperand(AndCqR, formatMask(), instSpecReg);
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(MoveRR, Arg0Reg, fillReg);
	genConvertSmallIntegerToIntegerInReg(fillReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, arrayFormat(), instSpecReg);
	jumpArrayFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), instSpecReg);
	jumpByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), instSpecReg);
	jumpFailCuzFixed = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 2) << 3) | 1), Arg0Reg);
	/* save num elements/slot size to instSpecReg */
	jumpLongTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, BytesPerWord / 4, TempReg);
	genoperandoperand(SubRR, instSpecReg, TempReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddCqR, (BytesPerWord / 4) - 1, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 2, instSpecReg);
	genoperandoperand(MoveCqR, 0, fillReg);
	/* go allocate */
	jumpLongPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpByteFormat, genoperandoperand(CmpCqR, (((usqInt)(maxSlots * BytesPerWord) << 3) | 1), Arg0Reg));
	/* save num elements to instSpecReg */
	jumpByteTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, BytesPerWord, TempReg);
	genoperandoperand(SubRR, instSpecReg, TempReg);
	genoperandoperand(AndCqR, BytesPerWord - 1, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddCqR, BytesPerWord - 1, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), instSpecReg);
	genoperandoperand(MoveCqR, 0, fillReg);
	/* go allocate */
	jumpBytePrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpArrayFormat, genoperandoperand(CmpCqR, (((usqInt)maxSlots << 3) | 1), Arg0Reg));
	/* save num elements/slot size to instSpecReg */
	jumpArrayTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, nilObject(), fillReg);
	jmpTarget(jumpBytePrepDone, jmpTarget(jumpLongPrepDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, numSlotsFullShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(CmpCqR, 0, byteSizeReg);
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	/* get result, increment freeStart and write it back. Arg1Reg holds new freeStart, the limit of the object */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	genoperandoperand(MoveRAw, byteSizeReg, freeStartAddress());
	genoperandoperandoperand(MoveRMwr, headerReg, 0, ReceiverResultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, BaseHeaderSize, ReceiverResultReg, Arg1Reg);
	fillLoop = genoperandoperandoperand(MoveRMwr, fillReg, 0, Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, Arg1Reg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, byteSizeReg);
	/* begin JumpAbove: */
	genConditionalBranchoperand(JumpAbove, ((sqInt)fillLoop));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNoSpace, jmpTarget(jumpUnhashed, jmpTarget(jumpFailCuzFixed, jmpTarget(jumpArrayTooBig, jmpTarget(jumpByteTooBig, jmpTarget(jumpLongTooBig, jmpTarget(jumpNElementsNonInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))));
	return 0;
}


/*	Implement primitiveShallowCopy/primitiveClone for convenient cases:
	- the receiver is not a context
	- the receiver is not a compiled method
	- the result fits in eden (actually below scavengeThreshold) */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveShallowCopy */
static sqInt
genPrimitiveShallowCopy(void)
{
    AbstractInstruction *continuance;
    AbstractInstruction *copyLoop;
    sqInt formatReg;
    AbstractInstruction *jumpEmpty;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpTooBig;
    AbstractInstruction *jumpVariable;
    sqInt ptrReg;
    sqInt quickConstant;
    sqInt resultReg;
    sqInt slotsReg;

	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	resultReg = Arg0Reg;
	/* get freeStart as early as possible so as not to wait later... */
	slotsReg = Arg1Reg;
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), resultReg);
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (ptrReg = (formatReg = SendNumArgsReg)), NoReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, indexablePointersFormat(), formatReg);
	jumpVariable = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	continuance = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genGetRawSlotSizeOfNonImminto(ReceiverResultReg, slotsReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSlotsMask(), slotsReg);
	jumpTooBig = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, slotsReg);
	/* compute byte size for slots */
	jumpEmpty = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, slotsReg);
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), slotsReg);
	genoperandoperand(AddRR, resultReg, slotsReg);
	genoperandoperand(CmpCqR, getScavengeThreshold(), slotsReg);
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, resultReg, ptrReg);
	genoperandoperand(MoveRAw, slotsReg, freeStartAddress());
	genoperandoperand(SubCqR, BytesPerWord * 2, slotsReg);
	genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, TempReg);
	quickConstant = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, resultReg);
	copyLoop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BytesPerWord, ReceiverResultReg);
	genoperandoperand(AddCqR, BytesPerWord, ptrReg);
	genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ptrReg);
	assert(!((ptrReg == SPReg)));
	genoperandoperand(CmpRR, ptrReg, slotsReg);
	/* begin JumpAboveOrEqual: */
	genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)copyLoop));
	genoperandoperand(MoveRR, resultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpVariable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genGetClassIndexOfNonImminto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, ClassReg);
	genConditionalBranchoperand(JumpNonZero, ((sqInt)continuance));
	jmpTarget(jumpImmediate, jmpTarget(jumpNoSpace, jmpTarget(jumpIsMethod, jmpTarget(jumpTooBig, jmpTarget(jumpEmpty, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	return 0;
}


/*	Generate the code for primitive 173, instVarAt:. Defer to
	StackInterpreterPrimitives>>primitiveSlotAt for Contexts.
 */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveSlotAt */
static sqInt
genPrimitiveSlotAt(void)
{
    AbstractInstruction *convertToIntAndReturn;
    sqInt formatReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpLongsOutOfBounds;
    AbstractInstruction *jumpNonPointers;
    AbstractInstruction *jumpPointersOutOfBounds;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpToReturnLargeInteger;
    AbstractInstruction *jumpWordsOutOfBounds;
    sqInt nSlotsOrElementsReg;
    sqInt quickConstant;

	nSlotsOrElementsReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg1Reg);
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrElementsReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	jumpNonPointers = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, classIndexMask(), TempReg);
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	/* Fail to StackInterpreterPrimitives>>primitiveSlotAt for Context */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* index is (formatReg Arg1Reg (0-rel index) * wordSize + baseHeaderSize */
	jumpPointersOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg1Reg);
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonPointers, genoperandoperand(CmpCqR, firstByteFormat(), formatReg));
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	/* fall through to objectMemory sixtyFourBitIndexableFormat */
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpLongsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg1Reg);
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ClassReg);
	genoperandoperand(MoveRR, ClassReg, TempReg);
	quickConstant = (numSmallIntegerBits()) - 1;
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, TempReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, TempReg);
	}
	jumpToReturnLargeInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertIntegerInRegtoSmallIntegerInReg(ClassReg, ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpToReturnLargeInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrElementsReg));
	gAndCqRR(7, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, formatReg);
	genoperandoperand(MoveRR, formatReg, ReceiverResultReg);
	convertToIntAndReturn = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, 3, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpIsWords, gLogicalShiftLeftCqR((shiftForWord()) - 2, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg1Reg);
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpFailAlloc, jmpTarget(jumpLongsOutOfBounds, jmpTarget(jumpPointersOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpIsMethod, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpIsContext, jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))));
	return 0;
}


/*	Generate the code for primitive 174, instVarAt:put:. Defer to
	StackInterpreterPrimitives>>primitiveSlotAtPut for Contexts.
 */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveSlotAtPut */
static sqInt
genPrimitiveSlotAtPut(void)
{
    sqInt formatReg;
    AbstractInstruction *jump64BitArgIsImmediate;
    AbstractInstruction *jump64BitsOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpNonSmallIntegerValue;
    AbstractInstruction *jumpNot64BitIndexable;
    AbstractInstruction *jumpNot8ByteInteger;
    AbstractInstruction *jumpNotIndexableBits;
    AbstractInstruction *jumpNotPointers;
    AbstractInstruction *jumpPointersOutOfBounds;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    sqInt nSlotsOrBytesReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    AbstractInstruction *rejoin;

	jumpImmutable = ((AbstractInstruction *) 0);
	nSlotsOrBytesReg = Extra0Reg;
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg0Reg);
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY

	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	jumpNotPointers = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genGetClassIndexOfNonImminto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, ClassReg);
	/* Fail to StackInterpreterPrimitives>>primitiveSlotAtPut for Context */
	/* optimistic store check; assume index in range (almost always is). */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg1Reg, TempReg, 0);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpPointersOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg0Reg);
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotPointers, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNonSmallIntegerValue = genJumpNotSmallInteger(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	/* fall through to 64-bit words */
	jumpNotIndexableBits = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genoperandoperand(MoveRR, Arg1Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	rejoin = genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jump64BitsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg0Reg);
	genoperandoperandoperand(MoveRXwrR, SendNumArgsReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonSmallIntegerValue, genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg));
	jumpNot64BitIndexable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* Now check if the header is that of an 8 byte LargePositiveInteger */
	jump64BitArgIsImmediate = genJumpImmediate(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, Arg1Reg, SendNumArgsReg);
	quickConstant = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, SendNumArgsReg);
	quickConstant1 = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant1, SendNumArgsReg);
	jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, SendNumArgsReg);
	genoperand(Jump, ((sqInt)rejoin));
	jmpTarget(jumpIsWords, genoperandoperand(CmpCqR, (((usqInt)0xFFFFFFFFU << 3) | 1), Arg1Reg));
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, nSlotsOrBytesReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg0Reg);
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, genoperandoperand(CmpCqR, (((usqInt)0xFFFF << 3) | 1), Arg1Reg));
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, nSlotsOrBytesReg);
	genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, genoperandoperand(CmpCqR, (((usqInt)0xFF << 3) | 1), Arg1Reg));
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrBytesReg);
	gAndCqRR(BytesPerWord - 1, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrBytesReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNot8ByteInteger, jmpTarget(jump64BitArgIsImmediate, jmpTarget(jumpNot64BitIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpNotIndexableBits, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpPointersOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jump64BitsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
#  endif

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, Arg0Reg);
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveStringAt */
static sqInt
genPrimitiveStringAt(void)
{
    AbstractInstruction *done;
    sqInt formatReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpWordsDone;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordTooBig;


	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg1Reg);
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	jumpIsBytes = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	jumpIsShorts = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	/* fall through to double words */
	jumpNotIndexable = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, ClassReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg1Reg);
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	jumpWordTooBig = jumpNotCharacterUnsignedValueInRegister(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	jumpWordsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, BytesPerWord - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	genoperandoperand(AndCqR, 0xFF, ReceiverResultReg);
	jmpTarget(jumpWordsDone, (done = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genConvertIntegerToCharacterInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	genoperand(Jump, ((sqInt)done));
	jmpTarget(jumpWordTooBig, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpNotIndexable, jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))));
	return CompletePrimitive;
}


/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveStringAtPut */
static sqInt
genPrimitiveStringAtPut(void)
{
    sqInt formatReg;
    AbstractInstruction *jumpBadArg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotString;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    sqInt quickConstant;

	jumpImmutable = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	assert(0 < (numRegArgs()));
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	jumpBadArg = genJumpNotCharacter(Arg1Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, Arg0Reg);
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY

	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	jumpNotString = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	/* fall through to double words */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	quickConstant = characterObjectOf((1U << (numCharacterBits())) - 1);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant, Arg1Reg);
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, ClassReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1), Arg0Reg);
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gCmpCqR(characterObjectOf(0xFFFF), Arg1Reg));
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, ClassReg);
	genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, gCmpCqR(characterObjectOf(0xFF), Arg1Reg));
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg);
	genoperandoperand(AndCqR, BytesPerWord - 1, formatReg);
	genoperandoperand(SubRR, formatReg, ClassReg);
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotString, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpNotString->operands))[0])));
#  endif

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 1, Arg0Reg);
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadArg, jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Implement primitiveUninitializedNewWithArg for convenient cases:
	- the receiver has a hash
	- the receiver is variable and not compiled method
	- the result fits in eden
	See superclass method for dynamic frequencies of formats.
	For the moment we implement only arrayFormat, firstByteFormat &
	firstLongFormat 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveUninitializedNewWithArg */
static int
genPrimitiveUninitializedNewWithArg(void)
{
    sqInt byteSizeReg;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpByteFormat;
    AbstractInstruction *jumpByteTooBig;
    AbstractInstruction *jumpDoubleByteFormat;
    AbstractInstruction *jumpDoubleBytePrepDone;
    AbstractInstruction *jumpDoubleWordFormat;
    AbstractInstruction *jumpDoubleWordPrepDone;
    AbstractInstruction *jumpDoubleWordTooBig;
    AbstractInstruction *jumpFailCuzFixed;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpLongPrepDone;
    AbstractInstruction *jumpLongTooBig;
    AbstractInstruction *jumpNElementsNonInt;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpNoSpaceBigObjects;
    AbstractInstruction *jumpOverflowHeader;
    AbstractInstruction *jumpShortTooBig;
    AbstractInstruction *jumpUnhashed;
    sqInt maxSlots;
    sqInt quickConstant;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 1) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* Assume there's an available scratch register on 64-bit machines.  This holds the saved numFixedFields and then the value to fill with */
	headerReg = SendNumArgsReg;
	fillReg = Extra0Reg;
	assert(fillReg > 0);
	/* Allow a max of 1 MB */
	instSpecReg = (byteSizeReg = ClassReg);
	/* get freeStart as early as possible so as not to wait later... */
	maxSlots = maxSlotsForNewSpaceAlloc();
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), Arg1Reg);
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* get index and fail if not a +ve integer */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* get class's format inst var for inst spec (format field) */
	jumpNElementsNonInt = genJumpNotSmallInteger(Arg0Reg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, instSpecReg);
	quickConstant = (fixedFieldsFieldWidth()) + (numSmallIntegerTagBits());
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, instSpecReg);
	genoperandoperand(AndCqR, formatMask(), instSpecReg);
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(MoveRR, Arg0Reg, fillReg);
	genConvertSmallIntegerToIntegerInReg(fillReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), instSpecReg);
	jumpByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), instSpecReg);
	jumpDoubleByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), instSpecReg);
	jumpDoubleWordFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), instSpecReg);
	/* allocates a 32-bit array */
	jumpFailCuzFixed = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 2) << 3) | 1), Arg0Reg);
	/* save num elements/slot size to instSpecReg */
	jumpLongTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, BytesPerWord / 4, TempReg);
	genoperandoperand(SubRR, instSpecReg, TempReg);
	genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddCqR, (BytesPerWord / 4) - 1, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 2, instSpecReg);
	/* go allocate */
	/* allocates a 16-bit array */
	jumpLongPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpDoubleByteFormat, genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 4) << 3) | 1), Arg0Reg));
	/* save num elements to instSpecReg */
	jumpShortTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, BytesPerWord / 2, TempReg);
	genoperandoperand(SubRR, instSpecReg, TempReg);
	genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddCqR, (BytesPerWord / 2) - 1, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 1, instSpecReg);
	/* go allocate */
	/* allocates a 64-bit array */
	jumpDoubleBytePrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpDoubleWordFormat, genoperandoperand(CmpCqR, (((usqInt)maxSlots << 3) | 1), Arg0Reg));
	/* save num elements to instSpecReg */
	jumpDoubleWordTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* go allocate */
	/* allocates a byte array */
	jumpDoubleWordPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpByteFormat, genoperandoperand(CmpCqR, (((usqInt)(maxSlots * BytesPerWord) << 3) | 1), Arg0Reg));
	/* save num elements to instSpecReg */
	jumpByteTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	genoperandoperand(MoveCqR, BytesPerWord, TempReg);
	genoperandoperand(SubRR, instSpecReg, TempReg);
	genoperandoperand(AndCqR, BytesPerWord - 1, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, formatShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddCqR, BytesPerWord - 1, instSpecReg);
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), instSpecReg);
	jmpTarget(jumpDoubleWordPrepDone, jmpTarget(jumpDoubleBytePrepDone, jmpTarget(jumpLongPrepDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSlotsMask(), instSpecReg);
	/* fallthrough: allocate objects with < 255 slots */
	/* store numSlots to headerReg */
	jumpOverflowHeader = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	genoperandoperand(LogicalShiftLeftCqR, numSlotsFullShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(CmpCqR, 0, byteSizeReg);
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	/* get result, increment freeStart and write it back. Arg1Reg holds new freeStart, the limit of the object */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	genoperandoperand(MoveRAw, byteSizeReg, freeStartAddress());
	genoperandoperandoperand(MoveRMwr, headerReg, 0, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOverflowHeader, genoperandoperand(MoveCqR, 0xFF, TempReg));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, numSlotsFullShift(), TempReg);
	genoperandoperand(AddRR, TempReg, headerReg);
	genoperandoperand(AddRR, instSpecReg, TempReg);
	genoperandoperand(AddCqR, (BaseHeaderSize * 2) / BytesPerWord, byteSizeReg);
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	genoperandoperand(AddRR, Arg1Reg, byteSizeReg);
	genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	/* get result, increment freeStart and write it back. Arg1Reg holds new freeStart, the limit of the object */
	jumpNoSpaceBigObjects = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	genoperandoperand(MoveRAw, byteSizeReg, freeStartAddress());
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	genoperandoperandoperand(MoveRMwr, headerReg, BaseHeaderSize, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpShortTooBig, jmpTarget(jumpDoubleWordTooBig, jmpTarget(jumpNoSpaceBigObjects, jmpTarget(jumpNoSpace, jmpTarget(jumpUnhashed, jmpTarget(jumpFailCuzFixed, jmpTarget(jumpByteTooBig, jmpTarget(jumpLongTooBig, jmpTarget(jumpNElementsNonInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))))));
	return 0;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentationFor64BitSpur>>#genPureFloatArithmetic:preOpCheck:boxed: */
static NoDbgRegParms sqInt
genPureFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed)
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;

	jumpFailCheck = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (!(preOpCheckOrNil == null)) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!(preOpCheckOrNil == null)) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailAlloc->operands))[0])));
	}
	return 0;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentationFor64BitSpur>>#genPureFloatComparison:invert:boxed: */
static NoDbgRegParms sqInt
genPureFloatComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt invertComparison, sqInt rcvrBoxed)
{
    AbstractInstruction *compareFloat;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;


	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	if (invertComparison) {
		/* May need to invert for NaNs */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}
	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, 
	/* begin genMoveConstant:R: */
genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentationFor64BitSpur>>#genRemoveSmallIntegerTagsInScratchReg: */
static NoDbgRegParms sqInt
genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, 1, scratchReg);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genShiftAwaySmallIntegerTagsInScratchReg: */
static NoDbgRegParms sqInt
genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg)
{

	/* begin ArithmeticShiftRightCq:R: */
	genoperandoperand(ArithmeticShiftRightCqR, numTagBits(), scratchReg);
	return 0;
}


/*	Stack looks like
	return address */

	/* CogObjectRepresentationFor64BitSpur>>#genSmallIntegerComparison:orDoubleComparison:invert: */
static NoDbgRegParms int
genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison)
{
    AbstractInstruction *compareIntFloat;
    AbstractInstruction *jumpAmbiguous;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotFloatAtAll;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpTrue;
    int r;
    AbstractInstruction *returnTrue;

	r = genSmallIntegerComparison(jumpOpcode);
	if (r < 0) {
		return r;
	}
#  if defined(DPFPReg0)
	/* Fall through on non-SmallInteger argument.  Argument may be a Float : let us check or fail */
	/* check for Small Float argument */
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	compareIntFloat = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, ReceiverResultReg, DPFPReg0);
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	/* Case of non ambiguity, use compareFloat((double) intRcvr,floatArg) */
	jumpAmbiguous = gJumpFPEqual(0);
	if (invertComparison) {
		/* May need to invert for NaNs */
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}
	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, (returnTrue = 
	/* begin genMoveConstant:R: */
genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpAmbiguous, genoperandoperand(ConvertRdR, DPFPReg1, Arg0Reg));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, returnTrue);
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotFloatAtAll = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* It was a Boxed Float, so convert the receiver to double and perform the (int compare: float) operation */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareIntFloat));
	jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpNotFloatAtAll, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
#  endif // defined(DPFPReg0)

	return CompletePrimitive;
}


/*	Get the literal count of a CompiledMethod into headerReg, plus one if
	requested. If inBytes is true, scale the count by the word size. Deal with
	the possibility of
	the method being cogged. */

	/* CogObjectRepresentationFor64BitSpur>>#getLiteralCountOf:plusOne:inBytes:into:scratch: */
static NoDbgRegParms sqInt
getLiteralCountOfplusOneinBytesintoscratch(sqInt methodReg, sqInt plusOne, sqInt inBytes, sqInt litCountReg, sqInt scratchReg)
{
    sqInt quickConstant;

	genGetMethodHeaderOfintoscratch(methodReg, litCountReg, scratchReg);
	assert((1U << (numTagBits())) == BytesPerWord);
	if (inBytes) {
		/* begin AndCq:R: */
		quickConstant = ((sqInt)((usqInt)((alternateHeaderNumLiteralsMask())) << (numTagBits())));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, quickConstant, litCountReg);
	}
	else {
		/* begin LogicalShiftRightCq:R: */
		genoperandoperand(LogicalShiftRightCqR, numTagBits(), litCountReg);
		genoperandoperand(AndCqR, alternateHeaderNumLiteralsMask(), litCountReg);
	}
	if (plusOne) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, (inBytes
			? LiteralStart * BytesPerWord
			: LiteralStart), litCountReg);
	}
	return 0;
}


/*	Answer the relevant inline cache tag for an instance.
	c.f. getInlineCacheClassTagFrom:into: & inlineCacheTagForClass: */

	/* CogObjectRepresentationFor64BitSpur>>#inlineCacheTagForInstance: */
static NoDbgRegParms sqInt
inlineCacheTagForInstance(sqInt oop)
{
	return (isImmediate(oop)
		? oop & (tagMask())
		: classIndexOf(oop));
}

	/* CogObjectRepresentationFor64BitSpur>>#log2BytesPerWord */
static sqInt
log2BytesPerWord(void)
{
	return 3;
}


/*	Generate the routine that converts selector indices into selector objects.
	It is called from the send trampolines.
	If the selector index is negative, convert it into a positive index into
	the special selectors array and index that. Otherwise, index the current
	method. The routine uses Extra0Reg & Extra1Reg, which are available, since
	they are not live at point of send. */

	/* CogObjectRepresentationFor64BitSpur>>#maybeGenerateSelectorIndexDereferenceRoutine */
static void
maybeGenerateSelectorIndexDereferenceRoutine(void)
{
    sqInt bitmask;
    sqInt byteOffset;
    sqInt byteOffset1;
    AbstractInstruction *jumpFullBlock;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNotBlock;
    sqInt offset;

	bitmask = 0;
	byteOffset = 0;
	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ClassReg);
	jumpNegative = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, FoxMethod, FPReg, Extra0Reg);
	genoperandoperand(AddCqR, 2, ClassReg);
	genoperandoperand(TstCqR, MFMethodFlagIsBlockFlag, Extra0Reg);
	/* If in a block, need to find the home method...  If using full blocks, need to test the cpicHasMNUCaseOrCMIsFullBlock bit */
	jumpNotBlock = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genoperandoperand(AndCqR, -(zoneAlignment()), Extra0Reg);
	byteOffset1 = BaseHeaderSize + 1;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMbrR, byteOffset1, Extra0Reg, Extra1Reg);
	genoperandoperand(TstCqR, 16, Extra1Reg);
	jumpFullBlock = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveM16rR, 0, Extra0Reg, Extra1Reg);
	genoperandoperand(SubRR, Extra1Reg, Extra0Reg);
	jmpTarget(jumpNotBlock, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jmpTarget(jumpFullBlock, ((AbstractInstruction *) (((jumpNotBlock->operands))[0])));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, -(zoneAlignment()), Extra0Reg);
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, Extra0Reg, Extra1Reg);
	genoperandoperandoperand(MoveXwrRR, ClassReg, Extra1Reg, ClassReg);
	genoperand(RetN, 0);
	jmpTarget(jumpNegative, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin NegateR: */
	genoperand(NegateR, ClassReg);
	genoperandoperand(LogicalShiftLeftCqR, 1, ClassReg);
	genoperandoperand(MoveAwR, specialObjectsOopAddress(), Extra0Reg);
	genoperandoperand(SubCqR, 1, ClassReg);
	genoperandoperandoperand(MoveMwrR, (SpecialSelectors + 1) * BytesPerWord, Extra0Reg, Extra1Reg);
	genoperandoperandoperand(MoveXwrRR, ClassReg, Extra1Reg, ClassReg);
	genoperand(RetN, 0);
	ceDereferenceSelectorIndex = methodZoneBase();
	outputInstructionsForGeneratedRuntimeAt(ceDereferenceSelectorIndex);
	recordGeneratedRunTimeaddress("ceDereferenceSelectorIndex", ceDereferenceSelectorIndex);
	recordRunTimeObjectReferences();
}

	/* CogObjectRepresentationFor64BitSpur>>#numSmallIntegerBits */
static sqInt
numSmallIntegerBits(void)
{
	return 61;
}

	/* CogObjectRepresentationFor64BitSpur>>#numSmallIntegerTagBits */
static sqInt
numSmallIntegerTagBits(void)
{
	return 3;
}


/*	The three valid tag patterns are 1 (SmallInteger), 2 (Character) and 3
	(SmallFloat64). 
 */

	/* CogObjectRepresentationFor64BitSpur>>#validInlineCacheTag: */
static NoDbgRegParms sqInt
validInlineCacheTag(sqInt classIndexOrTagPattern)
{
	return ((classIndexOrTagPattern >= 1)
	 && (classIndexOrTagPattern <= 3))
	 || ((classAtIndex(classIndexOrTagPattern)));
}

	/* CogObjectRepresentationForSpur>>#callStoreCheckTrampoline */
static void
callStoreCheckTrampoline(void)
{
    AbstractInstruction *abstractInstruction;


	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
}

	/* CogObjectRepresentationForSpur>>#checkValidDerivedObjectReference: */
static NoDbgRegParms int
checkValidDerivedObjectReference(sqInt bodyAddress)
{
	return (heapMapAtWord(pointerForOop(bodyAddress - BaseHeaderSize))) != 0;
}

	/* CogObjectRepresentationForSpur>>#checkValidOopReference: */
static NoDbgRegParms sqInt
checkValidOopReference(sqInt anOop)
{
	return (isImmediate(anOop))
	 || ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentationForSpur>>#couldBeDerivedObject: */
static NoDbgRegParms int
couldBeDerivedObject(sqInt bodyAddress)
{
	return oopisGreaterThanOrEqualTo(bodyAddress - BaseHeaderSize, startOfMemory());
}

	/* CogObjectRepresentationForSpur>>#couldBeObject: */
static NoDbgRegParms sqInt
couldBeObject(sqInt literal)
{
	return (isNonImmediate(literal))
	 && (oopisGreaterThanOrEqualTo(literal, startOfMemory()));
}


/*	Create a trampoline to answer the active context that will
	answer it if a frame is already married, and create it otherwise.
	Assume numArgs is in SendNumArgsReg and ClassReg is free. */

	/* CogObjectRepresentationForSpur>>#genActiveContextTrampolineLarge:inBlock:called: */
static NoDbgRegParms usqInt
genActiveContextTrampolineLargeinBlockcalled(sqInt isLarge, sqInt isInBlock, char *aString)
{
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	genGetActiveContextLargeinBlock(isLarge, isInBlock);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(aString, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Check the remembered bit of the object in objReg; answer the jump taken if
	the bit is already set.
	Only need to fetch the byte containing it, which reduces the size of the
	mask constant.
 */

	/* CogObjectRepresentationForSpur>>#genCheckRememberedBitOf:scratch: */
static NoDbgRegParms AbstractInstruction *
genCheckRememberedBitOfscratch(sqInt objReg, sqInt scratchReg)
{
    sqInt mask;
    sqInt rememberedBitByteOffset;

	rememberedBitByteOffset = (rememberedBitShift()) / 8;
	mask = 1ULL << ((rememberedBitShift()) % 8);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMbrR, rememberedBitByteOffset, objReg, scratchReg);
	genoperandoperand(TstCqR, mask, scratchReg);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}

	/* CogObjectRepresentationForSpur>>#genConvertCharacterToCodeInReg: */
static NoDbgRegParms sqInt
genConvertCharacterToCodeInReg(sqInt reg)
{

	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, numTagBits(), reg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genConvertIntegerToCharacterInReg: */
static NoDbgRegParms sqInt
genConvertIntegerToCharacterInReg(sqInt reg)
{

	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, numTagBits(), reg);
	genoperandoperand(AddCqR, characterTag(), reg);
	return 0;
}


/*	Create a full closure with the given values. */

	/* CogObjectRepresentationForSpur>>#genCreateFullClosure:numArgs:numCopied:ignoreContext:contextNumArgs:large:inBlock: */
static NoDbgRegParms sqInt
genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(sqInt compiledBlock, sqInt numArgs, sqInt numCopied, sqInt ignoreContext, sqInt contextNumArgs, sqInt contextIsLarge, sqInt contextIsBlock)
{
    AbstractInstruction *abstractInstruction;
    usqInt byteSize;
    usqLong header;
    sqInt numSlots;
    AbstractInstruction *skip;


	/* First get thisContext into ReceiverResultReg and thence in ClassReg. */
	if (ignoreContext) {
		/* begin genMoveConstant:R: */
		genoperandoperand(MoveCqR, nilObject(), ClassReg);
	}
	else {
		genGetActiveContextNumArgslargeinBlock(contextNumArgs, contextIsLarge, contextIsBlock);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	}
	numSlots = FullClosureFirstCopiedValueIndex + numCopied;
	byteSize = smallObjectBytesForSlots(numSlots);
	assert(ClassFullBlockClosureCompactIndex != 0);
	header = headerForSlotsformatclassIndex(numSlots, indexablePointersFormat(), ClassFullBlockClosureCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), ReceiverResultReg);
	genoperandoperand(MoveCqR, header, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, byteSize, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, freeStartAddress());
	genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, ClassReg, (ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(compiledBlock))
	 && ((oopisGreaterThan(compiledBlock, classTableRootObj()))
	 || (oopisLessThan(compiledBlock, nilObject())))) {
		annotateobjRef(genoperandoperand(MoveCwR, compiledBlock, TempReg), compiledBlock);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, compiledBlock, TempReg);
	}
	genoperandoperandoperand(MoveRMwr, TempReg, (ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveCqR, (((usqInt)numArgs << 3) | 1), TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, (ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	return 0;
}


/*	Make sure that the oop in reg is not forwarded. 
	Use the fact that isForwardedObjectClassIndexPun is a power of two to save
	an instruction. */
/*	maybe a fixup or an instruction */
/*	maybe a fixup or an instruction */

	/* CogObjectRepresentationForSpur>>#genEnsureOopInRegNotForwarded:scratchReg:ifForwarder:ifNotForwarder: */
static NoDbgRegParms sqInt
genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(sqInt reg, sqInt scratch, void *fwdJumpTarget, void *nonFwdJumpTargetOrZero)
{
    AbstractInstruction *finished;
    AbstractInstruction *imm;
    AbstractInstruction *ok;
    sqInt quickConstant;

	assert(reg != scratch);
	/* notionally
	   self genGetClassIndexOfNonImm: reg into: scratch.
	   cogit CmpCq: objectMemory isForwardedObjectClassIndexPun R: TempReg.
	   but the following is an instruction shorter: */
	imm = genJumpImmediate(reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, reg, scratch);
	quickConstant = (classIndexMask()) - (isForwardedObjectClassIndexPun());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, scratch);
	ok = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, reg, reg);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)fwdJumpTarget));
	if ((((usqInt)nonFwdJumpTargetOrZero)) == 0) {
		finished = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	else {
		finished = nonFwdJumpTargetOrZero;
	}
	jmpTarget(imm, jmpTarget(ok, finished));
	return 0;
}


/*	Make sure that the oop in reg is not forwarded, updating the slot in
	objReg with the value.
 */

	/* CogObjectRepresentationForSpur>>#genEnsureOopInRegNotForwarded:scratchReg:updatingSlot:in: */
static NoDbgRegParms sqInt
genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(sqInt reg, sqInt scratch, sqInt index, sqInt objReg)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *imm;
    AbstractInstruction *loop;
    AbstractInstruction *ok;
    sqInt quickConstant;


	/* Open-code
	   self genEnsureOopInRegNotForwarded: reg
	   scratchReg: scratch
	   updatingMw: index * objectMemory wordSize + objectMemory baseHeaderSize
	   r: objReg.
	   to avoid calling the store check unless the receiver is forwarded. */
	assert((reg != scratch)
	 && (objReg != scratch));
	loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* notionally
	   self genGetClassIndexOfNonImm: reg into: scratch.
	   cogit CmpCq: objectMemory isForwardedObjectClassIndexPun R: TempReg.
	   but the following is an instruction shorter: */
	imm = genJumpImmediate(reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, reg, scratch);
	quickConstant = (classIndexMask()) - (isForwardedObjectClassIndexPun());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, quickConstant, scratch);
	ok = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, reg, reg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, reg, (index * BytesPerWord) + BaseHeaderSize, objReg);
	assert((reg == Arg0Reg)
	 && ((scratch == TempReg)
	 && (objReg == ReceiverResultReg)));
	if (needsFrame()) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreCheckContextReceiverTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreCheckContextReceiverTrampoline);
		(abstractInstruction1->annotation = IsRelativeCall);
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)loop));
	jmpTarget(ok, jmpTarget(imm, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	Do the store check. Answer the argument for the benefit of the code
	generator; ReceiverResultReg may be caller-saved and hence smashed by this
	call. Answering
	it allows the code generator to reload ReceiverResultReg cheaply.
	In Spur the only thing we leave to the run-time is adding the receiver to
	the remembered set and setting its isRemembered bit. */

	/* CogObjectRepresentationForSpur>>#generateObjectRepresentationTrampolines */
static void
generateObjectRepresentationTrampolines(void)
{
    sqInt instVarIndex;
    AbstractInstruction *jumpSC;


#  if IMMUTABILITY
	for (instVarIndex = 0; instVarIndex < NumStoreTrampolines; instVarIndex += 1) {
		ceStoreTrampolines[instVarIndex] = (genStoreTrampolineCalledinstVarIndex(trampolineNamenumArgslimit("ceStoreTrampoline", instVarIndex, NumStoreTrampolines - 2), instVarIndex));
	}
#  endif // IMMUTABILITY

	ceNewHashTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNewHashOf, "ceNewHash", 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, ReceiverResultReg, 0);
	ceInlineNewHashTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNewHashOf, "ceInlineNewHash", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, ReceiverResultReg, 0);
	/* begin genStoreCheckTrampoline */
	if (CheckRememberedInTrampoline) {
		zeroOpcodeIndex();
		jumpSC = genCheckRememberedBitOfscratch(ReceiverResultReg, ABIResultReg);
		assert(((jumpSC->opcode)) == JumpNonZero);
		(jumpSC->opcode = JumpZero);
		/* begin RetN: */
		genoperand(RetN, 0);
		jmpTarget(jumpSC, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	ceStoreCheckTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(remember, "ceStoreCheckTrampoline", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, 
	/* begin returnRegForStoreCheck */
(((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
		? ReceiverResultReg
		: ABIResultReg), CheckRememberedInTrampoline);
	ceStoreCheckContextReceiverTrampoline = genStoreCheckContextReceiverTrampoline();
	ceScheduleScavengeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceScheduleScavenge, "ceScheduleScavengeTrampoline", 0, null, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceSmallActiveContextInMethodTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, 0, "ceSmallMethodContext");
	ceSmallActiveContextInBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, InVanillaBlock, "ceSmallBlockContext");
	ceSmallActiveContextInFullBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, InFullBlock, "ceSmallFullBlockContext");
	ceLargeActiveContextInMethodTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, 0, "ceLargeMethodContext");
	ceLargeActiveContextInBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, InVanillaBlock, "ceLargeBlockContext");
	ceLargeActiveContextInFullBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, InFullBlock, "ceLargeFullBlockContext");
	generateLowcodeObjectTrampolines();
}


/*	Create a trampoline to answer the active context that will
	answer it if a frame is already married, and create it otherwise.
	Assume numArgs is in SendNumArgsReg and ClassReg is free. */

	/* CogObjectRepresentationForSpur>>#genGetActiveContextLarge:inBlock: */
static NoDbgRegParms sqInt
genGetActiveContextLargeinBlock(sqInt isLarge, sqInt isInBlock)
{
    AbstractInstruction *continuation;
    AbstractInstruction *exit;
    usqLong header;
    AbstractInstruction *jumpNeedScavenge;
    AbstractInstruction *jumpSingle;
    AbstractInstruction *loopHead;
    sqInt offset;
    sqInt offset1;
    int slotSize;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, FoxMethod, FPReg, ClassReg);
	genoperandoperand(TstCqR, MFMethodFlagHasContextFlag, ClassReg);
	/* jump if flag bit not set */
	jumpSingle = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genoperandoperandoperand(MoveMwrR, FoxThisContext, FPReg, ReceiverResultReg);
	genoperand(RetN, 0);
	jmpTarget(jumpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(OrCqR, MFMethodFlagHasContextFlag, ClassReg);
	genoperandoperandoperand(MoveRMwr, ClassReg, FoxMethod, FPReg);
	switch (isInBlock) {
	case InFullBlock:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, 3, ClassReg);
		break;
	case InVanillaBlock:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, 3, ClassReg);
		genoperandoperandoperand(MoveM16rR, 0, ClassReg, TempReg);
		genoperandoperand(SubRR, TempReg, ClassReg);
		break;
	case 0:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, 1, ClassReg);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	slotSize = (isLarge
		? LargeContextSlots
		: SmallContextSlots);
	header = headerForSlotsformatclassIndex(slotSize, indexablePointersFormat(), ClassMethodContextCompactIndex);
	flag("endianness");
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), ReceiverResultReg);
	genoperandoperand(MoveCqR, header, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	offset = smallObjectBytesForSlots(slotSize);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, freeStartAddress());
	genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	/* Now initialize the fields of the context.  See CoInterpreter>>marryFrame:SP:copyTemps: */
	/* sender gets frame pointer as a SmallInteger */
	jumpNeedScavenge = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	continuation = genoperandoperandoperand(LoadEffectiveAddressMwrR, 1, FPReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (SenderIndex * BytesPerOop), ReceiverResultReg);
	genoperandoperandoperand(MoveMwrR, FoxSavedFP, FPReg, TempReg);
	genSetSmallIntegerTagsIn(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (InstructionPointerIndex * BytesPerOop), ReceiverResultReg);
	offset1 = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (MethodIndex * BytesPerWord), ReceiverResultReg);
	genoperandoperandoperand(MoveRMwr, ReceiverResultReg, FoxThisContext, FPReg);
	gSubRRR(SPReg, FPReg, TempReg);
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, log2BytesPerWord(), TempReg);
	genoperandoperand(SubCqR, 4, TempReg);
	genoperandoperand(AddRR, SendNumArgsReg, TempReg);
	genConvertIntegerToSmallIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (StackPointerIndex * BytesPerOop), ReceiverResultReg);
	if (isInBlock > 0) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(LoadEffectiveAddressMwrR, 2, SendNumArgsReg, TempReg);
		genoperandoperandoperand(MoveXwrRR, TempReg, FPReg, TempReg);
	}
	else {
		/* begin genMoveConstant:R: */
		genoperandoperand(MoveCqR, nilObject(), TempReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (ClosureIndex * BytesPerOop), ReceiverResultReg);
	genoperandoperandoperand(MoveMwrR, FoxMFReceiver, FPReg, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (ReceiverIndex * BytesPerOop), ReceiverResultReg);
	genoperandoperand(MoveCqR, 1, ClassReg);
	assert(!((SendNumArgsReg == SPReg)));
	loopHead = genoperandoperand(CmpRR, SendNumArgsReg, ClassReg);
	exit = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, TempReg);
	genoperandoperand(SubRR, ClassReg, TempReg);
	genoperandoperand(AddCqR, 2, TempReg);
	genoperandoperandoperand(MoveXwrRR, TempReg, FPReg, TempReg);
	genoperandoperand(AddCqR, ReceiverIndex + (BaseHeaderSize / BytesPerWord), ClassReg);
	genoperandoperandoperand(MoveRXwrR, TempReg, ClassReg, ReceiverResultReg);
	genoperandoperand(SubCqR, (ReceiverIndex + (BaseHeaderSize / BytesPerWord)) - 1, ClassReg);
	genoperand(Jump, ((sqInt)loopHead));
	jmpTarget(exit, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genoperandoperand(MoveCqR, nilObject(), TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(LoadEffectiveAddressMwrR, FoxMFReceiver, FPReg, ClassReg);
	genoperandoperand(AddCqR, (ReceiverIndex + 1) + (BaseHeaderSize / BytesPerWord), SendNumArgsReg);
	loopHead = genoperandoperand(SubCqR, BytesPerWord, ClassReg);
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, SPReg);
	exit = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, TempReg, SendNumArgsReg, ReceiverResultReg);
	genoperandoperand(AddCqR, 1, SendNumArgsReg);
	genoperand(Jump, ((sqInt)loopHead));
	jmpTarget(exit, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jumpNeedScavenge, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	CallRTregistersToBeSavedMask(ceScheduleScavengeTrampoline, ((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg));
	/* begin Jump: */
	genoperand(Jump, ((sqInt)continuation));
	return 0;
}


/*	Get the active context into ReceiverResultReg, creating it if necessary. */

	/* CogObjectRepresentationForSpur>>#genGetActiveContextNumArgs:large:inBlock: */
static NoDbgRegParms sqInt
genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;
    sqInt routine;

	if (isLargeContext) {
		switch (isInBlock) {
		case 0:
						routine = ceLargeActiveContextInMethodTrampoline;
			break;

		case InVanillaBlock:
						routine = ceLargeActiveContextInBlockTrampoline;
			break;

		case InFullBlock:
						routine = ceLargeActiveContextInFullBlockTrampoline;
			break;

		default:
			error("Case not found and no otherwise clause");
			routine = -1;
		}
	}
	else {
		switch (isInBlock) {
		case 0:
						routine = ceSmallActiveContextInMethodTrampoline;
			break;

		case InVanillaBlock:
						routine = ceSmallActiveContextInBlockTrampoline;
			break;

		case InFullBlock:
						routine = ceSmallActiveContextInFullBlockTrampoline;
			break;

		default:
			error("Case not found and no otherwise clause");
			routine = -1;
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, numArgs, SendNumArgsReg);
	abstractInstruction = genoperand(Call, routine);
	(abstractInstruction->annotation = IsRelativeCall);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genGetBits:ofFormatByteOf:into: */
static NoDbgRegParms sqInt
genGetBitsofFormatByteOfinto(sqInt mask, sqInt sourceReg, sqInt destReg)
{
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMbrR, 3, sourceReg, destReg);
	genoperandoperand(AndCqR, mask, destReg);
	return 0;
}


/*	Fetch the instance's class index into destReg. */

	/* CogObjectRepresentationForSpur>>#genGetClassIndexOfNonImm:into: */
static NoDbgRegParms sqInt
genGetClassIndexOfNonImminto(sqInt sourceReg, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, sourceReg, destReg);
	genoperandoperand(AndCqR, classIndexMask(), destReg);
	return 0;
}


/*	Fetch the class object whose index is in instReg into destReg.
	It is non-obvious, but the Cogit assumes loading a class does not involve
	a runtime call, so do not call classAtIndex: */

	/* CogObjectRepresentationForSpur>>#genGetClassObjectOfClassIndex:into:scratchReg: */
static NoDbgRegParms sqInt
genGetClassObjectOfClassIndexintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
	assert(instReg != destReg);
	assert(instReg != scratchReg);
	assert(destReg != scratchReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instReg, scratchReg);
	genoperandoperand(LogicalShiftRightCqR, classTableMajorIndexShift(), scratchReg);
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), scratchReg);
	assert(!(shouldAnnotateObjectReference(classTableRootObj())));
	if (isWithinMwOffsetRange(backEnd(), (classTableRootObj()) + BaseHeaderSize)) {
		/* begin MoveMw:r:R: */
		genoperandoperandoperand(MoveMwrR, (classTableRootObj()) + BaseHeaderSize, scratchReg, destReg);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, classTableRootObj(), scratchReg);
		genoperandoperandoperand(MoveMwrR, BaseHeaderSize, scratchReg, destReg);
	}
	genoperandoperand(MoveRR, instReg, scratchReg);
	genoperandoperand(AndCqR, classTableMinorIndexMask(), scratchReg);
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), scratchReg);
	genoperandoperandoperand(MoveXwrRR, scratchReg, destReg, destReg);
	return 0;
}


/*	Fetch the instance's class into destReg. If the instance is not the
	receiver and is forwarded, follow forwarding. */

	/* CogObjectRepresentationForSpur>>#genGetClassObjectOf:into:scratchReg:mayBeAForwarder: */
static NoDbgRegParms sqInt
genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeForwarder)
{
    AbstractInstruction *jumpIsImm;
    AbstractInstruction *jumpNotForwarded;
    AbstractInstruction *loop;

	if ((instReg == destReg)
	 || ((instReg == scratchReg)
	 || (destReg == scratchReg))) {
		return BadRegisterSet;
	}
	loop = genoperandoperand(MoveRR, instReg, scratchReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, tagMask(), scratchReg);
	jumpIsImm = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, instReg, scratchReg);
	genoperandoperand(AndCqR, classIndexMask(), scratchReg);
	if (mayBeForwarder) {
		/* if it is forwarded... */
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, isForwardedObjectClassIndexPun(), scratchReg);
		/* ...follow the forwarding pointer and loop to fetch its classIndex */
		jumpNotForwarded = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		genoperandoperandoperand(MoveMwrR, BaseHeaderSize, instReg, instReg);
		genoperand(Jump, ((sqInt)loop));
		jmpTarget(jumpNotForwarded, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	jmpTarget(jumpIsImm, genoperandoperand(MoveRR, scratchReg, destReg));
	if (scratchReg == TempReg) {
		/* begin PushR: */
		genoperand(PushR, instReg);
		genGetClassObjectOfClassIndexintoscratchReg(destReg, instReg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, instReg, destReg);
		genoperand(PopR, instReg);
	}
	else {
		genGetClassObjectOfClassIndexintoscratchReg(destReg, scratchReg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, scratchReg, destReg);
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genGetClassTagOf:into:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
	return genGetInlineCacheClassTagFromintoforEntry(instReg, destReg, 1);
}


/*	Fetch the instance's class index into destReg. */

	/* CogObjectRepresentationForSpur>>#genGetCompactClassIndexNonImmOf:into: */
static NoDbgRegParms sqInt
genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg)
{
	return genGetClassIndexOfNonImminto(instReg, destReg);
}

	/* CogObjectRepresentationForSpur>>#genGetDoubleValueOf:into: */
static NoDbgRegParms sqInt
genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveM64rRd, BaseHeaderSize, srcReg, destFPReg);
	return 0;
}


/*	Get the format field of the object in srcReg into destReg.
	srcReg may equal destReg. */

	/* CogObjectRepresentationForSpur>>#genGetFormatOf:into: */
static NoDbgRegParms sqInt
genGetFormatOfinto(sqInt srcReg, sqInt destReg)
{
	return genGetBitsofFormatByteOfinto(formatMask(), srcReg, destReg);
}


/*	Get the format of the object in sourceReg into destReg. If
	scratchRegOrNone is not NoReg, load at least the least significant 32-bits
	(64-bits in 64-bits) of the
	header word, which contains the format, into scratchRegOrNone. */

	/* CogObjectRepresentationForSpur>>#genGetFormatOf:into:leastSignificantHalfOfBaseHeaderIntoScratch: */
static NoDbgRegParms sqInt
genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(sqInt sourceReg, sqInt destReg, sqInt scratchRegOrNone)
{
	if (scratchRegOrNone == NoReg) {
		flag("endianness");
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMbrR, 3, sourceReg, destReg);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchRegOrNone);
		gLogicalShiftRightCqRR(formatShift(), scratchRegOrNone, destReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, formatMask(), destReg);
	return 0;
}


/*	Get the size in word-sized slots of the object in srcReg into destReg.
	srcReg may equal destReg. */

	/* CogObjectRepresentationForSpur>>#genGetNumSlotsOf:into: */
static NoDbgRegParms sqInt
genGetNumSlotsOfinto(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *jmp;

	assert(srcReg != destReg);
	genGetRawSlotSizeOfNonImminto(srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numSlotsMask(), destReg);
	jmp = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genGetOverflowSlotsOfinto(srcReg, destReg);
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	The raw numSlots field is the most significant byte of the 64-bit header
	word. MoveMbrR zero-extends. */

	/* CogObjectRepresentationForSpur>>#genGetRawSlotSizeOfNonImm:into: */
static NoDbgRegParms sqInt
genGetRawSlotSizeOfNonImminto(sqInt sourceReg, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, destReg);
	genoperandoperandoperand(MoveMbrR, 7, sourceReg, destReg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genJumpImmediate: */
static NoDbgRegParms AbstractInstruction *
genJumpImmediate(sqInt aRegister)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(TstCqR, tagMask(), aRegister);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}

	/* CogObjectRepresentationForSpur>>#genJumpImmutable:scratchReg: */
#if IMMUTABILITY
static NoDbgRegParms AbstractInstruction *
genJumpImmutablescratchReg(sqInt sourceReg, sqInt scratchReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchReg);
	genoperandoperand(TstCqR, immutableBitMask(), scratchReg);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}
#endif /* IMMUTABILITY */

	/* CogObjectRepresentationForSpur>>#genJumpMutable:scratchReg: */
#if IMMUTABILITY
static NoDbgRegParms AbstractInstruction *
genJumpMutablescratchReg(sqInt sourceReg, sqInt scratchReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchReg);
	genoperandoperand(TstCqR, immutableBitMask(), scratchReg);
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}
#endif /* IMMUTABILITY */

	/* CogObjectRepresentationForSpur>>#genLcFirstFieldPointer: */
static NoDbgRegParms void
genLcFirstFieldPointer(sqInt objectReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, objectReg);
	ssPushNativeRegister(objectReg);
}


/*	TODO: Retrieve the number of fixed fields. */

	/* CogObjectRepresentationForSpur>>#genLcFirstIndexableFieldPointer: */
static NoDbgRegParms void
genLcFirstIndexableFieldPointer(sqInt objectReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, objectReg);
	ssPushNativeRegister(objectReg);
}


/*	Check for integer */

	/* CogObjectRepresentationForSpur>>#genLcIsBytes:to: */
static NoDbgRegParms void
genLcIsBytesto(sqInt objectReg, sqInt valueReg)
{
    AbstractInstruction *cont;
    AbstractInstruction *falseTarget;
    AbstractInstruction *isCompiledMethod;
    AbstractInstruction *isImmediate;
    AbstractInstruction *isNotBytes;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, objectReg, valueReg);
	genoperandoperand(AndCqR, tagMask(), valueReg);
	/* Get the format */
	isImmediate = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetFormatOfinto(objectReg, valueReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), valueReg);
	isNotBytes = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), valueReg);
	/* True result */
	isCompiledMethod = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	genoperandoperand(MoveCqR, 1, valueReg);
	/* False result */
	cont = genoperand(Jump, ((sqInt)0));
	falseTarget = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(isImmediate, falseTarget);
	jmpTarget(isNotBytes, falseTarget);
	jmpTarget(isCompiledMethod, falseTarget);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, valueReg);
	jmpTarget(cont, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushNativeRegister(valueReg);
}


/*	TODO: Implement this one */

	/* CogObjectRepresentationForSpur>>#genLcIsFloatObject:to: */
static NoDbgRegParms void
genLcIsFloatObjectto(sqInt objectReg, sqInt valueReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 1, valueReg);
	ssPushNativeRegister(valueReg);
}


/*	TODO: Implement this one */

	/* CogObjectRepresentationForSpur>>#genLcIsIndexable:to: */
static NoDbgRegParms void
genLcIsIndexableto(sqInt objectReg, sqInt valueReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 1, valueReg);
	ssPushNativeRegister(valueReg);
}


/*	Check for the immediate case */

	/* CogObjectRepresentationForSpur>>#genLcIsIntegerObject:to: */
static NoDbgRegParms void
genLcIsIntegerObjectto(sqInt objectReg, sqInt valueReg)
{
    AbstractInstruction *cont;
    AbstractInstruction *falseResult;
    AbstractInstruction *isImmediate;
    AbstractInstruction *isLargeNegativeInteger;
    AbstractInstruction *isLargePositiveInteger;
    AbstractInstruction *trueResult;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, objectReg, valueReg);
	genoperandoperand(AndCqR, smallIntegerTag(), valueReg);
	/* Check the non-immediate case */
	isImmediate = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetClassIndexOfNonImminto(objectReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassLargePositiveInteger, TempReg);
	isLargePositiveInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassLargeNegativeInteger, TempReg);
	isLargeNegativeInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin XorR:R: */
	genoperandoperand(XorRR, valueReg, valueReg);
	/* True result */
	falseResult = genoperand(Jump, ((sqInt)0));
	trueResult = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(isLargePositiveInteger, trueResult);
	jmpTarget(isLargeNegativeInteger, trueResult);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 1, valueReg);
	cont = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(falseResult, cont);
	jmpTarget(isImmediate, cont);
	ssPushNativeRegister(valueReg);
}


/*	Check for immediate */

	/* CogObjectRepresentationForSpur>>#genLcIsPointers:to: */
static NoDbgRegParms void
genLcIsPointersto(sqInt objectReg, sqInt valueReg)
{
    AbstractInstruction *cont;
    AbstractInstruction *falseTarget;
    AbstractInstruction *isImmediate;
    AbstractInstruction *isNotPointers;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, objectReg, valueReg);
	genoperandoperand(AndCqR, tagMask(), valueReg);
	/* Get the format */
	isImmediate = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetFormatOfinto(objectReg, valueReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 9, valueReg);
	/* True result */
	isNotPointers = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	genoperandoperand(MoveCqR, 1, valueReg);
	/* False result */
	cont = genoperand(Jump, ((sqInt)0));
	falseTarget = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(isImmediate, falseTarget);
	jmpTarget(isNotPointers, falseTarget);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, valueReg);
	jmpTarget(cont, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushNativeRegister(valueReg);
}


/*	Check for immediate */

	/* CogObjectRepresentationForSpur>>#genLcIsWordsOrBytes:to: */
static NoDbgRegParms void
genLcIsWordsOrBytesto(sqInt objectReg, sqInt valueReg)
{
    AbstractInstruction *cont;
    AbstractInstruction *falseTarget;
    AbstractInstruction *isCompiledMethod;
    AbstractInstruction *isImmediate;
    AbstractInstruction *isNotBits;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, objectReg, valueReg);
	genoperandoperand(AndCqR, tagMask(), valueReg);
	/* Get the format */
	isImmediate = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetFormatOfinto(objectReg, valueReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 9, valueReg);
	isNotBits = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 24, valueReg);
	/* True result */
	isCompiledMethod = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	genoperandoperand(MoveCqR, 1, valueReg);
	/* False result */
	cont = genoperand(Jump, ((sqInt)0));
	falseTarget = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(isImmediate, falseTarget);
	jmpTarget(isNotBits, falseTarget);
	jmpTarget(isCompiledMethod, falseTarget);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, valueReg);
	jmpTarget(cont, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushNativeRegister(valueReg);
}


/*	Check for immediate */

	/* CogObjectRepresentationForSpur>>#genLcIsWords:to: */
static NoDbgRegParms void
genLcIsWordsto(sqInt objectReg, sqInt valueReg)
{
    AbstractInstruction *cont;
    AbstractInstruction *falseTarget;
    AbstractInstruction *isAboveRange;
    AbstractInstruction *isBelowRange;
    AbstractInstruction *isImmediate;

	isAboveRange = ((AbstractInstruction *) 0);
	isBelowRange = ((AbstractInstruction *) 0);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, objectReg, valueReg);
	genoperandoperand(AndCqR, tagMask(), valueReg);
	/* Get the format */
	isImmediate = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetFormatOfinto(objectReg, valueReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), valueReg);
	isBelowRange = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), valueReg);
	isAboveRange = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 1, valueReg);
	/* False result */
	cont = genoperand(Jump, ((sqInt)0));
	falseTarget = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	jmpTarget(isImmediate, falseTarget);
		jmpTarget(isBelowRange, falseTarget);
	jmpTarget(isAboveRange, falseTarget);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, valueReg);
	jmpTarget(cont, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushNativeRegister(valueReg);
}

	/* CogObjectRepresentationForSpur>>#genLcLoadObject:at: */
static NoDbgRegParms void
genLcLoadObjectat(sqInt object, sqInt fieldIndex)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, object);
	genoperandoperandoperand(MoveXwrRR, fieldIndex, object, object);
	ssPushRegister(object);
}

	/* CogObjectRepresentationForSpur>>#genLcLoadObject:field: */
static NoDbgRegParms void
genLcLoadObjectfield(sqInt object, sqInt fieldIndex)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BaseHeaderSize + (BytesPerOop * fieldIndex), object, object);
	ssPushRegister(object);
}

	/* CogObjectRepresentationForSpur>>#genLcStore:object:at: */
static NoDbgRegParms void
genLcStoreobjectat(sqInt value, sqInt object, sqInt fieldIndex)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 8, object);
	genoperandoperandoperand(MoveRXwrR, value, fieldIndex, object);
}

	/* CogObjectRepresentationForSpur>>#genLcStore:object:field: */
static NoDbgRegParms void
genLcStoreobjectfield(sqInt value, sqInt object, sqInt fieldIndex)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, value, BaseHeaderSize + (fieldIndex * BytesPerOop), object);
}


/*	Generate a call to code that allocates a new Array of size.
	The Array should be initialized with nils iff initialize is true.
	The size arg is passed in SendNumArgsReg, the result
	must come back in ReceiverResultReg. */

	/* CogObjectRepresentationForSpur>>#genNewArrayOfSize:initialized: */
static NoDbgRegParms sqInt
genNewArrayOfSizeinitialized(sqInt size, sqInt initialize)
{
    AbstractInstruction *abstractInstruction;
    usqLong header;
    sqInt i;
    sqInt offset;
    AbstractInstruction *skip;

	assert(size < (numSlotsMask()));
	header = headerForSlotsformatclassIndex(size, arrayFormat(), ClassArrayCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), ReceiverResultReg);
	genoperandoperand(MoveCqR, header, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (initialize
	 && (size > 0)) {
		/* begin genMoveConstant:R: */
		genoperandoperand(MoveCqR, nilObject(), TempReg);
		for (i = 0; i < size; i += 1) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveRMwr, TempReg, (i * BytesPerWord) + BaseHeaderSize, ReceiverResultReg);
		}
	}
	offset = smallObjectBytesForSlots(size);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, freeStartAddress());
	genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Create a closure with the given startpc, numArgs and numCopied
	within a context with ctxtNumArgs, large if isLargeCtxt that is in a
	block if isInBlock. Do /not/ initialize the copied values. */

	/* CogObjectRepresentationForSpur>>#genNoPopCreateClosureAt:numArgs:numCopied:contextNumArgs:large:inBlock: */
static NoDbgRegParms sqInt
genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;
    usqInt byteSize;
    usqLong header;
    sqInt numSlots;
    AbstractInstruction *skip;


	/* First get thisContext into ReceiverResultRega and thence in ClassReg. */
	genGetActiveContextNumArgslargeinBlock(ctxtNumArgs, isLargeCtxt, isInBlock);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	numSlots = ClosureFirstCopiedValueIndex + numCopied;
	byteSize = smallObjectBytesForSlots(numSlots);
	header = headerForSlotsformatclassIndex(numSlots, indexablePointersFormat(), ClassBlockClosureCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), ReceiverResultReg);
	genoperandoperand(MoveCqR, header, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, byteSize, ReceiverResultReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, freeStartAddress());
	genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, ClassReg, (ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveCqR, (((usqInt)bcpc << 3) | 1), TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, (ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	genoperandoperand(MoveCqR, (((usqInt)numArgs << 3) | 1), TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, (ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveAsCharacter */
static int
genPrimitiveAsCharacter(void)
{
    AbstractInstruction *jumpNotInt;
    AbstractInstruction *jumpOutOfRange;
    sqInt reg;

	jumpNotInt = ((AbstractInstruction *) 0);
	if (methodOrBlockNumArgs == 0) {
		reg = ReceiverResultReg;
	}
	else {
		if (methodOrBlockNumArgs > 1) {
			return UnimplementedPrimitive;
		}
		reg = Arg0Reg;
		/* begin genLoadArgAtDepth:into: */
		assert(0 < (numRegArgs()));
		jumpNotInt = genJumpNotSmallInteger(reg);
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	jumpOutOfRange = jumpNotCharacterUnsignedValueInRegister(TempReg);
	genConvertSmallIntegerToCharacterInReg(reg);
	if (reg != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, reg, ReceiverResultReg);
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOutOfRange, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (reg != ReceiverResultReg) {
		jmpTarget(jumpNotInt, ((AbstractInstruction *) (((jumpOutOfRange->operands))[0])));
	}
	return CompletePrimitive;
}


/*	Generate primitive 60, at: with unsigned access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveAt */
static sqInt
genPrimitiveAt(void)
{
	return genPrimitiveAtSigned(0);
}


/*	Generate primitive 61, at:put: with unsigned access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveAtPut */
static sqInt
genPrimitiveAtPut(void)
{
	return genPrimitiveAtPutSigned(0);
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genPrimitiveIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *comp;
    AbstractInstruction *jumpCmp;


	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	assert(!((Arg0Reg == SPReg)));
	comp = genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	if (orNot) {
		jumpCmp = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(Arg0Reg, TempReg, comp, 0);
	}
	else {
		jumpCmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	}
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (!orNot) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(Arg0Reg, TempReg, comp, 0);
	}
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}


/*	Generate primitive 164, at: with signed access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveIntegerAt */
static sqInt
genPrimitiveIntegerAt(void)
{
	return genPrimitiveAtSigned(1);
}


/*	Generate primitive 165, at:put: with signed access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveIntegerAtPut */
static sqInt
genPrimitiveIntegerAtPut(void)
{
	return genPrimitiveAtPutSigned(1);
}


/*	<returnTypeC: #'AbstractInstruction *'> */

	/* CogObjectRepresentationForSpur>>#genPrimitiveMakePoint */
static sqInt
genPrimitiveMakePoint(void)
{
    sqInt allocSize;
    AbstractInstruction *jumpFail;
    usqLong newPointHeader;
    sqInt resultReg;
    sqInt scratchReg;

	resultReg = ClassReg;
	/* <var: #jumpFail type: #'AbstractInstruction *'> */
	scratchReg = SendNumArgsReg;
	allocSize = BaseHeaderSize + (BytesPerWord * 2);
	newPointHeader = headerForSlotsformatclassIndex(2, nonIndexablePointerFormat(), ClassPointCompactIndex);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, freeStartAddress(), resultReg);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratchReg);
	genoperandoperand(CmpCqR, getScavengeThreshold(), scratchReg);
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveRAw, scratchReg, freeStartAddress());
	genoperandoperand(MoveCqR, newPointHeader, scratchReg);
	genoperandoperandoperand(MoveRMwr, scratchReg, 0, resultReg);
	genoperandoperandoperand(MoveRMwr, ReceiverResultReg, BaseHeaderSize, resultReg);
	genoperandoperandoperand(MoveRMwr, Arg0Reg, BaseHeaderSize + BytesPerWord, resultReg);
	genoperandoperand(MoveRR, resultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveObjectAt */
static sqInt
genPrimitiveObjectAt(void)
{
    sqInt headerReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBounds;
    AbstractInstruction *jumpNotHeaderIndex;
    sqInt quickConstant;


	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genGetMethodHeaderOfintoscratch(ReceiverResultReg, (headerReg = Arg1Reg), TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)1 << 3) | 1), Arg0Reg);
	jumpNotHeaderIndex = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, headerReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotHeaderIndex, gAndCqR((((usqInt)(alternateHeaderNumLiteralsMask()) << 3) | 1), headerReg));
	/* begin SubCq:R: */
	quickConstant = ((((usqInt)1 << 3) | 1)) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(SubCqR, quickConstant, Arg0Reg);
	assert(!((headerReg == SPReg)));
	genoperandoperand(CmpRR, headerReg, Arg0Reg);
	jumpBounds = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin AddCq:R: */
	genoperandoperand(AddCqR, ((usqInt)(BaseHeaderSize)) >> (shiftForWord()), Arg0Reg);
	genoperandoperandoperand(MoveXwrRR, Arg0Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpBounds, gAddCqR(((((usqInt)1 << 3) | 1)) - (smallIntegerTag()), Arg0Reg));
	jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	c.f. StackInterpreter>>stSizeOf: lengthOf:baseHeader:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationForSpur>>#genPrimitiveSize */
static sqInt
genPrimitiveSize(void)
{
    sqInt jic;
    sqInt jnx;
    AbstractInstruction *jump32BitLongsDone;
    AbstractInstruction *jump64BitLongsDone;
    AbstractInstruction *jumpArrayDone;
    AbstractInstruction *jumpBytesDone;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpIs32BitLongs;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsContext1;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotIndexable1;
    AbstractInstruction *jumpShortsDone;

	jic = 0;
	jnx = 0;
	jumpImm = genJumpImmediate(ReceiverResultReg);
	/* begin genGetSizeOf:into:formatReg:scratchReg:abortJumpsInto: */
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, SendNumArgsReg, TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), SendNumArgsReg);
	jumpIsBytes = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, arrayFormat(), SendNumArgsReg);
	jumpArrayDone = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jumpNotIndexable1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, weakArrayFormat(), SendNumArgsReg);
	jumpHasFixedFields = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstShortFormat(), SendNumArgsReg);
	jumpIsShorts = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstLongFormat(), SendNumArgsReg);
	jumpIs32BitLongs = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), SendNumArgsReg);
	jump64BitLongsDone = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jmpTarget(jumpNotIndexable1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotIndexable1 = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, BytesPerWord - 1, SendNumArgsReg);
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jumpBytesDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, (((usqInt)(BytesPerWord)) >> 1) - 1, SendNumArgsReg);
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jumpShortsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIs32BitLongs, gLogicalShiftLeftCqR((shiftForWord()) - 2, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, (((usqInt)(BytesPerWord)) >> 2) - 1, SendNumArgsReg);
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	/* formatReg contains fmt, now up for grabs.
	   destReg contains numSlots, precious.
	   sourceReg must be preserved */
	jump32BitLongsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasFixedFields, genoperandoperand(AndCqR, classIndexMask(), TempReg));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, SendNumArgsReg);
	genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	jumpIsContext1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(SendNumArgsReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), SendNumArgsReg);
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jmpTarget(jumpArrayDone, jmpTarget(jump64BitLongsDone, jmpTarget(jump32BitLongsDone, jmpTarget(jumpShortsDone, jmpTarget(jumpBytesDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	jumpNotIndexable = jumpNotIndexable1;
	jumpIsContext = jumpIsContext1;
	genConvertIntegerInRegtoSmallIntegerInReg(ClassReg, ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImm, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return CompletePrimitive;
}


/*	primitiveCompareWith: */

	/* CogObjectRepresentationForSpur>>#genPrimitiveStringCompareWith */
static sqInt
genPrimitiveStringCompareWith(void)
{
    AbstractInstruction *instr;
    AbstractInstruction *jump;
    AbstractInstruction *jumpAbove;
    AbstractInstruction *jumpIncorrectFormat1;
    AbstractInstruction *jumpIncorrectFormat2;
    AbstractInstruction *jumpIncorrectFormat3;
    AbstractInstruction *jumpIncorrectFormat4;
    AbstractInstruction *jumpMidFailure;
    AbstractInstruction *jumpSuccess;
    sqInt minSizeReg;
    sqInt string1CharOrByteSizeReg;
    sqInt string1Reg;
    sqInt string2CharOrByteSizeReg;
    sqInt string2Reg;


	/* I redefine those name to ease program comprehension */
	string1Reg = ReceiverResultReg;
	string2Reg = Arg0Reg;
	string1CharOrByteSizeReg = Arg1Reg;
	string2CharOrByteSizeReg = ClassReg;
	/* Load arguments in reg */
	minSizeReg = SendNumArgsReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genGetFormatOfinto(string1Reg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
	jumpIncorrectFormat1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	jumpIncorrectFormat2 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genGetNumSlotsOfinto(string1Reg, string1CharOrByteSizeReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), string1CharOrByteSizeReg);
	gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, string1CharOrByteSizeReg);
	genGetFormatOfinto(string2Reg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
	jumpIncorrectFormat3 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	jumpIncorrectFormat4 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genGetNumSlotsOfinto(string2Reg, string2CharOrByteSizeReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), string2CharOrByteSizeReg);
	gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, string2CharOrByteSizeReg);
	assert(!((string1CharOrByteSizeReg == SPReg)));
	genoperandoperand(CmpRR, string1CharOrByteSizeReg, string2CharOrByteSizeReg);
	jumpAbove = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, string1CharOrByteSizeReg, minSizeReg);
	jump = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpAbove, genoperandoperand(MoveRR, string2CharOrByteSizeReg, minSizeReg));
	jmpTarget(jump, genoperandoperand(CmpCqR, 0, minSizeReg));
	/* if one of the string is empty, no need to go through the comparing loop */
	/* Compare the bytes */
	jumpSuccess = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, BaseHeaderSize, TempReg);
	genoperandoperand(AddCqR, BaseHeaderSize, minSizeReg);
	instr = genoperandoperandoperand(MoveXbrRR, TempReg, string1Reg, string1CharOrByteSizeReg);
	/* begin MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, TempReg, string2Reg, string2CharOrByteSizeReg);
	genoperandoperand(SubRR, string2CharOrByteSizeReg, string1CharOrByteSizeReg);
	/* the 2 compared characters are different, exit the loop */
	jumpMidFailure = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genoperandoperand(AddCqR, 1, TempReg);
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, minSizeReg);
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)instr));
	genGetNumBytesOfinto(string1Reg, string1CharOrByteSizeReg);
	genGetNumBytesOfinto(string2Reg, string2CharOrByteSizeReg);
	jmpTarget(jumpSuccess, genoperandoperand(SubRR, string2CharOrByteSizeReg, string1CharOrByteSizeReg));
	jmpTarget(jumpMidFailure, genoperandoperand(MoveRR, string1CharOrByteSizeReg, ReceiverResultReg));
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIncorrectFormat4, jmpTarget(jumpIncorrectFormat3, jmpTarget(jumpIncorrectFormat2, jmpTarget(jumpIncorrectFormat1, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return CompletePrimitive;
}


/*	replaceFrom: start to: stop with: replacement startingAt: repStart. 
	
	The primitive in the JIT tries to deal with two pathological cases, copy
	of arrays and byteStrings,
	which often copies only a dozen of fields and where switching to the C
	runtime cost a lot.
	
	Based on heuristics on the method class, I generate a quick array path
	(typically for Array),
	a quick byteString path (typically for ByteString, ByteArray and
	LargeInteger) or no quick 
	path at all (Typically for Bitmap).
	
	The many tests to ensure that the primitive won't fail are not super
	optimised (multiple reloading
	or stack arguments in registers) but this is still good enough and worth
	it since we're avoiding 
	the Smalltalk to C stack switch. The tight copying loops are optimised. 
	
	It is possible to build a bigger version with the 2 different paths but I
	(Clement) believe this 
	is too big machine code wise to be worth it.
 */

	/* CogObjectRepresentationForSpur>>#genPrimitiveStringReplace */
static sqInt
genPrimitiveStringReplace(void)
{
    sqInt adjust;
    sqInt arrayReg;
    AbstractInstruction *instr;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jumpEmpty;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIncorrectFormat1;
    AbstractInstruction *jumpIncorrectFormat2;
    AbstractInstruction *jumpIncorrectFormat3;
    AbstractInstruction *jumpIncorrectFormat4;
    AbstractInstruction *jumpNotSmi1;
    AbstractInstruction *jumpNotSmi2;
    AbstractInstruction *jumpNotSmi3;
    AbstractInstruction *jumpOutOfBounds1;
    AbstractInstruction *jumpOutOfBounds2;
    AbstractInstruction *jumpOutOfBounds3;
    AbstractInstruction *jumpOutOfBounds4;
    sqInt offset;
    sqInt offset1;
    sqInt offset2;
    sqInt offset3;
    sqInt offset4;
    sqInt offset5;
    sqInt offset6;
    sqInt offset7;
    sqInt offset8;
    sqInt offset9;
    sqInt replReg;
    sqInt repStartReg;
    sqInt result;
    sqInt startReg;
    sqInt stopReg;


	/* Can I generate a quick path for this method ? */
	jumpImmutable = ((AbstractInstruction *) 0);
	jumpOutOfBounds3 = ((AbstractInstruction *) 0);
	jumpOutOfBounds4 = ((AbstractInstruction *) 0);
	if (!((maybeMethodClassOfseemsToBeInstantiating(methodObj, arrayFormat()))
		 || (maybeMethodClassOfseemsToBeInstantiating(methodObj, firstByteFormat())))) {
		return UnimplementedPrimitive;
	}
	arrayReg = ReceiverResultReg;
	startReg = Arg0Reg;
	stopReg = Arg1Reg;
	replReg = ClassReg;
	/* Load arguments in reg */
	repStartReg = SendNumArgsReg;
	/* begin genStackArgAt:into: */
	offset = (1) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, SPReg, repStartReg);
	/* begin genStackArgAt:into: */
	offset1 = (2) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset1, SPReg, replReg);
	/* begin genStackArgAt:into: */
	offset2 = (3) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset2, SPReg, stopReg);
	/* begin genStackArgAt:into: */
	offset3 = (4) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset3, SPReg, startReg);
	jumpNotSmi1 = genJumpNotSmallInteger(repStartReg);
	jumpNotSmi2 = genJumpNotSmallInteger(stopReg);
	/* repl non immediate or fail the primitive */
	jumpNotSmi3 = genJumpNotSmallInteger(startReg);
	/* if start>stop primitive success */
	jumpImm = genJumpImmediate(replReg);
	/* begin CmpR:R: */
	assert(!((startReg == SPReg)));
	genoperandoperand(CmpRR, startReg, stopReg);
	/* If receiver immutable fail the primitive */
	jumpEmpty = genConditionalBranchoperand(JumpLess, ((sqInt)0));
#  if IMMUTABILITY
	jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, TempReg);
#  endif

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)0 << 3) | 1), startReg);
	/* 0 >= replStart, fail */
	jumpOutOfBounds1 = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	genoperandoperand(CmpCqR, (((usqInt)0 << 3) | 1), repStartReg);
	/* --- Pointer object version --- */
	jumpOutOfBounds2 = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	if (maybeMethodClassOfseemsToBeInstantiating(methodObj, arrayFormat())) {
		/* Are they both array format ? */
		genGetFormatOfinto(arrayReg, TempReg);
		genGetFormatOfinto(replReg, startReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, arrayFormat(), startReg);
		jumpIncorrectFormat1 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, arrayFormat(), TempReg);
		/* Both objects are arrays, */
		jumpIncorrectFormat2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		genGetNumSlotsOfinto(arrayReg, TempReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin CmpR:R: */
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, stopReg);
		/* rep size < repStart - start + stop */
		jumpOutOfBounds3 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		genGetNumSlotsOfinto(replReg, TempReg);
		/* begin genStackArgAt:into: */
		offset4 = (4) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset4, SPReg, startReg);
		genConvertSmallIntegerToIntegerInReg(startReg);
		genConvertSmallIntegerToIntegerInReg(repStartReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, stopReg);
		genoperandoperand(AddRR, repStartReg, stopReg);
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, stopReg);
		/* Everything in bounds */
		/* PossibleRemembered object */
		jumpOutOfBounds4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveCwR, storeCheckBoundary(), TempReg);
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, arrayReg);
		jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(arrayReg, TempReg);
		callStoreCheckTrampoline();
		jmpTarget(jmpDestYoung, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jmpTarget(jmpAlreadyRemembered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin genStackArgAt:into: */
		offset5 = (3) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset5, SPReg, stopReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, repStartReg);
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), repStartReg);
		genoperandoperand(AddRR, repStartReg, replReg);
		adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(AddCqR, adjust, startReg);
			genoperandoperand(AddCqR, adjust, stopReg);
		}
		instr = genoperandoperandoperand(MoveXwrRR, startReg, replReg, TempReg);
		/* begin MoveR:Xwr:R: */
		genoperandoperandoperand(MoveRXwrR, TempReg, startReg, arrayReg);
		genoperandoperand(AddCqR, 1, startReg);
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpAboveOrEqual: */
		genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)instr));
		jmpTarget(jumpEmpty, 
		/* begin genPrimReturn */
(methodOrBlockNumArgs <= (numRegArgs())
			? genoperand(RetN, 0)
			: genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord)));
		jmpTarget(jumpIncorrectFormat1, jmpTarget(jumpIncorrectFormat2, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	}
	if (maybeMethodClassOfseemsToBeInstantiating(methodObj, firstByteFormat())) {
		/* Are they both byte array format ? CompiledMethod excluded */
		genGetFormatOfinto(arrayReg, TempReg);
		genGetFormatOfinto(replReg, repStartReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, firstByteFormat(), repStartReg);
		jumpIncorrectFormat1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, firstCompiledMethodFormat(), repStartReg);
		jumpIncorrectFormat2 = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
		jumpIncorrectFormat3 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
		/* Both objects are byte arrays */
		jumpIncorrectFormat4 = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		genGetNumSlotsOfinto(arrayReg, startReg);
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), startReg);
		gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, startReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin CmpR:R: */
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* rep size < repStart - start + stop */
		jumpOutOfBounds3 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, repStartReg, TempReg);
		offset6 = (1) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset6, SPReg, repStartReg);
		/* begin genStackArgAt:into: */
		offset7 = (4) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset7, SPReg, startReg);
		genConvertSmallIntegerToIntegerInReg(startReg);
		genConvertSmallIntegerToIntegerInReg(repStartReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, stopReg);
		genoperandoperand(AddRR, repStartReg, stopReg);
		genGetNumSlotsOfinto(replReg, startReg);
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), startReg);
		gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, startReg);
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* Everything in bounds */
		/* Copy the bytes */
		jumpOutOfBounds4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin genStackArgAt:into: */
		offset8 = (4) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset8, SPReg, startReg);
		genConvertSmallIntegerToIntegerInReg(startReg);
		/* begin genStackArgAt:into: */
		offset9 = (3) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset9, SPReg, stopReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, repStartReg);
		genoperandoperand(AddRR, repStartReg, replReg);
		adjust = BaseHeaderSize - 1;
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(AddCqR, adjust, startReg);
			genoperandoperand(AddCqR, adjust, stopReg);
		}
		instr = genoperandoperandoperand(MoveXbrRR, startReg, replReg, TempReg);
		/* begin MoveR:Xbr:R: */
		genoperandoperandoperand(MoveRXbrR, TempReg, startReg, arrayReg);
		genoperandoperand(AddCqR, 1, startReg);
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpAboveOrEqual: */
		genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)instr));
		jmpTarget(jumpEmpty, 
		/* begin genPrimReturn */
(methodOrBlockNumArgs <= (numRegArgs())
			? genoperand(RetN, 0)
			: genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord)));
		jmpTarget(jumpIncorrectFormat4, jmpTarget(jumpIncorrectFormat3, jmpTarget(jumpIncorrectFormat2, jmpTarget(jumpIncorrectFormat1, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	}
	if (((result = compileInterpreterPrimitive())) < 0) {
		return result;
	}
	jmpTarget(jumpImm, jmpTarget(jumpNotSmi1, jmpTarget(jumpNotSmi2, jmpTarget(jumpNotSmi3, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	jmpTarget(jumpOutOfBounds1, jmpTarget(jumpOutOfBounds2, jmpTarget(jumpOutOfBounds3, jmpTarget(jumpOutOfBounds4, ((AbstractInstruction *) (((jumpImm->operands))[0]))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpImm->operands))[0])));
#  endif

	return CompletePrimitive;
}

	/* CogObjectRepresentationForSpur>>#genSetSmallIntegerTagsIn: */
static NoDbgRegParms sqInt
genSetSmallIntegerTagsIn(sqInt scratchReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(OrCqR, 1, scratchReg);
	return 0;
}


/*	Create a trampoline to store-check the update of the receiver in a
	closure's outerContext in compileBlockFrameBuild:. */

	/* CogObjectRepresentationForSpur>>#genStoreCheckContextReceiverTrampoline */
static usqInt
genStoreCheckContextReceiverTrampoline(void)
{
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg0Reg, TempReg, 0);
	/* begin RetN: */
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceStoreCheckContextReceiver", startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Generate the code for a store check of valueReg into destReg. */

	/* CogObjectRepresentationForSpur>>#genStoreCheckReceiverReg:valueReg:scratchReg:inFrame: */
static NoDbgRegParms sqInt
genStoreCheckReceiverRegvalueRegscratchReginFrame(sqInt destReg, sqInt valueReg, sqInt scratchReg, sqInt inFrame)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;


	/* Is value stored an immediate?  If so we're done */
	jmpAlreadyRemembered = ((AbstractInstruction *) 0);
	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(valueReg);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveCwR, storeCheckBoundary(), scratchReg);
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, destReg);
	/* Is value stored old?  If so we're done. */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, valueReg);
	/* value is young and target is old.
	   Need to remember this only if the remembered bit is not already set. */
	jmpSourceOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	if (!CheckRememberedInTrampoline) {
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(destReg, scratchReg);
	}
	assert(destReg == ReceiverResultReg);
	/* begin evaluateTrampolineCallBlock:protectLinkRegIfNot: */
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jmpAlreadyRemembered, ((AbstractInstruction *) (((jmpSourceOld->operands))[0])));
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genStoreSourceReg:slotIndex:destReg:scratchReg:inFrame:needsStoreCheck: */
static NoDbgRegParms sqInt
genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck)
{
    AbstractInstruction *abstractInstruction;


	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		abstractInstruction = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	if (needsStoreCheck) {
		return genStoreCheckReceiverRegvalueRegscratchReginFrame(destReg, sourceReg, scratchReg, inFrame);
	}
	return 0;
}


/*	This method is used for unchecked stores in objects after their creation
	(typically, inlined creation of Array, closures and some temp vectors). 
	Currently there is no need to do the immutability check here
 */

	/* CogObjectRepresentationForSpur>>#genStoreSourceReg:slotIndex:intoNewObjectInDestReg: */
static NoDbgRegParms sqInt
genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	return 0;
}


/*	Convention:
	- RcvrResultReg holds the object mutated.
	If immutability failure:
	- TempReg holds the instance variable index mutated 
	if instVarIndex > numDedicatedStoreTrampoline
	- ClassReg holds the value to store
	Registers are not lived across this trampoline as the 
	immutability failure may need new stack frames. */

	/* CogObjectRepresentationForSpur>>#genStoreTrampolineCalled:instVarIndex: */
#if IMMUTABILITY
static NoDbgRegParms usqInt
genStoreTrampolineCalledinstVarIndex(char *trampolineName, sqInt instVarIndex)
{
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpImmutable1;
    AbstractInstruction *jumpRC;
    sqInt pushLinkReg;
    sqInt quickConstant;
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	if (CheckRememberedInTrampoline) {
		/* begin genStoreTrampolineCheckingRememberedCalled:instVarIndex: */
		/* Store check */
		/* If on 64-bits and doing the remembered bit test here, we can combine the tests to fetch the header once. */
		jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, SendNumArgsReg);
				/* begin TstCq:R: */
		quickConstant = 1U << (rememberedBitShift());
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(TstCqR, quickConstant, SendNumArgsReg);
		jumpRC = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin RetN: */
		genoperand(RetN, 0);
		jmpTarget(jumpRC, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(remember, 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, 
		/* begin returnRegForStoreCheck */
(((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
			? ReceiverResultReg
			: ABIResultReg));
		jmpTarget(jumpImmutable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceCannotAssignTowithIndexvalueToAssign, 3, ReceiverResultReg, (instVarIndex < (NumStoreTrampolines - 1)
			? (				/* begin trampolineArgConstant: */
					assert(instVarIndex >= 0),
				-2 - instVarIndex)
			: TempReg), ClassReg, null, 0 /* begin emptyRegisterMask */, 1, NoReg);
	}
	else {
		/* begin genStoreTrampolineNotCheckingRememberedCalled:instVarIndex: */
		genSmalltalkToCStackSwitch((pushLinkReg = 0 /* begin hasLinkRegister */));
		/* Store check */
		jumpImmutable1 = genJumpImmutablescratchReg(ReceiverResultReg, SendNumArgsReg);
		compileCallFornumArgsargargargargresultRegregsToSave(remember, 1, ReceiverResultReg, null, null, null, 
		/* begin returnRegForStoreCheck */
(((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
			? ReceiverResultReg
			: ABIResultReg), 0 /* begin emptyRegisterMask */);
		genLoadStackPointers(backEnd());
		genTrampolineReturn(pushLinkReg);
		jmpTarget(jumpImmutable1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileCallFornumArgsargargargargresultRegregsToSave(ceCannotAssignTowithIndexvalueToAssign, 3, ReceiverResultReg, (instVarIndex < (NumStoreTrampolines - 1)
			? (				/* begin trampolineArgConstant: */
					assert(instVarIndex >= 0),
				-2 - instVarIndex)
			: TempReg), ClassReg, null, NoReg, 0 /* begin emptyRegisterMask */);
		genLoadStackPointers(backEnd());
		genTrampolineReturn(pushLinkReg);
	}
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}
#endif /* IMMUTABILITY */


/*	Store check code is duplicated to use a single trampoline */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityAndStoreCheckSourceReg:slotIndex:destReg:scratchReg:needRestoreRcvr: */
#if IMMUTABILITY
static NoDbgRegParms sqInt
genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *immutableJump;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;

	jmpAlreadyRemembered = ((AbstractInstruction *) 0);
	immutableJump = genJumpImmutablescratchReg(destReg, scratchReg);
	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		abstractInstruction3 = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction3->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(sourceReg);
	genoperandoperand(MoveCwR, storeCheckBoundary(), scratchReg);
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, destReg);
	/* Is value stored old?  If so we're done. */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, sourceReg);
	/* value is young and target is old.
	   Need to remember this only if the remembered bit is not already set. */
	jmpSourceOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	if (!CheckRememberedInTrampoline) {
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(destReg, scratchReg);
	}
	jmpTarget(immutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genStoreTrampolineCall: */
	assert(IMMUTABILITY);
	if (index >= (NumStoreTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, index, TempReg);
		abstractInstruction = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreTrampolines[index]);
		(abstractInstruction1->annotation = IsRelativeCall);
	}
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin annotateBytecode: */
	(abstractInstruction2->annotation = HasBytecodePC);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	if (needRestoreRcvr) {
		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
	}
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jmpAlreadyRemembered, ((AbstractInstruction *) (((jmpSourceOld->operands))[0])));
	}
	return 0;
}
#endif /* IMMUTABILITY */


/*	Gen an immutability check with no store check (e.g. assigning an immediate
	literal) 
 */
/*	imm check has its own trampoline */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityButNoStoreCheckSourceReg:slotIndex:destReg:scratchReg:needRestoreRcvr: */
#if IMMUTABILITY
static NoDbgRegParms sqInt
genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *immutabilityFailure;
    AbstractInstruction *mutableJump;

	mutableJump = genJumpMutablescratchReg(destReg, scratchReg);
	/* begin genStoreTrampolineCall: */
	assert(IMMUTABILITY);
	if (index >= (NumStoreTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, index, TempReg);
		abstractInstruction = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreTrampolines[index]);
		(abstractInstruction1->annotation = IsRelativeCall);
	}
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin annotateBytecode: */
	(abstractInstruction2->annotation = HasBytecodePC);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	if (needRestoreRcvr) {
		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
	}
	immutabilityFailure = genoperand(Jump, ((sqInt)0));
	jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		abstractInstruction3 = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction3->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}
#endif /* IMMUTABILITY */


/*	We know there is a frame as immutability check requires a frame */
/*	needRestoreRcvr has to be true to keep RcvrResultReg live with the
	receiver in it across the trampoline
 */
/*	Trampoline convention... */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityCheckSourceReg:slotIndex:destReg:scratchReg:needsStoreCheck:needRestoreRcvr: */
#if IMMUTABILITY
static NoDbgRegParms sqInt
genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needsStoreCheck, sqInt needRestoreRcvr)
{
	assert(destReg == ReceiverResultReg);
	assert(scratchReg == TempReg);
	assert(sourceReg == ClassReg);
	if (needsStoreCheck) {
		genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sourceReg, index, destReg, scratchReg, needRestoreRcvr);
	}
	else {
		genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sourceReg, index, destReg, scratchReg, needRestoreRcvr);
	}
	return 0;
}
#endif /* IMMUTABILITY */


/*	Make sure SendNumArgsReg and ClassReg are available in addition to
	ReceiverResultReg and TempReg in
	genGetActiveContextNumArgs:large:inBlock:. 
 */

	/* CogObjectRepresentationForSpur>>#getActiveContextAllocatesInMachineCode */
static sqInt
getActiveContextAllocatesInMachineCode(void)
{
	return 1;
}


/*	Since all cache tags in Spur are class indices none of
	them are young or have to be updated in a scavenge. */

	/* CogObjectRepresentationForSpur>>#inlineCacheTagIsYoung: */
static NoDbgRegParms sqInt
inlineCacheTagIsYoung(sqInt cacheTag)
{
	return 0;
}

	/* CogObjectRepresentationForSpur>>#jumpNotCharacterUnsignedValueInRegister: */
static NoDbgRegParms AbstractInstruction *
jumpNotCharacterUnsignedValueInRegister(sqInt reg)
{
    sqInt quickConstant;

	/* begin CmpCq:R: */
	quickConstant = (1U << (numCharacterBits())) - 1;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant, reg);
	return genConditionalBranchoperand(JumpAbove, ((sqInt)0));
}


/*	Mark and trace a literal in a machine code instruction preceding address
	in cogMethodOrNil.
	Answer if code was modified. */

	/* CogObjectRepresentationForSpur>>#markAndTraceLiteral:in:atpc: */
static NoDbgRegParms sqInt
markAndTraceLiteralinatpc(sqInt literal, CogMethod *cogMethodOrNil, usqInt address)
{
    sqInt objOop;

	if (!(couldBeObject(literal))) {
		return 0;
	}
	assert(addressCouldBeObj(literal));
	if (!(isForwarded(literal))) {
		markAndTrace(literal);
		return 0;
	}
	/* begin setCodeModified */
#  if DUAL_MAPPED_CODE_ZONE
	codeModified = 1;
#  else
	codeModified = 1;
#  endif

	objOop = followForwarded(literal);
	storeLiteralbeforeFollowingAddress(backEnd(), objOop, address);
	markAndTraceUpdatedLiteralin(objOop, cogMethodOrNil);
	return 1;
}


/*	Mark and trace a literal in a sqInt variable of cogMethod. */

	/* CogObjectRepresentationForSpur>>#markAndTraceLiteral:in:at: */
static NoDbgRegParms void
markAndTraceLiteralinat(sqInt literal, CogMethod *cogMethod, sqInt *address)
{
    sqInt objOop;

	if (!(couldBeObject(literal))) {
		return;
	}
	assert(addressCouldBeObj(literal));
	if (!(isForwarded(literal))) {
		markAndTrace(literal);
		return;
	}
	objOop = followForwarded(literal);
	address[0] = objOop;
	markAndTraceUpdatedLiteralin(objOop, cogMethod);
}


/*	Common code to mark a literal in cogMethod and add
	the cogMethod to youngReferrers if the literal is young. */

	/* CogObjectRepresentationForSpur>>#markAndTraceUpdatedLiteral:in: */
static NoDbgRegParms void
markAndTraceUpdatedLiteralin(sqInt objOop, CogMethod *cogMethodOrNil)
{
	if (isNonImmediate(objOop)) {
		if ((cogMethodOrNil)
		 && (isYoungObject(objOop))) {
			ensureInYoungReferrers(cogMethodOrNil);
		}
		markAndTrace(objOop);
	}
}


/*	If primIndex has an accessorDepth and fails, or it is external and fails
	with PrimErrNoMemory,
	call ceCheckAndMaybeRetryPrimitive if so If ceCheck.... answers true,
	retry the primitive. */

	/* CogObjectRepresentationForSpur>>#maybeCompileRetryOf:onPrimitiveFail:flags: */
static NoDbgRegParms sqInt
maybeCompileRetryOfonPrimitiveFailflags(void (*primitiveRoutine)(void), sqInt primIndex, sqInt flags)
{
    AbstractInstruction *jmp;

	if ((accessorDepthForPrimitiveIndex(primIndex)) >= 0) {
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
		genoperandoperand(CmpCqR, 0, TempReg);
		jmp = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	}
	else {
		if (PrimNumberExternalCall != primIndex) {
			return 0;
		}
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
		genoperandoperand(CmpCqR, PrimErrNoMemory, TempReg);
		jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	}
	if (!(((flags & PrimCallNeedsNewMethod) != 0))) {
		genLoadNewMethod();
	}
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveCwR, ((sqInt)primitiveRoutine), TempReg);
	genoperandoperand(MoveRAw, TempReg, primitiveFunctionPointerAddress());
	compileCallFornumArgsargargargargresultRegregsToSave(
		ceCheckAndMaybeRetryPrimitive,
		1,
		(		/* begin trampolineArgConstant: */
			assert(primIndex >= 0),
		-2 - primIndex),
		null,
		null,
		null,
		TempReg,
		0 /* begin emptyRegisterMask */);
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Generate a shift of the register containing the class tag in a method
	cache probe.
	c.f. SpurMemoryManager>>methodCacheHashOf:with: */

	/* CogObjectRepresentationForSpur>>#maybeShiftClassTagRegisterForMethodCacheProbe: */
static NoDbgRegParms sqInt
maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg)
{

	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, 2, classTagReg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#numCharacterBits */
static sqInt
numCharacterBits(void)
{
	return 30;
}

	/* CogObjectRepresentationForSpur>>#remapObject: */
static NoDbgRegParms sqInt
remapObject(sqInt objOop)
{
	assert(addressCouldBeObj(objOop));
	return (shouldRemapObj(objOop)
		? remapObj(objOop)
		: objOop);
}

	/* CogObjectRepresentationForSpur>>#remapOop: */
static NoDbgRegParms sqInt
remapOop(sqInt objOop)
{
	return (shouldRemapOop(objOop)
		? remapObj(objOop)
		: objOop);
}


/*	Objects in newSpace or oldSpace except nil, true, false &
	classTableRootObj need to be annotated.
 */

	/* CogObjectRepresentationForSpur>>#shouldAnnotateObjectReference: */
static NoDbgRegParms sqInt
shouldAnnotateObjectReference(sqInt anOop)
{
	return (isNonImmediate(anOop))
	 && ((oopisGreaterThan(anOop, classTableRootObj()))
	 || (oopisLessThan(anOop, nilObject())));
}

	/* CogObjectRepresentationForSpur>>#slotOffsetOfInstVarIndex: */
static NoDbgRegParms sqInt
slotOffsetOfInstVarIndex(sqInt index)
{
	return (index * BytesPerWord) + BaseHeaderSize;
}

	/* CogSimStackEntry>>#ensureSpilledAt:from: */
static NoDbgRegParms SimStackEntry *
ensureSpilledAtfrom(SimStackEntry *self_in_CogSimStackEntry, sqInt baseOffset, sqInt baseRegister)
{
    AbstractInstruction *inst;

	if ((self_in_CogSimStackEntry->spilled)) {
		if (((self_in_CogSimStackEntry->type)) == SSSpill) {
			assert(((((self_in_CogSimStackEntry->offset)) == baseOffset)
			 && (((self_in_CogSimStackEntry->registerr)) == baseRegister))
			 || (violatesEnsureSpilledSpillAssert()));
			return self_in_CogSimStackEntry;
		}
	}
	assert(((self_in_CogSimStackEntry->type)) != SSSpill);
	traceSpill(self_in_CogSimStackEntry);
	if (((self_in_CogSimStackEntry->type)) == SSConstant) {
		inst = 
		/* begin genPushConstant: */
((isNonImmediate((self_in_CogSimStackEntry->constant)))
		 && ((oopisGreaterThan((self_in_CogSimStackEntry->constant), classTableRootObj()))
		 || (oopisLessThan((self_in_CogSimStackEntry->constant), nilObject())))
			? annotateobjRef(genoperand(PushCw, (self_in_CogSimStackEntry->constant)), (self_in_CogSimStackEntry->constant))
			: genoperand(PushCq, (self_in_CogSimStackEntry->constant)));
	}
	else {
		if (((self_in_CogSimStackEntry->type)) == SSBaseOffset) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, (self_in_CogSimStackEntry->offset), (self_in_CogSimStackEntry->registerr), TempReg);
			inst = genoperand(PushR, TempReg);
		}
		else {
			assert(((self_in_CogSimStackEntry->type)) == SSRegister);
			inst = genoperand(PushR, (self_in_CogSimStackEntry->registerr));
		}
		(self_in_CogSimStackEntry->type) = SSSpill;
		(self_in_CogSimStackEntry->offset) = baseOffset;
		(self_in_CogSimStackEntry->registerr) = baseRegister;
	}
	(self_in_CogSimStackEntry->spilled) = 1;
	return self_in_CogSimStackEntry;
}

	/* CogSimStackEntry>>#floatRegisterMask */
static NoDbgRegParms sqInt
floatRegisterMask(SimStackEntry *self_in_CogSimStackEntry)
{
	return 0;
}

	/* CogSimStackEntry>>#isSameEntryAs: */
static NoDbgRegParms sqInt
isSameEntryAs(SimStackEntry *self_in_CogSimStackEntry, CogSimStackEntry *ssEntry)
{
	return (((self_in_CogSimStackEntry->type)) == ((ssEntry->type)))
	 && ((((((self_in_CogSimStackEntry->type)) == SSBaseOffset)
	 || (((self_in_CogSimStackEntry->type)) == SSSpill))
	 && ((((self_in_CogSimStackEntry->offset)) == ((ssEntry->offset)))
	 && (((self_in_CogSimStackEntry->registerr)) == ((ssEntry->registerr)))))
	 || (((((self_in_CogSimStackEntry->type)) == SSRegister)
	 && (((self_in_CogSimStackEntry->registerr)) == ((ssEntry->registerr))))
	 || ((((self_in_CogSimStackEntry->type)) == SSConstant)
	 && (((self_in_CogSimStackEntry->constant)) == ((ssEntry->constant))))));
}


/*	Receiver is not a forwarder, except in blocks with no inst var access.
	For now we optimize only the case where receiver is accessed in a method. */

	/* CogSimStackEntry>>#mayBeAForwarder */
static NoDbgRegParms sqInt
mayBeAForwarder(SimStackEntry *self_in_CogSimStackEntry)
{
	if ((((self_in_CogSimStackEntry->type)) == SSRegister)
	 && (isNonForwarderReceiver((self_in_CogSimStackEntry->registerr)))) {
		return 0;
	}
	return ((self_in_CogSimStackEntry->type)) != SSConstant;
}

	/* CogSimStackEntry>>#popToReg: */
static NoDbgRegParms SimStackEntry *
popToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg)
{
	if ((self_in_CogSimStackEntry->spilled)) {
		/* begin PopR: */
		genoperand(PopR, reg);
	}
	else {
		switch ((self_in_CogSimStackEntry->type)) {
		case SSBaseOffset:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, (self_in_CogSimStackEntry->offset), (self_in_CogSimStackEntry->registerr), reg);
			break;
		case SSConstant:
			/* begin genMoveConstant:R: */
			if (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate((self_in_CogSimStackEntry->constant)))
			 && ((oopisGreaterThan((self_in_CogSimStackEntry->constant), classTableRootObj()))
			 || (oopisLessThan((self_in_CogSimStackEntry->constant), nilObject())))) {
				annotateobjRef(genoperandoperand(MoveCwR, (self_in_CogSimStackEntry->constant), reg), (self_in_CogSimStackEntry->constant));
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(MoveCqR, (self_in_CogSimStackEntry->constant), reg);
			}
			break;
		case SSRegister:
			if (reg != ((self_in_CogSimStackEntry->registerr))) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, (self_in_CogSimStackEntry->registerr), reg);
			}
			else {
				/* begin Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	return self_in_CogSimStackEntry;
}


/*	Answer a bit mask for the receiver's register, if any. */

	/* CogSimStackEntry>>#registerMask */
static NoDbgRegParms sqInt
registerMask(SimStackEntry *self_in_CogSimStackEntry)
{
	return ((((self_in_CogSimStackEntry->type)) == SSBaseOffset)
	 || (((self_in_CogSimStackEntry->type)) == SSRegister)
		? ((((self_in_CogSimStackEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackEntry->registerr)))) : (1ULL << ((self_in_CogSimStackEntry->registerr))))
		: 0);
}

	/* CogSimStackEntry>>#registerMaskOrNone */
static NoDbgRegParms sqInt
registerMaskOrNone(SimStackEntry *self_in_CogSimStackEntry)
{
	return (((self_in_CogSimStackEntry->type)) == SSRegister
		? ((((self_in_CogSimStackEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackEntry->registerr)))) : (1ULL << ((self_in_CogSimStackEntry->registerr))))
		: 0);
}

	/* CogSimStackEntry>>#registerOrNone */
static NoDbgRegParms sqInt
registerOrNone(SimStackEntry *self_in_CogSimStackEntry)
{
	return (((self_in_CogSimStackEntry->type)) == SSRegister
		? (self_in_CogSimStackEntry->registerr)
		: NoReg);
}

	/* CogSimStackEntry>>#storeToReg: */
static NoDbgRegParms SimStackEntry *
storeToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg)
{
	switch ((self_in_CogSimStackEntry->type)) {
	case SSBaseOffset:
	case SSSpill:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, (self_in_CogSimStackEntry->offset), (self_in_CogSimStackEntry->registerr), reg);
		break;
	case SSConstant:
		/* begin genMoveConstant:R: */
		if (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate((self_in_CogSimStackEntry->constant)))
		 && ((oopisGreaterThan((self_in_CogSimStackEntry->constant), classTableRootObj()))
		 || (oopisLessThan((self_in_CogSimStackEntry->constant), nilObject())))) {
			annotateobjRef(genoperandoperand(MoveCwR, (self_in_CogSimStackEntry->constant), reg), (self_in_CogSimStackEntry->constant));
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, (self_in_CogSimStackEntry->constant), reg);
		}
		break;
	case SSRegister:
		if (reg != ((self_in_CogSimStackEntry->registerr))) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, (self_in_CogSimStackEntry->registerr), reg);
		}
		else {
			/* begin Label */
			genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	return self_in_CogSimStackEntry;
}

	/* CogSimStackNativeEntry>>#ensureIsMarkedAsSpilled */
static NoDbgRegParms CogSimStackNativeEntry *
ensureIsMarkedAsSpilled(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	if (!((self_in_CogSimStackNativeEntry->spilled))) {
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSNativeRegister:
		case SSConstantInt32:
		case SSConstantNativePointer:
			(self_in_CogSimStackNativeEntry->type) = SSSpillNative;
			break;
		case SSRegisterSingleFloat:
		case SSConstantFloat32:
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat32;
			break;
		case SSRegisterDoubleFloat:
		case SSConstantFloat64:
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat64;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	(self_in_CogSimStackNativeEntry->spilled) = 1;
	return self_in_CogSimStackNativeEntry;
}

	/* CogSimStackNativeEntry>>#ensureSpilledSP:scratchRegister: */
static NoDbgRegParms CogSimStackNativeEntry *
ensureSpilledSPscratchRegister(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt spRegister, sqInt scratchRegister)
{
    sqInt wordConstant;
    sqInt wordConstant1;

	if (!((self_in_CogSimStackNativeEntry->spilled))) {
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSNativeRegister:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveRMwr, (self_in_CogSimStackNativeEntry->registerr), (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillNative;
			break;
		case SSRegisterSingleFloat:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveRsM32r, (self_in_CogSimStackNativeEntry->registerr), (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat32;
			break;
		case SSRegisterDoubleFloat:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveRdM64r, (self_in_CogSimStackNativeEntry->registerr), (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat64;
			break;
		case SSConstantFloat32:
			/* begin checkLiteral:forInstruction: */
			genoperandoperand(MoveCwR, asIEEE32BitWord((self_in_CogSimStackNativeEntry->constantFloat32)), scratchRegister);
			genoperandoperandoperand(MoveRM32r, scratchRegister, (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat32;
			break;
		case SSConstantFloat64:
			if (BytesPerWord == 4) {
				/* begin MoveCw:R: */
				wordConstant = (asIEEE64BitWord((self_in_CogSimStackNativeEntry->constantFloat64))) & 0xFFFFFFFFU;
				/* begin checkLiteral:forInstruction: */
				genoperandoperand(MoveCwR, wordConstant, scratchRegister);
				genoperandoperandoperand(MoveRM32r, scratchRegister, (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
				wordConstant1 = (((((sqLong) -32)) < 0) ? (((usqInt)((asIEEE64BitWord((self_in_CogSimStackNativeEntry->constantFloat64))))) >> (-(((sqLong) -32)))) : (((sqInt)((usqInt)((asIEEE64BitWord((self_in_CogSimStackNativeEntry->constantFloat64)))) << (((sqLong) -32))))));
				/* begin checkLiteral:forInstruction: */
				genoperandoperand(MoveCwR, wordConstant1, scratchRegister);
				genoperandoperandoperand(MoveRM32r, scratchRegister, ((-((self_in_CogSimStackNativeEntry->offset))) - 1) + 4, spRegister);
			}
			else {
				/* begin checkLiteral:forInstruction: */
				genoperandoperand(MoveCwR, asIEEE64BitWord((self_in_CogSimStackNativeEntry->constantFloat32)), scratchRegister);
				genoperandoperandoperand(MoveRMwr, scratchRegister, (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			}
			(self_in_CogSimStackNativeEntry->type) = SSSpillFloat64;
			break;
		case SSConstantInt32:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, (self_in_CogSimStackNativeEntry->constantInt32), scratchRegister);
			genoperandoperandoperand(MoveRMwr, scratchRegister, (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillNative;
			break;
		case SSConstantNativePointer:
			/* begin checkLiteral:forInstruction: */
			genoperandoperand(MoveCwR, (self_in_CogSimStackNativeEntry->constantNativePointer), scratchRegister);
			genoperandoperandoperand(MoveRMwr, scratchRegister, (-((self_in_CogSimStackNativeEntry->offset))) - 1, spRegister);
			(self_in_CogSimStackNativeEntry->type) = SSSpillNative;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	(self_in_CogSimStackNativeEntry->spilled) = 1;
	return self_in_CogSimStackNativeEntry;
}


/*	Answer a bit mask for the receiver's register, if any. */

	/* CogSimStackNativeEntry>>#nativeFloatRegisterMask */
static NoDbgRegParms sqInt
nativeFloatRegisterMask(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	return ((((self_in_CogSimStackNativeEntry->type)) == SSRegisterSingleFloat)
	 || (((self_in_CogSimStackNativeEntry->type)) == SSRegisterDoubleFloat)
		? ((((self_in_CogSimStackNativeEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackNativeEntry->registerr)))) : (1ULL << ((self_in_CogSimStackNativeEntry->registerr))))
		: 0);
}

	/* CogSimStackNativeEntry>>#nativeFloatRegisterOrNone */
static NoDbgRegParms sqInt
nativeFloatRegisterOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	return ((((self_in_CogSimStackNativeEntry->type)) == SSRegisterSingleFloat)
	 || (((self_in_CogSimStackNativeEntry->type)) == SSRegisterDoubleFloat)
		? (self_in_CogSimStackNativeEntry->registerr)
		: NoReg);
}

	/* CogSimStackNativeEntry>>#nativePopToReg: */
static NoDbgRegParms CogSimStackNativeEntry *
nativePopToReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg)
{
    float constantFloat32;
    double constantFloat64;
    AbstractInstruction *inst;
    AbstractInstruction *inst1;
    AbstractInstruction *inst11;
    AbstractInstruction *inst2;

	if ((self_in_CogSimStackNativeEntry->spilled)) {
		loadNativeFramePointerInto(TempReg);
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSSpillNative:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, (-((self_in_CogSimStackNativeEntry->offset))) - 1, TempReg, reg);
			break;
		case SSSpillInt64:
			assert(BytesPerWord == 8);
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, (-((self_in_CogSimStackNativeEntry->offset))) - 1, TempReg, reg);
			break;
		case SSSpillFloat32:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveM32rRs, (-((self_in_CogSimStackNativeEntry->offset))) - 1, TempReg, reg);
			break;
		case SSSpillFloat64:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveM64rRd, (-((self_in_CogSimStackNativeEntry->offset))) - 1, TempReg, reg);
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	else {
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSNativeRegister:
			if (reg != ((self_in_CogSimStackNativeEntry->registerr))) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, (self_in_CogSimStackNativeEntry->registerr), reg);
			}
			break;
		case SSRegisterSingleFloat:
			if (reg != ((self_in_CogSimStackNativeEntry->registerr))) {
				/* begin MoveRs:Rs: */
				genoperandoperand(MoveRsRs, (self_in_CogSimStackNativeEntry->registerr), reg);
			}
			break;
		case SSRegisterDoubleFloat:
			if (reg != ((self_in_CogSimStackNativeEntry->registerr))) {
				/* begin MoveRd:Rd: */
				genoperandoperand(MoveRdRd, (self_in_CogSimStackNativeEntry->registerr), reg);
			}
			break;
		case SSConstantInt32:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, (self_in_CogSimStackNativeEntry->constantInt32), reg);
			break;
		case SSConstantNativePointer:
			/* begin checkLiteral:forInstruction: */
			genoperandoperand(MoveCwR, (self_in_CogSimStackNativeEntry->constantNativePointer), reg);
			break;
		case SSConstantFloat32:
			/* begin MoveCf32:Rs: */
			constantFloat32 = (self_in_CogSimStackNativeEntry->constantFloat32);
			/* begin genMoveCf32:Rs: */
			inst = genoperand(PushCw, asIEEE32BitWord(constantFloat32));
			inst1 = genoperandoperandoperand(MoveM32rRs, 0, SPReg, reg);
			genoperandoperand(AddCqR, 8, SPReg);
			break;
		case SSConstantFloat64:
			/* begin MoveCf64:Rd: */
			constantFloat64 = (self_in_CogSimStackNativeEntry->constantFloat64);
			/* begin genMoveCf64:Rd: */
			inst2 = genoperand(PushCw, asIEEE64BitWord(constantFloat64));
			inst11 = genoperandoperandoperand(MoveM64rRd, 0, SPReg, reg);
			genoperandoperand(AddCqR, 8, SPReg);
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	return self_in_CogSimStackNativeEntry;
}

	/* CogSimStackNativeEntry>>#nativePopToReg:secondReg: */
static NoDbgRegParms CogSimStackNativeEntry *
nativePopToRegsecondReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg, sqInt secondReg)
{
	assert(BytesPerWord == 4);
	if ((self_in_CogSimStackNativeEntry->spilled)) {
		/* begin PopR: */
		genoperand(PopR, reg);
		genoperand(PopR, secondReg);
	}
	else {
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSConstantInt64:
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, ((self_in_CogSimStackNativeEntry->constantInt64)) & 0xFFFFFFFFU, reg);
			genoperandoperand(MoveCqR, (((usqLong)(((self_in_CogSimStackNativeEntry->constantInt64)))) >> 32) & 0xFFFFFFFFU, secondReg);
			break;
		case SSRegisterPair:
			/* begin Label */
			genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			if (reg != ((self_in_CogSimStackNativeEntry->registerr))) {
				if (((self_in_CogSimStackNativeEntry->registerSecond)) == reg) {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, (self_in_CogSimStackNativeEntry->registerSecond), TempReg);
				}
				genoperandoperand(MoveRR, (self_in_CogSimStackNativeEntry->registerr), reg);
			}
			if (((self_in_CogSimStackNativeEntry->registerSecond)) != secondReg) {
				if (((self_in_CogSimStackNativeEntry->registerSecond)) == reg) {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, TempReg, secondReg);
				}
				else {
					/* begin MoveR:R: */
					genoperandoperand(MoveRR, (self_in_CogSimStackNativeEntry->registerSecond), secondReg);
				}
			}
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	return self_in_CogSimStackNativeEntry;
}


/*	Answer a bit mask for the receiver's register, if any. */

	/* CogSimStackNativeEntry>>#nativeRegisterMask */
static NoDbgRegParms sqInt
nativeRegisterMask(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	return ((((((self_in_CogSimStackNativeEntry->type)) == SSBaseOffset)
	 || (((self_in_CogSimStackNativeEntry->type)) == SSNativeRegister))
	 || (((self_in_CogSimStackNativeEntry->type)) == SSRegisterSingleFloat))
	 || (((self_in_CogSimStackNativeEntry->type)) == SSRegisterDoubleFloat)
		? ((((self_in_CogSimStackNativeEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackNativeEntry->registerr)))) : (1ULL << ((self_in_CogSimStackNativeEntry->registerr))))
		: (((self_in_CogSimStackNativeEntry->type)) == SSRegisterPair
				? (((((self_in_CogSimStackNativeEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackNativeEntry->registerr)))) : (1ULL << ((self_in_CogSimStackNativeEntry->registerr))))) | (((((self_in_CogSimStackNativeEntry->registerSecond)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackNativeEntry->registerSecond)))) : (1ULL << ((self_in_CogSimStackNativeEntry->registerSecond)))))
				: 0));
}

	/* CogSimStackNativeEntry>>#nativeRegisterOrNone */
static NoDbgRegParms sqInt
nativeRegisterOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	return ((((self_in_CogSimStackNativeEntry->type)) == SSNativeRegister)
	 || (((self_in_CogSimStackNativeEntry->type)) == SSRegisterPair)
		? (self_in_CogSimStackNativeEntry->registerr)
		: NoReg);
}

	/* CogSimStackNativeEntry>>#nativeRegisterSecondOrNone */
static NoDbgRegParms sqInt
nativeRegisterSecondOrNone(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	return (((self_in_CogSimStackNativeEntry->type)) == SSRegisterPair
		? (self_in_CogSimStackNativeEntry->registerSecond)
		: NoReg);
}

	/* CogSimStackNativeEntry>>#nativeStackPopToReg: */
static NoDbgRegParms CogSimStackNativeEntry *
nativeStackPopToReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg)
{
	assert((self_in_CogSimStackNativeEntry->spilled));
	switch ((self_in_CogSimStackNativeEntry->type)) {
	case SSSpillNative:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, -((self_in_CogSimStackNativeEntry->offset)), FPReg, reg);
		break;
	case SSSpillInt64:
		assert(BytesPerWord == 8);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, -((self_in_CogSimStackNativeEntry->offset)), FPReg, reg);
		break;
	case SSSpillFloat32:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rRs, -((self_in_CogSimStackNativeEntry->offset)), FPReg, reg);
		break;
	case SSSpillFloat64:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM64rRd, -((self_in_CogSimStackNativeEntry->offset)), FPReg, reg);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	return self_in_CogSimStackNativeEntry;
}

	/* CogSimStackNativeEntry>>#nativeStackPopToReg:secondReg: */
static NoDbgRegParms CogSimStackNativeEntry *
nativeStackPopToRegsecondReg(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry, sqInt reg, sqInt secondReg)
{
	assert((self_in_CogSimStackNativeEntry->spilled));
	switch ((self_in_CogSimStackNativeEntry->type)) {
	case SSSpillInt64:
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, (-((self_in_CogSimStackNativeEntry->offset))) + 4, FPReg, reg);
		genoperandoperandoperand(MoveMwrR, -((self_in_CogSimStackNativeEntry->offset)), FPReg, secondReg);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	return self_in_CogSimStackNativeEntry;
}

	/* CogSimStackNativeEntry>>#spillingNeedsScratchRegister */
static NoDbgRegParms sqInt
spillingNeedsScratchRegister(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	if (!((self_in_CogSimStackNativeEntry->spilled))) {
		switch ((self_in_CogSimStackNativeEntry->type)) {
		case SSConstantInt32:
		case SSConstantInt64:
		case SSConstantFloat32:
		case SSConstantFloat64:
		case SSConstantNativePointer:
			return 1;

		default:
			return 0;
		}
	}
	return 0;
}

	/* CogSimStackNativeEntry>>#stackSpillSize */
static NoDbgRegParms sqInt
stackSpillSize(CogSimStackNativeEntry *self_in_CogSimStackNativeEntry)
{
	switch ((self_in_CogSimStackNativeEntry->type)) {
	case SSConstantInt64:
	case SSConstantFloat64:
	case SSRegisterDoubleFloat:
	case SSRegisterPair:
	case SSSpillFloat64:
	case SSSpillInt64:
		return 8;

	default:
		return BytesPerOop;
	}
	return 0;
}

	/* CogSSBytecodeFixup>>#isMergeFixup */
static NoDbgRegParms int
isMergeFixup(BytecodeFixup *self_in_CogSSBytecodeFixup)
{
	return (((usqInt)((self_in_CogSSBytecodeFixup->targetInstruction)))) == NeedsMergeFixupFlag;
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them.
	N.B. Do /not/ allocate TempReg. */
/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them.
	N.B. Do /not/ allocate TempReg. */

	/* CogX64Compiler>>#availableRegisterOrNoneFor: */
static NoDbgRegParms sqInt
availableRegisterOrNoneFor(AbstractInstruction *self_in_CogX64Compiler, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << Extra5Reg))) != 0))) {
		return Extra5Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra4Reg))) != 0))) {
		return Extra4Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra3Reg))) != 0))) {
		return Extra3Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra2Reg))) != 0))) {
		return Extra2Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra1Reg))) != 0))) {
		return Extra1Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra0Reg))) != 0))) {
		return Extra0Reg;
	}
	if (!(((liveRegsMask & ((1U << Arg1Reg))) != 0))) {
		return Arg1Reg;
	}
	if (!(((liveRegsMask & ((1U << Arg0Reg))) != 0))) {
		return Arg0Reg;
	}
	if (!(((liveRegsMask & ((1U << SendNumArgsReg))) != 0))) {
		return SendNumArgsReg;
	}
	if (!(((liveRegsMask & ((1U << ClassReg))) != 0))) {
		return ClassReg;
	}
	if (!(((liveRegsMask & ((1U << ReceiverResultReg))) != 0))) {
		return ReceiverResultReg;
	}
	return NoReg;
}

	/* CogX64Compiler>>#callFullInstructionByteSize */
static NoDbgRegParms sqInt
callFullInstructionByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 12;
}


/*	Answer the address the full call immediately preceding
	callSiteReturnAddress will jump to.
 */

	/* CogX64Compiler>>#callFullTargetFromReturnAddress: */
static NoDbgRegParms sqInt
callFullTargetFromReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress)
{
	return unalignedLongAt((callSiteReturnAddress - 2) - 8);
}

	/* CogX64Compiler>>#callInstructionByteSize */
static NoDbgRegParms sqInt
callInstructionByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 5;
}


/*	Answer the address the call immediately preceding callSiteReturnAddress
	will jump to.
 */

	/* CogX64Compiler>>#callTargetFromReturnAddress: */
static NoDbgRegParms sqInt
callTargetFromReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress)
{
    unsigned int callDistance;

	callDistance = literal32BeforeFollowingAddress(self_in_CogX64Compiler, callSiteReturnAddress);
	return callSiteReturnAddress + (((int) callDistance));
}

	/* CogX64Compiler>>#cFloatResultToRd: */
static NoDbgRegParms AbstractInstruction *
cFloatResultToRd(AbstractInstruction *self_in_CogX64Compiler, sqInt reg)
{
	if (XMM0L != reg) {
		/* begin MoveRd:Rd: */
		genoperandoperand(MoveRdRd, XMM0L, reg);
	}
	return self_in_CogX64Compiler;
}

	/* CogX64Compiler>>#cFloatResultToRs: */
static NoDbgRegParms AbstractInstruction *
cFloatResultToRs(AbstractInstruction *self_in_CogX64Compiler, sqInt reg)
{
	if (XMM0L != reg) {
		/* begin MoveRs:Rs: */
		genoperandoperand(MoveRsRs, XMM0L, reg);
	}
	return self_in_CogX64Compiler;
}

	/* CogX64Compiler>>#cmpC32RTempByteSize */
static NoDbgRegParms sqInt
cmpC32RTempByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 5;
}


/*	Compute the maximum size for each opcode. This allows jump offsets to
	be determined, provided that all backward branches are long branches. */
/*	N.B. The ^N forms are to get around the bytecode compiler's long branch
	limits which are exceeded when each case jumps around the otherwise. */

	/* CogX64Compiler>>#computeMaximumSize */
static NoDbgRegParms sqInt
computeMaximumSize(AbstractInstruction *self_in_CogX64Compiler)
{
	switch ((self_in_CogX64Compiler->opcode)) {
	case Label:
		return 0;

	case AlignmentNops:
		return (((self_in_CogX64Compiler->operands))[0]) - 1;

	case Fill32:
	case IMULRR:
	case CMPXCHGRMr:
	case SETE:
	case BSR:
	case SignExtend8RR:
	case SignExtend16RR:
	case ZeroExtend8RR:
	case ZeroExtend16RR:
		return 4;

	case Nop:
	case LOCK:
	case REP:
	case CLD:
	case MOVSB:
	case Stop:
		return 1;

	case CDQ:
	case CPUID:
	case MOVSQ:
		return 2;

	case IDIVR:
	case LFENCE:
	case MFENCE:
	case SFENCE:
	case CallR:
	case JumpR:
	case AddRR:
	case AddcRR:
	case AndRR:
	case CmpRR:
	case OrRR:
	case XorRR:
	case SubRR:
	case SubbRR:
	case NegateR:
	case MoveRR:
	case SignExtend32RR:
		return 3;

	case XCHGRR:
		return (((((self_in_CogX64Compiler->operands))[0]) == RAX)
		 || ((((self_in_CogX64Compiler->operands))[1]) == RAX)
			? 2
			: 3);

	case MoveRAwNoVBR:
		return ((((self_in_CogX64Compiler->operands))[0]) == RAX
			? 10
			: (((((self_in_CogX64Compiler->operands))[0]) == RBP)
				 || ((((self_in_CogX64Compiler->operands))[0]) == RSP)
					? 13
					: 14));

	case CallFull:
		return 12;

	case Call:
	case ClzRR:
	case MoveRRd:
	case MoveRdR:
	case ConvertRRd:
	case ConvertRdR:
		return 5;

	case JumpFull:
		resolveJumpTarget(self_in_CogX64Compiler);
		return 12;

	case JumpLong:
	case Jump:
		resolveJumpTarget(self_in_CogX64Compiler);
		return 5;

	case JumpZero:
	case JumpNonZero:
	case JumpNegative:
	case JumpNonNegative:
	case JumpOverflow:
	case JumpNoOverflow:
	case JumpCarry:
	case JumpNoCarry:
	case JumpLess:
	case JumpGreaterOrEqual:
	case JumpGreater:
	case JumpLessOrEqual:
	case JumpBelow:
	case JumpAboveOrEqual:
	case JumpAbove:
	case JumpBelowOrEqual:
	case JumpLongZero:
	case JumpLongNonZero:
	case JumpFPEqual:
	case JumpFPNotEqual:
	case JumpFPLess:
	case JumpFPGreaterOrEqual:
	case JumpFPGreater:
	case JumpFPLessOrEqual:
	case JumpFPOrdered:
	case JumpFPUnordered:
		resolveJumpTarget(self_in_CogX64Compiler);
		return 6;

	case RetN:
		return ((((self_in_CogX64Compiler->operands))[0]) == 0
			? 1
			: 3);

	case AddCqR:
	case AddcCqR:
	case AndCqR:
	case CmpCqR:
	case OrCqR:
	case SubCqR:
	case SubbCqR:
	case TstCqR:
		/* begin computeSizeOfArithCqR */
		if (isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])) {
			return 4;
		}
		if (is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])) {
			return ((((self_in_CogX64Compiler->operands))[1]) == RAX
				? 6
				: 7);
		}
		return 13;

	case AddCwR:
	case AndCwR:
	case CmpCwR:
	case OrCwR:
	case SubCwR:
	case XorCwR:
		return 13;

	case CmpC32R:
		return ((((self_in_CogX64Compiler->operands))[1]) <= 7
			? ((((self_in_CogX64Compiler->operands))[1]) == RAX
					? 5
					: 6)
			: 7);

	case LoadEffectiveAddressMwrR:
		return ((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
	? 4
	: 7)) + ((((((self_in_CogX64Compiler->operands))[1]) & 7) == RSP
	? 1
	: 0));

	case LogicalShiftLeftCqR:
	case LogicalShiftRightCqR:
	case ArithmeticShiftRightCqR:
	case RotateRightCqR:
	case RotateLeftCqR:
		return ((((self_in_CogX64Compiler->operands))[0]) == 1
			? 3
			: 4);

	case LogicalShiftLeftRR:
	case LogicalShiftRightRR:
	case ArithmeticShiftRightRR:
		return computeShiftRRSize(self_in_CogX64Compiler);

	case AddRdRd:
	case CmpRdRd:
	case SubRdRd:
	case MulRdRd:
	case DivRdRd:
	case XorRdRd:
	case AddRsRs:
	case SubRsRs:
	case MulRsRs:
	case DivRsRs:
	case MoveRdRd:
	case MoveRsRs:
	case ConvertRRs:
	case ConvertRsR:
	case ConvertRsRd:
	case ConvertRdRs:
		return 4 + ((((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[0]) > 7)
	? 1
	: 0));

	case SqrtRd:
	case SqrtRs:
		return 4 + (((((self_in_CogX64Compiler->operands))[0]) > 7
	? 1
	: 0));

	case CmpRsRs:
	case XorRsRs:
		return 3 + ((((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[0]) > 7)
	? 1
	: 0));

	case MoveCqR:
		return ((((self_in_CogX64Compiler->operands))[0]) == 0
			? 3
			: (is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
					? 7
					: 10));

	case MoveCwR:
		return (		/* begin inCurrentCompilation: */
			((addressIsInInstructions(((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]))))
		 || ((((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]))) == (methodLabel())))
		 || (((((usqInt)(((self_in_CogX64Compiler->operands))[0]))) >= ((methodLabel->address)))
		 && ((((usqInt)(((self_in_CogX64Compiler->operands))[0]))) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize)))))
			? 7
			: 11 /* begin moveCwRByteSize */);

	case MoveC32R:
		return 7;

	case MoveAwR:
		return (		/* begin isAddressRelativeToVarBase: */
			((((self_in_CogX64Compiler->operands))[0]))
		 && (((((self_in_CogX64Compiler->operands))[0]) >= (varBaseAddress()))
		 && (((((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress())) < (0x100000)))
			? (isQuick(self_in_CogX64Compiler, (((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress()))
					? ((((((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress())) == 0)
						 && (((((self_in_CogX64Compiler->operands))[1]) & 7) != RBP)
							? 3
							: 4)
					: 7)
			: ((((self_in_CogX64Compiler->operands))[1]) == RAX
					? 10
					: (((((self_in_CogX64Compiler->operands))[1]) == RBP)
						 || ((((self_in_CogX64Compiler->operands))[1]) == RSP)
							? 12
							: 14)));

	case MoveA32R:
		return ((((self_in_CogX64Compiler->operands))[1]) == RAX
			? 9
			: 13);

	case MoveRAw:
		return (		/* begin isAddressRelativeToVarBase: */
			((((self_in_CogX64Compiler->operands))[1]))
		 && (((((self_in_CogX64Compiler->operands))[1]) >= (varBaseAddress()))
		 && (((((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress())) < (0x100000)))
			? (isQuick(self_in_CogX64Compiler, (((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress()))
					? ((((((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress())) == 0)
						 && (((((self_in_CogX64Compiler->operands))[0]) & 7) != RBP)
							? 3
							: 4)
					: 7)
			: ((((self_in_CogX64Compiler->operands))[0]) == RAX
					? 10
					: (((((self_in_CogX64Compiler->operands))[0]) == RBP)
						 || ((((self_in_CogX64Compiler->operands))[0]) == RSP)
							? 13
							: 14)));

	case MoveRA32:
		return ((((self_in_CogX64Compiler->operands))[0]) == RAX
			? 9
			: 13);

	case MoveAbR:
		return (		/* begin isAddressRelativeToVarBase: */
			((((self_in_CogX64Compiler->operands))[0]))
		 && (((((self_in_CogX64Compiler->operands))[0]) >= (varBaseAddress()))
		 && (((((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress())) < (0x100000)))
			? (isQuick(self_in_CogX64Compiler, (((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress()))
					? ((((((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress())) == 0)
						 && (((((self_in_CogX64Compiler->operands))[1]) & 7) != RBP)
							? 3
							: 4)
					: 7)
			: ((((self_in_CogX64Compiler->operands))[1]) == RAX
					? 10
					: 14));

	case MoveRAb:
		return (		/* begin isAddressRelativeToVarBase: */
			((((self_in_CogX64Compiler->operands))[1]))
		 && (((((self_in_CogX64Compiler->operands))[1]) >= (varBaseAddress()))
		 && (((((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress())) < (0x100000)))
			? (isQuick(self_in_CogX64Compiler, (((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress()))
					? ((((((self_in_CogX64Compiler->operands))[1]) - (varBaseAddress())) == 0)
						 && (((((self_in_CogX64Compiler->operands))[0]) & 7) != RBP)
							? 3
							: 4)
					: 7)
			: ((((self_in_CogX64Compiler->operands))[0]) == RAX
					? 10
					: 14));

	case MoveRMwr:
		assert(is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1]));
		return ((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1])
	? (((((self_in_CogX64Compiler->operands))[1]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[2]) & 7) != RBP)
			? 3
			: 4)
	: 7)) + ((((((self_in_CogX64Compiler->operands))[2]) & 7) == RSP
	? 1
	: 0));

	case MoveRM32r:
		return (((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1])
	? (((((self_in_CogX64Compiler->operands))[1]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[2]) & 6) != RSP)
			? 2
			: 3)
	: 6)) + (((((((self_in_CogX64Compiler->operands))[2]) & 7) == RSP)
 && ((((self_in_CogX64Compiler->operands))[1]) != 0)
	? 1
	: 0))) + ((((((self_in_CogX64Compiler->operands))[2]) > 7)
 || ((((self_in_CogX64Compiler->operands))[0]) > 7)
	? 1
	: 0));

	case MoveRsM32r:
	case MoveRdM64r:
		return (((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1])
	? (((((self_in_CogX64Compiler->operands))[1]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[2]) & 6) != RSP)
			? 4
			: 5)
	: 8)) + (((((((self_in_CogX64Compiler->operands))[2]) & 7) == RSP)
 && ((((self_in_CogX64Compiler->operands))[1]) != 0)
	? 1
	: 0))) + ((((((self_in_CogX64Compiler->operands))[2]) > 7)
 || ((((self_in_CogX64Compiler->operands))[0]) > 7)
	? 1
	: 0));

	case MoveMbrR:
	case MoveM8rR:
	case MoveMs8rR:
	case MoveMwrR:
		assert(is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0]));
		return ((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
	? (((((self_in_CogX64Compiler->operands))[0]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[1]) & 7) != RBP)
			? 3
			: 4)
	: 7)) + ((((((self_in_CogX64Compiler->operands))[1]) & 7) == RSP
	? 1
	: 0));

	case MoveRMbr:
	case MoveRM8r:
		assert(is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1]));
		return ((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1])
	? (((((self_in_CogX64Compiler->operands))[1]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[0]) & 7) != RBP)
			? 3
			: 4)
	: 7)) + ((((((self_in_CogX64Compiler->operands))[2]) & 7) == RSP
	? 1
	: 0));

	case MoveM16rR:
		assert(is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0]));
		return ((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
	? (((((self_in_CogX64Compiler->operands))[0]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[1]) & 7) != RBP)
			? 4
			: 5)
	: 8)) + ((((((self_in_CogX64Compiler->operands))[1]) & 7) == RSP
	? 1
	: 0));

	case MoveRM16r:
		assert(is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1]));
		return (((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[1])
	? 4
	: 7)) + ((((((self_in_CogX64Compiler->operands))[2]) & 7) == RSP
	? 1
	: 0))) + ((((((self_in_CogX64Compiler->operands))[0]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7)
	? 1
	: 0));

	case MoveM32rR:
		return (((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
	? (((((self_in_CogX64Compiler->operands))[0]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[1]) & 6) != RSP)
			? 2
			: 3)
	: 6)) + (((((((self_in_CogX64Compiler->operands))[1]) & 7) == RSP)
 && ((((self_in_CogX64Compiler->operands))[0]) != 0)
	? 1
	: 0))) + ((((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7)
	? 1
	: 0));

	case MoveM32rRs:
	case MoveM64rRd:
		return (((isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
	? (((((self_in_CogX64Compiler->operands))[0]) == 0)
		 && (((((self_in_CogX64Compiler->operands))[1]) & 6) != RSP)
			? 4
			: 5)
	: 8)) + (((((((self_in_CogX64Compiler->operands))[1]) & 7) == RSP)
 && ((((self_in_CogX64Compiler->operands))[0]) != 0)
	? 1
	: 0))) + ((((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7)
	? 1
	: 0));

	case MoveXbrRR:
		assert((((self_in_CogX64Compiler->operands))[0]) != RSP);
		return (((((self_in_CogX64Compiler->operands))[1]) & 7) == RBP
			? 5
			: 4);

	case MoveRXbrR:
		assert((((self_in_CogX64Compiler->operands))[1]) != RSP);
		return ((((((self_in_CogX64Compiler->operands))[0]) > 3)
 || (((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7))
	? 4
	: 3)) + ((((((self_in_CogX64Compiler->operands))[2]) & 7) == RBP
	? 1
	: 0));

	case MoveXwrRR:
		assert((((self_in_CogX64Compiler->operands))[0]) != RSP);
		return (((((self_in_CogX64Compiler->operands))[1]) == RBP)
		 || ((((self_in_CogX64Compiler->operands))[1]) == R13)
			? 5
			: 4);

	case MoveRXwrR:
		assert((((self_in_CogX64Compiler->operands))[1]) != RSP);
		return (((((self_in_CogX64Compiler->operands))[2]) == RBP)
		 || ((((self_in_CogX64Compiler->operands))[2]) == R13)
			? 5
			: 4);

	case MoveX32rRR:
		assert((((self_in_CogX64Compiler->operands))[0]) != RSP);
		return ((((((self_in_CogX64Compiler->operands))[1]) == RBP)
 || ((((self_in_CogX64Compiler->operands))[1]) == R13)
	? 7
	: 6)) + ((((((self_in_CogX64Compiler->operands))[0]) > 7)
 || (((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7))
	? 1
	: 0));

	case MoveRX32rR:
		assert((((self_in_CogX64Compiler->operands))[1]) != RSP);
		return ((((((self_in_CogX64Compiler->operands))[2]) == RBP)
 || ((((self_in_CogX64Compiler->operands))[2]) == R13)
	? 4
	: 3)) + ((((((self_in_CogX64Compiler->operands))[0]) > 7)
 || (((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[2]) > 7))
	? 1
	: 0));

	case PopR:
	case PushR:
		return ((((self_in_CogX64Compiler->operands))[0]) < 8
			? 1
			: 2);

	case PushCq:
		return (isQuick(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
			? 2
			: (is32BitSignedImmediate(self_in_CogX64Compiler, ((self_in_CogX64Compiler->operands))[0])
					? 5
					: computeSizeOfPushCw(self_in_CogX64Compiler)));

	case PushCw:
		return computeSizeOfPushCw(self_in_CogX64Compiler);

	case PrefetchAw:
		return (		/* begin isAddressRelativeToVarBase: */
			((((self_in_CogX64Compiler->operands))[0]))
		 && (((((self_in_CogX64Compiler->operands))[0]) >= (varBaseAddress()))
		 && (((((self_in_CogX64Compiler->operands))[0]) - (varBaseAddress())) < (0x100000)))
			? 7
			: 0);

	case ZeroExtend32RR:
		return 2 + ((((((self_in_CogX64Compiler->operands))[1]) > 7)
 || ((((self_in_CogX64Compiler->operands))[0]) > 7)
	? 1
	: 0));

	case MovePerfCnt64RL:
		/* it is easier to calculate the right value by concretizing.  We concretize twice, but so what? */
		return concretizeMovePerfCnt64RL(self_in_CogX64Compiler);

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	On the x86 the only instructions that shift by the value of a
	register require the shift count to be in %ecx. So we may
	have to use swap instructions to get the count into ecx. */

	/* CogX64Compiler>>#computeShiftRRSize */
static NoDbgRegParms int
computeShiftRRSize(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt shiftCountReg;

	shiftCountReg = ((self_in_CogX64Compiler->operands))[0];
	return (shiftCountReg == RCX
		? 3
		: (shiftCountReg == RAX
				? 7
				: 9));
}

	/* CogX64Compiler>>#computeSizeOfPushCw */
static NoDbgRegParms sqInt
computeSizeOfPushCw(AbstractInstruction *self_in_CogX64Compiler)
{
	return (	/* begin inCurrentCompilation: */
		((addressIsInInstructions(((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]))))
	 || ((((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]))) == (methodLabel())))
	 || (((((usqInt)(((self_in_CogX64Compiler->operands))[0]))) >= ((methodLabel->address)))
	 && ((((usqInt)(((self_in_CogX64Compiler->operands))[0]))) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize)))))
		? 9
		: 12 /* begin pushCwByteSize */);
}


/*	Will get inlined into concretizeAt: switch. */

	/* CogX64Compiler>>#concretizeArithCqRWithRO:raxOpcode: */
static NoDbgRegParms sqInt
concretizeArithCqRWithROraxOpcode(AbstractInstruction *self_in_CogX64Compiler, sqInt regOpcode, sqInt raxOpcode)
{
    usqInt reg;
    usqInt value;

	value = ((self_in_CogX64Compiler->operands))[0];
	reg = ((self_in_CogX64Compiler->operands))[1];
	((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg));
	if (isQuick(self_in_CogX64Compiler, value)) {
		((self_in_CogX64Compiler->machineCode))[1] = 131;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg, regOpcode));
		((self_in_CogX64Compiler->machineCode))[3] = (value & 0xFF);
		return 4;
	}
	if (is32BitSignedImmediate(self_in_CogX64Compiler, value)) {
		if (reg == RAX) {
			((self_in_CogX64Compiler->machineCode))[1] = raxOpcode;
			((self_in_CogX64Compiler->machineCode))[2] = (value & 0xFF);
			((self_in_CogX64Compiler->machineCode))[3] = (((value) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((value) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((value) >> 24) & 0xFF);
			return 6;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 129;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg, regOpcode));
		((self_in_CogX64Compiler->machineCode))[3] = (value & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value) >> 24) & 0xFF);
		return 7;
	}
	return concretizeArithCwR(self_in_CogX64Compiler, (raxOpcode == 61
		? 57
		: raxOpcode - 2));
}

	/* CogX64Compiler>>#concretizeFill32 */
static NoDbgRegParms sqInt
concretizeFill32(AbstractInstruction *self_in_CogX64Compiler)
{
    usqIntptr_t word;

	word = ((self_in_CogX64Compiler->operands))[0];
	((self_in_CogX64Compiler->machineCode))[0] = (word & 0xFF);
	((self_in_CogX64Compiler->machineCode))[1] = (((word) >> 8) & 0xFF);
	((self_in_CogX64Compiler->machineCode))[2] = (((word) >> 16) & 0xFF);
	((self_in_CogX64Compiler->machineCode))[3] = (((word) >> 24) & 0xFF);
	return 4;
}


/*	Generate code for
	0x0: 50					pushq	%rax
	0x1: 52					pushq	%rdx
	0x2: 0f 31				rdtsc
	0x4: 48 c1 e2 20		shlq	$0x20, %rdx
	0x8: 48 09 d0			orq		%rdx, %rax
	0xb: 48 89 f8			movq	%rdi, %rax
	0xe: 5a					popq	%rdx
	0xf: 58					popq	%rax
	et al */

	/* CogX64Compiler>>#concretizeMovePerfCnt64RL */
static NoDbgRegParms sqInt
concretizeMovePerfCnt64RL(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt liveRegisterMask;
    sqInt offset;
    usqInt reg;

	reg = ((self_in_CogX64Compiler->operands))[0];
	liveRegisterMask = ((self_in_CogX64Compiler->operands))[1];
	offset = 0;
	if (((liveRegisterMask & ((1U << RAX))) != 0)) {
		((self_in_CogX64Compiler->machineCode))[0] = 80;
		offset += 1;
	}
	if (((liveRegisterMask & ((1U << RDX))) != 0)) {
		((self_in_CogX64Compiler->machineCode))[offset] = 82;
		offset += 1;
	}
	assert(!((reg == RDX)));
	((self_in_CogX64Compiler->machineCode))[offset] = 15;
	((self_in_CogX64Compiler->machineCode))[offset + 1] = 49;
	((self_in_CogX64Compiler->machineCode))[offset + 2] = (rexRxb(self_in_CogX64Compiler, 0, 0, RDX));
	((self_in_CogX64Compiler->machineCode))[offset + 3] = 193;
	((self_in_CogX64Compiler->machineCode))[offset + 4] = (modRMRO(self_in_CogX64Compiler, ModReg, RDX, 4));
	((self_in_CogX64Compiler->machineCode))[offset + 5] = 32;
	((self_in_CogX64Compiler->machineCode))[offset + 6] = (rexRxb(self_in_CogX64Compiler, RDX, 0, RAX));
	((self_in_CogX64Compiler->machineCode))[offset + 7] = 11;
	((self_in_CogX64Compiler->machineCode))[offset + 8] = (modRMRO(self_in_CogX64Compiler, ModReg, RAX, RDX));
	offset += 9;
	if (reg != RAX) {
		((self_in_CogX64Compiler->machineCode))[offset] = (rexRxb(self_in_CogX64Compiler, RAX, 0, reg));
		((self_in_CogX64Compiler->machineCode))[offset + 1] = 137;
		((self_in_CogX64Compiler->machineCode))[offset + 2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg, RAX));
		offset += 3;
	}
	if (((liveRegisterMask & ((1U << RDX))) != 0)) {
		((self_in_CogX64Compiler->machineCode))[offset] = 90;
		offset += 1;
	}
	if (((liveRegisterMask & ((1U << RAX))) != 0)) {
		((self_in_CogX64Compiler->machineCode))[offset] = 88;
		offset += 1;
	}
	return offset;
}

	/* CogX64Compiler>>#concretizeMoveRX32rR */
static NoDbgRegParms sqInt
concretizeMoveRX32rR(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt base;
    usqInt index;
    sqInt offset;
    usqInt src;

	src = ((self_in_CogX64Compiler->operands))[0];
	index = ((self_in_CogX64Compiler->operands))[1];
	base = ((self_in_CogX64Compiler->operands))[2];
	if ((index > 7)
	 || ((base > 7)
	 || (src > 7))) {
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 0, src, index, base));
		offset = 1;
	}
	else {
		offset = 0;
	}
	if ((base & 7) != RBP) {
		((self_in_CogX64Compiler->machineCode))[offset] = 137;
		((self_in_CogX64Compiler->machineCode))[offset + 1] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, src));
		((self_in_CogX64Compiler->machineCode))[offset + 2] = (sib(self_in_CogX64Compiler, SIB4, index, base));
		return offset + 3;
	}
	((self_in_CogX64Compiler->machineCode))[offset] = 137;
	((self_in_CogX64Compiler->machineCode))[offset + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, src));
	((self_in_CogX64Compiler->machineCode))[offset + 2] = (sib(self_in_CogX64Compiler, SIB4, index, base));
	((self_in_CogX64Compiler->machineCode))[offset + 3] = 0;
	return offset + 4;
}


/*	MoveX32rRR is expected to zero-extend, so explicitly zero the destination. */

	/* CogX64Compiler>>#concretizeMoveX32rRR */
static NoDbgRegParms sqInt
concretizeMoveX32rRR(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt base;
    usqInt dest;
    usqInt index;
    sqInt offset;

	index = ((self_in_CogX64Compiler->operands))[0];
	base = ((self_in_CogX64Compiler->operands))[1];
	dest = ((self_in_CogX64Compiler->operands))[2];
	((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, dest, 0, dest));
	((self_in_CogX64Compiler->machineCode))[1] = 49;
	((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, dest, dest));
	if ((index > 7)
	 || ((base > 7)
	 || (dest > 7))) {
		((self_in_CogX64Compiler->machineCode))[3] = (rexwrxb(self_in_CogX64Compiler, 0, dest, index, base));
		offset = 1;
	}
	else {
		offset = 0;
	}
	if ((base & 7) != RBP) {
		((self_in_CogX64Compiler->machineCode))[offset + 3] = 139;
		((self_in_CogX64Compiler->machineCode))[offset + 4] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, dest));
		((self_in_CogX64Compiler->machineCode))[offset + 5] = (sib(self_in_CogX64Compiler, SIB4, index, base));
		return offset + 6;
	}
	((self_in_CogX64Compiler->machineCode))[offset + 3] = 139;
	((self_in_CogX64Compiler->machineCode))[offset + 4] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, dest));
	((self_in_CogX64Compiler->machineCode))[offset + 5] = (sib(self_in_CogX64Compiler, SIB4, index, base));
	((self_in_CogX64Compiler->machineCode))[offset + 6] = 0;
	return offset + 7;
}

	/* CogX64Compiler>>#concretizeOpRR: */
static NoDbgRegParms sqInt
concretizeOpRR(AbstractInstruction *self_in_CogX64Compiler, sqInt x64opcode)
{
    usqInt regLHS;
    usqInt regRHS;

	regLHS = ((self_in_CogX64Compiler->operands))[0];
	regRHS = ((self_in_CogX64Compiler->operands))[1];
	((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, regRHS, 0, regLHS));
	((self_in_CogX64Compiler->machineCode))[1] = x64opcode;
	((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, regLHS, regRHS));
	return 3;
}


/*	We support only prefetches for addresses that are variables relative to
	VarBase 
 */

	/* CogX64Compiler>>#concretizePrefetchAw */
static NoDbgRegParms sqInt
concretizePrefetchAw(AbstractInstruction *self_in_CogX64Compiler)
{
    sqInt offset;
    usqInt operand;

	operand = ((self_in_CogX64Compiler->operands))[0];
	if (!(		/* begin isAddressRelativeToVarBase: */
			(operand)
		 && ((operand >= (varBaseAddress()))
		 && ((operand - (varBaseAddress())) < (0x100000))))) {
		return 0;
	}
	offset = operand - (varBaseAddress());
	((self_in_CogX64Compiler->machineCode))[0] = 15;
	((self_in_CogX64Compiler->machineCode))[1] = 24;
	((self_in_CogX64Compiler->machineCode))[2] = 147;
	((self_in_CogX64Compiler->machineCode))[3] = (offset & 0xFF);
	((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset)) >> 16) & 0xFF);
	((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset)) >> 8) & 0xFF);
	((self_in_CogX64Compiler->machineCode))[6] = (((usqInt)(offset)) >> 24);
	return 7;
}


/*	CmpRR/MoveRR RHS LHS computes LHS - RHS, i.e. apparently reversed. You
	have to think subtract. */

	/* CogX64Compiler>>#concretizeReverseOpRR: */
static NoDbgRegParms sqInt
concretizeReverseOpRR(AbstractInstruction *self_in_CogX64Compiler, sqInt x64opcode)
{
    usqInt regLHS;
    usqInt regRHS;

	regRHS = ((self_in_CogX64Compiler->operands))[0];
	regLHS = ((self_in_CogX64Compiler->operands))[1];
	((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, regRHS, 0, regLHS));
	((self_in_CogX64Compiler->machineCode))[1] = x64opcode;
	((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, regLHS, regRHS));
	return 3;
}

	/* CogX64Compiler>>#concretizeSet: */
static NoDbgRegParms sqInt
concretizeSet(AbstractInstruction *self_in_CogX64Compiler, sqInt conditionCode)
{
    usqInt reg;

	reg = ((self_in_CogX64Compiler->operands))[0];
	((self_in_CogX64Compiler->machineCode))[0] = ((reg >= R8
	? 68
	: 64));
	((self_in_CogX64Compiler->machineCode))[1] = 15;
	((self_in_CogX64Compiler->machineCode))[2] = (144 + conditionCode);
	((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, reg & 7, 0));
	return 4;
}

	/* CogX64Compiler>>#concretizeXCHGRR */
static NoDbgRegParms sqInt
concretizeXCHGRR(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt r1;
    usqInt r2;

	r1 = ((self_in_CogX64Compiler->operands))[0];
	r2 = ((self_in_CogX64Compiler->operands))[1];
	if (r2 == RAX) {
		r2 = r1;
		r1 = RAX;
	}
	if (r1 == RAX) {
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, r2));
		((self_in_CogX64Compiler->machineCode))[1] = (144 + (r2 % 8));
		return 2;
	}
	((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, r1, 0, r2));
	((self_in_CogX64Compiler->machineCode))[1] = 135;
	((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, r2, r1));
	return 3;
}


/*	Do a throw-away compilation to get at the cpuid info and initialize
	cpuidWord1 N.B. All of MSVC, gcc & clang have intrinsics for this, so if
	you have the energy
	by all means reimplement as an #if _MSC_VER...#elif __GNUC__ #else ...
	saga. 
 */

	/* CogX64Compiler>>#detectFeatures */
static NoDbgRegParms AbstractInstruction *
detectFeatures(AbstractInstruction *self_in_CogX64Compiler)
{
    usqIntptr_t (*cpuid)(void);
    sqInt fixupSize;
    sqInt opcodeSize;
    usqInt startAddress;

	startAddress = methodZoneBase();
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 10;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	cpuid = ((usqIntptr_t (*)(void)) startAddress);
	/* begin PushR: */
	genoperand(PushR, RDX);
	genoperand(PushR, RCX);
	genoperand(PushR, RBX);
	genoperandoperand(MoveCqR, 0x80000001U, RAX);
	gen(CPUID);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, RCX, RAX);
	genoperand(PopR, RBX);
	genoperand(PopR, RCX);
	genoperand(PopR, RDX);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	/* begin resetMethodZoneBase: */
	methodZoneBase = startAddress;
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	setCpuidWord1(self_in_CogX64Compiler, cpuid());
	return self_in_CogX64Compiler;
}


/*	Attempt to generate concrete machine code for the instruction at address.
	This is the inner dispatch of concretizeAt: actualAddress which exists
	only to get around the branch size limits in the SqueakV3 (blue book
	derived) bytecode set. */

	/* CogX64Compiler>>#dispatchConcretize */
static NoDbgRegParms sqInt
dispatchConcretize(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt addressOperand;
    usqInt addressOperand1;
    usqInt addressOperand2;
    usqInt addressOperand3;
    usqInt addressOperand4;
    usqInt addressOperand5;
    usqInt base;
    usqInt base1;
    usqInt base2;
    usqInt base3;
    usqInt baseReg;
    usqInt baseReg1;
    AbstractInstruction *dependentChain;
    usqInt dest;
    usqInt dest1;
    usqInt dest2;
    usqInt destReg;
    usqInt destReg1;
    usqInt destReg10;
    usqInt destReg11;
    usqInt destReg12;
    usqInt destReg13;
    usqInt destReg14;
    usqInt destReg15;
    usqInt destReg16;
    usqInt destReg17;
    usqInt destReg18;
    usqInt destReg19;
    usqInt destReg2;
    usqInt destReg20;
    usqInt destReg21;
    usqInt destReg22;
    usqInt destReg23;
    usqInt destReg24;
    usqInt destReg25;
    usqInt destReg26;
    usqInt destReg27;
    usqInt destReg28;
    usqInt destReg29;
    usqInt destReg3;
    usqInt destReg30;
    usqInt destReg31;
    usqInt destReg32;
    usqInt destReg4;
    usqInt destReg5;
    usqInt destReg6;
    usqInt destReg7;
    usqInt destReg8;
    usqInt destReg9;
    usqInt distance;
    usqInt distance1;
    usqInt distance2;
    usqInt distance3;
    usqInt distance4;
    sqInt i;
    usqInt index;
    usqInt index1;
    usqInt index2;
    usqInt index3;
    AbstractInstruction *jumpTarget;
    AbstractInstruction *jumpTarget1;
    AbstractInstruction *jumpTarget10;
    AbstractInstruction *jumpTarget11;
    AbstractInstruction *jumpTarget110;
    AbstractInstruction *jumpTarget111;
    AbstractInstruction *jumpTarget1110;
    AbstractInstruction *jumpTarget1111;
    AbstractInstruction *jumpTarget1112;
    AbstractInstruction *jumpTarget1113;
    AbstractInstruction *jumpTarget1114;
    AbstractInstruction *jumpTarget1115;
    AbstractInstruction *jumpTarget1116;
    AbstractInstruction *jumpTarget112;
    AbstractInstruction *jumpTarget113;
    AbstractInstruction *jumpTarget114;
    AbstractInstruction *jumpTarget115;
    AbstractInstruction *jumpTarget116;
    AbstractInstruction *jumpTarget117;
    AbstractInstruction *jumpTarget118;
    AbstractInstruction *jumpTarget119;
    AbstractInstruction *jumpTarget12;
    AbstractInstruction *jumpTarget120;
    AbstractInstruction *jumpTarget121;
    AbstractInstruction *jumpTarget122;
    AbstractInstruction *jumpTarget123;
    AbstractInstruction *jumpTarget124;
    AbstractInstruction *jumpTarget125;
    AbstractInstruction *jumpTarget13;
    AbstractInstruction *jumpTarget14;
    AbstractInstruction *jumpTarget15;
    AbstractInstruction *jumpTarget16;
    AbstractInstruction *jumpTarget17;
    AbstractInstruction *jumpTarget18;
    AbstractInstruction *jumpTarget19;
    AbstractInstruction *jumpTarget2;
    AbstractInstruction *jumpTarget20;
    AbstractInstruction *jumpTarget21;
    AbstractInstruction *jumpTarget210;
    AbstractInstruction *jumpTarget211;
    AbstractInstruction *jumpTarget212;
    AbstractInstruction *jumpTarget213;
    AbstractInstruction *jumpTarget214;
    AbstractInstruction *jumpTarget215;
    AbstractInstruction *jumpTarget216;
    AbstractInstruction *jumpTarget22;
    AbstractInstruction *jumpTarget23;
    AbstractInstruction *jumpTarget24;
    AbstractInstruction *jumpTarget25;
    AbstractInstruction *jumpTarget26;
    AbstractInstruction *jumpTarget27;
    AbstractInstruction *jumpTarget28;
    AbstractInstruction *jumpTarget29;
    AbstractInstruction *jumpTarget3;
    AbstractInstruction *jumpTarget30;
    AbstractInstruction *jumpTarget31;
    AbstractInstruction *jumpTarget32;
    AbstractInstruction *jumpTarget33;
    AbstractInstruction *jumpTarget34;
    AbstractInstruction *jumpTarget35;
    AbstractInstruction *jumpTarget4;
    AbstractInstruction *jumpTarget5;
    AbstractInstruction *jumpTarget6;
    AbstractInstruction *jumpTarget7;
    AbstractInstruction *jumpTarget8;
    AbstractInstruction *jumpTarget9;
    usqInt maskReg;
    int offset;
    int offset1;
    usqInt offset10;
    usqInt offset11;
    usqInt offset110;
    usqInt offset111;
    sqInt offset112;
    sqInt offset113;
    sqInt offset114;
    sqInt offset115;
    sqInt offset116;
    sqInt offset117;
    sqInt offset118;
    sqInt offset119;
    usqInt offset12;
    sqInt offset120;
    sqInt offset121;
    sqInt offset122;
    sqInt offset123;
    sqInt offset124;
    sqInt offset125;
    sqInt offset126;
    sqInt offset127;
    usqInt offset13;
    usqInt offset14;
    usqInt offset15;
    usqInt offset16;
    usqInt offset17;
    usqInt offset18;
    usqInt offset19;
    int offset2;
    sqInt offset20;
    usqInt offset21;
    usqInt offset22;
    usqInt offset23;
    usqInt offset24;
    sqInt offset25;
    sqInt offset26;
    sqInt offset27;
    sqInt offset28;
    sqInt offset29;
    usqInt offset3;
    sqInt offset30;
    sqInt offset31;
    sqInt offset32;
    sqInt offset33;
    sqInt offset34;
    sqInt offset35;
    sqInt offset36;
    sqInt offset37;
    sqInt offset38;
    sqInt offset39;
    usqInt offset4;
    sqInt offset40;
    sqInt offset41;
    sqInt offset42;
    sqInt offset43;
    sqInt offset44;
    sqInt offset5;
    sqInt offset6;
    sqInt offset7;
    sqInt offset8;
    sqInt offset9;
    usqInt operand;
    usqInt operand1;
    usqInt reg;
    usqInt reg1;
    usqInt reg10;
    usqInt reg11;
    usqInt reg12;
    usqInt reg13;
    usqInt reg14;
    usqInt reg15;
    usqInt reg16;
    usqInt reg17;
    usqInt reg18;
    usqInt reg19;
    usqInt reg2;
    usqInt reg20;
    usqInt reg21;
    usqInt reg22;
    usqInt reg3;
    usqInt reg4;
    usqInt reg5;
    usqInt reg6;
    usqInt reg7;
    usqInt reg8;
    usqInt regLHS;
    usqInt regLHS1;
    usqInt regLHS10;
    usqInt regLHS11;
    usqInt regLHS2;
    usqInt regLHS3;
    usqInt regLHS4;
    usqInt regLHS5;
    usqInt regLHS6;
    usqInt regLHS7;
    usqInt regLHS8;
    usqInt regLHS9;
    usqInt regRHS;
    usqInt regRHS1;
    usqInt regRHS10;
    usqInt regRHS11;
    usqInt regRHS2;
    usqInt regRHS3;
    usqInt regRHS4;
    usqInt regRHS5;
    usqInt regRHS6;
    usqInt regRHS7;
    usqInt regRHS8;
    usqInt regRHS9;
    sqInt regToShift;
    sqInt regToShift1;
    usqInt reg9;
    usqInt save0;
    usqInt save01;
    usqInt save1;
    usqInt save11;
    usqInt save12;
    usqInt save13;
    sqInt savedSize;
    sqInt savedSize1;
    sqInt savedSize2;
    sqInt savedSize3;
    usqInt shiftCountReg;
    usqInt shiftCountReg1;
    sqInt skip;
    sqInt skip1;
    sqInt skip10;
    sqInt skip11;
    sqInt skip12;
    sqInt skip13;
    sqInt skip14;
    sqInt skip15;
    sqInt skip16;
    sqInt skip17;
    sqInt skip18;
    sqInt skip19;
    sqInt skip2;
    sqInt skip20;
    sqInt skip21;
    sqInt skip22;
    sqInt skip23;
    sqInt skip24;
    sqInt skip25;
    sqInt skip26;
    sqInt skip27;
    sqInt skip28;
    sqInt skip3;
    sqInt skip4;
    sqInt skip5;
    sqInt skip6;
    sqInt skip7;
    sqInt skip8;
    sqInt skip9;
    usqInt src;
    usqInt src1;
    usqInt srcReg;
    usqInt srcReg1;
    usqInt srcReg10;
    usqInt srcReg11;
    usqInt srcReg12;
    usqInt srcReg13;
    usqInt srcReg14;
    usqInt srcReg15;
    usqInt srcReg16;
    usqInt srcReg17;
    usqInt srcReg18;
    usqInt srcReg19;
    usqInt srcReg2;
    usqInt srcReg20;
    usqInt srcReg21;
    usqInt srcReg22;
    usqInt srcReg23;
    usqInt srcReg24;
    usqInt srcReg25;
    usqInt srcReg26;
    usqInt srcReg27;
    usqInt srcReg28;
    usqInt srcReg29;
    usqInt srcReg3;
    usqInt srcReg30;
    usqInt srcReg31;
    usqInt srcReg32;
    usqInt srcReg4;
    usqInt srcReg5;
    usqInt srcReg6;
    usqInt srcReg7;
    usqInt srcReg8;
    usqInt srcReg9;
    usqInt value;
    usqInt value1;
    usqInt value11;
    usqInt value2;
    usqInt value3;
    usqInt value4;
    usqInt value5;
    usqInt value6;
    usqIntptr_t word;

	if (((self_in_CogX64Compiler->opcode)) >= CPUID) {
		return dispatchConcretizeProcessorSpecific(self_in_CogX64Compiler);
	}
	switch ((self_in_CogX64Compiler->opcode)) {
	case Label:
		/* begin concretizeLabel */
		dependentChain = (self_in_CogX64Compiler->dependent);
		while (!(!dependentChain)) {
			/* begin updateLabel: */
			assert((((dependentChain->opcode)) == MoveCwR)
			 || (((dependentChain->opcode)) == PushCw));
			((dependentChain->operands))[0] = (((self_in_CogX64Compiler->address)) + (((self_in_CogX64Compiler->operands))[1]));
			dependentChain = (dependentChain->dependent);
		}
		return 0;

	case AlignmentNops:
		/* begin concretizeAlignmentNops */
		for (i = 0; i < ((self_in_CogX64Compiler->machineCodeSize)); i += 1) {
			((self_in_CogX64Compiler->machineCode))[i] = 144;
		}
		return (self_in_CogX64Compiler->machineCodeSize);

	case Fill32:
		/* begin concretizeFill32 */
		word = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = (word & 0xFF);
		((self_in_CogX64Compiler->machineCode))[1] = (((word) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2] = (((word) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((word) >> 24) & 0xFF);
		return 4;

	case Nop:
		/* begin concretizeNop */
		((self_in_CogX64Compiler->machineCode))[0] = 144;
		return 1;

	case Call:
		/* begin concretizeCall */
		assert((((self_in_CogX64Compiler->operands))[0]) != 0);
		offset = (((int) (((self_in_CogX64Compiler->operands))[0]))) - (((int) (((self_in_CogX64Compiler->address)) + 5)));
		((self_in_CogX64Compiler->machineCode))[0] = 232;
		((self_in_CogX64Compiler->machineCode))[1] = (offset & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2] = ((((usqInt)(offset)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset)) >> 24) & 0xFF);
		return 5;

	case CallR:
		/* begin concretizeCallR */
		reg = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg));
		((self_in_CogX64Compiler->machineCode))[1] = 0xFF;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg, 2));
		return 3;

	case CallFull:
		/* begin concretizeCallFull */
		operand = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = 72;
		((self_in_CogX64Compiler->machineCode))[1] = 184;
		((self_in_CogX64Compiler->machineCode))[2] = (operand & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((operand) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((operand) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((operand) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((operand) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((operand) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((operand) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((operand) >> 56) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[10] = 0xFF;
		((self_in_CogX64Compiler->machineCode))[11] = (modRMRO(self_in_CogX64Compiler, ModReg, RAX, 2));
		return 12;

	case JumpR:
		/* begin concretizeJumpR */
		reg1 = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg1));
		((self_in_CogX64Compiler->machineCode))[1] = 0xFF;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg1, 4));
		return 3;

	case JumpFull:
		/* begin concretizeJumpFull */
		operand1 = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = 72;
		((self_in_CogX64Compiler->machineCode))[1] = 184;
		((self_in_CogX64Compiler->machineCode))[2] = (operand1 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((operand1) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((operand1) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((operand1) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((operand1) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((operand1) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((operand1) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((operand1) >> 56) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[10] = 0xFF;
		((self_in_CogX64Compiler->machineCode))[11] = (modRMRO(self_in_CogX64Compiler, ModReg, RAX, 4));
		return 12;

	case JumpLong:
		/* begin concretizeJumpLong */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel()))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offset1 = (((int) jumpTarget)) - (((int) (((self_in_CogX64Compiler->address)) + 5)));
		((self_in_CogX64Compiler->machineCode))[0] = 233;
		((self_in_CogX64Compiler->machineCode))[1] = (offset1 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2] = ((((usqInt)(offset1)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset1)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset1)) >> 24) & 0xFF);
		return 5;

	case JumpLongZero:
	case JumpZero:
	case JumpFPEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget12 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget12);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget12))
		 || (jumpTarget12 == (methodLabel()))) {
			jumpTarget12 = ((AbstractInstruction *) ((jumpTarget12->address)));
		}
		assert(jumpTarget12 != 0);
		jumpTarget3 = jumpTarget12;
		offset29 = (((sqLong) jumpTarget3)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset29)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (116);
			((self_in_CogX64Compiler->machineCode))[1] = (offset29 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget11 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget11);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget11))
		 || (jumpTarget11 == (methodLabel()))) {
			jumpTarget11 = ((AbstractInstruction *) ((jumpTarget11->address)));
		}
		assert(jumpTarget11 != 0);
		jumpTarget2 = jumpTarget11;
		offset112 = (((sqLong) jumpTarget2)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (132);
		((self_in_CogX64Compiler->machineCode))[2] = (offset112 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset112)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset112)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset112)) >> 24) & 0xFF);
		return 6;

	case JumpLongNonZero:
	case JumpNonZero:
	case JumpFPNotEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget13 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget13);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget13))
		 || (jumpTarget13 == (methodLabel()))) {
			jumpTarget13 = ((AbstractInstruction *) ((jumpTarget13->address)));
		}
		assert(jumpTarget13 != 0);
		jumpTarget4 = jumpTarget13;
		offset30 = (((sqLong) jumpTarget4)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset30)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (117);
			((self_in_CogX64Compiler->machineCode))[1] = (offset30 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget111 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget111);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget111))
		 || (jumpTarget111 == (methodLabel()))) {
			jumpTarget111 = ((AbstractInstruction *) ((jumpTarget111->address)));
		}
		assert(jumpTarget111 != 0);
		jumpTarget21 = jumpTarget111;
		offset113 = (((sqLong) jumpTarget21)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (133);
		((self_in_CogX64Compiler->machineCode))[2] = (offset113 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset113)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset113)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset113)) >> 24) & 0xFF);
		return 6;

	case Jump:
		/* begin concretizeJump */
		jumpTarget1 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1))
		 || (jumpTarget1 == (methodLabel()))) {
			jumpTarget1 = ((AbstractInstruction *) ((jumpTarget1->address)));
		}
		assert(jumpTarget1 != 0);
		offset2 = (((int) jumpTarget1)) - (((int) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset2)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = 235;
			((self_in_CogX64Compiler->machineCode))[1] = (offset2 & 0xFF);
			return 2;
		}
		offset2 = (((int) jumpTarget1)) - (((int) (((self_in_CogX64Compiler->address)) + 5)));
		((self_in_CogX64Compiler->machineCode))[0] = 233;
		((self_in_CogX64Compiler->machineCode))[1] = (offset2 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2] = ((((usqInt)(offset2)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset2)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset2)) >> 24) & 0xFF);
		return 5;

	case JumpNegative:
		/* begin concretizeConditionalJump: */
		jumpTarget14 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget14);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget14))
		 || (jumpTarget14 == (methodLabel()))) {
			jumpTarget14 = ((AbstractInstruction *) ((jumpTarget14->address)));
		}
		assert(jumpTarget14 != 0);
		jumpTarget5 = jumpTarget14;
		offset31 = (((sqLong) jumpTarget5)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset31)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (120);
			((self_in_CogX64Compiler->machineCode))[1] = (offset31 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget112 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget112);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget112))
		 || (jumpTarget112 == (methodLabel()))) {
			jumpTarget112 = ((AbstractInstruction *) ((jumpTarget112->address)));
		}
		assert(jumpTarget112 != 0);
		jumpTarget22 = jumpTarget112;
		offset114 = (((sqLong) jumpTarget22)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (136);
		((self_in_CogX64Compiler->machineCode))[2] = (offset114 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset114)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset114)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset114)) >> 24) & 0xFF);
		return 6;

	case JumpNonNegative:
		/* begin concretizeConditionalJump: */
		jumpTarget15 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget15);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget15))
		 || (jumpTarget15 == (methodLabel()))) {
			jumpTarget15 = ((AbstractInstruction *) ((jumpTarget15->address)));
		}
		assert(jumpTarget15 != 0);
		jumpTarget6 = jumpTarget15;
		offset32 = (((sqLong) jumpTarget6)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset32)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (121);
			((self_in_CogX64Compiler->machineCode))[1] = (offset32 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget113 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget113);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget113))
		 || (jumpTarget113 == (methodLabel()))) {
			jumpTarget113 = ((AbstractInstruction *) ((jumpTarget113->address)));
		}
		assert(jumpTarget113 != 0);
		jumpTarget23 = jumpTarget113;
		offset115 = (((sqLong) jumpTarget23)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (137);
		((self_in_CogX64Compiler->machineCode))[2] = (offset115 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset115)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset115)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset115)) >> 24) & 0xFF);
		return 6;

	case JumpOverflow:
		/* begin concretizeConditionalJump: */
		jumpTarget16 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget16);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget16))
		 || (jumpTarget16 == (methodLabel()))) {
			jumpTarget16 = ((AbstractInstruction *) ((jumpTarget16->address)));
		}
		assert(jumpTarget16 != 0);
		jumpTarget7 = jumpTarget16;
		offset33 = (((sqLong) jumpTarget7)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset33)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (112);
			((self_in_CogX64Compiler->machineCode))[1] = (offset33 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget114 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget114);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget114))
		 || (jumpTarget114 == (methodLabel()))) {
			jumpTarget114 = ((AbstractInstruction *) ((jumpTarget114->address)));
		}
		assert(jumpTarget114 != 0);
		jumpTarget24 = jumpTarget114;
		offset116 = (((sqLong) jumpTarget24)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (128);
		((self_in_CogX64Compiler->machineCode))[2] = (offset116 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset116)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset116)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset116)) >> 24) & 0xFF);
		return 6;

	case JumpNoOverflow:
		/* begin concretizeConditionalJump: */
		jumpTarget17 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget17);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget17))
		 || (jumpTarget17 == (methodLabel()))) {
			jumpTarget17 = ((AbstractInstruction *) ((jumpTarget17->address)));
		}
		assert(jumpTarget17 != 0);
		jumpTarget8 = jumpTarget17;
		offset34 = (((sqLong) jumpTarget8)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset34)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (113);
			((self_in_CogX64Compiler->machineCode))[1] = (offset34 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget115 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget115);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget115))
		 || (jumpTarget115 == (methodLabel()))) {
			jumpTarget115 = ((AbstractInstruction *) ((jumpTarget115->address)));
		}
		assert(jumpTarget115 != 0);
		jumpTarget25 = jumpTarget115;
		offset117 = (((sqLong) jumpTarget25)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (129);
		((self_in_CogX64Compiler->machineCode))[2] = (offset117 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset117)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset117)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset117)) >> 24) & 0xFF);
		return 6;

	case JumpCarry:
	case JumpBelow:
	case JumpFPLess:
		/* begin concretizeConditionalJump: */
		jumpTarget18 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget18);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget18))
		 || (jumpTarget18 == (methodLabel()))) {
			jumpTarget18 = ((AbstractInstruction *) ((jumpTarget18->address)));
		}
		assert(jumpTarget18 != 0);
		jumpTarget9 = jumpTarget18;
		offset35 = (((sqLong) jumpTarget9)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset35)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (114);
			((self_in_CogX64Compiler->machineCode))[1] = (offset35 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget116 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget116);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget116))
		 || (jumpTarget116 == (methodLabel()))) {
			jumpTarget116 = ((AbstractInstruction *) ((jumpTarget116->address)));
		}
		assert(jumpTarget116 != 0);
		jumpTarget26 = jumpTarget116;
		offset118 = (((sqLong) jumpTarget26)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (130);
		((self_in_CogX64Compiler->machineCode))[2] = (offset118 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset118)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset118)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset118)) >> 24) & 0xFF);
		return 6;

	case JumpNoCarry:
	case JumpAboveOrEqual:
	case JumpFPGreaterOrEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget19 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget19);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget19))
		 || (jumpTarget19 == (methodLabel()))) {
			jumpTarget19 = ((AbstractInstruction *) ((jumpTarget19->address)));
		}
		assert(jumpTarget19 != 0);
		jumpTarget10 = jumpTarget19;
		offset36 = (((sqLong) jumpTarget10)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset36)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (115);
			((self_in_CogX64Compiler->machineCode))[1] = (offset36 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget117 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget117);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget117))
		 || (jumpTarget117 == (methodLabel()))) {
			jumpTarget117 = ((AbstractInstruction *) ((jumpTarget117->address)));
		}
		assert(jumpTarget117 != 0);
		jumpTarget27 = jumpTarget117;
		offset119 = (((sqLong) jumpTarget27)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (131);
		((self_in_CogX64Compiler->machineCode))[2] = (offset119 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset119)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset119)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset119)) >> 24) & 0xFF);
		return 6;

	case JumpLess:
		/* begin concretizeConditionalJump: */
		jumpTarget110 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget110);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget110))
		 || (jumpTarget110 == (methodLabel()))) {
			jumpTarget110 = ((AbstractInstruction *) ((jumpTarget110->address)));
		}
		assert(jumpTarget110 != 0);
		jumpTarget20 = jumpTarget110;
		offset37 = (((sqLong) jumpTarget20)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset37)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (0x7C);
			((self_in_CogX64Compiler->machineCode))[1] = (offset37 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget118 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget118);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget118))
		 || (jumpTarget118 == (methodLabel()))) {
			jumpTarget118 = ((AbstractInstruction *) ((jumpTarget118->address)));
		}
		assert(jumpTarget118 != 0);
		jumpTarget28 = jumpTarget118;
		offset120 = (((sqLong) jumpTarget28)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (140);
		((self_in_CogX64Compiler->machineCode))[2] = (offset120 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset120)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset120)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset120)) >> 24) & 0xFF);
		return 6;

	case JumpGreaterOrEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget119 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget119);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget119))
		 || (jumpTarget119 == (methodLabel()))) {
			jumpTarget119 = ((AbstractInstruction *) ((jumpTarget119->address)));
		}
		assert(jumpTarget119 != 0);
		jumpTarget29 = jumpTarget119;
		offset38 = (((sqLong) jumpTarget29)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset38)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (125);
			((self_in_CogX64Compiler->machineCode))[1] = (offset38 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1110 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1110);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1110))
		 || (jumpTarget1110 == (methodLabel()))) {
			jumpTarget1110 = ((AbstractInstruction *) ((jumpTarget1110->address)));
		}
		assert(jumpTarget1110 != 0);
		jumpTarget210 = jumpTarget1110;
		offset121 = (((sqLong) jumpTarget210)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (141);
		((self_in_CogX64Compiler->machineCode))[2] = (offset121 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset121)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset121)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset121)) >> 24) & 0xFF);
		return 6;

	case JumpGreater:
		/* begin concretizeConditionalJump: */
		jumpTarget120 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget120);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget120))
		 || (jumpTarget120 == (methodLabel()))) {
			jumpTarget120 = ((AbstractInstruction *) ((jumpTarget120->address)));
		}
		assert(jumpTarget120 != 0);
		jumpTarget30 = jumpTarget120;
		offset39 = (((sqLong) jumpTarget30)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset39)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (0x7F);
			((self_in_CogX64Compiler->machineCode))[1] = (offset39 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1111 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1111);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1111))
		 || (jumpTarget1111 == (methodLabel()))) {
			jumpTarget1111 = ((AbstractInstruction *) ((jumpTarget1111->address)));
		}
		assert(jumpTarget1111 != 0);
		jumpTarget211 = jumpTarget1111;
		offset122 = (((sqLong) jumpTarget211)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (143);
		((self_in_CogX64Compiler->machineCode))[2] = (offset122 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset122)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset122)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset122)) >> 24) & 0xFF);
		return 6;

	case JumpLessOrEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget121 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget121);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget121))
		 || (jumpTarget121 == (methodLabel()))) {
			jumpTarget121 = ((AbstractInstruction *) ((jumpTarget121->address)));
		}
		assert(jumpTarget121 != 0);
		jumpTarget31 = jumpTarget121;
		offset40 = (((sqLong) jumpTarget31)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset40)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (0x7E);
			((self_in_CogX64Compiler->machineCode))[1] = (offset40 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1112 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1112);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1112))
		 || (jumpTarget1112 == (methodLabel()))) {
			jumpTarget1112 = ((AbstractInstruction *) ((jumpTarget1112->address)));
		}
		assert(jumpTarget1112 != 0);
		jumpTarget212 = jumpTarget1112;
		offset123 = (((sqLong) jumpTarget212)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (142);
		((self_in_CogX64Compiler->machineCode))[2] = (offset123 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset123)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset123)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset123)) >> 24) & 0xFF);
		return 6;

	case JumpAbove:
	case JumpFPGreater:
		/* begin concretizeConditionalJump: */
		jumpTarget122 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget122);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget122))
		 || (jumpTarget122 == (methodLabel()))) {
			jumpTarget122 = ((AbstractInstruction *) ((jumpTarget122->address)));
		}
		assert(jumpTarget122 != 0);
		jumpTarget32 = jumpTarget122;
		offset41 = (((sqLong) jumpTarget32)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset41)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (119);
			((self_in_CogX64Compiler->machineCode))[1] = (offset41 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1113 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1113);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1113))
		 || (jumpTarget1113 == (methodLabel()))) {
			jumpTarget1113 = ((AbstractInstruction *) ((jumpTarget1113->address)));
		}
		assert(jumpTarget1113 != 0);
		jumpTarget213 = jumpTarget1113;
		offset124 = (((sqLong) jumpTarget213)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (135);
		((self_in_CogX64Compiler->machineCode))[2] = (offset124 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset124)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset124)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset124)) >> 24) & 0xFF);
		return 6;

	case JumpBelowOrEqual:
	case JumpFPLessOrEqual:
		/* begin concretizeConditionalJump: */
		jumpTarget123 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget123);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget123))
		 || (jumpTarget123 == (methodLabel()))) {
			jumpTarget123 = ((AbstractInstruction *) ((jumpTarget123->address)));
		}
		assert(jumpTarget123 != 0);
		jumpTarget33 = jumpTarget123;
		offset42 = (((sqLong) jumpTarget33)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset42)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (118);
			((self_in_CogX64Compiler->machineCode))[1] = (offset42 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1114 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1114);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1114))
		 || (jumpTarget1114 == (methodLabel()))) {
			jumpTarget1114 = ((AbstractInstruction *) ((jumpTarget1114->address)));
		}
		assert(jumpTarget1114 != 0);
		jumpTarget214 = jumpTarget1114;
		offset125 = (((sqLong) jumpTarget214)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (134);
		((self_in_CogX64Compiler->machineCode))[2] = (offset125 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset125)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset125)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset125)) >> 24) & 0xFF);
		return 6;

	case JumpFPOrdered:
		/* begin concretizeConditionalJump: */
		jumpTarget124 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget124);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget124))
		 || (jumpTarget124 == (methodLabel()))) {
			jumpTarget124 = ((AbstractInstruction *) ((jumpTarget124->address)));
		}
		assert(jumpTarget124 != 0);
		jumpTarget34 = jumpTarget124;
		offset43 = (((sqLong) jumpTarget34)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset43)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (123);
			((self_in_CogX64Compiler->machineCode))[1] = (offset43 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1115 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1115);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1115))
		 || (jumpTarget1115 == (methodLabel()))) {
			jumpTarget1115 = ((AbstractInstruction *) ((jumpTarget1115->address)));
		}
		assert(jumpTarget1115 != 0);
		jumpTarget215 = jumpTarget1115;
		offset126 = (((sqLong) jumpTarget215)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (139);
		((self_in_CogX64Compiler->machineCode))[2] = (offset126 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset126)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset126)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset126)) >> 24) & 0xFF);
		return 6;

	case JumpFPUnordered:
		/* begin concretizeConditionalJump: */
		jumpTarget125 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget125);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget125))
		 || (jumpTarget125 == (methodLabel()))) {
			jumpTarget125 = ((AbstractInstruction *) ((jumpTarget125->address)));
		}
		assert(jumpTarget125 != 0);
		jumpTarget35 = jumpTarget125;
		offset44 = (((sqLong) jumpTarget35)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 2)));
		if ((((self_in_CogX64Compiler->machineCodeSize)) == 0
			? isQuick(self_in_CogX64Compiler, offset44)
			: ((self_in_CogX64Compiler->machineCodeSize)) == 2)) {
			((self_in_CogX64Compiler->machineCode))[0] = (122);
			((self_in_CogX64Compiler->machineCode))[1] = (offset44 & 0xFF);
			return 2;
		}
		/* begin concretizeConditionalJumpLong: */
		jumpTarget1116 = ((AbstractInstruction *) (((self_in_CogX64Compiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget1116);
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(jumpTarget1116))
		 || (jumpTarget1116 == (methodLabel()))) {
			jumpTarget1116 = ((AbstractInstruction *) ((jumpTarget1116->address)));
		}
		assert(jumpTarget1116 != 0);
		jumpTarget216 = jumpTarget1116;
		offset127 = (((sqLong) jumpTarget216)) - (((sqLong) (((self_in_CogX64Compiler->address)) + 6)));
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = (138);
		((self_in_CogX64Compiler->machineCode))[2] = (offset127 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = ((((usqInt)(offset127)) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset127)) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset127)) >> 24) & 0xFF);
		return 6;

	case RetN:
		/* begin concretizeRetN */
		offset3 = ((self_in_CogX64Compiler->operands))[0];
		if (offset3 == 0) {
			((self_in_CogX64Compiler->machineCode))[0] = 195;
			return 1;
		}
		((self_in_CogX64Compiler->machineCode))[0] = 194;
		((self_in_CogX64Compiler->machineCode))[1] = (offset3 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2] = ((offset3) >> 8);
		return 3;

	case Stop:
		/* begin concretizeStop */
		((self_in_CogX64Compiler->machineCode))[0] = 204;
		return 1;

	case AddCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 0, 5);

	case AddcCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 2, 20);

	case AddCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 3);

	case AddRR:
		return concretizeOpRR(self_in_CogX64Compiler, 3);

	case AddRsRs:
		/* begin concretizeSEEOpRsRs: */
		regRHS = ((self_in_CogX64Compiler->operands))[0];
		regLHS = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((regLHS <= 7)
		 && (regRHS <= 7)) {
			skip = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS, 0, regRHS));
		}
		((self_in_CogX64Compiler->machineCode))[skip + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip + 2] = 88;
		((self_in_CogX64Compiler->machineCode))[skip + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS, regLHS));
		return skip + 4;

	case AddRdRd:
		/* begin concretizeSEE2OpRdRd: */
		regRHS1 = ((self_in_CogX64Compiler->operands))[0];
		regLHS1 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((regLHS1 <= 7)
		 && (regRHS1 <= 7)) {
			skip1 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip1 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS1, 0, regRHS1));
		}
		((self_in_CogX64Compiler->machineCode))[skip1 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip1 + 2] = 88;
		((self_in_CogX64Compiler->machineCode))[skip1 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS1, regLHS1));
		return skip1 + 4;

	case AndCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 4, 37);

	case AndCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 35);

	case AndRR:
		return concretizeOpRR(self_in_CogX64Compiler, 35);

	case TstCqR:
		/* begin concretizeTstCqR */
		value = ((self_in_CogX64Compiler->operands))[0];
		reg2 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg2));
		if (isQuick(self_in_CogX64Compiler, value)) {
			((self_in_CogX64Compiler->machineCode))[1] = 246;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg2, 0));
			((self_in_CogX64Compiler->machineCode))[3] = (value & 0xFF);
			return 4;
		}
		if (is32BitSignedImmediate(self_in_CogX64Compiler, value)) {
			if (reg2 == RAX) {
				((self_in_CogX64Compiler->machineCode))[1] = 169;
				((self_in_CogX64Compiler->machineCode))[2] = (value & 0xFF);
				((self_in_CogX64Compiler->machineCode))[3] = (((value) >> 8) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[4] = (((value) >> 16) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[5] = (((value) >> 24) & 0xFF);
				return 6;
			}
			((self_in_CogX64Compiler->machineCode))[1] = 247;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg2, 0));
			((self_in_CogX64Compiler->machineCode))[3] = (value & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((value) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((value) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((value) >> 24) & 0xFF);
			return 7;
		}
		return concretizeArithCwR(self_in_CogX64Compiler, 133);

	case CmpCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 7, 61);

	case CmpCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 57);

	case CmpC32R:
		/* begin concretizeCmpC32R */
		value1 = ((self_in_CogX64Compiler->operands))[0];
		reg3 = ((self_in_CogX64Compiler->operands))[1];
		if (reg3 == RAX) {
			((self_in_CogX64Compiler->machineCode))[0] = 61;
			skip2 = 0;
		}
		else {
			if (reg3 > 7) {
				((self_in_CogX64Compiler->machineCode))[0] = 65;
				skip2 = 2;
			}
			else {
				skip2 = 1;
			}
			((self_in_CogX64Compiler->machineCode))[skip2 - 1] = 129;
			((self_in_CogX64Compiler->machineCode))[skip2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg3, 7));
		}
		((self_in_CogX64Compiler->machineCode))[skip2 + 1] = (value1 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip2 + 2] = (((value1) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip2 + 3] = (((value1) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip2 + 4] = (((value1) >> 24) & 0xFF);
		return 5 + skip2;

	case CmpRR:
		return concretizeReverseOpRR(self_in_CogX64Compiler, 57);

	case CmpRdRd:
		/* begin concretizeCmpRdRd */
		regRHS2 = ((self_in_CogX64Compiler->operands))[0];
		regLHS2 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((regLHS2 <= 7)
		 && (regRHS2 <= 7)) {
			skip3 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip3 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS2, 0, regRHS2));
		}
		((self_in_CogX64Compiler->machineCode))[skip3 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip3 + 2] = 46;
		((self_in_CogX64Compiler->machineCode))[skip3 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS2, regLHS2));
		return skip3 + 4;

	case CmpRsRs:
		/* begin concretizeCmpRsRs */
		regRHS3 = ((self_in_CogX64Compiler->operands))[0];
		regLHS3 = ((self_in_CogX64Compiler->operands))[1];
		if ((regLHS3 <= 7)
		 && (regRHS3 <= 7)) {
			skip4 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip4 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS3, 0, regRHS3));
		}
		((self_in_CogX64Compiler->machineCode))[skip4] = 15;
		((self_in_CogX64Compiler->machineCode))[skip4 + 1] = 46;
		((self_in_CogX64Compiler->machineCode))[skip4 + 2] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS3, regLHS3));
		return skip4 + 3;

	case DivRdRd:
		/* begin concretizeSEE2OpRdRd: */
		regRHS4 = ((self_in_CogX64Compiler->operands))[0];
		regLHS4 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((regLHS4 <= 7)
		 && (regRHS4 <= 7)) {
			skip5 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip5 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS4, 0, regRHS4));
		}
		((self_in_CogX64Compiler->machineCode))[skip5 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip5 + 2] = 94;
		((self_in_CogX64Compiler->machineCode))[skip5 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS4, regLHS4));
		return skip5 + 4;

	case DivRsRs:
		/* begin concretizeSEEOpRsRs: */
		regRHS5 = ((self_in_CogX64Compiler->operands))[0];
		regLHS5 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((regLHS5 <= 7)
		 && (regRHS5 <= 7)) {
			skip6 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip6 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS5, 0, regRHS5));
		}
		((self_in_CogX64Compiler->machineCode))[skip6 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip6 + 2] = 94;
		((self_in_CogX64Compiler->machineCode))[skip6 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS5, regLHS5));
		return skip6 + 4;

	case MulRdRd:
		/* begin concretizeSEE2OpRdRd: */
		regRHS6 = ((self_in_CogX64Compiler->operands))[0];
		regLHS6 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((regLHS6 <= 7)
		 && (regRHS6 <= 7)) {
			skip7 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip7 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS6, 0, regRHS6));
		}
		((self_in_CogX64Compiler->machineCode))[skip7 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip7 + 2] = 89;
		((self_in_CogX64Compiler->machineCode))[skip7 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS6, regLHS6));
		return skip7 + 4;

	case MulRsRs:
		/* begin concretizeSEEOpRsRs: */
		regRHS7 = ((self_in_CogX64Compiler->operands))[0];
		regLHS7 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((regLHS7 <= 7)
		 && (regRHS7 <= 7)) {
			skip8 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip8 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS7, 0, regRHS7));
		}
		((self_in_CogX64Compiler->machineCode))[skip8 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip8 + 2] = 89;
		((self_in_CogX64Compiler->machineCode))[skip8 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS7, regLHS7));
		return skip8 + 4;

	case OrCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 1, 13);

	case OrCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 11);

	case OrRR:
		return concretizeOpRR(self_in_CogX64Compiler, 11);

	case SubCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 5, 45);

	case SubbCqR:
		return concretizeArithCqRWithROraxOpcode(self_in_CogX64Compiler, 3, 29);

	case SubCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 43);

	case SubRR:
		return concretizeOpRR(self_in_CogX64Compiler, 43);

	case SubRdRd:
		/* begin concretizeSEE2OpRdRd: */
		regRHS8 = ((self_in_CogX64Compiler->operands))[0];
		regLHS8 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((regLHS8 <= 7)
		 && (regRHS8 <= 7)) {
			skip9 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip9 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS8, 0, regRHS8));
		}
		((self_in_CogX64Compiler->machineCode))[skip9 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip9 + 2] = 92;
		((self_in_CogX64Compiler->machineCode))[skip9 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS8, regLHS8));
		return skip9 + 4;

	case SubRsRs:
		/* begin concretizeSEEOpRsRs: */
		regRHS9 = ((self_in_CogX64Compiler->operands))[0];
		regLHS9 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((regLHS9 <= 7)
		 && (regRHS9 <= 7)) {
			skip10 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip10 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS9, 0, regRHS9));
		}
		((self_in_CogX64Compiler->machineCode))[skip10 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip10 + 2] = 92;
		((self_in_CogX64Compiler->machineCode))[skip10 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS9, regLHS9));
		return skip10 + 4;

	case SqrtRd:
		/* begin concretizeSqrtRd */
		reg4 = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if (reg4 <= 7) {
			skip11 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip11 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, reg4, 0, reg4));
		}
		((self_in_CogX64Compiler->machineCode))[skip11 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip11 + 2] = 81;
		((self_in_CogX64Compiler->machineCode))[skip11 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, reg4, reg4));
		return skip11 + 4;

	case SqrtRs:
		/* begin concretizeSqrtRs */
		reg5 = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if (reg5 <= 7) {
			skip12 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip12 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, reg5, 0, reg5));
		}
		((self_in_CogX64Compiler->machineCode))[skip12 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip12 + 2] = 81;
		((self_in_CogX64Compiler->machineCode))[skip12 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, reg5, reg5));
		return skip12 + 4;

	case XorCwR:
		return concretizeArithCwR(self_in_CogX64Compiler, 51);

	case XorRR:
		return concretizeOpRR(self_in_CogX64Compiler, 51);

	case XorRdRd:
		/* begin concretizeXorRdRd */
		regRHS10 = ((self_in_CogX64Compiler->operands))[0];
		regLHS10 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((regLHS10 <= 7)
		 && (regRHS10 <= 7)) {
			skip13 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip13 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS10, 0, regRHS10));
		}
		((self_in_CogX64Compiler->machineCode))[skip13 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip13 + 2] = 87;
		((self_in_CogX64Compiler->machineCode))[skip13 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS10, regLHS10));
		return skip13 + 4;

	case XorRsRs:
		/* begin concretizeXorRsRs */
		regRHS11 = ((self_in_CogX64Compiler->operands))[0];
		regLHS11 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		if ((regLHS11 <= 7)
		 && (regRHS11 <= 7)) {
			skip14 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip14 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, regLHS11, 0, regRHS11));
		}
		((self_in_CogX64Compiler->machineCode))[skip14 + 1] = 87;
		((self_in_CogX64Compiler->machineCode))[skip14 + 2] = (modRMRO(self_in_CogX64Compiler, ModReg, regRHS11, regLHS11));
		return skip14 + 3;

	case NegateR:
		/* begin concretizeNegateR */
		reg6 = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg6));
		((self_in_CogX64Compiler->machineCode))[1] = 247;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg6, 3));
		return 3;

	case LoadEffectiveAddressMwrR:
		/* begin concretizeLoadEffectiveAddressMwrR */
		offset4 = ((self_in_CogX64Compiler->operands))[0];
		srcReg = ((self_in_CogX64Compiler->operands))[1];
		destReg = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg, 0, srcReg));
		((self_in_CogX64Compiler->machineCode))[1] = 141;
		if ((srcReg != RSP)
		 && (srcReg != R12)) {
			if (isQuick(self_in_CogX64Compiler, offset4)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg, destReg));
				((self_in_CogX64Compiler->machineCode))[3] = (offset4 & 0xFF);
				return 4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg, destReg));
			((self_in_CogX64Compiler->machineCode))[3] = (offset4 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((offset4) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset4) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset4) >> 24) & 0xFF);
			return 7;
		}
		if (isQuick(self_in_CogX64Compiler, offset4)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg, destReg));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg));
			((self_in_CogX64Compiler->machineCode))[4] = (offset4 & 0xFF);
			return 5;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg, destReg));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg));
		((self_in_CogX64Compiler->machineCode))[4] = (offset4 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((offset4) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset4) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset4) >> 24) & 0xFF);
		return 8;

	case RotateLeftCqR:
		/* begin concretizeShiftCqRegOpcode: */
		distance = ((self_in_CogX64Compiler->operands))[0];
		assert(((distance >= 1) && (distance <= 0x3F)));
		reg7 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg7));
		if (distance == 1) {
			((self_in_CogX64Compiler->machineCode))[1] = 209;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg7, 0));
			return 3;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 193;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg7, 0));
		((self_in_CogX64Compiler->machineCode))[3] = distance;
		return 4;

	case RotateRightCqR:
		/* begin concretizeShiftCqRegOpcode: */
		distance1 = ((self_in_CogX64Compiler->operands))[0];
		assert(((distance1 >= 1) && (distance1 <= 0x3F)));
		reg8 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg8));
		if (distance1 == 1) {
			((self_in_CogX64Compiler->machineCode))[1] = 209;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg8, 1));
			return 3;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 193;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg8, 1));
		((self_in_CogX64Compiler->machineCode))[3] = distance1;
		return 4;

	case ArithmeticShiftRightCqR:
		/* begin concretizeShiftCqRegOpcode: */
		distance2 = ((self_in_CogX64Compiler->operands))[0];
		assert(((distance2 >= 1) && (distance2 <= 0x3F)));
		reg9 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg9));
		if (distance2 == 1) {
			((self_in_CogX64Compiler->machineCode))[1] = 209;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg9, 7));
			return 3;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 193;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg9, 7));
		((self_in_CogX64Compiler->machineCode))[3] = distance2;
		return 4;

	case LogicalShiftRightCqR:
		/* begin concretizeShiftCqRegOpcode: */
		distance3 = ((self_in_CogX64Compiler->operands))[0];
		assert(((distance3 >= 1) && (distance3 <= 0x3F)));
		reg10 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg10));
		if (distance3 == 1) {
			((self_in_CogX64Compiler->machineCode))[1] = 209;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg10, 5));
			return 3;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 193;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg10, 5));
		((self_in_CogX64Compiler->machineCode))[3] = distance3;
		return 4;

	case LogicalShiftLeftCqR:
		/* begin concretizeShiftCqRegOpcode: */
		distance4 = ((self_in_CogX64Compiler->operands))[0];
		assert(((distance4 >= 1) && (distance4 <= 0x3F)));
		reg11 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg11));
		if (distance4 == 1) {
			((self_in_CogX64Compiler->machineCode))[1] = 209;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg11, 4));
			return 3;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 193;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg11, 4));
		((self_in_CogX64Compiler->machineCode))[3] = distance4;
		return 4;

	case ArithmeticShiftRightRR:
		/* begin concretizeShiftRegRegOpcode: */
		shiftCountReg = ((self_in_CogX64Compiler->operands))[0];
		destReg1 = ((self_in_CogX64Compiler->operands))[1];
		if (shiftCountReg == RCX) {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, destReg1));
			((self_in_CogX64Compiler->machineCode))[1] = 211;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, destReg1, 7));
			return 3;
		}
		regToShift = (destReg1 == shiftCountReg
			? RCX
			: (destReg1 == RCX
					? shiftCountReg
					: destReg1));
		if (shiftCountReg == RAX) {
			((self_in_CogX64Compiler->machineCode))[0] = 72;
			((self_in_CogX64Compiler->machineCode))[1] = (144 + RCX);
			((self_in_CogX64Compiler->machineCode))[2] = (rexRxb(self_in_CogX64Compiler, 0, 0, regToShift));
			((self_in_CogX64Compiler->machineCode))[3] = 211;
			((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, regToShift, 7));
			((self_in_CogX64Compiler->machineCode))[5] = 72;
			((self_in_CogX64Compiler->machineCode))[6] = (144 + RCX);
			return 7;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, shiftCountReg, 0, RCX));
		((self_in_CogX64Compiler->machineCode))[1] = 135;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, RCX, shiftCountReg));
		((self_in_CogX64Compiler->machineCode))[3] = (rexRxb(self_in_CogX64Compiler, 0, 0, regToShift));
		((self_in_CogX64Compiler->machineCode))[4] = 211;
		((self_in_CogX64Compiler->machineCode))[5] = (modRMRO(self_in_CogX64Compiler, ModReg, regToShift, 7));
		((self_in_CogX64Compiler->machineCode))[6] = (rexRxb(self_in_CogX64Compiler, shiftCountReg, 0, RCX));
		((self_in_CogX64Compiler->machineCode))[7] = 135;
		((self_in_CogX64Compiler->machineCode))[8] = (modRMRO(self_in_CogX64Compiler, ModReg, RCX, shiftCountReg));
		return 9;

	case LogicalShiftLeftRR:
		/* begin concretizeShiftRegRegOpcode: */
		shiftCountReg1 = ((self_in_CogX64Compiler->operands))[0];
		destReg2 = ((self_in_CogX64Compiler->operands))[1];
		if (shiftCountReg1 == RCX) {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, destReg2));
			((self_in_CogX64Compiler->machineCode))[1] = 211;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, destReg2, 4));
			return 3;
		}
		regToShift1 = (destReg2 == shiftCountReg1
			? RCX
			: (destReg2 == RCX
					? shiftCountReg1
					: destReg2));
		if (shiftCountReg1 == RAX) {
			((self_in_CogX64Compiler->machineCode))[0] = 72;
			((self_in_CogX64Compiler->machineCode))[1] = (144 + RCX);
			((self_in_CogX64Compiler->machineCode))[2] = (rexRxb(self_in_CogX64Compiler, 0, 0, regToShift1));
			((self_in_CogX64Compiler->machineCode))[3] = 211;
			((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, regToShift1, 4));
			((self_in_CogX64Compiler->machineCode))[5] = 72;
			((self_in_CogX64Compiler->machineCode))[6] = (144 + RCX);
			return 7;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, shiftCountReg1, 0, RCX));
		((self_in_CogX64Compiler->machineCode))[1] = 135;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, RCX, shiftCountReg1));
		((self_in_CogX64Compiler->machineCode))[3] = (rexRxb(self_in_CogX64Compiler, 0, 0, regToShift1));
		((self_in_CogX64Compiler->machineCode))[4] = 211;
		((self_in_CogX64Compiler->machineCode))[5] = (modRMRO(self_in_CogX64Compiler, ModReg, regToShift1, 4));
		((self_in_CogX64Compiler->machineCode))[6] = (rexRxb(self_in_CogX64Compiler, shiftCountReg1, 0, RCX));
		((self_in_CogX64Compiler->machineCode))[7] = 135;
		((self_in_CogX64Compiler->machineCode))[8] = (modRMRO(self_in_CogX64Compiler, ModReg, RCX, shiftCountReg1));
		return 9;

	case ClzRR:
		/* begin concretizeClzRR */
		maskReg = ((self_in_CogX64Compiler->operands))[0];
		dest = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		((self_in_CogX64Compiler->machineCode))[1] = (rexwrxb(self_in_CogX64Compiler, 1, dest, 0, maskReg));
		((self_in_CogX64Compiler->machineCode))[2] = 15;
		((self_in_CogX64Compiler->machineCode))[3] = 189;
		((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, maskReg, dest));
		return 5;

	case MoveCqR:
		/* begin concretizeMoveCqR */
		value2 = ((self_in_CogX64Compiler->operands))[0];
		reg12 = ((self_in_CogX64Compiler->operands))[1];
		if (is32BitSignedImmediate(self_in_CogX64Compiler, value2)) {
			if (value2 == 0) {
				((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, reg12, 0, reg12));
				((self_in_CogX64Compiler->machineCode))[1] = 49;
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg12, reg12));
				return 3;
			}
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg12));
			((self_in_CogX64Compiler->machineCode))[1] = 199;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg12, 0));
			((self_in_CogX64Compiler->machineCode))[3] = (value2 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((value2) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((value2) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((value2) >> 24) & 0xFF);
			return 7;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg12));
		((self_in_CogX64Compiler->machineCode))[1] = (184 + (reg12 & 7));
		((self_in_CogX64Compiler->machineCode))[2] = (value2 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((value2) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value2) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value2) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value2) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((value2) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((value2) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((value2) >> 56) & 0xFF);
		return 10;

	case MoveCwR:
		/* begin concretizeMoveCwR */
		value3 = ((self_in_CogX64Compiler->operands))[0];
		reg13 = ((self_in_CogX64Compiler->operands))[1];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) value3)))
		 || ((((AbstractInstruction *) value3)) == (methodLabel()))) {
			value3 = ((((AbstractInstruction *) value3))->address);
		}
		if (		/* begin addressIsInCurrentCompilation: */
			((((usqInt)value3)) >= ((methodLabel->address)))
		 && ((((usqInt)value3)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))) {
			offset5 = value3 - (((self_in_CogX64Compiler->address)) + 7);
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, reg13, 0, 0));
			((self_in_CogX64Compiler->machineCode))[1] = 141;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 5, reg13));
			((self_in_CogX64Compiler->machineCode))[3] = (offset5 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset5)) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset5)) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = ((((usqInt)(offset5)) >> 24) & 0xFF);
			return 7;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg13));
		((self_in_CogX64Compiler->machineCode))[1] = (184 + (reg13 & 7));
		((self_in_CogX64Compiler->machineCode))[2] = (value3 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((value3) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value3) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value3) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value3) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((value3) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((value3) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((value3) >> 56) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[10] = 144;
		return 11;

	case MoveC32R:
		/* begin concretizeMoveC32R */
		value4 = ((self_in_CogX64Compiler->operands))[0];
		reg14 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg14));
		((self_in_CogX64Compiler->machineCode))[1] = 199;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, reg14, 0));
		((self_in_CogX64Compiler->machineCode))[3] = (value4 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value4) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value4) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value4) >> 24) & 0xFF);
		return 7;

	case MoveRR:
		return concretizeReverseOpRR(self_in_CogX64Compiler, 137);

	case MoveAwR:
		/* begin concretizeMoveAwR */
		addressOperand = ((self_in_CogX64Compiler->operands))[0];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand)))
		 || ((((AbstractInstruction *) addressOperand)) == (methodLabel()))) {
			addressOperand = ((((AbstractInstruction *) addressOperand))->address);
		}
		if (		/* begin isAddressRelativeToVarBase: */
			(addressOperand)
		 && ((addressOperand >= (varBaseAddress()))
		 && ((addressOperand - (varBaseAddress())) < (0x100000)))) {
			save0 = ((self_in_CogX64Compiler->operands))[0];
			save1 = ((self_in_CogX64Compiler->operands))[1];
			((self_in_CogX64Compiler->operands))[0] = (addressOperand - (varBaseAddress()));
			((self_in_CogX64Compiler->operands))[1] = RBX;
			((self_in_CogX64Compiler->operands))[2] = save1;
			/* begin concretizeMoveMwrR */
			offset11 = ((self_in_CogX64Compiler->operands))[0];
			srcReg1 = ((self_in_CogX64Compiler->operands))[1];
			destReg3 = ((self_in_CogX64Compiler->operands))[2];
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg3, 0, srcReg1));
			((self_in_CogX64Compiler->machineCode))[1] = 139;
			if ((srcReg1 != RSP)
			 && (srcReg1 != R12)) {
				if ((offset11 == 0)
				 && ((srcReg1 != RBP)
				 && (srcReg1 != R13))) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg1, destReg3));
					savedSize = 3;
					goto l1;
				}
				if (isQuick(self_in_CogX64Compiler, offset11)) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg1, destReg3));
					((self_in_CogX64Compiler->machineCode))[3] = (offset11 & 0xFF);
					savedSize = 4;
					goto l1;
				}
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg1, destReg3));
				((self_in_CogX64Compiler->machineCode))[3] = (offset11 & 0xFF);
				((self_in_CogX64Compiler->machineCode))[4] = (((offset11) >> 8) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[5] = (((offset11) >> 16) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[6] = (((offset11) >> 24) & 0xFF);
				savedSize = 7;
				goto l1;
			}
			if (offset11 == 0) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg1, destReg3));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg1));
				savedSize = 4;
				goto l1;
			}
			if (isQuick(self_in_CogX64Compiler, offset11)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg1, destReg3));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg1));
				((self_in_CogX64Compiler->machineCode))[4] = (offset11 & 0xFF);
				savedSize = 5;
				goto l1;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg1, destReg3));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg1));
			((self_in_CogX64Compiler->machineCode))[4] = (offset11 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset11) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset11) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = (((offset11) >> 24) & 0xFF);
			savedSize = 8;
	l1:	/* end concretizeMoveMwrR */;
			((self_in_CogX64Compiler->operands))[0] = save0;
			((self_in_CogX64Compiler->operands))[1] = save1;
			((self_in_CogX64Compiler->operands))[2] = 0;
			return savedSize;
		}
		/* If fetching RAX, fetch directly, otherwise, because of instruction encoding limitations, the register
		   _must_ be fetched through RAX.  If reg = RBP or RSP simply fetch directly, otherwise swap RAX with
		   the register before and after the fetch through RAX.  We avoid swapping before hand with RBP
		   and RSP because setting RSP to whatever the contents of RAX is can cause disastrous results if
		   an interrupt is delivered immediately after that point.  See mail threads beginning with
		   http://lists.squeakfoundation.org/pipermail/vm-dev/2019-September/031428.html
		   http://lists.squeakfoundation.org/pipermail/vm-dev/2019-October/031499.html */
		reg15 = ((self_in_CogX64Compiler->operands))[1];
		if ((reg15 == RAX)
		 || ((reg15 == RBP)
		 || (reg15 == RSP))) {
			offset6 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg15));
			((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg15 % 8));
			offset6 = 2;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset6] = 72;
		((self_in_CogX64Compiler->machineCode))[1 + offset6] = 161;
		((self_in_CogX64Compiler->machineCode))[2 + offset6] = (addressOperand & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset6] = (((addressOperand) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset6] = (((addressOperand) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset6] = (((addressOperand) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset6] = (((addressOperand) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset6] = (((addressOperand) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset6] = (((addressOperand) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9 + offset6] = (((addressOperand) >> 56) & 0xFF);
		if (reg15 == RAX) {
			return 10;
		}
		if ((reg15 == RBP)
		 || (reg15 == RSP)) {
			((self_in_CogX64Compiler->machineCode))[10] = (rexRxb(self_in_CogX64Compiler, RAX, 0, reg15));
			((self_in_CogX64Compiler->machineCode))[11] = (144 + (reg15 % 8));
			return 12;
		}
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[13] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 14;

	case MoveA32R:
		/* begin concretizeMoveA32R */
		addressOperand1 = ((self_in_CogX64Compiler->operands))[0];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand1)))
		 || ((((AbstractInstruction *) addressOperand1)) == (methodLabel()))) {
			addressOperand1 = ((((AbstractInstruction *) addressOperand1))->address);
		}
		reg16 = ((self_in_CogX64Compiler->operands))[1];
		if (reg16 == RAX) {
			offset7 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg16));
			((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg16 % 8));
			offset7 = 2;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset7] = 161;
		((self_in_CogX64Compiler->machineCode))[1 + offset7] = (addressOperand1 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2 + offset7] = (((addressOperand1) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset7] = (((addressOperand1) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset7] = (((addressOperand1) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset7] = (((addressOperand1) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset7] = (((addressOperand1) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset7] = (((addressOperand1) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset7] = (((addressOperand1) >> 56) & 0xFF);
		if (reg16 == RAX) {
			return 9;
		}
		((self_in_CogX64Compiler->machineCode))[11] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 13;

	case MoveRAw:
		/* begin concretizeMoveRAw */
		reg17 = ((self_in_CogX64Compiler->operands))[0];
		addressOperand2 = ((self_in_CogX64Compiler->operands))[1];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand2)))
		 || ((((AbstractInstruction *) addressOperand2)) == (methodLabel()))) {
			addressOperand2 = ((((AbstractInstruction *) addressOperand2))->address);
		}
		if (		/* begin isAddressRelativeToVarBase: */
			(addressOperand2)
		 && ((addressOperand2 >= (varBaseAddress()))
		 && ((addressOperand2 - (varBaseAddress())) < (0x100000)))) {
			save11 = ((self_in_CogX64Compiler->operands))[1];
			((self_in_CogX64Compiler->operands))[1] = (addressOperand2 - (varBaseAddress()));
			((self_in_CogX64Compiler->operands))[2] = RBX;
			/* begin concretizeMoveRMwr */
			srcReg2 = ((self_in_CogX64Compiler->operands))[0];
			offset12 = ((self_in_CogX64Compiler->operands))[1];
			destReg4 = ((self_in_CogX64Compiler->operands))[2];
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, srcReg2, 0, destReg4));
			((self_in_CogX64Compiler->machineCode))[1] = 137;
			if ((destReg4 != RSP)
			 && (destReg4 != R12)) {
				if ((offset12 == 0)
				 && ((destReg4 != RBP)
				 && (destReg4 != R13))) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg4, srcReg2));
					savedSize1 = 3;
					goto l2;
				}
				if (isQuick(self_in_CogX64Compiler, offset12)) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg4, srcReg2));
					((self_in_CogX64Compiler->machineCode))[3] = (offset12 & 0xFF);
					savedSize1 = 4;
					goto l2;
				}
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg4, srcReg2));
				((self_in_CogX64Compiler->machineCode))[3] = (offset12 & 0xFF);
				((self_in_CogX64Compiler->machineCode))[4] = (((offset12) >> 8) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[5] = (((offset12) >> 16) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[6] = (((offset12) >> 24) & 0xFF);
				savedSize1 = 7;
				goto l2;
			}
			if (offset12 == 0) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg4, srcReg2));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg4));
				savedSize1 = 4;
				goto l2;
			}
			if (isQuick(self_in_CogX64Compiler, offset12)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg4, srcReg2));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg4));
				((self_in_CogX64Compiler->machineCode))[4] = (offset12 & 0xFF);
				savedSize1 = 5;
				goto l2;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg4, srcReg2));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg4));
			((self_in_CogX64Compiler->machineCode))[4] = (offset12 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset12) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset12) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = (((offset12) >> 24) & 0xFF);
			savedSize1 = 8;
	l2:	/* end concretizeMoveRMwr */;
			((self_in_CogX64Compiler->operands))[1] = save11;
			((self_in_CogX64Compiler->operands))[2] = 0;
			return savedSize1;
		}
		if ((reg17 == RAX)
		 || ((reg17 == RBP)
		 || (reg17 == RSP))) {
			offset8 = 0;
		}
		else {
			if ((reg17 == RBP)
			 || (reg17 == RSP)) {
				((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, reg17, 0, RAX));
				((self_in_CogX64Compiler->machineCode))[1] = 137;
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, RAX, reg17));
				offset8 = 3;
			}
			else {
				((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RAX, 0, reg17));
				((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg17 % 8));
				offset8 = 2;
			}
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset8] = 72;
		((self_in_CogX64Compiler->machineCode))[1 + offset8] = 163;
		((self_in_CogX64Compiler->machineCode))[2 + offset8] = (addressOperand2 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset8] = (((addressOperand2) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset8] = (((addressOperand2) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset8] = (((addressOperand2) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset8] = (((addressOperand2) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset8] = (((addressOperand2) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset8] = (((addressOperand2) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9 + offset8] = (((addressOperand2) >> 56) & 0xFF);
		if (reg17 == RAX) {
			return 10;
		}
		if ((reg17 == RBP)
		 || (reg17 == RSP)) {
			return 13;
		}
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[13] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 14;

	case MoveRA32:
		/* begin concretizeMoveRA32 */
		reg18 = ((self_in_CogX64Compiler->operands))[0];
		addressOperand3 = ((self_in_CogX64Compiler->operands))[1];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand3)))
		 || ((((AbstractInstruction *) addressOperand3)) == (methodLabel()))) {
			addressOperand3 = ((((AbstractInstruction *) addressOperand3))->address);
		}
		if (reg18 == RAX) {
			offset9 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg18));
			((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg18 % 8));
			offset9 = 2;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset9] = 163;
		((self_in_CogX64Compiler->machineCode))[1 + offset9] = (addressOperand3 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[2 + offset9] = (((addressOperand3) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset9] = (((addressOperand3) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset9] = (((addressOperand3) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset9] = (((addressOperand3) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset9] = (((addressOperand3) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset9] = (((addressOperand3) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset9] = (((addressOperand3) >> 56) & 0xFF);
		if (reg18 == RAX) {
			return 9;
		}
		((self_in_CogX64Compiler->machineCode))[11] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 13;

	case MoveAbR:
		/* begin concretizeMoveAbR */
		addressOperand4 = ((self_in_CogX64Compiler->operands))[0];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand4)))
		 || ((((AbstractInstruction *) addressOperand4)) == (methodLabel()))) {
			addressOperand4 = ((((AbstractInstruction *) addressOperand4))->address);
		}
		if (		/* begin isAddressRelativeToVarBase: */
			(addressOperand4)
		 && ((addressOperand4 >= (varBaseAddress()))
		 && ((addressOperand4 - (varBaseAddress())) < (0x100000)))) {
			save01 = ((self_in_CogX64Compiler->operands))[0];
			save12 = ((self_in_CogX64Compiler->operands))[1];
			((self_in_CogX64Compiler->operands))[0] = (addressOperand4 - (varBaseAddress()));
			((self_in_CogX64Compiler->operands))[1] = RBX;
			((self_in_CogX64Compiler->operands))[2] = save12;
			/* begin concretizeMoveMbrR */
			offset110 = ((self_in_CogX64Compiler->operands))[0];
			srcReg31 = ((self_in_CogX64Compiler->operands))[1];
			destReg32 = ((self_in_CogX64Compiler->operands))[2];
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg32, 0, srcReg31));
			((self_in_CogX64Compiler->machineCode))[1] = 138;
			if ((srcReg31 != RSP)
			 && (srcReg31 != R12)) {
				if ((offset110 == 0)
				 && ((srcReg31 != RBP)
				 && (srcReg31 != R13))) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg31, destReg32));
					savedSize2 = 3;
					goto l4;
				}
				if (isQuick(self_in_CogX64Compiler, offset110)) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg31, destReg32));
					((self_in_CogX64Compiler->machineCode))[3] = (offset110 & 0xFF);
					savedSize2 = 4;
					goto l4;
				}
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg31, destReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (offset110 & 0xFF);
				((self_in_CogX64Compiler->machineCode))[4] = (((offset110) >> 8) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[5] = (((offset110) >> 16) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[6] = (((offset110) >> 24) & 0xFF);
				savedSize2 = 7;
				goto l4;
			}
			if (offset110 == 0) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg31, destReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg31));
				savedSize2 = 4;
				goto l4;
			}
			if (isQuick(self_in_CogX64Compiler, offset110)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg31, destReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg31));
				((self_in_CogX64Compiler->machineCode))[4] = (offset110 & 0xFF);
				savedSize2 = 5;
				goto l4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg31, destReg32));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg31));
			((self_in_CogX64Compiler->machineCode))[4] = (offset110 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset110) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset110) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = (((offset110) >> 24) & 0xFF);
			savedSize2 = 8;
	l4:	/* end concretizeMoveMbrR */;
			((self_in_CogX64Compiler->operands))[0] = save01;
			((self_in_CogX64Compiler->operands))[1] = save12;
			((self_in_CogX64Compiler->operands))[2] = 0;
			return savedSize2;
		}
		reg21 = ((self_in_CogX64Compiler->operands))[1];
		if (reg21 == RAX) {
			offset26 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg21));
			((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg21 % 8));
			offset26 = 2;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset26] = 72;
		((self_in_CogX64Compiler->machineCode))[1 + offset26] = 160;
		((self_in_CogX64Compiler->machineCode))[2 + offset26] = (addressOperand4 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset26] = (((addressOperand4) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset26] = (((addressOperand4) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset26] = (((addressOperand4) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset26] = (((addressOperand4) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset26] = (((addressOperand4) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset26] = (((addressOperand4) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9 + offset26] = (((addressOperand4) >> 56) & 0xFF);
		if (reg21 == RAX) {
			return 10;
		}
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[13] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 14;

	case MoveRAb:
		/* begin concretizeMoveRAb */
		reg22 = ((self_in_CogX64Compiler->operands))[0];
		addressOperand5 = ((self_in_CogX64Compiler->operands))[1];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand5)))
		 || ((((AbstractInstruction *) addressOperand5)) == (methodLabel()))) {
			addressOperand5 = ((((AbstractInstruction *) addressOperand5))->address);
		}
		if (		/* begin isAddressRelativeToVarBase: */
			(addressOperand5)
		 && ((addressOperand5 >= (varBaseAddress()))
		 && ((addressOperand5 - (varBaseAddress())) < (0x100000)))) {
			save13 = ((self_in_CogX64Compiler->operands))[1];
			((self_in_CogX64Compiler->operands))[1] = (addressOperand5 - (varBaseAddress()));
			((self_in_CogX64Compiler->operands))[2] = RBX;
			/* begin concretizeMoveRMbr */
			srcReg32 = ((self_in_CogX64Compiler->operands))[0];
			offset111 = ((self_in_CogX64Compiler->operands))[1];
			baseReg1 = ((self_in_CogX64Compiler->operands))[2];
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, srcReg32, 0, baseReg1));
			((self_in_CogX64Compiler->machineCode))[1] = 136;
			if ((baseReg1 != RSP)
			 && (baseReg1 != R12)) {
				if ((offset111 == 0)
				 && ((baseReg1 != RBP)
				 && (baseReg1 != R13))) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, baseReg1, srcReg32));
					savedSize3 = 3;
					goto l5;
				}
				if (isQuick(self_in_CogX64Compiler, offset111)) {
					((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, baseReg1, srcReg32));
					((self_in_CogX64Compiler->machineCode))[3] = (offset111 & 0xFF);
					savedSize3 = 4;
					goto l5;
				}
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, baseReg1, srcReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (offset111 & 0xFF);
				((self_in_CogX64Compiler->machineCode))[4] = (((offset111) >> 8) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[5] = (((offset111) >> 16) & 0xFF);
				((self_in_CogX64Compiler->machineCode))[6] = (((offset111) >> 24) & 0xFF);
				savedSize3 = 7;
				goto l5;
			}
			if (offset111 == 0) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, baseReg1, srcReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg1));
				savedSize3 = 4;
				goto l5;
			}
			if (isQuick(self_in_CogX64Compiler, offset111)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, baseReg1, srcReg32));
				((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg1));
				((self_in_CogX64Compiler->machineCode))[4] = (offset111 & 0xFF);
				savedSize3 = 5;
				goto l5;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, baseReg1, srcReg32));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg1));
			((self_in_CogX64Compiler->machineCode))[4] = (offset111 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset111) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset111) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = (((offset111) >> 24) & 0xFF);
			savedSize3 = 8;
	l5:	/* end concretizeMoveRMbr */;
			((self_in_CogX64Compiler->operands))[1] = save13;
			((self_in_CogX64Compiler->operands))[2] = 0;
			return savedSize3;
		}
		if (reg22 == RAX) {
			offset27 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, reg22));
			((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg22 % 8));
			offset27 = 2;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset27] = 72;
		((self_in_CogX64Compiler->machineCode))[1 + offset27] = 162;
		((self_in_CogX64Compiler->machineCode))[2 + offset27] = (addressOperand5 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset27] = (((addressOperand5) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset27] = (((addressOperand5) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset27] = (((addressOperand5) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset27] = (((addressOperand5) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset27] = (((addressOperand5) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset27] = (((addressOperand5) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9 + offset27] = (((addressOperand5) >> 56) & 0xFF);
		if (reg22 == RAX) {
			return 10;
		}
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[13] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 14;

	case MoveMbrR:
	case MoveM8rR:
		/* begin concretizeMoveMbrR */
		offset10 = ((self_in_CogX64Compiler->operands))[0];
		srcReg3 = ((self_in_CogX64Compiler->operands))[1];
		destReg5 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg5, 0, srcReg3));
		((self_in_CogX64Compiler->machineCode))[1] = 138;
		if ((srcReg3 != RSP)
		 && (srcReg3 != R12)) {
			if ((offset10 == 0)
			 && ((srcReg3 != RBP)
			 && (srcReg3 != R13))) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg3, destReg5));
				return 3;
			}
			if (isQuick(self_in_CogX64Compiler, offset10)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg3, destReg5));
				((self_in_CogX64Compiler->machineCode))[3] = (offset10 & 0xFF);
				return 4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg3, destReg5));
			((self_in_CogX64Compiler->machineCode))[3] = (offset10 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((offset10) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset10) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset10) >> 24) & 0xFF);
			return 7;
		}
		if (offset10 == 0) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg3, destReg5));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg3));
			return 4;
		}
		if (isQuick(self_in_CogX64Compiler, offset10)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg3, destReg5));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg3));
			((self_in_CogX64Compiler->machineCode))[4] = (offset10 & 0xFF);
			return 5;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg3, destReg5));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg3));
		((self_in_CogX64Compiler->machineCode))[4] = (offset10 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((offset10) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset10) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset10) >> 24) & 0xFF);
		return 8;

	case MoveRMbr:
	case MoveRM8r:
		/* begin concretizeMoveRMbr */
		srcReg4 = ((self_in_CogX64Compiler->operands))[0];
		offset13 = ((self_in_CogX64Compiler->operands))[1];
		baseReg = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, srcReg4, 0, baseReg));
		((self_in_CogX64Compiler->machineCode))[1] = 136;
		if ((baseReg != RSP)
		 && (baseReg != R12)) {
			if ((offset13 == 0)
			 && ((baseReg != RBP)
			 && (baseReg != R13))) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, baseReg, srcReg4));
				return 3;
			}
			if (isQuick(self_in_CogX64Compiler, offset13)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, baseReg, srcReg4));
				((self_in_CogX64Compiler->machineCode))[3] = (offset13 & 0xFF);
				return 4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, baseReg, srcReg4));
			((self_in_CogX64Compiler->machineCode))[3] = (offset13 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((offset13) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset13) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset13) >> 24) & 0xFF);
			return 7;
		}
		if (offset13 == 0) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, baseReg, srcReg4));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg));
			return 4;
		}
		if (isQuick(self_in_CogX64Compiler, offset13)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, baseReg, srcReg4));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg));
			((self_in_CogX64Compiler->machineCode))[4] = (offset13 & 0xFF);
			return 5;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, baseReg, srcReg4));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, baseReg));
		((self_in_CogX64Compiler->machineCode))[4] = (offset13 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((offset13) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset13) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset13) >> 24) & 0xFF);
		return 8;

	case MoveM16rR:
		/* begin concretizeMoveM16rR */
		offset14 = ((self_in_CogX64Compiler->operands))[0];
		srcReg5 = ((self_in_CogX64Compiler->operands))[1];
		destReg6 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg6, 0, srcReg5));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 183;
		if ((srcReg5 != RSP)
		 && (srcReg5 != R12)) {
			if ((offset14 == 0)
			 && ((srcReg5 != RBP)
			 && (srcReg5 != R13))) {
				((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg5, destReg6));
				return 4;
			}
			if (isQuick(self_in_CogX64Compiler, offset14)) {
				((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg5, destReg6));
				((self_in_CogX64Compiler->machineCode))[4] = (offset14 & 0xFF);
				return 5;
			}
			((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg5, destReg6));
			((self_in_CogX64Compiler->machineCode))[4] = (offset14 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset14) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset14) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = (((offset14) >> 24) & 0xFF);
			return 8;
		}
		if ((offset14 == 0)
		 && ((srcReg5 != RBP)
		 && (srcReg5 != R13))) {
			((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg5, destReg6));
			((self_in_CogX64Compiler->machineCode))[4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg5));
			return 5;
		}
		if (isQuick(self_in_CogX64Compiler, offset14)) {
			((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg5, destReg6));
			((self_in_CogX64Compiler->machineCode))[4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg5));
			((self_in_CogX64Compiler->machineCode))[5] = (offset14 & 0xFF);
			return 6;
		}
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg5, destReg6));
		((self_in_CogX64Compiler->machineCode))[4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg5));
		((self_in_CogX64Compiler->machineCode))[5] = (offset14 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset14) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset14) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((offset14) >> 24) & 0xFF);
		return 9;

	case MoveRM16r:
		/* begin concretizeMoveRM16r */
		srcReg6 = ((self_in_CogX64Compiler->operands))[0];
		offset15 = ((self_in_CogX64Compiler->operands))[1];
		destReg7 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((srcReg6 > 7)
		 || (destReg7 > 7)) {
			((self_in_CogX64Compiler->machineCode))[1] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg6, 0, destReg7));
			skip15 = 1;
		}
		else {
			skip15 = 0;
		}
		if ((destReg7 & 7) != RSP) {
			if (isQuick(self_in_CogX64Compiler, offset15)) {
				((self_in_CogX64Compiler->machineCode))[skip15 + 1] = 137;
				((self_in_CogX64Compiler->machineCode))[skip15 + 2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg7, srcReg6));
				((self_in_CogX64Compiler->machineCode))[skip15 + 3] = (offset15 & 0xFF);
				return skip15 + 4;
			}
			((self_in_CogX64Compiler->machineCode))[skip15 + 1] = 137;
			((self_in_CogX64Compiler->machineCode))[skip15 + 2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg7, srcReg6));
			((self_in_CogX64Compiler->machineCode))[skip15 + 3] = (offset15 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[skip15 + 4] = (((offset15) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[skip15 + 5] = (((offset15) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[skip15 + 6] = (((offset15) >> 24) & 0xFF);
			return skip15 + 7;
		}
		if (isQuick(self_in_CogX64Compiler, offset15)) {
			((self_in_CogX64Compiler->machineCode))[skip15 + 1] = 137;
			((self_in_CogX64Compiler->machineCode))[skip15 + 2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg7, srcReg6));
			((self_in_CogX64Compiler->machineCode))[skip15 + 3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg7));
			((self_in_CogX64Compiler->machineCode))[skip15 + 4] = (offset15 & 0xFF);
			return skip15 + 5;
		}
		((self_in_CogX64Compiler->machineCode))[skip15 + 1] = 137;
		((self_in_CogX64Compiler->machineCode))[skip15 + 2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg7, srcReg6));
		((self_in_CogX64Compiler->machineCode))[skip15 + 3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg7));
		((self_in_CogX64Compiler->machineCode))[skip15 + 4] = (offset15 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip15 + 5] = (((offset15) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip15 + 6] = (((offset15) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip15 + 7] = (((offset15) >> 24) & 0xFF);
		return skip15 + 8;

	case MoveM32rR:
		/* begin concretizeMoveM32rR */
		offset16 = ((self_in_CogX64Compiler->operands))[0];
		srcReg7 = ((self_in_CogX64Compiler->operands))[1];
		destReg8 = ((self_in_CogX64Compiler->operands))[2];
		if ((srcReg7 <= 7)
		 && (destReg8 <= 7)) {
			skip16 = 0;
		}
		else {
			skip16 = 1;
			((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 0, destReg8, 0, srcReg7));
		}
		((self_in_CogX64Compiler->machineCode))[skip16] = 139;
		if (offset16 == 0) {
			if ((srcReg7 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip16 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg7, destReg8));
				return skip16 + 2;
			}
			if ((srcReg7 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip16 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg7, destReg8));
				((self_in_CogX64Compiler->machineCode))[skip16 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg7));
				return skip16 + 3;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset16)) {
			if ((srcReg7 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip16 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg7, destReg8));
				((self_in_CogX64Compiler->machineCode))[skip16 + 2] = (offset16 & 0xFF);
				return skip16 + 3;
			}
			((self_in_CogX64Compiler->machineCode))[skip16 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg7, destReg8));
			((self_in_CogX64Compiler->machineCode))[skip16 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg7));
			((self_in_CogX64Compiler->machineCode))[skip16 + 3] = (offset16 & 0xFF);
			return skip16 + 4;
		}
		((self_in_CogX64Compiler->machineCode))[skip16 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg7, destReg8));
		if ((srcReg7 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip16 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg7));
			skip16 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip16 + 2] = (offset16 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip16 + 3] = (((offset16) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip16 + 4] = (((offset16) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip16 + 5] = (((offset16) >> 24) & 0xFF);
		return skip16 + 6;

	case MoveM32rRs:
		/* begin concretizeMoveM32rRs */
		offset17 = ((self_in_CogX64Compiler->operands))[0];
		srcReg8 = ((self_in_CogX64Compiler->operands))[1];
		destReg9 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((srcReg8 <= 7)
		 && (destReg9 <= 7)) {
			skip17 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip17 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg9, 0, srcReg8));
		}
		((self_in_CogX64Compiler->machineCode))[skip17 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip17 + 2] = 110;
		if (offset17 == 0) {
			if ((srcReg8 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip17 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg8, destReg9));
				return skip17 + 4;
			}
			if ((srcReg8 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip17 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg8, destReg9));
				((self_in_CogX64Compiler->machineCode))[skip17 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg8));
				return skip17 + 5;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset17)) {
			if ((srcReg8 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip17 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg8, destReg9));
				((self_in_CogX64Compiler->machineCode))[skip17 + 4] = (offset17 & 0xFF);
				return skip17 + 5;
			}
			((self_in_CogX64Compiler->machineCode))[skip17 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg8, destReg9));
			((self_in_CogX64Compiler->machineCode))[skip17 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg8));
			((self_in_CogX64Compiler->machineCode))[skip17 + 5] = (offset17 & 0xFF);
			return skip17 + 6;
		}
		((self_in_CogX64Compiler->machineCode))[skip17 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg8, destReg9));
		if ((srcReg8 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip17 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg8));
			skip17 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip17 + 4] = (offset17 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip17 + 5] = (((offset17) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip17 + 6] = (((offset17) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip17 + 7] = (((offset17) >> 24) & 0xFF);
		return skip17 + 8;

	case MoveM64rRd:
		/* begin concretizeMoveM64rRd */
		offset18 = ((self_in_CogX64Compiler->operands))[0];
		srcReg9 = ((self_in_CogX64Compiler->operands))[1];
		destReg10 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((srcReg9 <= 7)
		 && (destReg10 <= 7)) {
			skip18 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip18 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg10, 0, srcReg9));
		}
		((self_in_CogX64Compiler->machineCode))[skip18 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip18 + 2] = 0x7E;
		if (offset18 == 0) {
			if ((srcReg9 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip18 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg9, destReg10));
				return skip18 + 4;
			}
			if ((srcReg9 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip18 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg9, destReg10));
				((self_in_CogX64Compiler->machineCode))[skip18 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg9));
				return skip18 + 5;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset18)) {
			if ((srcReg9 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip18 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg9, destReg10));
				((self_in_CogX64Compiler->machineCode))[skip18 + 4] = (offset18 & 0xFF);
				return skip18 + 5;
			}
			((self_in_CogX64Compiler->machineCode))[skip18 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg9, destReg10));
			((self_in_CogX64Compiler->machineCode))[skip18 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg9));
			((self_in_CogX64Compiler->machineCode))[skip18 + 5] = (offset18 & 0xFF);
			return skip18 + 6;
		}
		((self_in_CogX64Compiler->machineCode))[skip18 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg9, destReg10));
		if ((srcReg9 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip18 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg9));
			skip18 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip18 + 4] = (offset18 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip18 + 5] = (((offset18) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip18 + 6] = (((offset18) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip18 + 7] = (((offset18) >> 24) & 0xFF);
		return skip18 + 8;

	case MoveMwrR:
		/* begin concretizeMoveMwrR */
		offset19 = ((self_in_CogX64Compiler->operands))[0];
		srcReg10 = ((self_in_CogX64Compiler->operands))[1];
		destReg11 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, destReg11, 0, srcReg10));
		((self_in_CogX64Compiler->machineCode))[1] = 139;
		if ((srcReg10 != RSP)
		 && (srcReg10 != R12)) {
			if ((offset19 == 0)
			 && ((srcReg10 != RBP)
			 && (srcReg10 != R13))) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg10, destReg11));
				return 3;
			}
			if (isQuick(self_in_CogX64Compiler, offset19)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg10, destReg11));
				((self_in_CogX64Compiler->machineCode))[3] = (offset19 & 0xFF);
				return 4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg10, destReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (offset19 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((offset19) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset19) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset19) >> 24) & 0xFF);
			return 7;
		}
		if (offset19 == 0) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, srcReg10, destReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg10));
			return 4;
		}
		if (isQuick(self_in_CogX64Compiler, offset19)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, srcReg10, destReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg10));
			((self_in_CogX64Compiler->machineCode))[4] = (offset19 & 0xFF);
			return 5;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, srcReg10, destReg11));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, srcReg10));
		((self_in_CogX64Compiler->machineCode))[4] = (offset19 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((offset19) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset19) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset19) >> 24) & 0xFF);
		return 8;

	case MoveXbrRR:
		/* begin concretizeMoveXbrRR */
		index = ((self_in_CogX64Compiler->operands))[0];
		base = ((self_in_CogX64Compiler->operands))[1];
		dest1 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, dest1, index, base));
		((self_in_CogX64Compiler->machineCode))[1] = 138;
		if ((base != RBP)
		 && (base != R13)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, dest1));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, index, base));
			return 4;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, dest1));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, index, base));
		((self_in_CogX64Compiler->machineCode))[4] = 0;
		return 5;

	case MoveRXbrR:
		/* begin concretizeMoveRXbrR */
		src = ((self_in_CogX64Compiler->operands))[0];
		index1 = ((self_in_CogX64Compiler->operands))[1];
		base1 = ((self_in_CogX64Compiler->operands))[2];
		offset20 = 0;
		if ((src > 3)
		 || ((base1 > 7)
		 || (index1 > 7))) {
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, src, index1, base1));
			offset20 = 1;
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset20] = 136;
		if ((base1 & 7) != RBP) {
			/* RBP,R13 */
			((self_in_CogX64Compiler->machineCode))[1 + offset20] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, src));
			((self_in_CogX64Compiler->machineCode))[2 + offset20] = (sib(self_in_CogX64Compiler, SIB1, index1, base1));
			return 3 + offset20;
		}
		((self_in_CogX64Compiler->machineCode))[1 + offset20] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, src));
		((self_in_CogX64Compiler->machineCode))[2 + offset20] = (sib(self_in_CogX64Compiler, SIB1, index1, base1));
		((self_in_CogX64Compiler->machineCode))[3 + offset20] = 0;
		return 4 + offset20;

	case MoveXwrRR:
		/* begin concretizeMoveXwrRR */
		index2 = ((self_in_CogX64Compiler->operands))[0];
		base2 = ((self_in_CogX64Compiler->operands))[1];
		dest2 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, dest2, index2, base2));
		if ((base2 != RBP)
		 && (base2 != R13)) {
			((self_in_CogX64Compiler->machineCode))[1] = 139;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, dest2));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB8, index2, base2));
			return 4;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 139;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, dest2));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB8, index2, base2));
		((self_in_CogX64Compiler->machineCode))[4] = 0;
		return 5;

	case MoveRXwrR:
		/* begin concretizeMoveRXwrR */
		src1 = ((self_in_CogX64Compiler->operands))[0];
		index3 = ((self_in_CogX64Compiler->operands))[1];
		base3 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, src1, index3, base3));
		if ((base3 != RBP)
		 && (base3 != R13)) {
			((self_in_CogX64Compiler->machineCode))[1] = 137;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 4, src1));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB8, index3, base3));
			return 4;
		}
		((self_in_CogX64Compiler->machineCode))[1] = 137;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, 4, src1));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB8, index3, base3));
		((self_in_CogX64Compiler->machineCode))[4] = 0;
		return 5;

	case MoveX32rRR:
		return concretizeMoveX32rRR(self_in_CogX64Compiler);

	case MoveRX32rR:
		return concretizeMoveRX32rR(self_in_CogX64Compiler);

	case MoveRMwr:
		/* begin concretizeMoveRMwr */
		srcReg11 = ((self_in_CogX64Compiler->operands))[0];
		offset21 = ((self_in_CogX64Compiler->operands))[1];
		destReg12 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, srcReg11, 0, destReg12));
		((self_in_CogX64Compiler->machineCode))[1] = 137;
		if ((destReg12 != RSP)
		 && (destReg12 != R12)) {
			if ((offset21 == 0)
			 && ((destReg12 != RBP)
			 && (destReg12 != R13))) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg12, srcReg11));
				return 3;
			}
			if (isQuick(self_in_CogX64Compiler, offset21)) {
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg12, srcReg11));
				((self_in_CogX64Compiler->machineCode))[3] = (offset21 & 0xFF);
				return 4;
			}
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg12, srcReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (offset21 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((offset21) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = (((offset21) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = (((offset21) >> 24) & 0xFF);
			return 7;
		}
		if (offset21 == 0) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg12, srcReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg12));
			return 4;
		}
		if (isQuick(self_in_CogX64Compiler, offset21)) {
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg12, srcReg11));
			((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg12));
			((self_in_CogX64Compiler->machineCode))[4] = (offset21 & 0xFF);
			return 5;
		}
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg12, srcReg11));
		((self_in_CogX64Compiler->machineCode))[3] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg12));
		((self_in_CogX64Compiler->machineCode))[4] = (offset21 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((offset21) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((offset21) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((offset21) >> 24) & 0xFF);
		return 8;

	case MoveRM32r:
		/* begin concretizeMoveRM32r */
		srcReg12 = ((self_in_CogX64Compiler->operands))[0];
		offset22 = ((self_in_CogX64Compiler->operands))[1];
		destReg13 = ((self_in_CogX64Compiler->operands))[2];
		if ((srcReg12 <= 7)
		 && (destReg13 <= 7)) {
			skip19 = 0;
		}
		else {
			skip19 = 1;
			((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg12, 0, destReg13));
		}
		((self_in_CogX64Compiler->machineCode))[skip19] = 137;
		if (offset22 == 0) {
			if ((destReg13 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip19 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg13, srcReg12));
				return skip19 + 2;
			}
			if ((destReg13 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip19 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg13, srcReg12));
				((self_in_CogX64Compiler->machineCode))[skip19 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg13));
				return skip19 + 3;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset22)) {
			if ((destReg13 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip19 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg13, srcReg12));
				((self_in_CogX64Compiler->machineCode))[skip19 + 2] = (offset22 & 0xFF);
				return skip19 + 3;
			}
			((self_in_CogX64Compiler->machineCode))[skip19 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg13, srcReg12));
			((self_in_CogX64Compiler->machineCode))[skip19 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg13));
			((self_in_CogX64Compiler->machineCode))[skip19 + 3] = (offset22 & 0xFF);
			return skip19 + 4;
		}
		((self_in_CogX64Compiler->machineCode))[skip19 + 1] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg13, srcReg12));
		if ((destReg13 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip19 + 2] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg13));
			skip19 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip19 + 2] = (offset22 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip19 + 3] = (((offset22) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip19 + 4] = (((offset22) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip19 + 5] = (((offset22) >> 24) & 0xFF);
		return skip19 + 6;

	case MoveRsM32r:
		/* begin concretizeMoveRsM32r */
		srcReg13 = ((self_in_CogX64Compiler->operands))[0];
		offset23 = ((self_in_CogX64Compiler->operands))[1];
		destReg14 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((srcReg13 <= 7)
		 && (destReg14 <= 7)) {
			skip20 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip20 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg13, 0, destReg14));
		}
		((self_in_CogX64Compiler->machineCode))[skip20 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip20 + 2] = 0x7E;
		if (offset23 == 0) {
			if ((destReg14 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip20 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg14, srcReg13));
				return skip20 + 4;
			}
			if ((destReg14 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip20 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg14, srcReg13));
				((self_in_CogX64Compiler->machineCode))[skip20 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg14));
				return skip20 + 5;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset23)) {
			if ((destReg14 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip20 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg14, srcReg13));
				((self_in_CogX64Compiler->machineCode))[skip20 + 4] = (offset23 & 0xFF);
				return skip20 + 5;
			}
			((self_in_CogX64Compiler->machineCode))[skip20 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg14, srcReg13));
			((self_in_CogX64Compiler->machineCode))[skip20 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg14));
			((self_in_CogX64Compiler->machineCode))[skip20 + 5] = (offset23 & 0xFF);
			return skip20 + 6;
		}
		((self_in_CogX64Compiler->machineCode))[skip20 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg14, srcReg13));
		if ((destReg14 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip20 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg14));
			skip20 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip20 + 4] = (offset23 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip20 + 5] = (((offset23) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip20 + 6] = (((offset23) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip20 + 7] = (((offset23) >> 24) & 0xFF);
		return skip20 + 8;

	case MoveRdM64r:
		/* begin concretizeMoveRdM64r */
		srcReg14 = ((self_in_CogX64Compiler->operands))[0];
		offset24 = ((self_in_CogX64Compiler->operands))[1];
		destReg15 = ((self_in_CogX64Compiler->operands))[2];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		if ((srcReg14 <= 7)
		 && (destReg15 <= 7)) {
			skip21 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip21 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg14, 0, destReg15));
		}
		((self_in_CogX64Compiler->machineCode))[skip21 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip21 + 2] = 214;
		if (offset24 == 0) {
			if ((destReg15 & 6) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip21 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg15, srcReg14));
				return skip21 + 4;
			}
			if ((destReg15 & 7) == RSP) {
				/* RBP & R13 fall through */
				((self_in_CogX64Compiler->machineCode))[skip21 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, destReg15, srcReg14));
				((self_in_CogX64Compiler->machineCode))[skip21 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg15));
				return skip21 + 5;
			}
		}
		if (isQuick(self_in_CogX64Compiler, offset24)) {
			if ((destReg15 & 7) != RSP) {
				((self_in_CogX64Compiler->machineCode))[skip21 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg15, srcReg14));
				((self_in_CogX64Compiler->machineCode))[skip21 + 4] = (offset24 & 0xFF);
				return skip21 + 5;
			}
			((self_in_CogX64Compiler->machineCode))[skip21 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp8, destReg15, srcReg14));
			((self_in_CogX64Compiler->machineCode))[skip21 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg15));
			((self_in_CogX64Compiler->machineCode))[skip21 + 5] = (offset24 & 0xFF);
			return skip21 + 6;
		}
		((self_in_CogX64Compiler->machineCode))[skip21 + 3] = (modRMRO(self_in_CogX64Compiler, ModRegRegDisp32, destReg15, srcReg14));
		if ((destReg15 & 7) == RSP) {
			((self_in_CogX64Compiler->machineCode))[skip21 + 4] = (sib(self_in_CogX64Compiler, SIB1, 4, destReg15));
			skip21 += 1;
		}
		((self_in_CogX64Compiler->machineCode))[skip21 + 4] = (offset24 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip21 + 5] = (((offset24) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip21 + 6] = (((offset24) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[skip21 + 7] = (((offset24) >> 24) & 0xFF);
		return skip21 + 8;

	case MoveRdR:
		/* begin concretizeMoveRdR */
		srcReg15 = ((self_in_CogX64Compiler->operands))[0];
		destReg16 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		((self_in_CogX64Compiler->machineCode))[1] = (rexRxb(self_in_CogX64Compiler, srcReg15, 0, destReg16));
		((self_in_CogX64Compiler->machineCode))[2] = 15;
		((self_in_CogX64Compiler->machineCode))[3] = 0x7E;
		((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, destReg16, srcReg15));
		return 5;

	case MoveRRd:
		/* begin concretizeMoveRRd */
		srcReg16 = ((self_in_CogX64Compiler->operands))[0];
		destReg17 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 102;
		((self_in_CogX64Compiler->machineCode))[1] = (rexRxb(self_in_CogX64Compiler, destReg17, 0, srcReg16));
		((self_in_CogX64Compiler->machineCode))[2] = 15;
		((self_in_CogX64Compiler->machineCode))[3] = 110;
		((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg16, destReg17));
		return 5;

	case MoveRdRd:
		/* begin concretizeMoveRdRd */
		srcReg17 = ((self_in_CogX64Compiler->operands))[0];
		destReg18 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((srcReg17 <= 7)
		 && (destReg18 <= 7)) {
			skip22 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip22 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg17, 0, destReg18));
		}
		((self_in_CogX64Compiler->machineCode))[skip22 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip22 + 2] = 17;
		((self_in_CogX64Compiler->machineCode))[skip22 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, destReg18, srcReg17));
		return skip22 + 4;

	case MoveRsRs:
		/* begin concretizeMoveRsRs */
		srcReg18 = ((self_in_CogX64Compiler->operands))[0];
		destReg19 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((srcReg18 <= 7)
		 && (destReg19 <= 7)) {
			skip23 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip23 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, srcReg18, 0, destReg19));
		}
		((self_in_CogX64Compiler->machineCode))[skip23 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip23 + 2] = 17;
		((self_in_CogX64Compiler->machineCode))[skip23 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, destReg19, srcReg18));
		return skip23 + 4;

	case PopR:
		/* begin concretizePopR */
		reg19 = ((self_in_CogX64Compiler->operands))[0];
		if (reg19 < 8) {
			((self_in_CogX64Compiler->machineCode))[0] = (88 + reg19);
			return 1;
		}
		((self_in_CogX64Compiler->machineCode))[0] = 65;
		((self_in_CogX64Compiler->machineCode))[1] = (88 + (reg19 - 8));
		return 2;

	case PushR:
		/* begin concretizePushR */
		reg20 = ((self_in_CogX64Compiler->operands))[0];
		if (reg20 < 8) {
			((self_in_CogX64Compiler->machineCode))[0] = (80 + reg20);
			return 1;
		}
		((self_in_CogX64Compiler->machineCode))[0] = 65;
		((self_in_CogX64Compiler->machineCode))[1] = (80 + (reg20 - 8));
		return 2;

	case PushCq:
		/* begin concretizePushCq */
		value6 = ((self_in_CogX64Compiler->operands))[0];
		if (isQuick(self_in_CogX64Compiler, value6)) {
			((self_in_CogX64Compiler->machineCode))[0] = 106;
			((self_in_CogX64Compiler->machineCode))[1] = (value6 & 0xFF);
			return 2;
		}
		if (is32BitSignedImmediate(self_in_CogX64Compiler, value6)) {
			((self_in_CogX64Compiler->machineCode))[0] = 104;
			((self_in_CogX64Compiler->machineCode))[1] = (value6 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[2] = (((value6) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[3] = (((value6) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = (((value6) >> 24) & 0xFF);
			return 5;
		}
		/* begin concretizePushCw */
		value11 = ((self_in_CogX64Compiler->operands))[0];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) value11)))
		 || ((((AbstractInstruction *) value11)) == (methodLabel()))) {
			value11 = ((((AbstractInstruction *) value11))->address);
		}
		if (		/* begin addressIsInCurrentCompilation: */
			((((usqInt)value11)) >= ((methodLabel->address)))
		 && ((((usqInt)value11)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))) {
			offset28 = value11 - (((self_in_CogX64Compiler->address)) + 7);
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RISCTempReg, 0, 0));
			((self_in_CogX64Compiler->machineCode))[1] = 141;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 5, RISCTempReg));
			((self_in_CogX64Compiler->machineCode))[3] = (offset28 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset28)) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset28)) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = ((((usqInt)(offset28)) >> 24) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = 65;
			((self_in_CogX64Compiler->machineCode))[8] = (72 + RISCTempReg);
			return 9;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RISCTempReg, 0, RISCTempReg));
		((self_in_CogX64Compiler->machineCode))[1] = (184 + (RISCTempReg & 7));
		((self_in_CogX64Compiler->machineCode))[2] = (value11 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((value11) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value11) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value11) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value11) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((value11) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((value11) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((value11) >> 56) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[10] = 65;
		((self_in_CogX64Compiler->machineCode))[11] = (72 + RISCTempReg);
		assert((((self_in_CogX64Compiler->machineCode))[11]) < 144);
		return 12;

	case PushCw:
		/* begin concretizePushCw */
		value5 = ((self_in_CogX64Compiler->operands))[0];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) value5)))
		 || ((((AbstractInstruction *) value5)) == (methodLabel()))) {
			value5 = ((((AbstractInstruction *) value5))->address);
		}
		if (		/* begin addressIsInCurrentCompilation: */
			((((usqInt)value5)) >= ((methodLabel->address)))
		 && ((((usqInt)value5)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))) {
			offset25 = value5 - (((self_in_CogX64Compiler->address)) + 7);
			((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RISCTempReg, 0, 0));
			((self_in_CogX64Compiler->machineCode))[1] = 141;
			((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModRegInd, 5, RISCTempReg));
			((self_in_CogX64Compiler->machineCode))[3] = (offset25 & 0xFF);
			((self_in_CogX64Compiler->machineCode))[4] = ((((usqInt)(offset25)) >> 8) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[5] = ((((usqInt)(offset25)) >> 16) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[6] = ((((usqInt)(offset25)) >> 24) & 0xFF);
			((self_in_CogX64Compiler->machineCode))[7] = 65;
			((self_in_CogX64Compiler->machineCode))[8] = (72 + RISCTempReg);
			return 9;
		}
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RISCTempReg, 0, RISCTempReg));
		((self_in_CogX64Compiler->machineCode))[1] = (184 + (RISCTempReg & 7));
		((self_in_CogX64Compiler->machineCode))[2] = (value5 & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3] = (((value5) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4] = (((value5) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5] = (((value5) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6] = (((value5) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7] = (((value5) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8] = (((value5) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9] = (((value5) >> 56) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[10] = 65;
		((self_in_CogX64Compiler->machineCode))[11] = (72 + RISCTempReg);
		assert((((self_in_CogX64Compiler->machineCode))[11]) < 144);
		return 12;

	case PrefetchAw:
		return concretizePrefetchAw(self_in_CogX64Compiler);

	case ConvertRRd:
		/* begin concretizeConvertRRd */
		srcReg19 = ((self_in_CogX64Compiler->operands))[0];
		destReg20 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		((self_in_CogX64Compiler->machineCode))[1] = (rexRxb(self_in_CogX64Compiler, destReg20, 0, srcReg19));
		((self_in_CogX64Compiler->machineCode))[2] = 15;
		((self_in_CogX64Compiler->machineCode))[3] = 42;
		((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg19, destReg20));
		return 5;

	case ConvertRdR:
		/* begin concretizeConvertRdR */
		srcReg20 = ((self_in_CogX64Compiler->operands))[0];
		destReg21 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		((self_in_CogX64Compiler->machineCode))[1] = (rexRxb(self_in_CogX64Compiler, destReg21, 0, srcReg20));
		((self_in_CogX64Compiler->machineCode))[2] = 15;
		((self_in_CogX64Compiler->machineCode))[3] = 45;
		((self_in_CogX64Compiler->machineCode))[4] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg20, destReg21));
		return 5;

	case ConvertRRs:
		/* begin concretizeConvertRRs */
		srcReg21 = ((self_in_CogX64Compiler->operands))[0];
		destReg22 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((srcReg21 <= 7)
		 && (destReg22 <= 7)) {
			skip24 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip24 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg22, 0, srcReg21));
		}
		((self_in_CogX64Compiler->machineCode))[skip24 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip24 + 2] = 42;
		((self_in_CogX64Compiler->machineCode))[skip24 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg21, destReg22));
		return skip24 + 4;

	case ConvertRsR:
		/* begin concretizeConvertRsR */
		srcReg22 = ((self_in_CogX64Compiler->operands))[0];
		destReg23 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((srcReg22 <= 7)
		 && (destReg23 <= 7)) {
			skip25 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip25 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg23, 0, srcReg22));
		}
		((self_in_CogX64Compiler->machineCode))[skip25 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip25 + 2] = 45;
		((self_in_CogX64Compiler->machineCode))[skip25 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg22, destReg23));
		return skip25 + 4;

	case ConvertRsRd:
		/* begin concretizeConvertRsRd */
		srcReg23 = ((self_in_CogX64Compiler->operands))[0];
		destReg24 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		if ((srcReg23 <= 7)
		 && (destReg24 <= 7)) {
			skip26 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip26 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg24, 0, srcReg23));
		}
		((self_in_CogX64Compiler->machineCode))[skip26 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip26 + 2] = 90;
		((self_in_CogX64Compiler->machineCode))[skip26 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg23, destReg24));
		return skip26 + 4;

	case ConvertRdRs:
		/* begin concretizeConvertRdRs */
		srcReg24 = ((self_in_CogX64Compiler->operands))[0];
		destReg25 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = 242;
		if ((srcReg24 <= 7)
		 && (destReg25 <= 7)) {
			skip27 = 0;
		}
		else {
			((self_in_CogX64Compiler->machineCode))[(skip27 = 1)] = (rexwrxb(self_in_CogX64Compiler, 0, destReg25, 0, srcReg24));
		}
		((self_in_CogX64Compiler->machineCode))[skip27 + 1] = 15;
		((self_in_CogX64Compiler->machineCode))[skip27 + 2] = 90;
		((self_in_CogX64Compiler->machineCode))[skip27 + 3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg24, destReg25));
		return skip27 + 4;

	case SignExtend8RR:
		/* begin concretizeSignExtend8RR */
		srcReg25 = ((self_in_CogX64Compiler->operands))[0];
		destReg26 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, destReg26, 0, srcReg25));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 190;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg25, destReg26));
		return 4;

	case SignExtend16RR:
		/* begin concretizeSignExtend16RR */
		srcReg26 = ((self_in_CogX64Compiler->operands))[0];
		destReg27 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, destReg27, 0, srcReg26));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 191;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg26, destReg27));
		return 4;

	case SignExtend32RR:
		/* begin concretizeSignExtend32RR */
		srcReg27 = ((self_in_CogX64Compiler->operands))[0];
		destReg28 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, destReg28, 0, srcReg27));
		((self_in_CogX64Compiler->machineCode))[1] = 99;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg27, destReg28));
		return 3;

	case ZeroExtend8RR:
		/* begin concretizeZeroExtend8RR */
		srcReg28 = ((self_in_CogX64Compiler->operands))[0];
		destReg29 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, destReg29, 0, srcReg28));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 182;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg28, destReg29));
		return 4;

	case ZeroExtend16RR:
		/* begin concretizeZeroExtend16RR */
		srcReg29 = ((self_in_CogX64Compiler->operands))[0];
		destReg30 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, destReg30, 0, srcReg29));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 183;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg29, destReg30));
		return 4;

	case ZeroExtend32RR:
		/* begin concretizeZeroExtend32RR */
		srcReg30 = ((self_in_CogX64Compiler->operands))[0];
		destReg31 = ((self_in_CogX64Compiler->operands))[1];
		if ((srcReg30 <= 7)
		 && (destReg31 <= 7)) {
			skip28 = 0;
		}
		else {
			skip28 = 1;
			((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 0, destReg31, 0, srcReg30));
		}
		((self_in_CogX64Compiler->machineCode))[skip28] = 139;
		((self_in_CogX64Compiler->machineCode))[skip28 + 1] = (modRMRO(self_in_CogX64Compiler, ModReg, srcReg30, destReg31));
		return skip28 + 2;

	case MovePerfCnt64RL:
		return concretizeMovePerfCnt64RL(self_in_CogX64Compiler);

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	Attempt to generate concrete machine code for the instruction at address.
	This is part of the inner dispatch of concretizeAt: actualAddress which
	exists only
	to get around the number of literals limits in the SqueakV3 (blue book
	derived) bytecode set. */

	/* CogX64Compiler>>#dispatchConcretizeProcessorSpecific */
static NoDbgRegParms sqInt
dispatchConcretizeProcessorSpecific(AbstractInstruction *self_in_CogX64Compiler)
{
    usqInt addressOperand;
    usqInt addressReg;
    usqInt dest;
    usqInt maskReg;
    sqInt offset;
    usqInt reg;
    usqInt reg1;
    usqInt reg2;
    usqInt regDivisor;
    usqInt valueReg;

	switch ((self_in_CogX64Compiler->opcode)) {
	case CPUID:
		/* begin concretizeCPUID */
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = 162;
		return 2;

	case CDQ:
		/* begin concretizeCDQ */
		((self_in_CogX64Compiler->machineCode))[0] = 72;
		((self_in_CogX64Compiler->machineCode))[1] = 153;
		return 2;

	case IDIVR:
		/* begin concretizeIDIVR */
		regDivisor = ((self_in_CogX64Compiler->operands))[0];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, 0, 0, regDivisor));
		((self_in_CogX64Compiler->machineCode))[1] = 247;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, regDivisor, 7));
		return 3;

	case IMULRR:
		/* begin concretizeMulRR */
		reg1 = ((self_in_CogX64Compiler->operands))[0];
		reg2 = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, reg2, 0, reg1));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 175;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, reg1, reg2));
		return 4;

	case XCHGRR:
		return concretizeXCHGRR(self_in_CogX64Compiler);

	case REP:
		/* begin concretizeREP */
		((self_in_CogX64Compiler->machineCode))[0] = 243;
		return 1;

	case CLD:
		/* begin concretizeCLD */
		((self_in_CogX64Compiler->machineCode))[0] = 0xFC;
		return 1;

	case MOVSB:
		/* begin concretizeMOVSB */
		((self_in_CogX64Compiler->machineCode))[0] = 164;
		return 1;

	case MOVSQ:
		/* begin concretizeMOVSQ */
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, 0, 0, 0));
		((self_in_CogX64Compiler->machineCode))[1] = 165;
		return 2;

	case BSR:
		/* begin concretizeBSR */
		maskReg = ((self_in_CogX64Compiler->operands))[0];
		dest = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexwrxb(self_in_CogX64Compiler, 1, dest, 0, maskReg));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 189;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModReg, maskReg, dest));
		return 4;

	case MFENCE:
		/* begin concretizeFENCE: */
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = 174;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, 0, 6));
		return 3;

	case SFENCE:
		/* begin concretizeFENCE: */
		((self_in_CogX64Compiler->machineCode))[0] = 15;
		((self_in_CogX64Compiler->machineCode))[1] = 174;
		((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, 0, 7));
		return 3;

	case LOCK:
		/* begin concretizeLOCK */
		((self_in_CogX64Compiler->machineCode))[0] = 240;
		return 1;

	case MoveRAwNoVBR:
		/* begin concretizeMoveRAwNoVBR */
		reg = ((self_in_CogX64Compiler->operands))[0];
		addressOperand = ((self_in_CogX64Compiler->operands))[1];
		if (		/* begin isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) addressOperand)))
		 || ((((AbstractInstruction *) addressOperand)) == (methodLabel()))) {
			addressOperand = ((((AbstractInstruction *) addressOperand))->address);
		}
		if ((reg == RAX)
		 || ((reg == RBP)
		 || (reg == RSP))) {
			offset = 0;
		}
		else {
			if ((reg == RBP)
			 || (reg == RSP)) {
				((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, reg, 0, RAX));
				((self_in_CogX64Compiler->machineCode))[1] = 137;
				((self_in_CogX64Compiler->machineCode))[2] = (modRMRO(self_in_CogX64Compiler, ModReg, RAX, reg));
				offset = 3;
			}
			else {
				((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, RAX, 0, reg));
				((self_in_CogX64Compiler->machineCode))[1] = (144 + (reg % 8));
				offset = 2;
			}
		}
		((self_in_CogX64Compiler->machineCode))[0 + offset] = 72;
		((self_in_CogX64Compiler->machineCode))[1 + offset] = 163;
		((self_in_CogX64Compiler->machineCode))[2 + offset] = (addressOperand & 0xFF);
		((self_in_CogX64Compiler->machineCode))[3 + offset] = (((addressOperand) >> 8) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[4 + offset] = (((addressOperand) >> 16) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[5 + offset] = (((addressOperand) >> 24) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[6 + offset] = (((addressOperand) >> 32) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[7 + offset] = (((addressOperand) >> 40) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[8 + offset] = (((addressOperand) >> 48) & 0xFF);
		((self_in_CogX64Compiler->machineCode))[9 + offset] = (((addressOperand) >> 56) & 0xFF);
		if (reg == RAX) {
			return 10;
		}
		if ((reg == RBP)
		 || (reg == RSP)) {
			return 13;
		}
		((self_in_CogX64Compiler->machineCode))[12] = (((self_in_CogX64Compiler->machineCode))[0]);
		((self_in_CogX64Compiler->machineCode))[13] = (((self_in_CogX64Compiler->machineCode))[1]);
		return 14;

	case CMPXCHGRMr:
		/* begin concretizeCMPXCHGRMr */
		valueReg = ((self_in_CogX64Compiler->operands))[0];
		addressReg = ((self_in_CogX64Compiler->operands))[1];
		((self_in_CogX64Compiler->machineCode))[0] = (rexRxb(self_in_CogX64Compiler, valueReg, 0, addressReg));
		((self_in_CogX64Compiler->machineCode))[1] = 15;
		((self_in_CogX64Compiler->machineCode))[2] = 177;
		((self_in_CogX64Compiler->machineCode))[3] = (modRMRO(self_in_CogX64Compiler, ModRegInd, addressReg, valueReg));
		return 4;

	case SETE:
		return concretizeSet(self_in_CogX64Compiler, 4);

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}

	/* CogX64Compiler>>#fullCallsAreRelative */
static NoDbgRegParms sqInt
fullCallsAreRelative(AbstractInstruction *self_in_CogX64Compiler)
{
	return 0;
}

	/* CogX64Compiler>>#genDivR:R:Quo:Rem: */
static NoDbgRegParms AbstractInstruction *
genDivRRQuoRem(AbstractInstruction *self_in_CogX64Compiler, sqInt abstractRegDivisor, sqInt abstractRegDividend, sqInt abstractRegQuotient, sqInt abstractRegRemainder)
{
    sqInt rDividend;
    sqInt rDivisor;
    sqInt reg;
    sqInt rQuotient;
    sqInt rRemainder;
    sqInt rUnused;
    sqInt saveRestoreEAX;
    sqInt saveRestoreEDX;
    sqInt saveRestoreExchanged;

	assert(abstractRegDividend != abstractRegDivisor);
	assert(abstractRegQuotient != abstractRegRemainder);
	rDividend = abstractRegDividend;
	rDivisor = abstractRegDivisor;
	rQuotient = abstractRegQuotient;
	/* IDIV r does a signed divide of RDX:RAX by r, RAX := Quotient, RDX := Remainder.
	   Since we must sign extend the dividend into RDX we must substitute another register if RDX is an input. */
	rRemainder = abstractRegRemainder;
	if ((rDividend == RDX)
	 || (rDivisor == RDX)) {
		/* Slang, sigh... */
		rUnused = RAX;
		while (rUnused <= RDI) {
			if ((rUnused != RSP)
			 && ((rUnused != RBP)
			 && ((rUnused != RDX)
			 && ((rUnused != rDividend)
			 && ((rUnused != rDivisor)
			 && ((rUnused != rQuotient)
			 && (rUnused != rRemainder))))))) {
				/* begin PushR: */
				genoperand(PushR, rUnused);
				genoperandoperand(MoveRR, RDX, rUnused);
				if (rDividend == RDX) {
					genDivRRQuoRem(self_in_CogX64Compiler, rDivisor, rUnused, rQuotient, rRemainder);
				}
				else {
					genDivRRQuoRem(self_in_CogX64Compiler, rUnused, rDividend, rQuotient, rRemainder);
				}
				/* begin PopR: */
				genoperand(PopR, rUnused);
				return self_in_CogX64Compiler;
			}
			rUnused += 1;
		}
		error("couldn't find unused register in genDivR:R:Quo:Rem:");
	}
	if ((saveRestoreEAX = (rQuotient != RAX)
	 && (rRemainder != RAX))) {
		/* begin PushR: */
		genoperand(PushR, RAX);
	}
	if ((saveRestoreEDX = (rQuotient != RDX)
	 && (rRemainder != RDX))) {
		/* begin PushR: */
		genoperand(PushR, RDX);
	}
	saveRestoreExchanged = -1;
	if (rDividend != RAX) {
		if (rDivisor == RAX) {
			if (((rDividend != rQuotient)
			 && (rDividend != rRemainder))
			 && ((rDividend != RDX)
			 || (!saveRestoreEDX))) {
				reg = (saveRestoreExchanged = rDividend);
				/* begin PushR: */
				genoperand(PushR, reg);
			}
			genoperandoperand(XCHGRR, rDivisor, rDividend);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, rDividend, RAX);
		}
	}
	gen(CDQ);
	genoperand(IDIVR, (rDivisor == RAX
		? rDividend
		: rDivisor));
	if ((rQuotient == RDX)
	 && (rRemainder == RAX)) {
		genoperandoperand(XCHGRR, rQuotient, rRemainder);
	}
	else {
		if (rQuotient == RDX) {
			if (rRemainder != RDX) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, RDX, rRemainder);
			}
			if (rQuotient != RAX) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, RAX, rQuotient);
			}
		}
		else {
			if (rQuotient != RAX) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, RAX, rQuotient);
			}
			if (rRemainder != RDX) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, RDX, rRemainder);
			}
		}
	}
	if (saveRestoreExchanged >= 0) {
		/* begin PopR: */
		genoperand(PopR, saveRestoreExchanged);
	}
	if (saveRestoreEDX) {
		/* begin PopR: */
		genoperand(PopR, RDX);
	}
	if (saveRestoreEAX) {
		/* begin PopR: */
		genoperand(PopR, RAX);
	}
	return self_in_CogX64Compiler;
}


/*	Get the abstract registers for ECX, EDI and ESI */

	/* CogX64Compiler>>#genMemCopy:to:constantSize: */
static NoDbgRegParms AbstractInstruction *
genMemCopytoconstantSize(AbstractInstruction *self_in_CogX64Compiler, sqInt originalSourceReg, sqInt originalDestReg, sqInt size)
{
    sqInt countReg;
    sqInt destReg;
    AbstractInstruction *inst;
    sqInt numbytes;
    sqInt numwords;
    sqInt sourceReg;

	sourceReg = RSI;
	destReg = RDI;
	/* Put the source in ESI and the dest in EDI */
	countReg = RCX;
	inst = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (originalSourceReg != sourceReg) {
		if (originalDestReg == sourceReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, originalDestReg, TempReg);
		}
		genoperandoperand(MoveRR, originalSourceReg, sourceReg);
	}
	if (originalDestReg != destReg) {
		if (originalDestReg == sourceReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, TempReg, destReg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, originalDestReg, destReg);
		}
	}
	gen(CLD);
	numbytes = size & 7;
	if (numbytes > 0) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, numbytes, countReg);
		gen(REP);
		gen(MOVSB);
	}
	numwords = size / 8;
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, numwords, countReg);
	gen(REP);
	gen(MOVSQ);
	return self_in_CogX64Compiler;
}


/*	Get the abstract registers for ECX, EDI and ESI */

	/* CogX64Compiler>>#genMemCopy:to:size: */
static NoDbgRegParms AbstractInstruction *
genMemCopytosize(AbstractInstruction *self_in_CogX64Compiler, sqInt originalSourceReg, sqInt originalDestReg, sqInt originalSize)
{
    sqInt countReg;
    sqInt destReg;
    AbstractInstruction *inst;
    sqInt size;
    sqInt sourceReg;
    sqInt spilledSize;

	sourceReg = RSI;
	destReg = RDI;
	/* TODO: Avoid spilling */
	countReg = RCX;
	spilledSize = 0;
	if ((originalSize == sourceReg)
	 || (originalSize == destReg)) {
		/* begin PushR: */
		genoperand(PushR, originalSize);
		spilledSize = 1;
	}
	inst = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (originalSourceReg != sourceReg) {
		if (originalDestReg == sourceReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, originalDestReg, TempReg);
		}
		genoperandoperand(MoveRR, originalSourceReg, sourceReg);
	}
	if (originalDestReg != destReg) {
		if (originalDestReg == sourceReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, TempReg, destReg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, originalDestReg, destReg);
		}
	}
	if (spilledSize) {
		/* begin PopR: */
		genoperand(PopR, TempReg);
		size = TempReg;
	}
	else {
		if (originalSize == countReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, originalSize, TempReg);
			size = TempReg;
		}
		else {
			size = originalSize;
		}
	}
	gen(CLD);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, size, countReg);
	genoperandoperand(AndCqR, 7, countReg);
	gen(REP);
	gen(MOVSB);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, size, countReg);
	genoperandoperand(LogicalShiftRightCqR, 3, countReg);
	gen(REP);
	gen(MOVSQ);
	return self_in_CogX64Compiler;
}

	/* CogX64Compiler>>#genMulR:R: */
static NoDbgRegParms AbstractInstruction *
genMulRR(AbstractInstruction *self_in_CogX64Compiler, sqInt regSource, sqInt regDest)
{
	return genoperandoperand(IMULRR, regSource, regDest);
}


/*	Ensure that the register args are pushed before the outer and
	inner retpcs at an entry miss for arity <= self numRegArgs. The
	outer retpc is that of a call at a send site. The inner is the call
	from a method or PIC abort/miss to the trampoline. */
/*	This won't be as clumsy on a RISC. But putting the receiver and
	args above the return address means the CoInterpreter has a
	single machine-code frame format which saves us a lot of work. */
/*	Iff there are register args convert
	base	->	outerRetpc		(send site retpc)
	sp		->	innerRetpc		(PIC abort/miss retpc)
	to
	base	->	receiver
	(arg0)
	(arg1)
	outerRetpc
	sp		->	innerRetpc		(PIC abort/miss retpc) */

	/* CogX64Compiler>>#genPushRegisterArgsForAbortMissNumArgs: */
static NoDbgRegParms AbstractInstruction *
genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction *self_in_CogX64Compiler, sqInt numArgs)
{
	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);
		if (numArgs == 0) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, 0, SPReg, TempReg);
			genoperand(PushR, TempReg);
			genoperandoperandoperand(MoveMwrR, BytesPerWord * 2, SPReg, TempReg);
			genoperandoperandoperand(MoveRMwr, TempReg, BytesPerWord, SPReg);
			genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 2 * BytesPerWord, SPReg);
			return self_in_CogX64Compiler;
		}
		if (numArgs == 1) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, TempReg);
			genoperand(PushR, TempReg);
			genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, TempReg);
			genoperand(PushR, TempReg);
			genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 3 * BytesPerWord, SPReg);
			genoperandoperandoperand(MoveRMwr, Arg0Reg, 2 * BytesPerWord, SPReg);
			return self_in_CogX64Compiler;
		}
		if (numArgs == 2) {
			/* begin PushR: */
			genoperand(PushR, Arg1Reg);
			genoperandoperandoperand(MoveMwrR, BytesPerWord * 2, SPReg, TempReg);
			genoperand(PushR, TempReg);
			genoperandoperandoperand(MoveMwrR, BytesPerWord * 2, SPReg, TempReg);
			genoperand(PushR, TempReg);
			genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 4 * BytesPerWord, SPReg);
			genoperandoperandoperand(MoveRMwr, Arg0Reg, 3 * BytesPerWord, SPReg);
			return self_in_CogX64Compiler;
		}
	}
	return self_in_CogX64Compiler;
}


/*	Ensure that the register args are pushed before the retpc for arity <=
	self numRegArgs. This
	isn't as clumsy on a RISC. But putting the receiver and args above the
	return address
	means the CoInterpreter has a single machine-code frame format which saves
	us a lot of work.
	N.B. Take great care to /not/ smash TempReg, which is used in directed
	send marshalling.
	We could use XCHG to swap the ReceiverResultReg and top-of-stack return
	address, pushing the
	the ret pc (now in ReceiverResultReg) later, but XCHG is very slow. We can
	use SendNumArgsReg
	because it is only live in sends of arity >= (NumSendTrampolines - 1). */

	/* CogX64Compiler>>#genPushRegisterArgsForNumArgs:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction *self_in_CogX64Compiler, sqInt numArgs, sqInt scratchReg)
{
	assert((numRegArgs()) < (NumSendTrampolines - 1));
	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, SPReg, scratchReg);
		genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 0, SPReg);
		if (numArgs > 0) {
			/* begin PushR: */
			genoperand(PushR, Arg0Reg);
			if (numArgs > 1) {
				/* begin PushR: */
				genoperand(PushR, Arg1Reg);
			}
		}
		genoperand(PushR, scratchReg);
	}
	return self_in_CogX64Compiler;
}


/*	This is a no-op on x64 SysV since the ABI passes up to 6 args in registers
	and trampolines currently observe a limit of 4.
	But the WIN64 ABI always reserve shadow space for saving up to 4 parameter
	registers (even if less than 4 args).
 */

	/* CogX64Compiler>>#genRemoveNArgsFromStack: */
static NoDbgRegParms sqInt
genRemoveNArgsFromStack(AbstractInstruction *self_in_CogX64Compiler, sqInt n)
{
	assert(n <= 4);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, 32, RSP);
	return 0;
}


/*	Restore the registers in regMask as saved by genSaveRegs:. */

	/* CogX64Compiler>>#genRestoreRegs: */
static NoDbgRegParms sqInt
genRestoreRegs(AbstractInstruction *self_in_CogX64Compiler, sqInt regMask)
{
    sqInt reg;

	assert(!((((regMask & (registerMaskForand(RSP, RBP))) != 0))));
	for (reg = RAX; reg <= R15; reg += 1) {
		if (((regMask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0)) {
			/* begin PopR: */
			genoperand(PopR, reg);
		}
	}
	return 0;
}


/*	Save the registers in regMask for a call into the C run-time from a
	trampoline. 
 */

	/* CogX64Compiler>>#genSaveRegs: */
static NoDbgRegParms sqInt
genSaveRegs(AbstractInstruction *self_in_CogX64Compiler, sqInt regMask)
{
    sqInt reg;

	assert(((R15 - RAX) + 1) == 16);
	assert(!((((regMask & (registerMaskForand(RSP, RBP))) != 0))));
	for (reg = R15; reg >= RAX; reg += -1) {
		if (((regMask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0)) {
			/* begin PushR: */
			genoperand(PushR, reg);
		}
	}
	return 0;
}

	/* CogX64Compiler>>#genSubstituteReturnAddress: */
static NoDbgRegParms AbstractInstruction *
genSubstituteReturnAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt retpc)
{
	return genoperand(PushCw, retpc);
}

	/* CogX64Compiler>>#genSwapR:R:Scratch: */
static NoDbgRegParms AbstractInstruction *
genSwapRRScratch(AbstractInstruction *self_in_CogX64Compiler, sqInt regA, sqInt regB, sqInt regTmp)
{
	return genoperandoperand(XCHGRR, regA, regB);
}


/*	Answer if the processor has a dedicated callee-saved register to point to
	the base of commonly-accessed variables. */

	/* CogX64Compiler>>#hasVarBaseRegister */
static NoDbgRegParms sqInt
hasVarBaseRegister(AbstractInstruction *self_in_CogX64Compiler)
{
	return 1;
}


/*	Answer the instruction size at pc. This is used in method disassembly
	to decode the jumps in block dispatch to discover where block methods
	occur within a larger method. This is very far from a full decode. */

	/* CogX64Compiler>>#instructionSizeAt: */
static NoDbgRegParms sqInt
instructionSizeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc)
{
    sqInt op;

	op = byteAt(pc);
	if ((op & 0xF8) == 72) {
		return 1 + (instructionSizeAt(self_in_CogX64Compiler, pc + 1));
	}
	switch (op) {
	case 15:
		return twoByteInstructionSizeAt(self_in_CogX64Compiler, pc);

	case 61:
	case 233:
		return 5;

	case 112:
	case 113:
	case 114:
	case 115:
	case 116:
	case 117:
	case 118:
	case 119:
	case 120:
	case 121:
	case 122:
	case 123:
	case 0x7C:
	case 125:
	case 0x7E:
	case 0x7F:
	case 137:
	case 235:
		return 2;

	case 131:
		return sizeImmediateGroup1at(self_in_CogX64Compiler, op, pc);

	case 139:
		return sizeHasModrmat(self_in_CogX64Compiler, op, pc);

	case 144:
	case 204:
		return 1;

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	Top 32 bits all the same as the bottom 32 bits' sign bit implies we can
	use a sign-extended 4 byte offset.
 */

	/* CogX64Compiler>>#is32BitSignedImmediate: */
static NoDbgRegParms int
is32BitSignedImmediate(AbstractInstruction *self_in_CogX64Compiler, sqInt a64BitUnsignedOperand)
{
	return (((int) a64BitUnsignedOperand)) == (((sqLong) a64BitUnsignedOperand));
}


/*	Assuming mcpc is a send return pc answer if the instruction before it is a
	call (not a CallFull).
 */

	/* CogX64Compiler>>#isCallPrecedingReturnPC: */
static NoDbgRegParms int
isCallPrecedingReturnPC(AbstractInstruction *self_in_CogX64Compiler, sqInt mcpc)
{
	return (byteAt(mcpc - 5)) == 232;
}

	/* CogX64Compiler>>#isJumpAt: */
static NoDbgRegParms sqInt
isJumpAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc)
{
    sqInt op;

	op = byteAt(pc);
	return (((op >= 112) && (op <= 0x7F)))
	 || ((op == 233)
	 || ((op == 235)
	 || (((op == 15)
	 && ((((byteAt(pc + 1)) >= 128) && ((byteAt(pc + 1)) <= 143))))
	 || ((op == 72)
	 && (((byteAt(pc + 1)) == 161)
	 && (((byteAt(pc + 10)) == 0xFF)
	 && ((byteAt(pc + 11)) == 224)))))));
}

	/* CogX64Compiler>>#isQuick: */
static NoDbgRegParms int
isQuick(AbstractInstruction *self_in_CogX64Compiler, usqIntptr_t operand)
{
	return (((((sqLong) operand)) >= -128) && ((((sqLong) operand)) <= 0x7F));
}


/*	Answer if an address can be accessed using the offset in a MoveMw:r:R: or
	similar instruction.
	We assume this is true for 32-bit processors and expect 64-bit processors
	to answer false
	for values in the interpreter or the object memory. Restrict our use of
	offsets to reference
	addresses within the method zone, rather than checking for a 32-bit
	offset, so as to keep the
	simulator and real VM in sync. */

	/* CogX64Compiler>>#isWithinMwOffsetRange: */
static NoDbgRegParms sqInt
isWithinMwOffsetRange(AbstractInstruction *self_in_CogX64Compiler, sqInt anAddress)
{
	return 
	/* begin addressIsInCodeZone: */
((((usqInt)anAddress)) >= codeBase)
	 && ((((usqInt)anAddress)) < limitAddress);
}


/*	Set the target of a jump instruction. These all have the target in the
	first operand. */
/*	Set the target of a jump instruction. These all have the target in the
	first operand.
	Override to cope with JumpFPNotEqual where because of IEEE NaN conformance
	and the behaviour of COMISD/UCOMISD we generate two jumps to the same
	target.  */

	/* CogX64Compiler>>#jmpTarget: */
static NoDbgRegParms AbstractInstruction *
jmpTarget(AbstractInstruction *self_in_CogX64Compiler, AbstractInstruction *anAbstractInstruction)
{
    AbstractInstruction *aDependent;

	aDependent = (self_in_CogX64Compiler->dependent);
	while (aDependent) {
		jmpTarget(aDependent, anAbstractInstruction);
		aDependent = (aDependent->dependent);
	}
	((self_in_CogX64Compiler->operands))[0] = (((usqInt)anAbstractInstruction));
	return anAbstractInstruction;
}


/*	Branch/Call ranges. Jump[Cond] can be generated as short as possible.
	Call/Jump[Cond]Long must be generated
	in the same number of bytes irrespective of displacement since their
	targets may be updated, but they need only
	span 16Mb, the maximum size of the code zone. This allows e.g. ARM to use
	single-word call and jump instructions
	for most calls and jumps. CallFull/JumpFull must also be generated in the
	same number of bytes irrespective of
	displacement for the same reason, but they must be able to span the full
	(32-bit or 64-bit) address space because
	they are used to call code in the C runtime, which may be distant from the
	code zone
 */

	/* CogX64Compiler>>#jumpLongByteSize */
static NoDbgRegParms sqInt
jumpLongByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 5;
}

	/* CogX64Compiler>>#jumpLongConditionalByteSize */
static NoDbgRegParms sqInt
jumpLongConditionalByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 6;
}


/*	Answer the target address for the long jump immediately preceding mcpc */

	/* CogX64Compiler>>#jumpLongTargetBeforeFollowingAddress: */
static NoDbgRegParms sqInt
jumpLongTargetBeforeFollowingAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt mcpc)
{
	return callTargetFromReturnAddress(self_in_CogX64Compiler, mcpc);
}

	/* CogX64Compiler>>#jumpTargetPCAt: */
static NoDbgRegParms usqInt
jumpTargetPCAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc)
{
    sqInt byte;
    sqInt offset;
    sqInt size;

	size = instructionSizeAt(self_in_CogX64Compiler, pc);
	if (size == 2) {
		byte = byteAt(pc + 1);
		offset = ((byte & 128) == 0
			? byte
			: byte - 0x100);
	}
	else {
		byte = byteAt((pc + size) - 1);
		offset = ((byte & 128) == 0
			? byte
			: byte - 0x100);
		offset = (((sqInt)((usqInt)(offset) << 8))) + (byteAt((pc + size) - 2));
		offset = (((sqInt)((usqInt)(offset) << 8))) + (byteAt((pc + size) - 3));
		offset = (((sqInt)((usqInt)(offset) << 8))) + (byteAt((pc + size) - 4));
	}
	return (pc + size) + offset;
}


/*	Answer the delta from the stack pointer after a call to the stack pointer
	immediately prior to the call. This is used to compute the stack pointer
	immediately prior to a call from within a leaf routine, which in turn is
	used to capture the c stack pointer to use in trampolines back into the C
	run-time.  */

	/* CogX64Compiler>>#leafCallStackPointerDelta */
static NoDbgRegParms sqInt
leafCallStackPointerDelta(AbstractInstruction *self_in_CogX64Compiler)
{
	return 8;
}


/*	Answer a literal loaded before the inline cache tag load for the return
	address of a send.
 */

	/* CogX64Compiler>>#literalBeforeInlineCacheTagAt: */
static NoDbgRegParms sqInt
literalBeforeInlineCacheTagAt(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress)
{
	return literalBeforeFollowingAddress(self_in_CogX64Compiler, callSiteReturnAddress - 12);
}


/*	Answer the byte size of a MoveCwR opcode's corresponding machine code
	when the argument is a PIC. This is for the self-reference at the end of a
	closed PIC: leaq 0xffffffffffffff2b(%rip), %rcx : 48 8D 0D 2B FF FF FF */

	/* CogX64Compiler>>#loadPICLiteralByteSize */
static NoDbgRegParms sqInt
loadPICLiteralByteSize(AbstractInstruction *self_in_CogX64Compiler)
{
	return 7;
}

	/* CogX64Compiler>>#machineCodeAt: */
static NoDbgRegParms unsigned char
machineCodeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt anOffset)
{
	return ((self_in_CogX64Compiler->machineCode))[anOffset];
}


/*	Answer the maximum number of bytes of machine code generated for any
	abstract instruction.
	e.g. xchg %rdx, %rax; movq $0x12345678ABCDEF0, %(rax); xchg %rdx, %rax =>
	48 92 48 A3 F0 DE BC 9A 78 56 34 12 48 92
 */

	/* CogX64Compiler>>#machineCodeBytes */
static NoDbgRegParms sqInt
machineCodeBytes(AbstractInstruction *self_in_CogX64Compiler)
{
	return 14;
}


/*	See ModR/M byte & opcode syntax
	In addition to the notation shown above in 'Mnemonic Syntax' on page 43,
	the following notation indicates the size and type of operands in the
	syntax of an instruction opcode:
	/digit	Indicates that the ModRM byte specifies only one register or memory
	(r/m) operand.
	The digit is specified by the ModRM reg field and is used as an
	instruction-opcode extension.
	Valid digit values range from 0 to 7.
	/r		Indicates that the ModRM byte specifies both a register operand and a
	reg/mem (register or memory) operand. */

	/* CogX64Compiler>>#mod:RM:RO: */
static NoDbgRegParms sqInt
modRMRO(AbstractInstruction *self_in_CogX64Compiler, sqInt mod, sqInt regMode, sqInt regOpcode)
{
	return ((((sqInt)((usqInt)(mod) << 6))) + (((sqInt)((usqInt)((regOpcode & 7)) << 3)))) + (regMode & 7);
}

	/* CogX64Compiler>>#numIntRegArgs */
static NoDbgRegParms sqInt
numIntRegArgs(AbstractInstruction *self_in_CogX64Compiler)
{
	return 4;
}

	/* CogX64Compiler>>#padIfPossibleWithStopsFrom:to: */
static NoDbgRegParms AbstractInstruction *
padIfPossibleWithStopsFromto(AbstractInstruction *self_in_CogX64Compiler, sqInt startAddr, sqInt endAddr)
{
	stopsFromto(self_in_CogX64Compiler, startAddr, endAddr);
	return self_in_CogX64Compiler;
}


/*	Temporary register used for fetching the instruction pointer. This should
	not be used for passing parameters in a standard ABI */

	/* CogX64Compiler>>#registerToSaveIP */
static NoDbgRegParms sqInt
registerToSaveIP(AbstractInstruction *self_in_CogX64Compiler)
{
	return R15;
}

	/* CogX64Compiler>>#relocateCallBeforeReturnPC:by: */
static NoDbgRegParms AbstractInstruction *
relocateCallBeforeReturnPCby(AbstractInstruction *self_in_CogX64Compiler, sqInt retpc, sqInt delta)
{
    sqInt distance;

	if (delta != 0) {
		distance = (((((sqInt)((usqInt)((byteAt(retpc - 1))) << 24))) + (((sqInt)((usqInt)((byteAt(retpc - 2))) << 16)))) + (((sqInt)((usqInt)((byteAt(retpc - 3))) << 8)))) + (byteAt(retpc - 4));
		distance += delta;
		byteAtput(retpc - 1, (((usqInt)(distance)) >> 24) & 0xFF);
		byteAtput(retpc - 2, (((usqInt)(distance)) >> 16) & 0xFF);
		byteAtput(retpc - 3, (((usqInt)(distance)) >> 8) & 0xFF);
		byteAtput(retpc - 4, distance & 0xFF);
	}
	return self_in_CogX64Compiler;
}


/*	We generate the method address using pc-relative addressing.
	Simply check that rip-relative addressing is being used. c.f.
	concretizeMoveCwR */

	/* CogX64Compiler>>#relocateMethodReferenceBeforeAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateMethodReferenceBeforeAddressby(AbstractInstruction *self_in_CogX64Compiler, sqInt pc, sqInt delta)
{
	assert((((byteAt(pc - 6)) == 141)
	 && (((byteAt(pc - 5)) | (modRMRO(self_in_CogX64Compiler, 0, 0, 7))) == (modRMRO(self_in_CogX64Compiler, ModRegInd, 5, 7))))
	 || (((byteAt(pc - 8)) == 141)
	 && (((byteAt(pc - 7)) | (modRMRO(self_in_CogX64Compiler, 0, 0, 7))) == (modRMRO(self_in_CogX64Compiler, ModRegInd, 5, 7)))));
	return self_in_CogX64Compiler;
}


/*	Rewrite a call instruction to call a different target. This variant is
	used to link PICs
	in ceSendMiss et al, and to rewrite cached primitive calls. Answer the
	extent of
	the code change which is used to compute the range of the icache to flush. */
/*	self cCode: ''
	inSmalltalk: [cogit disassembleFrom: callSiteReturnAddress - 10 to:
	callSiteReturnAddress - 1]. */

	/* CogX64Compiler>>#rewriteCallAt:target: */
static NoDbgRegParms sqInt
rewriteCallAttarget(AbstractInstruction *self_in_CogX64Compiler, usqInt callSiteReturnAddress, usqInt callTargetAddress)
{
    usqInt callDistance;

	callDistance = ((usqInt) (int)(callTargetAddress - callSiteReturnAddress));
	byteAtput(callSiteReturnAddress - 1, ((callDistance) >> 24) & 0xFF);
	byteAtput(callSiteReturnAddress - 2, ((callDistance) >> 16) & 0xFF);
	byteAtput(callSiteReturnAddress - 3, ((callDistance) >> 8) & 0xFF);
	byteAtput(callSiteReturnAddress - 4, callDistance & 0xFF);
	assert((callTargetFromReturnAddress(self_in_CogX64Compiler, callSiteReturnAddress)) == callTargetAddress);
	return 5;
}


/*	Rewrite a CallFull instruction to call a different target. This variant is
	used to rewrite cached primitive calls.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush.
	On x64 this is a rewrite of
	movq #64bits, %rax : 48 A1 b0 b1 b2 b3 b4 b5 b6 b7
	jmp %rax : FF E0 */

	/* CogX64Compiler>>#rewriteCallFullAt:target: */
static NoDbgRegParms sqInt
rewriteCallFullAttarget(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	assert((byteAt(callSiteReturnAddress - 12)) == 72);
	unalignedLongAtput(callSiteReturnAddress - 10, callTargetAddress);
	assert((((usqInt) (callFullTargetFromReturnAddress(self_in_CogX64Compiler, callSiteReturnAddress)))) == callTargetAddress);
	return 12;
}


/*	Rewrite the short jump instruction to jump to a new cpic case target. */
/*	prevent type inference for avoiding warning on abs */

	/* CogX64Compiler>>#rewriteCPICJumpAt:target: */
static NoDbgRegParms AbstractInstruction *
rewriteCPICJumpAttarget(AbstractInstruction *self_in_CogX64Compiler, usqInt addressFollowingJump, usqInt jumpTargetAddr)
{
    sqInt callDistance;

	callDistance = jumpTargetAddr - addressFollowingJump;
	assert((SQABS(callDistance)) < 128);
	byteAtput(addressFollowingJump - 1, callDistance & 0xFF);
	return self_in_CogX64Compiler;
}


/*	Rewrite an inline cache to call a different target for a new tag. This
	variant is used
	to link unlinked sends in ceSend:to:numArgs: et al. Answer the extent of
	the code
	change which is used to compute the range of the icache to flush.
	N.B. On 64-bit platforms the inline cache tag is only 32-bits wide, hence
	this code
	is identical to that for the IA32. */
/*	self cCode: ''
	inSmalltalk: [cogit disassembleFrom: callSiteReturnAddress - 12 to:
	callSiteReturnAddress - 1]. */

	/* CogX64Compiler>>#rewriteInlineCacheAt:tag:target: */
static NoDbgRegParms sqInt
rewriteInlineCacheAttagtarget(AbstractInstruction *self_in_CogX64Compiler, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress)
{
    usqInt callDistance;

	callDistance = ((usqInt) (int)(callTargetAddress - callSiteReturnAddress));
	byteAtput(callSiteReturnAddress - 1, ((callDistance) >> 24) & 0xFF);
	byteAtput(callSiteReturnAddress - 2, ((callDistance) >> 16) & 0xFF);
	byteAtput(callSiteReturnAddress - 3, ((callDistance) >> 8) & 0xFF);
	byteAtput(callSiteReturnAddress - 4, callDistance & 0xFF);
	byteAtput(callSiteReturnAddress - 6, (((usqInt)(cacheTag)) >> 24) & 0xFF);
	byteAtput(callSiteReturnAddress - 7, (((usqInt)(cacheTag)) >> 16) & 0xFF);
	byteAtput(callSiteReturnAddress - 8, (((usqInt)(cacheTag)) >> 8) & 0xFF);
	byteAtput(callSiteReturnAddress - 9, cacheTag & 0xFF);
	assert((callTargetFromReturnAddress(self_in_CogX64Compiler, callSiteReturnAddress)) == callTargetAddress);
	return 12;
}


/*	Rewrite an inline cache with a new tag. This variant is used
	by the garbage collector. */

	/* CogX64Compiler>>#rewriteInlineCacheTag:at: */
static NoDbgRegParms AbstractInstruction *
rewriteInlineCacheTagat(AbstractInstruction *self_in_CogX64Compiler, sqInt cacheTag, sqInt callSiteReturnAddress)
{
	unalignedLong32Atput(self_in_CogX64Compiler, callSiteReturnAddress - 9, cacheTag);
	return self_in_CogX64Compiler;
}


/*	Rewrite a JumpFull instruction to jump to a different target. This variant
	is used to rewrite cached primitive calls.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush.
	On x64 this is a rewrite of
	movq #64bits, %rax : 48 A1 b0 b1 b2 b3 b4 b5 b6 b7
	jmp %rax : FF E0 */

	/* CogX64Compiler>>#rewriteJumpFullAt:target: */
static NoDbgRegParms sqInt
rewriteJumpFullAttarget(AbstractInstruction *self_in_CogX64Compiler, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteCallFullAttarget(self_in_CogX64Compiler, callSiteReturnAddress, callTargetAddress);
}


/*	<0-15> */
/*	<0-15> */
/*	<0-15> */

	/* CogX64Compiler>>#rexR:x:b: */
static NoDbgRegParms sqInt
rexRxb(AbstractInstruction *self_in_CogX64Compiler, sqInt reg, sqInt sibReg, sqInt fieldReg)
{
	return rexwrxb(self_in_CogX64Compiler, 1, reg, sibReg, fieldReg);
}


/*	<Boolean> */
/*	<0-15> */
/*	<0-15> */
/*	<0-15> */
/*	Given width64, the R register, sib register, and modrm/sib/reg field
	register, answer the correctly encoded REX prefix byte.
	See AMD64 Architecture Programmer's Manual Volume 3: General-Purpose and
	System Instructions, Table 1-11 */

	/* CogX64Compiler>>#rexw:r:x:b: */
static NoDbgRegParms sqInt
rexwrxb(AbstractInstruction *self_in_CogX64Compiler, sqInt width64, sqInt reg, sqInt sibReg, sqInt fieldReg)
{
    sqInt regBits;

	regBits = ((((usqInt)((reg & 8))) >> 1) + (((usqInt)((sibReg & 8))) >> 2)) + (((usqInt)(fieldReg)) >> 3);
	return ((width64
	? 72
	: 64)) + regBits;
}


/*	to save Slang from having to be a real compiler (it can't inline switches
	that return)
 */
/*	Answer if the receiver's opcode sets the condition codes correctly for the
	given conditional jump opcode.
 */

	/* CogX64Compiler>>#setsConditionCodesFor: */
static NoDbgRegParms sqInt
setsConditionCodesFor(AbstractInstruction *self_in_CogX64Compiler, sqInt aConditionalJumpOpcode)
{
	switch ((self_in_CogX64Compiler->opcode)) {
	case ArithmeticShiftRightCqR:
	case ArithmeticShiftRightRR:
	case LogicalShiftLeftCqR:
	case LogicalShiftLeftRR:
		return ((aConditionalJumpOpcode >= JumpZero) && (aConditionalJumpOpcode <= JumpNonNegative));

	case LogicalShiftRightCqR:
		return 0;

	case XorRR:
		return 1;

	case ClzRR:
		return (aConditionalJumpOpcode == JumpZero)
		 || ((aConditionalJumpOpcode == JumpNonZero)
		 || ((aConditionalJumpOpcode == JumpCarry)
		 || (aConditionalJumpOpcode == JumpNoCarry)));

	default:
		haltmsg("unhandled opcode in setsConditionCodesFor:");
		return 0;
	}
	return 0;
}

	/* CogX64Compiler>>#sizeHasModrm:at: */
static NoDbgRegParms int
sizeHasModrmat(AbstractInstruction *self_in_CogX64Compiler, sqInt op, sqInt pc)
{
    sqInt mod;
    sqInt modrm;
    sqInt rm;
    sqInt ro;

	modrm = byteAt(pc + 1);
	mod = ((usqInt)(modrm)) >> 6;
	ro = (((usqInt)(modrm)) >> 3) & 7;
	rm = modrm & 7;
	if (mod == 3) {
		return 2;
	}
	if (rm != 4) {
		/* no SIB byte */
		switch (mod) {
		case 0:
			return (rm == 5
				? 6
				: 3);

		case 1:
			return 3;

		case 2:
			return 6;

		default:
			error("Case not found and no otherwise clause");
		}
	}
	haltmsg("fall through in sizeHasModrm:at:");
	return 0;
}


/*	see [1] p A-7, p A-13 */

	/* CogX64Compiler>>#sizeImmediateGroup1:at: */
static NoDbgRegParms int
sizeImmediateGroup1at(AbstractInstruction *self_in_CogX64Compiler, sqInt op, sqInt pc)
{
    sqInt mod;
    sqInt modrm;
    sqInt rm;
    sqInt ro;

	modrm = byteAt(pc + 1);
	mod = ((usqInt)(modrm)) >> 6;
	ro = (((usqInt)(modrm)) >> 3) & 7;
	rm = modrm & 7;
	switch (ro) {
	case 7:
		return (op == 129
			? 6
			: 3);

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}

	/* CogX64Compiler>>#stopsFrom:to: */
static NoDbgRegParms AbstractInstruction *
stopsFromto(AbstractInstruction *self_in_CogX64Compiler, sqInt startAddr, sqInt endAddr)
{
	memset(((void *)startAddr), 204 /* begin stop */, (endAddr - startAddr) + 1);
	return self_in_CogX64Compiler;
}


/*	Rewrite the 32-bit literal in the instruction immediately preceding
	followingAddress. 
 */

	/* CogX64Compiler>>#storeLiteral32:beforeFollowingAddress: */
static NoDbgRegParms AbstractInstruction *
storeLiteral32beforeFollowingAddress(AbstractInstruction *self_in_CogX64Compiler, sqInt literal, sqInt followingAddress)
{
	byteAtput(followingAddress - 1, (((usqInt)(literal)) >> 24) & 0xFF);
	byteAtput(followingAddress - 2, (((usqInt)(literal)) >> 16) & 0xFF);
	byteAtput(followingAddress - 3, (((usqInt)(literal)) >> 8) & 0xFF);
	byteAtput(followingAddress - 4, literal & 0xFF);
	return self_in_CogX64Compiler;
}

	/* CogX64Compiler>>#s:i:b: */
static NoDbgRegParms sqInt
sib(AbstractInstruction *self_in_CogX64Compiler, sqInt scale, sqInt indexReg, sqInt baseReg)
{
	return ((((sqInt)((usqInt)(scale) << 6))) + (((sqInt)((usqInt)((indexReg & 7)) << 3)))) + (baseReg & 7);
}

	/* CogX64Compiler>>#twoByteInstructionSizeAt: */
static NoDbgRegParms int
twoByteInstructionSizeAt(AbstractInstruction *self_in_CogX64Compiler, sqInt pc)
{
    sqInt op;

	op = byteAt(pc + 1);
	switch (op & 240) {
	case 128:
		/* long conditional jumps */
		return 6;

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	Answer if Call and JumpLong are relative and hence need to take the
	caller's relocation delta into account during code compaction, rather than
	just the
	callee's delta. */

	/* CogX64Compiler>>#zoneCallsAreRelative */
static NoDbgRegParms sqInt
zoneCallsAreRelative(AbstractInstruction *self_in_CogX64Compiler)
{
	return 1;
}

	/* SimpleStackBasedCogit>>#cogMethodHasExternalPrim: */
sqInt
cogMethodHasExternalPrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (primIndex == PrimNumberExternalCall)
	 || (primIndex == PrimNumberFFICall);
}

	/* SimpleStackBasedCogit>>#cogMethodHasMachineCodePrim: */
sqInt
cogMethodHasMachineCodePrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (((primIndex >= 1) && (primIndex <= MaxCompiledPrimitiveIndex)))
	 && ((((primitiveGeneratorTable[primIndex]).primitiveGenerator)));
}


/*	Compile the jump instruction(s) at the end of the method that dispatch to
	each block body.
 */

	/* SimpleStackBasedCogit>>#compileBlockDispatch */
static sqInt
compileBlockDispatch(void)
{
    AbstractInstruction *jumpSkip;

	assert(blockCount > 0);
	/* Now generate a binary search through start pcs to jump to blocks. 1 block is a special case. */
	blockEntryNoContextSwitch = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	/* Set OK to context switch flag to non-zero. */
	jumpSkip = genoperand(Jump, ((sqInt)0));
	blockEntryLabel = genoperandoperand(MoveRR, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jumpSkip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (blockCount > 1) {
		genLoadSlotsourceRegdestReg(ClosureStartPCIndex, ReceiverResultReg, TempReg);
	}
	compileBlockDispatchFromto(0, blockCount - 1);
	return 0;
}


/*	After pushing the temporaries but before the stack limit check a primitive
	method needs to fetch the error code, if any. If the primitive has failed,
	call the trampoline
	that will assign it to the last temp. */

	/* SimpleStackBasedCogit>>#compileGetErrorCode */
static void
compileGetErrorCode(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *jmpNoError;


	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, TempReg);
	jmpNoError = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	addDependent(methodLabel, annotateAbsolutePCRef(genoperandoperand(MoveCwR, ((sqInt)methodLabel), ClassReg)));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceReapAndResetErrorCodeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(jmpNoError, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* SimpleStackBasedCogit>>#compileInterpreterPrimitive */
static sqInt
compileInterpreterPrimitive(void)
{
    sqInt flags;
    void (*primitiveRoutine)(void);

	flags = 0;
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, (&flags));
	if (((flags & PrimCallOnSmalltalkStack) != 0)) {
		return compileOnStackExternalPrimitiveflags(primitiveRoutine, flags);
	}
	return compileInterpreterPrimitiveflags(primitiveRoutine, flags);
}


/*	Compile a call to an interpreter primitive. Call the C routine with the
	usual stack-switching dance, test the primFailCode and then either
	return on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compileInterpreterPrimitive:flags: */
static NoDbgRegParms sqInt
compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags)
{
    AbstractInstruction *continueAfterProfileSample;
    AbstractInstruction *jmp;
    AbstractInstruction *jump;
    AbstractInstruction *jumpToTakeSample;
    sqInt liveRegisterMask;
    sqInt reg;
    AbstractInstruction *skip;

	jumpToTakeSample = ((AbstractInstruction *) 0);
	assert(!((registerisInMask(VarBaseReg, ABICallerSavedRegisterMask))));
	genExternalizePointersForPrimitiveCall();
	/* begin genLoadCStackPointersForPrimCall */
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	if (recordPrimTraceForMethod(methodObj)) {
		genFastPrimTraceUsingand(ClassReg, SendNumArgsReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, TempReg);
	genoperandoperand(MoveRAw, TempReg, primFailCodeAddress());
	if (methodOrBlockNumArgs != 0) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, methodOrBlockNumArgs, TempReg);
	}
	genoperandoperand(MoveRAw, TempReg, argumentCountAddress());
	if (((flags & PrimCallNeedsNewMethod) != 0)) {
		genLoadNewMethod();
	}
	/* begin checkLiteral:forInstruction: */
	genoperand(PrefetchAw, primFailCodeAddress());
	if (((flags & PrimCallMayEndureCodeCompaction) != 0)) {
		/* The ceActivateFailingPrimitiveMethod: machinery can't handle framelessness. */
		needsFrame = 1;
		/* begin genMarshallNArgs:arg:arg:arg:arg: */
		genoperandoperand(SubCqR, 32, RSP);
		assert(0 <= 4);
		/* begin checkLiteral:forInstruction: */
		genoperand(PushCw, (((flags & PrimCallCollectsProfileSamples) != 0)
			? cePrimReturnEnterCogCodeProfiling
			: cePrimReturnEnterCogCode));
		/* begin checkLiteral:forInstruction: */
		genoperand(JumpFull, ((sqInt)(((sqInt)primitiveRoutine))));
		return 0;
	}
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(0 <= 4);
	/* begin checkLiteral:forInstruction: */
	genoperand(CallFull, ((sqInt)primitiveRoutine));
	assert(0 <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	maybeCompileRetryOfonPrimitiveFailflags(primitiveRoutine, primitiveIndex, flags);
	genLoadStackPointersForPrimCall(backEnd, ClassReg);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, instructionPointerAddress(), ClassReg);
	genoperandoperandoperand(MoveRMwr, ClassReg, 0, SPReg);
	genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, TempReg);
	/* placing the test here attributes the tick to the primitive plus any checkForAndFollowForwardedPrimitiveState
	   scanning, but attributes all of a failing primitive to the current method (in ceStackOverflow: on frame build). */
	jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		liveRegisterMask = (0);
		/* begin genCheckForProfileTimerTick: */
				reg = RAX;
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, nextProfileTickAddress(), Arg1Reg);
		genoperandoperand(CmpCqR, 0, Arg1Reg);
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpToTakeSample = jump;
	}
	continueAfterProfileSample = genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, ReceiverResultReg);
	/* begin RetN: */
	genoperand(RetN, BytesPerWord);
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		jmpTarget(jumpToTakeSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genTakeProfileSample();
		genLoadStackPointerForPrimCall(backEnd, ClassReg);
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, instructionPointerAddress(), ClassReg);
		genoperandoperandoperand(MoveRMwr, ClassReg, 0, SPReg);
		genoperand(Jump, ((sqInt)continueAfterProfileSample));
	}
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BytesPerWord * (methodOrBlockNumArgs + 1), SPReg, ReceiverResultReg);
	return 0;
}


/*	Compile a fast call of a C primitive using the current stack page,
	avoiding the stack switch except on failure.
	This convention still uses stackPointer and argumentCount to access
	operands. Push all operands to the stack,
	assign stackPointer, argumentCount, and zero primFailCode. Make the call
	(saving a LinkReg if required).
	Test for failure and return. On failure on Spur, if there is an accessor
	depth, assign framePointer and newMethod,
	do the stack switch, call checkForAndFollowForwardedPrimitiveState, and
	loop back if forwarders are found.
	Fall through to frame build. */

	/* SimpleStackBasedCogit>>#compileOnStackExternalPrimitive:flags: */
static NoDbgRegParms sqInt
compileOnStackExternalPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt calleeSavedRegisterMask;
    AbstractInstruction *continueAfterProfileSample;
    AbstractInstruction *jmpFail;
    AbstractInstruction *jump;
    AbstractInstruction *jumpToTakeSample;
    sqInt liveRegisterMask;
    sqInt reg;
    sqInt retpcOffset;
    AbstractInstruction *retry;
    AbstractInstruction *skip;
    AbstractInstruction *skip1;
    sqInt spRegSaveRegister;

	jumpToTakeSample = ((AbstractInstruction *) 0);
	assert(((flags & PrimCallOnSmalltalkStack) != 0));
	assert(!((registerisInMask(VarBaseReg, ABICallerSavedRegisterMask))));
	if (recordFastCCallPrimTraceForMethod(methodObj)) {
		genFastPrimTraceUsingand(ClassReg, SendNumArgsReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, TempReg);
	genoperandoperand(MoveRAw, TempReg, primFailCodeAddress());
	if (methodOrBlockNumArgs != 0) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, methodOrBlockNumArgs, TempReg);
	}
	genoperandoperand(MoveRAw, TempReg, argumentCountAddress());
	genExternalizeStackPointerForFastPrimitiveCall();
	calleeSavedRegisterMask = ((ABICalleeSavedRegisterMask | ((1U << ClassReg))) - ((1U << ClassReg)));
	spRegSaveRegister = NoReg;
	retry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (((flags & PrimCallOnSmalltalkStackAlign2x) != 0)) {
		gAndCqRR((-1 - (((BytesPerWord * 2) - 1))), SPReg, NativeSPReg);
	}
	else {
	}
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(0 <= 4);
	if (((!(flags & PrimCallIsExternalCall)))
	 && (is32BitSignedImmediate(backEnd, maximumDistanceFromCodeZone(((sqInt)primitiveRoutine))))) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ((sqInt)primitiveRoutine));
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin checkLiteral:forInstruction: */
		genoperand(CallFull, ((sqInt)primitiveRoutine));
	}
	assert(0 <= 4);
	genoperandoperand(AddCqR, 32, RSP);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
	if (spRegSaveRegister != NoReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, spRegSaveRegister, SPReg);
	}
	genoperandoperand(CmpCqR, 0, TempReg);
	/* Remember to restore the native stack pointer to point to the C stack,
	   otherwise the Smalltalk frames will get overwritten on an interrupt. */
	jmpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		liveRegisterMask = (0);
		/* begin genCheckForProfileTimerTick: */
				reg = RAX;
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, nextProfileTickAddress(), Arg1Reg);
		genoperandoperand(CmpCqR, 0, Arg1Reg);
		skip1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpToTakeSample = jump;
	}
	/* get result and restore retpc */
	continueAfterProfileSample = genoperandoperand(MoveAwR, stackPointerAddress(), TempReg);
	retpcOffset = -((methodOrBlockNumArgs + 1) * BytesPerWord);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, retpcOffset, TempReg, ClassReg);
	genoperandoperand(MoveRR, TempReg, SPReg);
	genoperandoperandoperand(MoveMwrR, 0, TempReg, ReceiverResultReg);
	genoperandoperandoperand(MoveRMwr, ClassReg, 0, TempReg);
	genoperand(RetN, 0);
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		jmpTarget(jumpToTakeSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genTakeProfileSample();
		/* begin Jump: */
		genoperand(Jump, ((sqInt)continueAfterProfileSample));
	}
	jmpTarget(jmpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if ((accessorDepthForPrimitiveMethod(methodObj)) >= 0) {
		/* Given that following primitive state to the accessor depth is recursive, we're asking for
		   trouble if we run the fixup on the Smalltalk stack page.  Run it on the full C stack instead.
		   This won't be a performance issue since primitive failure should be very rare. */
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveRAw, FPReg, framePointerAddress());
		genoperandoperand(MoveCwR, ((sqInt)primitiveRoutine), TempReg);
		genoperandoperand(MoveRAw, TempReg, primitiveFunctionPointerAddress());
		genLoadNewMethod();
		/* begin genLoadCStackPointersForPrimCall */
		if (cFramePointerInUse) {
			genLoadCStackPointers(backEnd);
		}
		else {
			genLoadCStackPointer(backEnd);
		}
		/* begin genMarshallNArgs:arg:arg:arg:arg: */
		genoperandoperand(SubCqR, 32, RSP);
		assert(0 <= 4);
		if (is32BitSignedImmediate(backEnd, maximumDistanceFromCodeZone(((usqIntptr_t)checkForAndFollowForwardedPrimitiveState)))) {
			/* begin CallRT: */
			abstractInstruction1 = genoperand(Call, ((usqIntptr_t)checkForAndFollowForwardedPrimitiveState));
			(abstractInstruction1->annotation = IsRelativeCall);
		}
		else {
			/* begin checkLiteral:forInstruction: */
			genoperand(CallFull, ((usqIntptr_t)checkForAndFollowForwardedPrimitiveState));
		}
		genLoadStackPointersForPrimCall(backEnd, ClassReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, ABIResultReg);
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, TempReg);
		genoperandoperand(MoveRAw, TempReg, primFailCodeAddress());
		genoperand(Jump, ((sqInt)retry));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	else {
		/* must reload SPReg to undo any alignment change, */
		if (((flags & PrimCallOnSmalltalkStackAlign2x) != 0)) {
			/* begin checkLiteral:forInstruction: */
			genoperandoperand(MoveAwR, stackPointerAddress(), TempReg);
			genoperandoperand(SubCqR, BytesPerWord, TempReg);
			genoperandoperand(MoveRR, TempReg, SPReg);
		}
	}
	if (((ABICallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, (methodOrBlockNumArgs + 1) * BytesPerWord, SPReg, ReceiverResultReg);
	}
	return 0;
}


/*	Compile one method cache probe in an OpenPIC's lookup of selector.
	Answer the jump taken if the selector probe fails.
	The class tag of the receiver must be in SendNumArgsReg. ClassReg and
	TempReg are used as scratch registers.
	On a hit, the offset of the entry is in ClassReg. */

	/* SimpleStackBasedCogit>>#compileOpenPICMethodCacheProbeFor:withShift:baseRegOrNone: */
static NoDbgRegParms AbstractInstruction *
compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt offset1;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);
	annotateobjRef(genoperandoperand(XorCwR, selector, ClassReg), selector);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	genoperandoperand(AndCqR, ((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
	}
	else {
		/* begin AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);
		genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
	}
	annotateobjRef(genoperandoperand(CmpCwR, selector, TempReg), selector);
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset1 = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
	}
	else {
		/* begin MoveMw:r:R: */
		genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
	}
	assert(!((SendNumArgsReg == SPReg)));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile the code for an open PIC. Perform a probe of the first-level
	method lookup cache followed by a call of ceSendFromInLineCacheMiss: if
	the probe fails. */

	/* SimpleStackBasedCogit>>#compileOpenPIC:numArgs: */
static NoDbgRegParms void
compileOpenPICnumArgs(sqInt selector, sqInt numArgs)
{
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt quickConstant;
    sqInt reg;

	/* begin preenMethodLabel */
	((methodLabel->operands))[1] = 0;
	compilePICAbort(numArgs);
	entry = genGetClassTagOfintoscratchReg(ReceiverResultReg, SendNumArgsReg, TempReg);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	if (!(		/* begin addressIsInCodeZone: */
			((((usqInt)(methodCacheAddress()))) >= codeBase)
		 && ((((usqInt)(methodCacheAddress()))) < limitAddress))) {
		/* begin MoveCq:R: */
		quickConstant = methodCacheAddress();
		reg = (cacheBaseReg = Extra0Reg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, quickConstant, reg);
	}
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 0, cacheBaseReg);
	/* Fetch the method.  The interpret trampoline requires the bytecoded method in SendNumArgsReg */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin MoveMw:r:R: */
	itsAHit = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	jmpTarget(jumpBCMethod, picInterpretAbort);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genSmalltalkToCStackSwitch(1);
	addDependent(methodLabel, annotateAbsolutePCRef(genoperandoperand(MoveCwR, ((sqInt)methodLabel), SendNumArgsReg)));
	compileCallFornumArgsargargargargresultRegregsToSave(ceSendFromInLineCacheMiss, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* begin emptyRegisterMask */);
}


/*	Compile one method cache probe in a perform: primitive's lookup of
	selector. Answer the jump taken if the selector probe fails. */

	/* SimpleStackBasedCogit>>#compilePerformMethodCacheProbeFor:withShift:baseRegOrNone: */
static NoDbgRegParms AbstractInstruction *
compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt offset1;


	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);
	/* begin XorR:R: */
	genoperandoperand(XorRR, selectorReg, ClassReg);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	genoperandoperand(AndCqR, ((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
	}
	else {
		/* begin AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);
		genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
	}
	assert(!((selectorReg == SPReg)));
	genoperandoperand(CmpRR, selectorReg, TempReg);
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset1 = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
	}
	else {
		/* begin MoveMw:r:R: */
		genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
	}
	assert(!((SendNumArgsReg == SPReg)));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile a primitive. If possible, performance-critical primitives will
	be generated by their own routines (primitiveGenerator). Otherwise,
	if there is a primitive at all, we call the C routine with the usual
	stack-switching dance, test the primFailCode and then either return
	on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compilePrimitive */
static sqInt
compilePrimitive(void)
{
    sqInt code;
    sqInt flags;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpNotSmallInteger;
    sqInt opcodeIndexAtPrimitive;
    PrimitiveDescriptor *primitiveDescriptor;
    void (*primitiveRoutine)(void);
    sqInt reg;

	flags = 0;
	if (primitiveIndex == 0) {
		return 0;
	}
	if ((((primitiveDescriptor = primitiveGeneratorOrNil())))
	 && ((((primitiveDescriptor->primitiveGenerator)))
	 && ((((primitiveDescriptor->primNumArgs)) < 0)
	 || (((primitiveDescriptor->primNumArgs)) == methodOrBlockNumArgs)))) {
		/* Note opcodeIndex so that any arg load instructions
		   for unimplemented primitives can be discarded. */
		opcodeIndexAtPrimitive = opcodeIndex;
		code = ((primitiveDescriptor->primitiveGenerator))();
		if ((code < 0)
		 && (code != UnimplementedPrimitive)) {
			/* Generator failed, so no point continuing... */
			return code;
		}
		if (code == UnfailingPrimitive) {
			return 0;
		}
		if ((code == CompletePrimitive)
		 && (!(		/* begin methodUsesPrimitiveErrorCode:header: */
			((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
		 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))))) {
			return 0;
		}
		if (code == UnimplementedPrimitive) {
			opcodeIndex = opcodeIndexAtPrimitive;
		}
	}
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, (&flags));
	if ((primitiveRoutine == 0)
	 || (primitiveRoutine == (((void (*)(void)) primitiveFail)))) {
		return genFastPrimFail();
	}
	if ((primitiveRoutine == (((void (*)(void)) primitiveHighResClock)))
	 && (methodOrBlockNumArgs == 0)) {
		/* begin genPrimitiveHighResClock64 */
		reg = RAX;
		assert(registerisNotInMask(reg, registerMaskForandandand(ReceiverResultReg, Arg1Reg, Extra0Reg, Extra1Reg)));
		gMovePerfCnt64RL(reg, (0));
		gLogicalShiftRightCqRR((numSmallIntegerBits()) - 1, reg, Arg1Reg);
		if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, 0, Arg1Reg);
		}
		jumpNotSmallInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		genConvertIntegerInRegtoSmallIntegerInReg(reg, ReceiverResultReg);
		/* begin genPrimReturn */
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(reg, ReceiverResultReg, Extra0Reg, Extra1Reg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
		jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		code = 0;
		if (code != UnimplementedPrimitive) {
			return code;
		}
	}
	if (((flags & PrimCallOnSmalltalkStack) != 0)) {
		return compileOnStackExternalPrimitiveflags(primitiveRoutine, flags);
	}
	return compileInterpreterPrimitiveflags(primitiveRoutine, flags);
}

	/* SimpleStackBasedCogit>>#extendedPushBytecode */
static sqInt
extendedPushBytecode(void)
{
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genPushReceiverVariable(variableIndex);
	}
	if (variableType == 1) {
		return genPushTemporaryVariable(variableIndex);
	}
	if (variableType == 2) {
		return genPushLiteralIndex(variableIndex);
	}
	return genPushLiteralVariable(variableIndex);
}

	/* SimpleStackBasedCogit>>#extendedStoreAndPopBytecode */
static sqInt
extendedStoreAndPopBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, 
		/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(1, variableIndex);
#    if IMMUTABILITY
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY

		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, 
		/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#extendedStoreBytecode */
static sqInt
extendedStoreBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, 
		/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(0, variableIndex);
#    if IMMUTABILITY
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY

		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, 
		/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#frameOffsetOfNativeFrameMark */
static int
frameOffsetOfNativeFrameMark(void)
{
	return FoxMFReceiver - BytesPerWord;
}

	/* SimpleStackBasedCogit>>#frameOffsetOfNativeFramePointer */
static int
frameOffsetOfNativeFramePointer(void)
{
	return FoxMFReceiver - (BytesPerWord * 3);
}

	/* SimpleStackBasedCogit>>#frameOffsetOfNativeStackPointer */
static int
frameOffsetOfNativeStackPointer(void)
{
	return FoxMFReceiver - (BytesPerWord * 4);
}

	/* SimpleStackBasedCogit>>#frameOffsetOfPreviousNativeStackPointer */
static int
frameOffsetOfPreviousNativeStackPointer(void)
{
	return FoxMFReceiver - (BytesPerWord * 2);
}

	/* SimpleStackBasedCogit>>#frameOffsetOfTemporary: */
static NoDbgRegParms sqInt
frameOffsetOfTemporary(sqInt index)
{
	return 
	/* begin frameOffsetOfTemporary:numArgs: */
(index < methodOrBlockNumArgs
		? FoxCallerSavedIP + ((methodOrBlockNumArgs - index) * BytesPerWord)
		: (FoxMFReceiver - BytesPerWord) + ((methodOrBlockNumArgs - index) * BytesPerWord));
}


/*	Implemented with SistaCogit only */

	/* SimpleStackBasedCogit>>#genCallMappedInlinedPrimitive */
static int
genCallMappedInlinedPrimitive(void)
{
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#genDoubleFailIfZeroArgRcvr:arg: */
static NoDbgRegParms AbstractInstruction *
genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, TempReg);
	genoperandoperand(ConvertRRd, TempReg, DPFPReg2);
	genoperandoperand(CmpRdRd, DPFPReg2, argReg);
	return gJumpFPEqual(0);
}


/*	Can use any of the first 32 literals for the selector and pass up to 7
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genExtendedSendBytecode */
static sqInt
genExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}

	/* SimpleStackBasedCogit>>#genExtendedSuperBytecode */
static sqInt
genExtendedSuperBytecode(void)
{
	return genSendSupernumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}


/*	244		11110100	i i i i i i i i	Pop and Jump 0n False i i i i i i i i (+
	Extend B * 256, where Extend B >= 0)
 */

	/* SimpleStackBasedCogit>>#genExtJumpIfFalse */
static sqInt
genExtJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongForwardBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}


/*	243		11110011	i i i i i i i i	Pop and Jump 0n True i i i i i i i i (+
	Extend B * 256, where Extend B >= 0)
 */

	/* SimpleStackBasedCogit>>#genExtJumpIfTrue */
static sqInt
genExtJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongForwardBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}


/*	NewspeakV4: 221		11011101		Nop */
/*	SistaV1:		 91		01011011'		Nop */

	/* SimpleStackBasedCogit>>#genExtNopBytecode */
static sqInt
genExtNopBytecode(void)
{
	extA = (numExtB = (extB = 0));
	return 0;
}


/*	SistaV1:		233		11101001	iiiiiiii		Push Character #iiiiiiii (+ Extend A *
	256) 
 */

	/* SimpleStackBasedCogit>>#genExtPushCharacterBytecode */
static sqInt
genExtPushCharacterBytecode(void)
{
    sqInt literal;
    sqInt value;

	value = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	literal = characterObjectOf(value);
	/* begin genPushLiteral: */
	return ssPushConstant(literal);
}


/*	NewsqueakV4:	229		11100101	iiiiiiii	Push Integer #iiiiiiii (+ Extend B *
	256, where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
	SistaV1:		232		11101000	iiiiiiii	Push Integer #iiiiiiii (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#genExtPushIntegerBytecode */
static sqInt
genExtPushIntegerBytecode(void)
{
    sqInt value;

	value = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	extB = 0;
	numExtB = 0;
	return ssPushConstant((((usqInt)value << 3) | 1));
}


/*	228		11100100	i i i i i i i i	Push Literal #iiiiiiii (+ Extend A * 256) */

	/* SimpleStackBasedCogit>>#genExtPushLiteralBytecode */
static sqInt
genExtPushLiteralBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genPushLiteralIndex(index);
}


/*	227		11100011	i i i i i i i i	Push Literal Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtPushLiteralVariableBytecode */
static sqInt
genExtPushLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genPushLiteralVariable(index);
}


/*	SistaV1: *	82			01010010			Push thisContext, (then Extend B = 1 => push
	thisProcess) 
 */

	/* SimpleStackBasedCogit>>#genExtPushPseudoVariable */
static sqInt
genExtPushPseudoVariable(void)
{
    sqInt ext;

	ext = extB;
	extB = 0;
	numExtB = 0;
	switch (ext) {
	case 0:
		return genPushActiveContextBytecode();

	default:
		return EncounteredUnknownBytecode;
	}
	return 0;
}


/*	226		11100010	i i i i i i i i	Push Receiver Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtPushReceiverVariableBytecode */
static sqInt
genExtPushReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isReadMediatedContextInstVarIndex(index))
		? genPushMaybeContextReceiverVariable(index)
		: genPushReceiverVariable(index));
}


/*	238		11101110	i i i i i j j j	Send Literal Selector #iiiii (+ Extend A *
	32) with jjj (+ Extend B * 8) Arguments
 */

	/* SimpleStackBasedCogit>>#genExtSendBytecode */
static sqInt
genExtSendBytecode(void)
{
    sqInt litIndex;
    sqInt nArgs;

	litIndex = (((usqInt)(byte1)) >> 3) + (((sqInt)((usqInt)(extA) << 5)));
	extA = 0;
	nArgs = (byte1 & 7) + (((sqInt)((usqInt)(extB) << 3)));
	extB = 0;
	numExtB = 0;
	return genSendnumArgs(litIndex, nArgs);
}


/*	239		11101111	i i i i i j j j	Send To Superclass Literal Selector #iiiii
	(+ Extend A * 32) with jjj (+ Extend B * 8) Arguments
 */

	/* SimpleStackBasedCogit>>#genExtSendSuperBytecode */
static sqInt
genExtSendSuperBytecode(void)
{
    int isDirected;
    sqInt litIndex;
    sqInt nArgs;

	if ((isDirected = extB >= 64)) {
		extB = extB & 0x3F;
	}
	litIndex = (((usqInt)(byte1)) >> 3) + (((sqInt)((usqInt)(extA) << 5)));
	extA = 0;
	nArgs = (byte1 & 7) + (((sqInt)((usqInt)(extB) << 3)));
	extB = 0;
	numExtB = 0;
	return (isDirected
		? genSendDirectedSupernumArgs(litIndex, nArgs)
		: genSendSupernumArgs(litIndex, nArgs));
}


/*	236		11101100	i i i i i i i i	Pop and Store Literal Variable #iiiiiiii (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreAndPopLiteralVariableBytecode */
static sqInt
genExtStoreAndPopLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(1, index, 
	/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(((ssTop())->constant)))
	 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
	 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
}


/*	235		11101011	i i i i i i i i	Pop and Store Receiver Variable #iiiiiii (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreAndPopReceiverVariableBytecode */
static sqInt
genExtStoreAndPopReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isWriteMediatedContextInstVarIndex(index))
		? genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, index, 
			/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1)
		: genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, index, 
			/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1));
}


/*	233		11101001	i i i i i i i i	Store Literal Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreLiteralVariableBytecode */
static sqInt
genExtStoreLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, index, 
	/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(((ssTop())->constant)))
	 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
	 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
}


/*	232		11101000	i i i i i i i i	Store Receiver Variable #iiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreReceiverVariableBytecode */
static sqInt
genExtStoreReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isWriteMediatedContextInstVarIndex(index))
		? genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, index, 
			/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1)
		: genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, index, 
			/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#genExtUnconditionalJump */
static sqInt
genExtUnconditionalJump(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	if (distance < 0) {
		return genJumpBackTo(target);
	}
	genJumpTo(target);
	abstractInstruction = lastOpcode();
	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* SimpleStackBasedCogit>>#genFastPrimFail */
static sqInt
genFastPrimFail(void)
{
	primitiveIndex = 0;
	return UnfailingPrimitive;
}


/*	Support for compileInterpreterPrimitive. Generate inline code
	so as to record the primitive trace as fast as possible. */

	/* SimpleStackBasedCogit>>#genFastPrimTraceUsing:and: */
static NoDbgRegParms void
genFastPrimTraceUsingand(sqInt r1, sqInt r2)
{
    sqInt offset;


	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, 0, r2);
	genoperandoperand(MoveAbR, primTraceLogIndexAddress(), r2);
	genoperandoperandoperand(LoadEffectiveAddressMwrR, 1, r2, r1);
	genoperandoperand(MoveRAb, r1, primTraceLogIndexAddress());
	addDependent(methodLabel, annotateAbsolutePCRef(genoperandoperand(MoveCwR, ((sqInt)methodLabel), r1)));
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, r1, TempReg);
	genoperandoperand(MoveCwR, ((sqInt)(primTraceLogAddress())), r1);
	genoperandoperandoperand(MoveRXwrR, TempReg, r2, r1);
}

	/* SimpleStackBasedCogit>>#genLoadNewMethod */
static void
genLoadNewMethod(void)
{
    sqInt offset;

	addDependent(methodLabel, annotateAbsolutePCRef(genoperandoperand(MoveCwR, ((sqInt)methodLabel), ClassReg)));
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
	genoperandoperand(MoveRAw, TempReg, newMethodAddress());
#  if LRPCheck
	if (checkingLongRunningPrimitives()) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, TempReg);
		genoperandoperand(MoveRAw, TempReg, longRunningPrimitiveStopUsecsAddress());
	}
#  endif // LRPCheck
}

	/* SimpleStackBasedCogit>>#genLongJumpIfFalse */
static sqInt
genLongJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genLongJumpIfTrue */
static sqInt
genLongJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}


/*	230		11100110	i i i i i i i i	Push Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongPushTemporaryVariableBytecode */
static sqInt
genLongPushTemporaryVariableBytecode(void)
{
	return genPushTemporaryVariable(byte1);
}


/*	237		11101101	i i i i i i i i	Pop and Store Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongStoreAndPopTemporaryVariableBytecode */
static sqInt
genLongStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte1);
}


/*	234		11101010	i i i i i i i i	Store Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongStoreTemporaryVariableBytecode */
static sqInt
genLongStoreTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(0, byte1);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalBackwardJump */
static sqInt
genLongUnconditionalBackwardJump(void)
{
    sqInt distance;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance < 0);
	return genJumpBackTo((distance + 2) + bytecodePC);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalForwardJump */
static sqInt
genLongUnconditionalForwardJump(void)
{
    sqInt distance;
    sqInt targetpc;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance >= 0);
	targetpc = (distance + 2) + bytecodePC;
	return genJumpTo(targetpc);
}


/*	Compile the code for a probe of the first-level method cache for a perform
	primitive. The selector is assumed to be in Arg0Reg. Defer to
	adjustArgumentsForPerform: to
	adjust the arguments before the jump to the method. */
/*	N.B. Can't assume TempReg already contains the tag because a method can
	of course be invoked via the unchecked entry-point, e.g. as does perform:. */

	/* SimpleStackBasedCogit>>#genLookupForPerformNumArgs: */
static NoDbgRegParms sqInt
genLookupForPerformNumArgs(sqInt numArgs)
{
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBadNumArgs;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpInterpret;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt quickConstant;
    sqInt reg;

	genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, SendNumArgsReg, 0);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	if (!(		/* begin addressIsInCodeZone: */
			((((usqInt)(methodCacheAddress()))) >= codeBase)
		 && ((((usqInt)(methodCacheAddress()))) < limitAddress))) {
		/* begin MoveCq:R: */
		quickConstant = methodCacheAddress();
		reg = (cacheBaseReg = Extra0Reg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, quickConstant, reg);
	}
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 0, cacheBaseReg);
	/* Fetch the method, and check if it is cogged. */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin MoveMw:r:R: */
	itsAHit = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	/* check the argument count; if it's wrong fall back on the interpreter primitive. */
	jumpInterpret = genJumpImmediate(ClassReg);
	/* begin genLoadcmNumArgsOf:into: */
	genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	genoperandoperandoperand(MoveMbrR, BytesPerWord, ClassReg, SendNumArgsReg);
	genoperandoperand(CmpCqR, numArgs, SendNumArgsReg);
	/* Adjust arguments and jump to the method's unchecked entry-point. */
	jumpBadNumArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	adjustArgumentsForPerform(numArgs);
	/* begin JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpInterpret, jmpTarget(jumpBadNumArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return 0;
}

	/* SimpleStackBasedCogit>>#genMustBeBooleanTrampolineFor:called: */
static NoDbgRegParms usqInt
genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName)
{
	zeroOpcodeIndex();
	assert(!(shouldAnnotateObjectReference(boolean)));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, boolean, TempReg);
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSendMustBeBoolean, trampolineName, 1, TempReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, NoReg, 1);
}


/*	Implement 28-bit hashMultiply for SmallInteger and LargePositiveInteger
	receivers. 
 */

	/* SimpleStackBasedCogit>>#genPrimitiveHashMultiply */
static int
genPrimitiveHashMultiply(void)
{
    AbstractInstruction *jmpFailImm;
    AbstractInstruction *jmpFailNotPositiveLargeInt;

	if (mclassIsSmallInteger()) {
		genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
		genMulRR(backEnd, TempReg, ReceiverResultReg);
		genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
		genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
		/* begin RetN: */
		genoperand(RetN, 0);
		return CompletePrimitive;
	}
	jmpFailImm = genJumpImmediate(ReceiverResultReg);
	genGetCompactClassIndexNonImmOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, ClassLargePositiveIntegerCompactIndex, ClassReg);
	jmpFailNotPositiveLargeInt = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, ReceiverResultReg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
	genMulRR(backEnd, TempReg, ReceiverResultReg);
	genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jmpFailImm, jmpTarget(jmpFailNotPositiveLargeInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Generate the substitute return code for an external or FFI primitive call.
	On success simply return, extracting numArgs from newMethod.
	On primitive failure call ceActivateFailingPrimitiveMethod: newMethod. */

	/* SimpleStackBasedCogit>>#genPrimReturnEnterCogCodeEnilopmart: */
static NoDbgRegParms void
genPrimReturnEnterCogCodeEnilopmart(sqInt profiling)
{
    AbstractInstruction *continuePostSample;
    AbstractInstruction *jmpFail;
    AbstractInstruction *jmpSample;
    AbstractInstruction *jump;
    sqInt liveRegisterMask;
    sqInt reg;
    sqInt regOrConst0;
    AbstractInstruction *skip;

	continuePostSample = ((AbstractInstruction *) 0);
	jmpSample = ((AbstractInstruction *) 0);
	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg);
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, TempReg);
	jmpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (profiling) {
		liveRegisterMask = (0);
		/* begin genCheckForProfileTimerTick: */
				reg = RAX;
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, nextProfileTickAddress(), Arg1Reg);
		genoperandoperand(CmpCqR, 0, Arg1Reg);
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jmpSample = jump;
		continuePostSample = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, instructionPointerAddress(), ClassReg);
	genLoadStackPointers(backEnd);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, 0, SPReg, ReceiverResultReg);
	genoperandoperandoperand(MoveRMwr, ClassReg, 0, SPReg);
	genoperand(RetN, 0);
	jmpTarget(jmpFail, genoperandoperand(MoveAwR, newMethodAddress(), SendNumArgsReg));
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cStackPointerAddress(), SPReg);
	compileCallFornumArgsargargargargresultRegregsToSave(ceActivateFailingPrimitiveMethod, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* begin emptyRegisterMask */);
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, instructionPointerAddress(), ClassReg);
	genLoadStackPointers(backEnd);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, ReceiverResultReg);
	genoperand(PushR, ClassReg);
	genoperand(RetN, BytesPerWord);
	if (profiling) {
		/* Call ceTakeProfileSample: to record sample and then continue.  newMethod
		   should be up-to-date.  Need to save and restore the link reg around this call. */
		jmpTarget(jmpSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin genMarshallNArgs:arg:arg:arg:arg: */
		/* begin trampolineArgConstant: */
		assert(null >= 0);
		regOrConst0 = -2 - null;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, 32, RSP);
		assert(1 <= 4);
				if (regOrConst0 < NoReg) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		}
		else {
			if (regOrConst0 != CArg0Reg) {
				/* begin MoveR:R: */
				genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
			}
		}
		/* begin checkLiteral:forInstruction: */
		genoperand(CallFull, ((usqInt)ceTakeProfileSample));
		assert(1 <= 4);
		genoperandoperand(AddCqR, 32, RSP);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)continuePostSample));
	}
}

	/* SimpleStackBasedCogit>>#genPushConstantFalseBytecode */
static sqInt
genPushConstantFalseBytecode(void)
{
	return ssPushConstant(falseObject());
}

	/* SimpleStackBasedCogit>>#genPushConstantNilBytecode */
static sqInt
genPushConstantNilBytecode(void)
{
	return ssPushConstant(nilObject());
}


/*	79			01001111		Push 1 */

	/* SimpleStackBasedCogit>>#genPushConstantOneBytecode */
static sqInt
genPushConstantOneBytecode(void)
{
	return ssPushConstant((((usqInt)1 << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushConstantTrueBytecode */
static sqInt
genPushConstantTrueBytecode(void)
{
	return ssPushConstant(trueObject());
}


/*	78			01001110		Push 0 */

	/* SimpleStackBasedCogit>>#genPushConstantZeroBytecode */
static sqInt
genPushConstantZeroBytecode(void)
{
	return ssPushConstant((((usqInt)0 << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushLiteralConstantBytecode */
static sqInt
genPushLiteralConstantBytecode(void)
{
	return genPushLiteralIndex(byte0 & 0x1F);
}


/*	16-31		0001 i i i i		Push Literal Variable #iiii */

	/* SimpleStackBasedCogit>>#genPushLiteralVariable16CasesBytecode */
static sqInt
genPushLiteralVariable16CasesBytecode(void)
{
	return genPushLiteralVariable(byte0 & 15);
}

	/* SimpleStackBasedCogit>>#genPushLiteralVariableBytecode */
static sqInt
genPushLiteralVariableBytecode(void)
{
	return genPushLiteralVariable(byte0 & 0x1F);
}

	/* SimpleStackBasedCogit>>#genPushQuickIntegerConstantBytecode */
static sqInt
genPushQuickIntegerConstantBytecode(void)
{
	return ssPushConstant((((usqInt)(byte0 - 117) << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushReceiverVariableBytecode */
static sqInt
genPushReceiverVariableBytecode(void)
{
	return genPushReceiverVariable(byte0 & 15);
}

	/* SimpleStackBasedCogit>>#genPushTemporaryVariableBytecode */
static sqInt
genPushTemporaryVariableBytecode(void)
{
	return genPushTemporaryVariable(byte0 & 15);
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnConst */
sqInt
genQuickReturnConst(void)
{
    sqInt constant;

	constant = quickPrimitiveConstantFor(primitiveIndex);
	/* begin genMoveConstant:R: */
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(constant))
	 && ((oopisGreaterThan(constant, classTableRootObj()))
	 || (oopisLessThan(constant, nilObject())))) {
		annotateobjRef(genoperandoperand(MoveCwR, constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, constant, ReceiverResultReg);
	}
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnInstVar */
sqInt
genQuickReturnInstVar(void)
{
    sqInt index;

	index = quickPrimitiveInstVarIndexFor(primitiveIndex);
	genLoadSlotsourceRegdestReg(index, ReceiverResultReg, ReceiverResultReg);
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnSelf */
sqInt
genQuickReturnSelf(void)
{
	genUpArrowReturn();
	return UnfailingPrimitive;
}

	/* SimpleStackBasedCogit>>#genReturnFalse */
static sqInt
genReturnFalse(void)
{

	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnNil */
static sqInt
genReturnNil(void)
{

	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, nilObject(), ReceiverResultReg);
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnNilFromBlock */
static sqInt
genReturnNilFromBlock(void)
{
	assert(inBlock > 0);
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, nilObject(), ReceiverResultReg);
	return genBlockReturn();
}

	/* SimpleStackBasedCogit>>#genReturnTrue */
static sqInt
genReturnTrue(void)
{

	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg);
	return genUpArrowReturn();
}


/*	Can use any of the first 64 literals for the selector and pass up to 3
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genSecondExtendedSendBytecode */
static sqInt
genSecondExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x3F, ((usqInt)(byte1)) >> 6);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector0ArgsBytecode */
static sqInt
genSendLiteralSelector0ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 0);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector1ArgBytecode */
static sqInt
genSendLiteralSelector1ArgBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 1);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector2ArgsBytecode */
static sqInt
genSendLiteralSelector2ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 2);
}

	/* SimpleStackBasedCogit>>#genShortJumpIfFalse */
static sqInt
genShortJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genShortJumpIfTrue */
static sqInt
genShortJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}

	/* SimpleStackBasedCogit>>#genShortUnconditionalJump */
static sqInt
genShortUnconditionalJump(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpTo(target);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorEqualsEquals */
static sqInt
genSpecialSelectorEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(0);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorNotEqualsEquals */
static sqInt
genSpecialSelectorNotEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(1);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorSend */
static sqInt
genSpecialSelectorSend(void)
{
    sqInt index;
    sqInt numArgs;

	index = byte0 - (/* begin firstSpecialSelectorBytecodeOffset */
	(bytecodeSetOffset == 0x100
	? AltFirstSpecialSelector + 0x100
	: FirstSpecialSelector));
	numArgs = specialSelectorNumArgs(index);
	return genSendnumArgs((-index) - 1, numArgs);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopReceiverVariableBytecode */
static sqInt
genStoreAndPopReceiverVariableBytecode(void)
{
	return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, byte0 & 7, 
	/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(((ssTop())->constant)))
	 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
	 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopRemoteTempLongBytecode */
static sqInt
genStoreAndPopRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(1, byte1, byte2, 
	/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(((ssTop())->constant)))
	 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
	 || (oopisLessThan(((ssTop())->constant), nilObject()))))));
}

	/* SimpleStackBasedCogit>>#genStoreAndPopTemporaryVariableBytecode */
static sqInt
genStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte0 & 7);
}

	/* SimpleStackBasedCogit>>#genStoreRemoteTempLongBytecode */
static sqInt
genStoreRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(0, byte1, byte2, 
	/* begin ssTopNeedsStoreCheck */
((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(((ssTop())->constant)))
	 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
	 || (oopisLessThan(((ssTop())->constant), nilObject()))))));
}

	/* SimpleStackBasedCogit>>#genTakeProfileSample */
static void
genTakeProfileSample(void)
{
    AbstractInstruction *abstractInstruction;

	addDependent(methodLabel, annotateAbsolutePCRef(genoperandoperand(MoveCwR, ((sqInt)methodLabel), ClassReg)));
	/* begin genMarshallNArgs:arg:arg:arg:arg: */
	genoperandoperand(SubCqR, 32, RSP);
	assert(1 <= 4);
		genoperandoperand(MoveRR, ClassReg, CArg0Reg);;
	if (is32BitSignedImmediate(backEnd, maximumDistanceFromCodeZone(((sqInt)ceTakeProfileSample)))) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ((usqIntptr_t)ceTakeProfileSample));
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin checkLiteral:forInstruction: */
		genoperand(CallFull, ((usqIntptr_t)ceTakeProfileSample));
	}
	assert(1 <= 4);
	genoperandoperand(AddCqR, 32, RSP);
}


/*	SistaV1: *	217		Trap */

	/* SimpleStackBasedCogit>>#genUnconditionalTrapBytecode */
static int
genUnconditionalTrapBytecode(void)
{
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#loadNativeArgumentAddress:to: */
static NoDbgRegParms void
loadNativeArgumentAddressto(sqInt baseOffset, sqInt reg)
{
    sqInt offset;

	/* begin MoveMw:r:R: */
	offset = frameOffsetOfPreviousNativeStackPointer();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, reg);
	genoperandoperand(AddCqR, baseOffset - 1, reg);
}

	/* SimpleStackBasedCogit>>#loadNativeFramePointerInto: */
static NoDbgRegParms void
loadNativeFramePointerInto(sqInt reg)
{
    sqInt offset;

	/* begin MoveMw:r:R: */
	offset = frameOffsetOfNativeFramePointer();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, reg);
}

	/* SimpleStackBasedCogit>>#loadNativeLocalAddress:to: */
static NoDbgRegParms void
loadNativeLocalAddressto(sqInt baseOffset, sqInt reg)
{
    sqInt offset;

	/* begin MoveMw:r:R: */
	offset = frameOffsetOfNativeFramePointer();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, reg);
	genoperandoperand(AddCqR, baseOffset - 1, reg);
}


/*	Collect the branch and send data for cogMethod, storing it into arrayObj. */

	/* SimpleStackBasedCogit>>#mapPCDataFor:into: */
sqInt
mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    usqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;

	introspectionDataIndex = 0;
	introspectionData = arrayObj;
	if (((cogMethod->stackCheckOffset)) == 0) {
		assert(introspectionDataIndex == 0);
		if ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock)) {
			storePointerUncheckedofObjectwithValue(0, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(1, introspectionData, (((usqInt)cbNoSwitchEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(3, introspectionData, (((usqInt)cbEntryOffset << 3) | 1));
		}
		else {
			storePointerUncheckedofObjectwithValue(0, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(1, introspectionData, (((usqInt)cmEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 3) | 1));
		}
		return 4;
	}
	startbcpc = startPCOfMethod((cogMethod->methodObject));
	/* begin mapFor:bcpc:performUntil:arg: */
	latestContinuation = 0;
	assert((((((CogBlockMethod *) cogMethod))->stackCheckOffset)) > 0);
	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)(((CogBlockMethod *) cogMethod)))) + (((((CogBlockMethod *) cogMethod))->stackCheckOffset));
	result = pcDataForAnnotationMcpcBcpcMethod(null, 0 + (((sqInt)((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)cogMethod));
	if (result != 0) {
		errCode = result;
		goto l2;
	}
	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if ((((((CogBlockMethod *) cogMethod))->cmType)) >= CMMethod) {
		isInBlock = ((((CogBlockMethod *) cogMethod))->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) (((CogBlockMethod *) cogMethod)));
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == (((((CogBlockMethod *) cogMethod))->startpc)));
		homeMethod = cmHomeMethod(((CogBlockMethod *) cogMethod));
		map = findMapLocationForMcpcinMethod((((usqInt)(((CogBlockMethod *) cogMethod)))) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}
		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - (/* begin blockCreationBytecodeSizeForHeader: */
	(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
	? AltBlockCreationBytecodeSize
	: BlockCreationBytecodeSize));
		bsOffset = 
		/* begin bytecodeSetOffsetForHeader: */
(headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
			? 0x100
			: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask);
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l2;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l2;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((				/* begin isBackwardBranch:at:exts:in: */
					assert(((descriptor->spanFunction))),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = pcDataForAnnotationMcpcBcpcMethod(descriptor, (isBackwardBranch
					? (((sqInt)((usqInt)(annotation) << 1))) + 1
					: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
					? bcpc - (2 * nExts)
					: bcpc), ((void *)cogMethod));
				if (result != 0) {
					errCode = result;
					goto l2;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift)));
			}
		}
		map -= 1;
	}
	errCode = 0;
	l2:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode != 0) {
		assert(errCode == PrimErrNoMemory);
		return -1;
	}
	if (((cogMethod->blockEntryOffset)) != 0) {
		errCode = blockDispatchTargetsForperformarg(cogMethod, pcDataForBlockEntryMethod, ((sqInt)cogMethod));
		if (errCode != 0) {
			assert(errCode == PrimErrNoMemory);
			return -1;
		}
	}
	return introspectionDataIndex;
}

	/* SimpleStackBasedCogit>>#numSpecialSelectors */
static sqInt
numSpecialSelectors(void)
{
	return (bytecodeSetOffset == 0x100
			? AltNumSpecialSelectors
			: NumSpecialSelectors);
}


/*	Collect the branch and send data for the block method starting at
	blockEntryMcpc, storing it into picData.
 */

	/* SimpleStackBasedCogit>>#pcDataForBlockEntry:Method: */
static NoDbgRegParms usqInt
pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod)
{
	storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)(blockEntryMcpc - blockNoContextSwitchOffset) << 3) | 1));
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)blockEntryMcpc << 3) | 1));
	introspectionDataIndex += 4;
	return 0;
}

	/* SimpleStackBasedCogit>>#pcDataFor:Annotation:Mcpc:Bcpc:Method: */
static NoDbgRegParms sqInt
pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt actualBcpc;
    sqInt actualMcpc;

	if (!descriptor) {
		/* this is the stackCheck offset */
		assert(introspectionDataIndex == 0);
		if (((((CogMethod *) cogMethodArg))->cpicHasMNUCaseOrCMIsFullBlock)) {
			storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)cbNoSwitchEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)cbEntryOffset << 3) | 1));
		}
		else {
			storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)cmEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 3) | 1));
		}
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 4, introspectionData, (((usqInt)(bcpc + 1) << 3) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 5, introspectionData, (((((((CogMethod *) cogMethodArg))->stackCheckOffset)) << 3) | 1));
		introspectionDataIndex += 6;
		return 0;
	}
	if ((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= HasBytecodePC) {
		actualBcpc = (((isBackwardBranchAndAnnotation & 1) != 0)
			? bcpc + 1
			: (bcpc + ((descriptor->numBytes))) + 1);
		actualMcpc = (((usqInt)mcpc)) - (((usqInt)cogMethodArg));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, (((usqInt)actualBcpc << 3) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)actualMcpc << 3) | 1));
		introspectionDataIndex += 2;
	}
	return 0;
}


/*	If there is a generator for the current primitive then answer it;
	otherwise answer nil. */

	/* SimpleStackBasedCogit>>#primitiveGeneratorOrNil */
static PrimitiveDescriptor *
primitiveGeneratorOrNil(void)
{
    PrimitiveDescriptor *primitiveDescriptor;

	if (isQuickPrimitiveIndex(primitiveIndex)) {
		/* an unused one */
		primitiveDescriptor = (&(primitiveGeneratorTable[0]));
		(primitiveDescriptor->primitiveGenerator = quickPrimitiveGeneratorFor(primitiveIndex));
		return primitiveDescriptor;
	}
	if (((primitiveIndex >= 1) && (primitiveIndex <= MaxCompiledPrimitiveIndex))) {
		return (&(primitiveGeneratorTable[primitiveIndex]));
	}
	return null;
}

	/* SimpleStackBasedCogit>>#register:isInMask: */
static NoDbgRegParms int
registerisInMask(sqInt reg, sqInt mask)
{
	return ((mask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0);
}

	/* SimpleStackBasedCogit>>#register:isNotInMask: */
static NoDbgRegParms int
registerisNotInMask(sqInt reg, sqInt mask)
{
	return (!(mask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))));
}

	/* SimpleStackBasedCogit>>#v3:Block:Code:Size: */
static NoDbgRegParms sqInt
v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts <= 0);
	return (((sqInt)((usqInt)((fetchByteofObject(pc + 2, aMethodObj))) << 8))) + (fetchByteofObject(pc + 3, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:LongForward:Branch:Distance: */
static NoDbgRegParms sqInt
v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return (((sqInt)((usqInt)(((fetchByteofObject(pc, aMethodObj)) & 3)) << 8))) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:Long:Branch:Distance: */
static NoDbgRegParms sqInt
v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return (((sqInt)((usqInt)((((fetchByteofObject(pc, aMethodObj)) & 7) - 4)) << 8))) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	N.B. This serves for both BlueBook/V3 and V4 short jumps. */

	/* SimpleStackBasedCogit>>#v3:ShortForward:Branch:Distance: */
static NoDbgRegParms sqInt
v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return ((fetchByteofObject(pc, aMethodObj)) & 7) + 1;
}


/*	253		11111101 eei i i kkk	jjjjjjjj		Push Closure Num Copied iii (+ Ext A
	// 16 * 8) Num Args kkk (+ Ext A \\ 16 * 8) BlockSize jjjjjjjj (+ Ext B *
	256). ee = num extensions
 */

	/* SimpleStackBasedCogit>>#v4:Block:Code:Size: */
static NoDbgRegParms sqInt
v4BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt byteOne;
    sqInt eb;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;

	eb = 0;
	/* If nExts < 0 it isn't known and we rely on the number of extensions encoded in the eeiiikkk byte. */
	byteOne = fetchByteofObject(pc + 1, aMethodObj);
	assert((nExts < 0)
	 || (nExts == (((usqInt)(byteOne)) >> 6)));
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - (((usqInt)(byteOne)) >> 6)) - (((usqInt)(byteOne)) >> 6);
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 2, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */
/*	243		11110011	i i i i i i i i	Pop and Jump 0n True i i i i i i i i (+
	Extend A * 256)
 */
/*	244		11110100	i i i i i i i i	Pop and Jump 0n False i i i i i i i i (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#v4:LongForward:Branch:Distance: */
static NoDbgRegParms sqInt
v4LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt eb;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;

	eb = 0;
	assert(nExts >= 0);
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - nExts) - nExts;
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 1, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#v4:Long:Branch:Distance: */
static NoDbgRegParms sqInt
v4LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt eb;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;

	eb = 0;
	assert(nExts >= 0);
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - nExts) - nExts;
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 1, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}

	/* SistaMethodZone>>#getCogCodeZoneThreshold */
double
getCogCodeZoneThreshold(void)
{
	return thresholdRatio;
}

	/* SistaMethodZone>>#setCogCodeZoneThreshold: */
sqInt
setCogCodeZoneThreshold(double ratio)
{
	if (!((ratio >= 0.1)
		 && (ratio <= 1.0))) {
		return PrimErrBadArgument;
	}
	thresholdRatio = ratio;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	return 0;
}


/*	Add a blockStart for an embedded block. For a binary tree walk block
	dispatch blocks must be compiled in pc/depth-first order but are scanned
	in breadth-first
	order, so do an insertion sort (which of course is really a bubble sort
	because we
	have to move everything higher to make room). */

	/* StackToRegisterMappingCogit>>#addBlockStartAt:numArgs:numCopied:span: */
static NoDbgRegParms BlockStart *
addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span)
{
    BlockStart *blockStart;
    sqInt i;
    sqInt j;


	/* Transcript ensureCr; nextPutAll: 'addBlockStartAt: '; print: bytecodepc; cr; flush. */
	if (blockCount > 0) {
		i = blockCount - 1;
		while (1) {
			/* check for repeat addition during recompilation due to initialNil miscount. */
			blockStart = (&(blockStarts[i]));
			if (((blockStart->startpc)) == bytecodepc) {
				return blockStart;
			}
			if (!((((blockStart->startpc)) > bytecodepc)
			 && (i > 0))) break;
			i -= 1;
		}
		for (j = blockCount; j >= (i + 1); j += -1) {
			blockStarts[j] = (blockStarts[j - 1]);
		}
		blockStart = (&(blockStarts[i + 1]));
	}
	else {
		blockStart = (&(blockStarts[blockCount]));
	}
	blockCount += 1;
	(blockStart->startpc = bytecodepc);
	(blockStart->numArgs = numArgs);
	(blockStart->numCopied = numCopied);
	(blockStart->numInitialNils = 0);
	(blockStart->stackCheckLabel = null);
	(blockStart->hasInstVarRef = 0);
	(blockStart->span = span);
	return blockStart;
}


/*	e.g.	Receiver				Receiver	or	Receiver				Receiver	(RISC)
	Selector/Arg0	=>		Arg1			Selector/Arg0	=>		Arg1
	Arg1					Arg2			Arg1					Arg2
	Arg2					Arg3			Arg2			sp->	Arg3
	Arg3			sp->	retpc	sp->	Arg3
	sp->	retpc */
/*	Generate code to adjust the possibly stacked arguments immediately
	before jumping to a method looked up by a perform primitive. */

	/* StackToRegisterMappingCogit>>#adjustArgumentsForPerform: */
static NoDbgRegParms void
adjustArgumentsForPerform(sqInt numArgs)
{
    sqInt index;

	assert((numRegArgs()) <= 2);
	assert(numArgs >= 1);
	if (numArgs <= (numRegArgs())) {
		if (numArgs == 2) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, Arg1Reg, Arg0Reg);
		}
		return;
	}
	if (((numRegArgs()) + 1) == numArgs) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, SPReg, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, Arg1Reg);
		genoperandoperandoperand(MoveMwrR, BytesPerWord * 2, SPReg, Arg0Reg);
		genoperandoperand(AddCqR, (numArgs + 1) * BytesPerWord, SPReg);
		genoperandoperandoperand(MoveRMwr, TempReg, 0, SPReg);
		return;
	}
	for (index = (numArgs - 1); index >= 1; index += -1) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, index * BytesPerWord, SPReg, TempReg);
		genoperandoperandoperand(MoveRMwr, TempReg, (index + 1) * BytesPerWord, SPReg);
	}
	genoperand(PopR, TempReg);
	genoperandoperandoperand(MoveRMwr, TempReg, 0, SPReg);
}


/*	if there's a free register, use it */

	/* StackToRegisterMappingCogit>>#allocateFloatRegNotConflictingWith: */
static NoDbgRegParms sqInt
allocateFloatRegNotConflictingWith(sqInt regMask)
{
    sqInt reg;

	reg = availableFloatRegisterOrNoneFor(backEnd, (liveFloatRegisters()) | regMask);
	if (reg == NoReg) {
		/* No free register, choose one that does not conflict with regMask */
		reg = freeAnyFloatRegNotConflictingWith(regMask);
	}
	return reg;
}


/*	If the stack entry is already in a register not conflicting with regMask,
	answers it,
	else allocate a new register not conflicting with reg mask
 */

	/* StackToRegisterMappingCogit>>#allocateRegForStackEntryAt:notConflictingWith: */
static NoDbgRegParms sqInt
allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask)
{
    sqInt mask;
    CogSimStackEntry *stackEntry;

	stackEntry = ssValue(index);
	mask = registerMaskOrNone(stackEntry);
	if ((mask != 0)
	 && ((!(mask & regMask)))) {
		flag("TODO");
		return registerOrNone(stackEntry);
	}
	return allocateRegNotConflictingWith(regMask);
}


/*	if there's a free register, use it */

	/* StackToRegisterMappingCogit>>#allocateRegNotConflictingWith: */
static NoDbgRegParms sqInt
allocateRegNotConflictingWith(sqInt regMask)
{
    sqInt reg;

	reg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | regMask);
	if (reg == NoReg) {
		/* No free register, choose one that does not conflict with regMask */
		reg = freeAnyRegNotConflictingWith(regMask);
	}
	if (reg == ReceiverResultReg) {
		/* If we've allocated RcvrResultReg, it's not live anymore */
		voidReceiverResultRegContainsSelf();
	}
	return reg;
}

	/* StackToRegisterMappingCogit>>#anyReferencesToRegister:inTopNItems: */
static NoDbgRegParms sqInt
anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n)
{
    sqInt i;
    sqInt regMask;

	regMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((registerMask(simStackAt(i))) & regMask) != 0)) {
			return 1;
		}
	}
	return 0;
}


/*	Store the smalltalk pointers */

	/* StackToRegisterMappingCogit>>#beginHighLevelCall: */
static NoDbgRegParms void
beginHighLevelCall(sqInt alignment)
{
    sqInt actualAlignment;
    sqInt i;
    sqInt mask;
    sqInt offset;

	/* begin ssFlushAll */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= simStackPtr) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = simStackPtr + 1;
	}
	voidReceiverResultRegContainsSelf();
	genSaveStackPointers(backEnd);
	genLoadCStackPointer(backEnd);
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfNativeFramePointer();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, FPReg);
	genoperandoperand(SubCqR, 1, FPReg);
	actualAlignment = ((alignment < BytesPerWord) ? BytesPerWord : alignment);
	if (actualAlignment > BytesPerWord) {
		mask = -actualAlignment;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, mask, SPReg);
	}
	currentCallCleanUpSize = 0;
}


/*	This is a static version of ceCallCogCodePopReceiverArg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg0Regs */
void
callCogCodePopReceiverArg0Regs(void)
{
	realCECallCogCodePopReceiverArg0Regs();
}


/*	This is a static version of ceCallCogCodePopReceiverArg1Arg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg1Arg0Regs */
void
callCogCodePopReceiverArg1Arg0Regs(void)
{
	realCECallCogCodePopReceiverArg1Arg0Regs();
}

	/* StackToRegisterMappingCogit>>#callSwitchToCStack */
static sqInt
callSwitchToCStack(void)
{

	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, cFramePointerAddress(), FPReg);
	return 0;
}


/*	Restore the link register */

	/* StackToRegisterMappingCogit>>#callSwitchToSmalltalkStack */
static void
callSwitchToSmalltalkStack(void)
{

	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	genLoadStackPointers(backEnd);
}


/*	Loop over bytecodes, dispatching to the generator for each bytecode,
	handling fixups in due course.
 */

	/* StackToRegisterMappingCogit>>#compileAbstractInstructionsFrom:through: */
static NoDbgRegParms sqInt
compileAbstractInstructionsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt nextOpcodeIndex;
    sqInt result;

	traceSimStack();
	bytecodePC = start;
	nExts = (result = 0);
	descriptor = null;
	deadCode = 0;
	while (1) {
		maybeHaltIfDebugPC();
		mergeWithFixupIfRequired((fixup = fixupAt(bytecodePC)));
		descriptor = loadBytesAndGetDescriptor();
		nextOpcodeIndex = opcodeIndex;
		if (deadCode) {
			result = mapDeadDescriptorIfNeeded(descriptor);
		}
		else {
			result = ((descriptor->generator))();
		}
		if (result == 0) {
			/* begin assertExtsAreConsumed: */
			if (!((descriptor->isExtension))) {
				assert((extA == 0)
				 && ((extB == 0)
				 && (numExtB == 0)));
			}
		}
		traceDescriptor(descriptor);
		traceSimStack();
		/* begin patchFixupTargetIfNeeded:nextOpcodeIndex: */
		if ((((((usqInt)((fixup->targetInstruction)))) >= NeedsNonMergeFixupFlag) && ((((usqInt)((fixup->targetInstruction)))) <= NeedsMergeFixupFlag))) {
			/* There is a fixup for this bytecode.  It must point to the first generated
			   instruction for this bytecode.  If there isn't one we need to add a label. */
			if (opcodeIndex == nextOpcodeIndex) {
				/* begin Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			(fixup->targetInstruction = abstractInstructionAt(nextOpcodeIndex));
		}
		/* begin maybeDumpLiterals: */
		if ((		/* begin isUnconditionalBranch */
			(isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse)))))
		 || ((descriptor->isReturn))) {
			/* begin dumpLiterals: */
			!((			/* begin isUnconditionalBranch */
				(isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse)))))
			 || ((descriptor->isReturn)));
		}
		bytecodePC = (bytecodePC + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bytecodePC, nExts, methodObj)
	: 0));
		if (!((result == 0)
		 && (bytecodePC <= end))) break;
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
	}
	/* begin checkEnoughOpcodes */
	if (opcodeIndex > numAbstractOpcodes) {
		error("Cog JIT internal error. Too many abstract opcodes.  Num opcodes heuristic is too optimistic.");
	}
	return result;
}

	/* StackToRegisterMappingCogit>>#compileBlockBodies */
static sqInt
compileBlockBodies(void)
{
    BlockStart *blockStart;
    sqInt compiledBlocksCount;
    sqInt initialCounterIndex;
    sqInt initialOpcodeIndex;
    sqInt initialStackPtr;
    sqInt (* const pushNilSizeFunction)(sqInt,sqInt) = squeakV3orSistaV1PushNilSizenumInitialNils;
    sqInt result;
    sqInt savedNeedsFrame;
    sqInt savedNumArgs;
    sqInt savedNumTemps;

	assert(blockCount > 0);
	savedNeedsFrame = needsFrame;
	savedNumArgs = methodOrBlockNumArgs;
	savedNumTemps = methodOrBlockNumTemps;
	inBlock = InVanillaBlock;
	compiledBlocksCount = 0;
	while (compiledBlocksCount < blockCount) {
		compilationPass = 1;
		blockStart = blockStartAt(compiledBlocksCount);
		if (((result = scanBlock(blockStart))) < 0) {
			return result;
		}
		initialOpcodeIndex = opcodeIndex;
		/* for SistaCogit */
		initialCounterIndex = 0 /* begin maybeCounterIndex */;
		while (1) {
			compileBlockEntry(blockStart);
			initialStackPtr = simStackPtr;
			if (((result = compileAbstractInstructionsFromthrough(((blockStart->startpc)) + (pushNilSizeFunction(methodObj, ((blockStart->numInitialNils)))), (((blockStart->startpc)) + ((blockStart->span))) - 1))) < 0) {
				return result;
			}
			if (initialStackPtr == simStackPtr) break;
			assert((initialStackPtr > simStackPtr)
			 || (deadCode));
			/* for asserts */
			compilationPass += 1;
			(blockStart->numInitialNils = (((blockStart->numInitialNils)) + simStackPtr) - initialStackPtr);
			(((blockStart->fakeHeader))->dependent = null);
			reinitializeFixupsFromthrough(((blockStart->startpc)) + ((blockStart->numInitialNils)), (((blockStart->startpc)) + ((blockStart->span))) - 1);
			bzero(abstractOpcodes + initialOpcodeIndex,
									(opcodeIndex - initialOpcodeIndex) * sizeof(AbstractInstruction));
			opcodeIndex = initialOpcodeIndex;
		}
		compiledBlocksCount += 1;
	}
	needsFrame = savedNeedsFrame;
	methodOrBlockNumArgs = savedNumArgs;
	methodOrBlockNumTemps = savedNumTemps;
	return 0;
}


/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. closure (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	Avoid use of SendNumArgsReg which is the flag determining whether
	context switch is allowed on stack-overflow. */
/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any, and to correctly
	initialize the explicitly nilled/pushed temp entries (they are /not/ of
	type constant nil). */

	/* StackToRegisterMappingCogit>>#compileBlockFrameBuild: */
static NoDbgRegParms void
compileBlockFrameBuild(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *cascade0;
    sqInt i;
    sqInt ign;

	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);
	/* begin PushR: */
	genoperand(PushR, FPReg);
	genoperandoperand(MoveRR, SPReg, FPReg);
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	cascade0 = (blockStart->fakeHeader);
	addDependent(cascade0, annotateAbsolutePCRef(genoperand(PushCw, ((sqInt)((blockStart->fakeHeader))))));
	/* begin setLabelOffset: */
	((cascade0->operands))[1] = MFMethodFlagIsBlockFlag;
	genoperand(PushCq, nilObject());
	if ((blockStart->hasInstVarRef)) {
		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);
		genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, ReceiverIndex, ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, Arg0Reg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, Arg0Reg, ReceiverResultReg);
	}
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = 0; i < ((blockStart->numCopied)); i += 1) {
		genLoadSlotsourceRegdestReg(i + ClosureFirstCopiedValueIndex, ClassReg, TempReg);
		/* begin PushR: */
		genoperand(PushR, TempReg);
	}
	(blockStart->stackCheckLabel = compileStackOverflowCheck(1));
	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramefulMethod((blockStart->startpc));
	if (((blockStart->numInitialNils)) > 0) {
		if (((blockStart->numInitialNils)) > 1) {
			/* begin genMoveConstant:R: */
			genoperandoperand(MoveCqR, nilObject(), TempReg);
			for (ign = 1; ign <= ((blockStart->numInitialNils)); ign += 1) {
				/* begin PushR: */
				genoperand(PushR, TempReg);
			}
		}
		else {
			/* begin genPushConstant: */
			genoperand(PushCq, nilObject());
		}
	}
}


/*	Make sure ReceiverResultReg holds the receiver, loaded from the closure,
	which is what is initially in ReceiverResultReg. We must annotate the
	first instruction in vanilla blocks so that
	findMethodForStartBcpc:inHomeMethod: can function. We need two annotations
	because the first is a fiducial. */
/*	Make sure ReceiverResultReg holds the receiver, loaded from
	the closure, which is what is initially in ReceiverResultReg */

	/* StackToRegisterMappingCogit>>#compileBlockFramelessEntry: */
static NoDbgRegParms void
compileBlockFramelessEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;

	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramelessBlock((blockStart->startpc));
	if (!(((blockStart->entryLabel)) == null)) {
		abstractInstruction = (blockStart->entryLabel);
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
	}
	if ((blockStart->hasInstVarRef)) {
		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);
		genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, ReceiverIndex, ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, TempReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, TempReg, ReceiverResultReg);
	}
}

	/* StackToRegisterMappingCogit>>#compileCogFullBlockMethod: */
static NoDbgRegParms CogMethod *
compileCogFullBlockMethod(sqInt numCopied)
{
    sqInt allocBytes;
    sqInt fixupBytes;
    sqInt numBlocks;
    sqInt numBytecodes;
    sqInt numCleanBlocks;
    sqInt opcodeBytes;
    sqInt result;

	methodOrBlockNumTemps = tempCountOf(methodObj);
	setHasMovableLiteral(0);
	setHasYoungReferent(isYoungObject(methodObj));
	methodOrBlockNumArgs = argumentCountOf(methodObj);
	inBlock = InFullBlock;
	maxLitIndex = -1;
	assert((primitiveIndexOf(methodObj)) == 0);
	/* initial estimate.  Actual endPC is determined in scanMethod. */
	initialPC = startPCOfMethod(methodObj);
	endPC = numBytesOf(methodObj);
	numBytecodes = (endPC - initialPC) + 1;
	primitiveIndex = 0;
	/* begin allocateOpcodes:bytecodes:ifFail: */
	numAbstractOpcodes = (numBytecodes + 10) * 10 /* begin estimateOfAbstractOpcodesPerBytecodes */;
	opcodeBytes = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupBytes = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	/* Document the fact that the MaxStackAllocSize ensures that the number of abstract
	   opcodes fits in a 16 bit integer (e.g. CogBytecodeFixup's instructionIndex). */
	allocBytes = opcodeBytes + fixupBytes;
	assert((((sizeof(CogAbstractInstruction)) + (sizeof(CogBytecodeFixup))) * 0xC000) > MaxStackAllocSize);
	if (allocBytes > MaxStackAllocSize) {
		return ((CogMethod *) MethodTooBig);
		goto l1;
	}
	abstractOpcodes = alloca(allocBytes);
	bzero(abstractOpcodes, allocBytes);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeBytes));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	l1:	/* end allocateOpcodes:bytecodes:ifFail: */;
	flag("TODO");
	if (((numBlocks = scanMethod())) < 0) {
		return ((CogMethod *) numBlocks);
	}
	assert(numBlocks == 0);
	numCleanBlocks = scanForCleanBlocks();
	assert(numCleanBlocks == 0);
	allocateBlockStarts(numBlocks + numCleanBlocks);
	blockCount = 0;
	if (numCleanBlocks > 0) {
		addCleanBlockStarts();
	}
	blockEntryLabel = null;
	(methodLabel->dependent = null);
	if (((result = compileEntireFullBlockMethod(numCopied))) < 0) {
		return ((CogMethod *) result);
	}
	return generateCogFullBlock();
}

	/* StackToRegisterMappingCogit>>#compileCogMethod: */
static NoDbgRegParms CogMethod *
compileCogMethod(sqInt selector)
{
    sqInt allocBytes;
    int extra;
    sqInt fixupBytes;
    sqInt numBlocks;
    sqInt numBytecodes;
    sqInt numCleanBlocks;
    sqInt opcodeBytes;
    sqInt result;

	methodOrBlockNumTemps = tempCountOf(methodObj);
	setHasMovableLiteral(0);
	setHasYoungReferent((isYoungObject(methodObj))
	 || (isYoung(selector)));
	methodOrBlockNumArgs = argumentCountOf(methodObj);
	inBlock = 0;
	maxLitIndex = -1;
	extra = ((((primitiveIndex = primitiveIndexOf(methodObj))) > 0)
	 && (!(isQuickPrimitiveIndex(primitiveIndex)))
		? 30
		: 10);
	/* initial estimate.  Actual endPC is determined in scanMethod. */
	initialPC = startPCOfMethod(methodObj);
	endPC = (isQuickPrimitiveIndex(primitiveIndex)
		? initialPC - 1
		: numBytesOf(methodObj));
	numBytecodes = (endPC - initialPC) + 1;
	/* begin allocateOpcodes:bytecodes:ifFail: */
	numAbstractOpcodes = (numBytecodes + extra) * 10 /* begin estimateOfAbstractOpcodesPerBytecodes */;
	opcodeBytes = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupBytes = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	/* Document the fact that the MaxStackAllocSize ensures that the number of abstract
	   opcodes fits in a 16 bit integer (e.g. CogBytecodeFixup's instructionIndex). */
	allocBytes = opcodeBytes + fixupBytes;
	assert((((sizeof(CogAbstractInstruction)) + (sizeof(CogBytecodeFixup))) * 0xC000) > MaxStackAllocSize);
	if (allocBytes > MaxStackAllocSize) {
		return ((CogMethod *) MethodTooBig);
		goto l1;
	}
	abstractOpcodes = alloca(allocBytes);
	bzero(abstractOpcodes, allocBytes);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeBytes));
	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	labelCounter = 0;
	l1:	/* end allocateOpcodes:bytecodes:ifFail: */;
	if (((numBlocks = scanMethod())) < 0) {
		return ((CogMethod *) numBlocks);
	}
	numCleanBlocks = scanForCleanBlocks();
	if (methodFoundInvalidPostScan()) {
		return ((CogMethod *) ShouldNotJIT);
	}
	allocateBlockStarts(numBlocks + numCleanBlocks);
	blockCount = 0;
	if (numCleanBlocks > 0) {
		addCleanBlockStarts();
	}
	blockEntryLabel = null;
	(methodLabel->dependent = null);
	if (((result = compileEntireMethod())) < 0) {
		return ((CogMethod *) result);
	}
	return generateCogMethod(selector);
}


/*	Compile the abstract instructions for the entire method, including blocks. */
/*	Compile the abstract instructions for the entire method, including blocks. */

	/* StackToRegisterMappingCogit>>#compileEntireMethod */
static sqInt
compileEntireMethod(void)
{
    sqInt result;

	regArgsHaveBeenPushed = 0;
	/* begin preenMethodLabel */
	((methodLabel->operands))[1] = 0;
	compileAbort();
	compileEntry();
	if (((result = compilePrimitive())) < 0) {
		return result;
	}
	compileFrameBuild();
	if (((result = compileMethodBody())) < 0) {
		return result;
	}
	if (blockCount == 0) {
		return 0;
	}
	if (((result = compileBlockBodies())) < 0) {
		return result;
	}
	return compileBlockDispatch();
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any. */

	/* StackToRegisterMappingCogit>>#compileFrameBuild */
static void
compileFrameBuild(void)
{
    sqInt i;
    sqInt iLimiT;


#  if IMMUTABILITY
	if (useTwoPaths) {
		compileTwoPathFrameBuild();
		return;
	}
#  endif

	if (!needsFrame) {
		if (useTwoPaths) {
			compileTwoPathFramelessInit();
		}
		initSimStackForFramelessMethod(initialPC);
		return;
	}
	assert(!(useTwoPaths));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, FPReg);
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(genoperand(PushCw, ((sqInt)methodLabel))));
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, nilObject(), SendNumArgsReg);
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	genoperand(PushR, ReceiverResultReg);
	for (i = (methodOrBlockNumArgs + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (	/* begin methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
}


/*	Make sure ReceiverResultReg holds the receiver, loaded from the closure,
	which is what is initially in ReceiverResultReg.  */
/*	Make sure ReceiverResultReg holds the receiver, loaded from
	the closure, which is what is initially in ReceiverResultReg */

	/* StackToRegisterMappingCogit>>#compileFullBlockFramelessEntry: */
static NoDbgRegParms void
compileFullBlockFramelessEntry(sqInt numCopied)
{
	initSimStackForFramelessBlock(initialPC);
	flag("TODO");
	genLoadSlotsourceRegdestReg(FullClosureReceiverIndex, ReceiverResultReg, Arg0Reg);
	genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, FullClosureReceiverIndex, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
}


/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. closure (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	Avoid use of SendNumArgsReg which is the flag determining whether
	context switch is allowed on stack-overflow. */

	/* StackToRegisterMappingCogit>>#compileFullBlockMethodFrameBuild: */
static NoDbgRegParms void
compileFullBlockMethodFrameBuild(sqInt numCopied)
{
    sqInt i;
    sqInt iLimiT;

	if (useTwoPaths) {
		/* method with only inst var store, we compile only slow path for now */
		useTwoPaths = 0;
#    if IMMUTABILITY
		needsFrame = 1;
#    endif

	}
	if (!needsFrame) {
		/* it is OK for numCopied to be non-zero provided that the block does not actually use the copied values.
		   There are some blocks like this, e.g. that simply reference copied values to mark them as used for Slang.
		   See e.g. CroquetPlugin>>#primitiveGatherEntropy which contains the block [bufPtr. bufSize. false],
		   which the bytecode compiler optimizes to [false]. */
		compileFullBlockFramelessEntry(numCopied);
		initSimStackForFramelessBlock(initialPC);
		return;
	}
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, FPReg);
	genoperandoperand(MoveRR, SPReg, FPReg);
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	addDependent(methodLabel, annotateAbsolutePCRef(genoperand(PushCw, ((sqInt)methodLabel))));
	/* begin setLabelOffset: */
	((methodLabel->operands))[1] = MFMethodFlagIsBlockFlag;
	genoperandoperand(MoveCqR, nilObject(), SendNumArgsReg);
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	flag("TODO");
	genLoadSlotsourceRegdestReg(FullClosureReceiverIndex, ClassReg, Arg0Reg);
	genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, FullClosureReceiverIndex, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	genoperand(PushR, ReceiverResultReg);
	for (i = 0; i < numCopied; i += 1) {
		genLoadSlotsourceRegdestReg(i + FullClosureFirstCopiedValueIndex, ClassReg, TempReg);
		/* begin PushR: */
		genoperand(PushR, TempReg);
	}
	for (i = ((methodOrBlockNumArgs + numCopied) + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	stackCheckLabel = compileStackOverflowCheck(1);
	initSimStackForFramefulMethod(initialPC);
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	We are in a method where the frame is needed *only* for instance variable
	store, typically a setter method.
	This case has 20% overhead with Immutability compared to setter without
	immutability because of the stack
	frame creation. We compile two path, one where the object is immutable,
	one where it isn't. At the beginning 
	of the frame build, we take one path or the other depending on the
	receiver mutability.
	
	Note: this specific case happens only where there are only instance
	variabel stores. We could do something
	similar for literal variable stores, but we don't as it's too uncommon.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFrameBuild */
#if IMMUTABILITY
static void
compileTwoPathFrameBuild(void)
{
    sqInt i;
    sqInt iLimiT;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpOld;

	assert(useTwoPaths);
	assert(blockCount == 0);
	jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, TempReg);
	/* begin genJumpInOldSpace: */
	genoperandoperand(CmpCqR, storeCheckBoundary(), ReceiverResultReg);
	jumpOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	assert(!needsFrame);
	initSimStackForFramelessMethod(initialPC);
	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l2;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
	l2:	/* end compileMethodBody */;
	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	needsFrame = 1;
	jmpTarget(jumpOld, jmpTarget(jumpImmutable, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, FPReg);
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(genoperand(PushCw, ((sqInt)methodLabel))));
	/* begin genMoveConstant:R: */
	genoperandoperand(MoveCqR, nilObject(), SendNumArgsReg);
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	genoperand(PushR, ReceiverResultReg);
	for (i = (methodOrBlockNumArgs + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (	/* begin methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
}
#endif /* IMMUTABILITY */


/*	We are in a frameless method with at least two inst var stores. We compile
	two paths,
	one where the object is in new space, and one where it isn't. At the
	beginning 
	of the method, we take one path or the other depending on the receiver
	being in newSpace.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFramelessInit */
static void
compileTwoPathFramelessInit(void)
{
    AbstractInstruction *jumpOld;

	assert(!(IMMUTABILITY));
	assert(!(needsFrame));
	assert(useTwoPaths);
	/* begin genJumpInOldSpace: */
	genoperandoperand(CmpCqR, storeCheckBoundary(), ReceiverResultReg);
	jumpOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	initSimStackForFramelessMethod(initialPC);
	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l2;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
	l2:	/* end compileMethodBody */;
	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	jmpTarget(jumpOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* StackToRegisterMappingCogit>>#cPICMissTrampolineFor: */
static NoDbgRegParms sqInt
cPICMissTrampolineFor(sqInt numArgs)
{
	return picMissTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}


/*	Replaces the Blue Book double-extended send [132], in which the first byte
	was wasted on 8 bits of argument count. 
	Here we use 3 bits for the operation sub-type (opType), and the remaining
	5 bits for argument count where needed. 
	The last byte give access to 256 instVars or literals. 
	See also secondExtendedSendBytecode
 */

	/* StackToRegisterMappingCogit>>#doubleExtendedDoAnythingBytecode */
static sqInt
doubleExtendedDoAnythingBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    sqInt opType;

	opType = ((usqInt)(byte1)) >> 5;
	if (opType == 0) {
		return genSendnumArgs(byte2, byte1 & 0x1F);
	}
	if (opType == 1) {
		return genSendSupernumArgs(byte2, byte1 & 0x1F);
	}
	switch (opType) {
	case 2:
		if (isReadMediatedContextInstVarIndex(byte2)) {
			genPushMaybeContextReceiverVariable(byte2);
		}
		else {
			genPushReceiverVariable(byte2);
			/* begin annotateInstructionForBytecode */
			abstractInstruction = (prevInstIsPCAnnotated()
				? gen(Nop)
				: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
			/* begin annotateBytecode: */
			(abstractInstruction->annotation = HasBytecodePC);
			return 0;
		}
		break;
	case 3:
		genPushLiteralIndex(byte2);
		/* begin annotateInstructionForBytecode */
		abstractInstruction1 = (prevInstIsPCAnnotated()
			? gen(Nop)
			: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin annotateBytecode: */
		(abstractInstruction1->annotation = HasBytecodePC);
		return 0;

	case 4:
		genPushLiteralVariable(byte2);
		break;
	case 7:
		/* begin genStorePop:LiteralVariable: */
		genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, byte2, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
#    if IMMUTABILITY
		/* genStorePop:LiteralVariable: annotates; don't annotate twice */
		return 0;
#    endif


	default:
		/* 5 & 6 */
		if (isWriteMediatedContextInstVarIndex(byte2)) {
			/* begin genStorePop:MaybeContextReceiverVariable: */
			genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
		}
		else {
			/* begin genStorePop:ReceiverVariable: */
			genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssTop())->constant)))
			 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
			 || (oopisLessThan(((ssTop())->constant), nilObject()))))), 1);
		}
#    if IMMUTABILITY
		/* genStorePop:...ReceiverVariable: annotate; don't annotate twice */
		return 0;
#    endif

;
	}
	assert(needsFrame);
	assert(!(prevInstIsPCAnnotated()));
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin annotateBytecode: */
	(abstractInstruction2->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#duplicateTopBytecode */
static sqInt
duplicateTopBytecode(void)
{
    SimStackEntry desc;


	/* begin ssTopDescriptor */
	desc = simStack[simStackPtr];
	return ssPushDesc(desc);
}

	/* StackToRegisterMappingCogit>>#endHighLevelCallWithCleanup */
static void
endHighLevelCallWithCleanup(void)
{
	if (currentCallCleanUpSize > 0) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, currentCallCleanUpSize, SPReg);
	}
	callSwitchToSmalltalkStack();
}

	/* StackToRegisterMappingCogit>>#endHighLevelCallWithoutCleanup */
static void
endHighLevelCallWithoutCleanup(void)
{
	callSwitchToSmalltalkStack();
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureFixupAt: */
static NoDbgRegParms BytecodeFixup *
ensureFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;


	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if ((((usqInt)((fixup->targetInstruction)))) <= NeedsNonMergeFixupFlag) {
		/* convert a non-merge into a merge */
		/* begin becomeMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
		(fixup->simStackPtr = simStackPtr);
		(fixup->simNativeStackPtr = simNativeStackPtr);
		(fixup->simNativeStackSize = simNativeStackSize);
	}
	else {
		if ((fixup->isTargetOfBackwardBranch)) {
			/* this is the target of a backward branch and
			   so doesn't have a simStackPtr assigned yet. */
			(fixup->simStackPtr = simStackPtr);
			(fixup->simNativeStackPtr = simNativeStackPtr);
			(fixup->simNativeStackSize = simNativeStackSize);
		}
		else {
			assert(((fixup->simStackPtr)) == simStackPtr);
			assert(((fixup->simNativeStackPtr)) == simNativeStackPtr);
			assert(((fixup->simNativeStackSize)) == simNativeStackSize);
		}
	}
	return fixup;
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureNonMergeFixupAt: */
static NoDbgRegParms BytecodeFixup *
ensureNonMergeFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;


	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if (((fixup->targetInstruction)) == 0) {
		/* begin becomeNonMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsNonMergeFixupFlag);
	}
	return fixup;
}

	/* StackToRegisterMappingCogit>>#ensureReceiverResultRegContainsSelf */
static void
ensureReceiverResultRegContainsSelf(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin ssAllocateRequiredReg: */
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
			storeToReg(simSelf(), ReceiverResultReg);
			((simSelf())->liveRegister = ReceiverResultReg);
		}
	}
	else {
		assert(((((simSelf())->type)) == SSRegister)
		 && (((((simSelf())->registerr)) == ReceiverResultReg)
		 && (receiverIsInReceiverResultReg())));
	}
}

	/* StackToRegisterMappingCogit>>#evaluate:at: */
static NoDbgRegParms void
evaluateat(BytecodeDescriptor *descriptor, sqInt pc)
{
	byte0 = fetchByteofObject(pc, methodObj);
	assert(descriptor == (generatorAt(bytecodeSetOffset + byte0)));
	loadSubsequentBytesForDescriptorat(descriptor, pc);
	((descriptor->generator))();
}


/*	Attempt to follow a branch to a pc. Handle branches to unconditional jumps
	and branches to push: aBoolean; conditional branch pairs. If the branch
	cannot be
	followed answer targetBytecodePC. It is not possible to follow jumps to
	conditional branches because the stack changes depth. That following is
	left to the genJumpIf:to:
	clients. */

	/* StackToRegisterMappingCogit>>#eventualTargetOf: */
static NoDbgRegParms sqInt
eventualTargetOf(sqInt targetBytecodePC)
{
    sqInt cond;
    sqInt currentTarget;
    BytecodeDescriptor *descriptor;
    sqInt nExts;
    sqInt nextPC;
    BytecodeFixup *self_in_CogSSBytecodeFixup;
    sqInt span;

	cond = 0;
	nextPC = (currentTarget = targetBytecodePC);
	while (1) {
		nExts = 0;
		while (1) {
			/* begin generatorForPC: */
			descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
			if ((descriptor->isReturn)) {
				return currentTarget;
			}
			if (!((descriptor->isExtension))) break;
			nExts += 1;
			nextPC += (descriptor->numBytes);
		}
		if (		/* begin isUnconditionalBranch */
			(isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse))))) {
			span = ((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj);
			if (span < 0) {
				/* Do *not* follow backward branches; these are interrupt points and should not be elided. */
				return currentTarget;
			}
			nextPC = (nextPC + ((descriptor->numBytes))) + span;
		}
		else {
			if (((descriptor->generator)) == genPushConstantTrueBytecode) {
				cond = 1;
			}
			else {
				if (((descriptor->generator)) == genPushConstantFalseBytecode) {
					cond = 0;
				}
				else {
					return currentTarget;
				}
			}
			/* begin fixupAt: */
			self_in_CogSSBytecodeFixup = fixupAtIndex(nextPC - initialPC);
			if ((self_in_CogSSBytecodeFixup->isTargetOfBackwardBranch)) {
				return currentTarget;
			}
			else {
				goto l2;
			}
	l2:;
			nextPC = eventualTargetOf(nextPC + ((descriptor->numBytes)));
			nExts = 0;
			while (1) {
				/* begin generatorForPC: */
				descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
				if ((descriptor->isReturn)) {
					return currentTarget;
				}
				if (!((descriptor->isExtension))) break;
				nExts += 1;
				nextPC += (descriptor->numBytes);
			}
			if (!(isBranch(descriptor))) {
				return currentTarget;
			}
			if (			/* begin isUnconditionalBranch */
				(isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse))))) {
				return currentTarget;
			}
			nextPC = (cond == ((descriptor->isBranchTrue))
				? (nextPC + ((descriptor->numBytes))) + (((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj))
				: nextPC + ((descriptor->numBytes)));
		}
		currentTarget = nextPC;
	}
	return 0;
}


/*	Spill the closest register on stack not conflicting with regMask. 
	Assertion Failure if regMask has already all the registers */

	/* StackToRegisterMappingCogit>>#freeAnyFloatRegNotConflictingWith: */
static NoDbgRegParms sqInt
freeAnyFloatRegNotConflictingWith(sqInt regMask)
{
    CogSimStackEntry *desc;
    sqInt index;
    sqInt reg;

	assert(needsFrame);
	reg = NoReg;
	index = ((simSpillBase < 0) ? 0 : simSpillBase);
	index = ((simNativeSpillBase < 0) ? 0 : simNativeSpillBase);
	while ((reg == NoReg)
	 && (index < simNativeStackPtr)) {
		desc = simNativeStackAt(index);
		if ((((desc->type)) == SSRegisterSingleFloat)
		 || (((desc->type)) == SSRegisterDoubleFloat)) {
			if (!(((regMask & (((((desc->registerr)) < 0) ? (((usqInt)(1)) >> (-((desc->registerr)))) : (1ULL << ((desc->registerr)))))) != 0))) {
				reg = (desc->registerr);
			}
		}
		index += 1;
	}
	assert(!((reg == NoReg)));
	ssAllocateRequiredFloatReg(reg);
	return reg;
}


/*	Spill the closest register on stack not conflicting with regMask. 
	Assertion Failure if regMask has already all the registers */

	/* StackToRegisterMappingCogit>>#freeAnyRegNotConflictingWith: */
static NoDbgRegParms sqInt
freeAnyRegNotConflictingWith(sqInt regMask)
{
    CogSimStackEntry *desc;
    sqInt index;
    sqInt reg;

	assert(needsFrame);
	reg = NoReg;
	index = ((simSpillBase < 0) ? 0 : simSpillBase);
	while ((reg == NoReg)
	 && (index < simStackPtr)) {
		desc = simStackAt(index);
		if (((desc->type)) == SSRegister) {
			if (!(((regMask & (((((desc->registerr)) < 0) ? (((usqInt)(1)) >> (-((desc->registerr)))) : (1ULL << ((desc->registerr)))))) != 0))) {
				reg = (desc->registerr);
			}
		}
		index += 1;
	}
	assert(!((reg == NoReg)));
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)), simStackPtr, simNativeStackPtr);
	return reg;
}


/*	Return from block, assuming result already loaded into ReceiverResultReg. */
/*	Return from block, assuming result already loaded into ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#genBlockReturn */
static sqInt
genBlockReturn(void)
{
	if (needsFrame) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);
		genoperand(PopR, FPReg);
	}
	/* begin RetN: */
	genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	/* can't fall through */
	deadCode = 1;
	return 0;
}


/*	Generate special versions of the ceCallCogCodePopReceiverAndClassRegs
	enilopmart that also pop register args from the stack to undo the pushing
	of register args in the abort/miss trampolines. */

	/* StackToRegisterMappingCogit>>#genCallPICEnilopmartNumArgs: */
static NoDbgRegParms void
(*genCallPICEnilopmartNumArgs(sqInt numArgs))(void)
{
    sqInt endAddress;
    usqInt enilopmart;
    sqInt size;

	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, varBaseAddress(), VarBaseReg);
	genLoadStackPointers(backEnd);
	/* begin PopR: */
	genoperand(PopR, ClassReg);
	genoperand(PopR, TempReg);
	genoperand(PopR, SendNumArgsReg);
	if (numArgs > 0) {
		if (numArgs > 1) {
			/* begin PopR: */
			genoperand(PopR, Arg1Reg);
			assert((numRegArgs()) == 2);
		}
		/* begin PopR: */
		genoperand(PopR, Arg0Reg);
	}
	genoperand(PopR, ReceiverResultReg);
	genoperand(PushR, SendNumArgsReg);
	genoperand(JumpR, TempReg);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineNamenumRegArgs("ceCallPIC", numArgs), enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	SistaV1:	**	248	(2)	11111000 iiiiiiii		mssjjjjj		Call Primitive #iiiiiiii
	+ (jjjjj * 256) 
	m=1 means inlined primitive, no hard return after execution. 
	ss defines the unsafe operation set used to encode the operations. 
	(ss = 0 means sista unsafe operations, ss = 01 means lowcode operations,
	other numbers are as yet used).
	See SistaCogit genCallPrimitiveBytecode, EncoderForSistaV1's class comment
	and StackInterpreter>>#callPrimitiveBytecode for more information. */

	/* StackToRegisterMappingCogit>>#genCallPrimitiveBytecode */
static sqInt
genCallPrimitiveBytecode(void)
{
    sqInt prim;
    sqInt primSet;

	if (byte2 < 128) {
		return (bytecodePC == initialPC
			? 0
			: EncounteredUnknownBytecode);
	}
	prim = (((sqInt)((usqInt)((byte2 - 128)) << 8))) + byte1;
	primSet = (((usqInt)(prim)) >> 13) & 3;
	prim = prim & 0x1FFF;
	if (primSet == 1) {
		if (prim < 1000) {
			return genLowcodeNullaryInlinePrimitive(prim);
		}
		if (prim < 2000) {
			return genLowcodeUnaryInlinePrimitive(prim - 1000);
		}
		if (prim < 3000) {
			return genLowcodeBinaryInlinePrimitive(prim - 2000);
		}
		if (prim < 4000) {
			return genLowcodeTrinaryInlinePrimitive(prim - 3000);
		}
	}
	return EncounteredUnknownBytecode;
}


/*	Override to push the register receiver and register arguments, if any. */

	/* StackToRegisterMappingCogit>>#genExternalizePointersForPrimitiveCall */
static sqInt
genExternalizePointersForPrimitiveCall(void)
{
	genPushRegisterArgs();
	/* begin PopR: */
	genoperand(PopR, TempReg);
	genoperandoperand(MoveRAw, TempReg, instructionPointerAddress());
	return genSaveStackPointers(backEnd);
}


/*	Override to push the register receiver and register arguments, if any. */

	/* StackToRegisterMappingCogit>>#genExternalizeStackPointerForFastPrimitiveCall */
static AbstractInstruction *
genExternalizeStackPointerForFastPrimitiveCall(void)
{
	genPushRegisterArgs();
	return (gAddCqRR(BytesPerWord, SPReg, TempReg),
		/* begin checkLiteral:forInstruction: */
genoperandoperand(MoveRAw, TempReg, stackPointerAddress()));
}


/*	Block compilation. At this point in the method create the block. Note its
	start and defer generating code for it until after the method and any
	other preceding
	blocks. The block's actual code will be compiled later. */
/*	253		11111101 eei i i kkk	jjjjjjjj		Push Closure Num Copied iii (+ Ext A
	// 16 * 8) Num Args kkk (+ Ext A \\ 16 * 8) BlockSize jjjjjjjj (+ Ext B *
	256). ee = num extensions
 */

	/* StackToRegisterMappingCogit>>#genExtPushClosureBytecode */
static sqInt
genExtPushClosureBytecode(void)
{
    sqInt i;
    sqInt numArgs;
    sqInt numCopied;
    sqInt reg;
    sqInt startpc;

	assert(needsFrame);
	startpc = bytecodePC + (((generatorAt(byte0))->numBytes));
	addBlockStartAtnumArgsnumCopiedspan(startpc, (numArgs = (byte1 & 7) + ((extA % 16) * 8)), (numCopied = ((((usqInt)(byte1)) >> 3) & 7) + ((extA / 16) * 8)), byte2 + (((sqInt)((usqInt)(extB) << 8))));
	extA = (numExtB = (extB = 0));
	/* begin genInlineClosure:numArgs:numCopied: */
	assert(getActiveContextAllocatesInMachineCode());
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(startpc + 1, numArgs, numCopied, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (ClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	Full Block creation compilation. The block's actual code will be compiled
	separatedly. 
 */
/*	*	255		11111111	xxxxxxxx	siyyyyyy	push Closure Compiled block literal
	index xxxxxxxx (+ Extend A * 256) numCopied yyyyyy receiverOnStack: s = 1
	ignoreOuterContext: i = 1
 */

	/* StackToRegisterMappingCogit>>#genExtPushFullClosureBytecode */
static sqInt
genExtPushFullClosureBytecode(void)
{
    sqInt compiledBlock;
    sqInt i;
    int ignoreContext;
    sqInt numCopied;
    int receiverIsOnStack;
    sqInt reg;

	assert(needsFrame);
	compiledBlock = getLiteral(byte1 + (((sqInt)((usqInt)(extA) << 8))));
	extA = 0;
	numCopied = byte2 & (0x3F);
	receiverIsOnStack = ((byte2 & (128)) != 0);
	ignoreContext = ((byte2 & (64)) != 0);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(compiledBlock, argumentCountOf(compiledBlock), numCopied, ignoreContext, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (FullClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	if (receiverIsOnStack) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
	}
	else {
		storeToReg(simSelf(), (reg = TempReg));
	}
	genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, FullClosureReceiverIndex, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). 
 */
/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). Override to add version for generic and PIC-specific entry
	with reg args. */

	/* StackToRegisterMappingCogit>>#generateEnilopmarts */
static void
generateEnilopmarts(void)
{

#  if Debug
	/* begin genEnilopmartFor:forCall:called: */
	realCEEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "realCEEnterCogCodePopReceiverReg");
	ceEnterCogCodePopReceiverReg = enterCogCodePopReceiver;
	/* begin genEnilopmartFor:forCall:called: */
	realCECallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "realCECallCogCodePopReceiverReg");
	ceCallCogCodePopReceiverReg = callCogCodePopReceiver;
	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "realCECallCogCodePopReceiverAndClassRegs");
	ceCallCogCodePopReceiverAndClassRegs = callCogCodePopReceiverAndClassRegs;
#  else // Debug
	/* begin genEnilopmartFor:forCall:called: */
	ceEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "ceEnterCogCodePopReceiverReg");
	ceCallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "ceCallCogCodePopReceiverReg");
	ceCallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "ceCallCogCodePopReceiverAndClassRegs");
#  endif // Debug

	genPrimReturnEnterCogCodeEnilopmart(0);
	cePrimReturnEnterCogCode = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCode);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCode", cePrimReturnEnterCogCode);
	genPrimReturnEnterCogCodeEnilopmart(1);
	cePrimReturnEnterCogCodeProfiling = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCodeProfiling);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCodeProfiling", cePrimReturnEnterCogCodeProfiling);
#  if Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "realCECallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg0Regs = callCogCodePopReceiverArg0Regs;
	realCECallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "realCECallCogCodePopReceiverArg1Arg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = callCogCodePopReceiverArg1Arg0Regs;
#  else // Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	ceCallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "ceCallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "ceCallCogCodePopReceiverArg1Arg0Regs");
#  endif // Debug

	ceCall0ArgsPIC = genCallPICEnilopmartNumArgs(0);
		ceCall1ArgsPIC = genCallPICEnilopmartNumArgs(1);
		ceCall2ArgsPIC = genCallPICEnilopmartNumArgs(2);
	assert((numRegArgs()) == 2);
}


/*	Size pc-dependent instructions and assign eventual addresses to all
	instructions. Answer the size of the code.
	Compute forward branches based on virtual address (abstract code starts at
	0), assuming that any branches branched over are long.
	Compute backward branches based on actual address.
	Reuse the fixups array to record the pc-dependent instructions that need
	to have
	their code generation postponed until after the others.
	
	Override to add handling for null branches (branches to the immediately
	following instruction) occasioned by StackToRegisterMapping's following of
	jumps.  */

	/* StackToRegisterMappingCogit>>#generateInstructionsAt: */
static NoDbgRegParms sqInt
generateInstructionsAt(sqInt eventualAbsoluteAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    BytecodeFixup *fixup;
    sqInt i;
    sqInt j;
    sqInt pcDependentIndex;

	absoluteAddress = eventualAbsoluteAddress;
	pcDependentIndex = 0;
	for (i = 0; i < opcodeIndex; i += 1) {
		/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
		   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
		abstractInstruction = abstractInstructionAt(i);
		maybeBreakGeneratingFromto(absoluteAddress, absoluteAddress + ((abstractInstruction->maxSize)));
		if (isPCDependent(abstractInstruction)) {
			sizePCDependentInstructionAt(abstractInstruction, absoluteAddress);
			if ((isJump(abstractInstruction))
			 && ((((i + 1) < opcodeIndex)
			 && ((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 1))))
			 || (((i + 2) < opcodeIndex)
			 && (((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 2)))
			 && ((((abstractInstructionAt(i + 1))->opcode)) == Nop))))) {
				(abstractInstruction->opcode = Nop);
				concretizeAt(abstractInstruction, absoluteAddress);
			}
			else {
				fixup = fixupAtIndex(pcDependentIndex);
				pcDependentIndex += 1;
				(fixup->instructionIndex = i);
			}
			absoluteAddress += (abstractInstruction->machineCodeSize);
		}
		else {
			/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
			   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
			absoluteAddress = concretizeAt(abstractInstruction, absoluteAddress);
			assert(((abstractInstruction->machineCodeSize)) == ((abstractInstruction->maxSize)));
		}
	}
	for (j = 0; j < pcDependentIndex; j += 1) {
		fixup = fixupAtIndex(j);
		abstractInstruction = abstractInstructionAt((fixup->instructionIndex));
		maybeBreakGeneratingFromto((abstractInstruction->address), (((abstractInstruction->address)) + ((abstractInstruction->maxSize))) - 1);
		concretizeAt(abstractInstruction, (abstractInstruction->address));
	}
	return absoluteAddress - eventualAbsoluteAddress;
}


/*	Generate the run-time entries for the various method and PIC entry misses
	and aborts.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* StackToRegisterMappingCogit>>#generateMissAbortTrampolines */
static void
generateMissAbortTrampolines(void)
{
    sqInt numArgs;
    sqInt numArgsLimiT;

	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		methodAbortTrampolines[numArgs] = (genMethodAbortTrampolineFor(numArgs));
	}
	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		picAbortTrampolines[numArgs] = (genPICAbortTrampolineFor(numArgs));
	}
	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		picMissTrampolines[numArgs] = (genPICMissTrampolineFor(numArgs));
	}
	ceReapAndResetErrorCodeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceReapAndResetErrorCodeFor, "ceReapAndResetErrorCodeTrampoline", 1, ClassReg, null, null, null, 0 /* begin emptyRegisterMask */, 1, NoReg, 0);
}


/*	Override to generate code to push the register arg(s) for <= numRegArg
	arity sends.
 */

	/* StackToRegisterMappingCogit>>#generateSendTrampolines */
static void
generateSendTrampolines(void)
{
    sqInt numArgs;

	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		ordinarySendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(
	ceSendsupertonumArgs,
	numArgs,
	trampolineNamenumArgs("ceSend", numArgs),
	ClassReg,
	(	/* begin trampolineArgConstant: */
		assert(0 >= 0),
	-2 - 0),
	ReceiverResultReg,
	/* begin numArgsOrSendNumArgsReg: */
(numArgs <= (NumSendTrampolines - 2)
		? (assert(numArgs >= 0),
			-2 - numArgs)
		: SendNumArgsReg)));
	}
	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		directedSuperSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendabovetonumArgs, numArgs, trampolineNamenumArgs("ceDirectedSuperSend", numArgs), ClassReg, TempReg, ReceiverResultReg, 
/* begin numArgsOrSendNumArgsReg: */
(numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
		directedSuperBindingSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendaboveClassBindingtonumArgs, numArgs, trampolineNamenumArgs("ceDirectedSuperBindingSend", numArgs), ClassReg, TempReg, ReceiverResultReg, 
/* begin numArgsOrSendNumArgsReg: */
(numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
	}
	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		superSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(
	ceSendsupertonumArgs,
	numArgs,
	trampolineNamenumArgs("ceSuperSend", numArgs),
	ClassReg,
	(	/* begin trampolineArgConstant: */
		assert(1 >= 0),
	-2 - 1),
	ReceiverResultReg,
	/* begin numArgsOrSendNumArgsReg: */
(numArgs <= (NumSendTrampolines - 2)
		? (assert(numArgs >= 0),
			-2 - numArgs)
		: SendNumArgsReg)));
	}
	firstSend = ordinarySendTrampolines[0];
	lastSend = superSendTrampolines[NumSendTrampolines - 1];
}


/*	Generate trampolines for tracing. In the simulator we can save a lot of
	time and avoid noise instructions in the lastNInstructions log by
	short-cutting these
	trampolines, but we need them in the real vm. */

	/* StackToRegisterMappingCogit>>#generateTracingTrampolines */
static void
generateTracingTrampolines(void)
{
	ceTraceLinkedSendTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceLinkedSend, "ceTraceLinkedSendTrampoline", 1, ReceiverResultReg, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceTraceBlockActivationTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceBlockActivation, "ceTraceBlockActivationTrampoline", 0, null, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceTraceStoreTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceStoreOfinto, "ceTraceStoreTrampoline", 2, TempReg, ReceiverResultReg, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
}

	/* StackToRegisterMappingCogit>>#genForwardersInlinedIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genForwardersInlinedIdenticalOrNotIf(sqInt orNot)
{
    sqInt arg;
    sqInt argConstant;
    sqInt argNeedsReg;
    sqInt argReg;
    sqInt argReg1;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt descr;
    BytecodeFixup *fixup;
    sqInt i;
    void *jumpTarget;
    void *jumpTarget1;
    AbstractInstruction *label;
    sqInt nExts;
    sqInt next;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranch;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvr;
    sqInt rcvrConstant;
    sqInt rcvrNeedsReg;
    sqInt rcvrReg;
    sqInt rcvrReg1;
    sqInt reg;
    sqInt rNext;
    sqInt rNext1;
    sqInt rTop;
    sqInt rTop1;
    CogSimStackEntry *simStackEntry;
    CogSimStackEntry *simStackEntry1;
    sqInt target;
    sqInt targetBytecodePC;
    sqInt targetBytecodePC1;
    sqInt topRegistersMask;
    sqInt unforwardArg;
    sqInt unforwardRcvr;

	arg = 0;
	descr = 0;
	next = 0;
	postBranch = 0;
	rcvr = 0;
	target = 0;
	unforwardRcvr = mayBeAForwarder(ssValue(1));
	unforwardArg = mayBeAForwarder(ssTop());
	if ((!unforwardRcvr)
	 && (!unforwardArg)) {
		return genVanillaInlinedIdenticalOrNotIf(orNot);
	}
	assert(unforwardArg
	 || (unforwardRcvr));
	simStackEntry = ssValue(1);
	/* begin isUnannotatableConstant: */
	rcvrConstant = (((simStackEntry->type)) == SSConstant)
	 && ((isImmediate((simStackEntry->constant)))
	 || (!(	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate((simStackEntry->constant)))
	 && ((oopisGreaterThan((simStackEntry->constant), classTableRootObj()))
	 || (oopisLessThan((simStackEntry->constant), nilObject()))))));
	simStackEntry1 = ssTop();
	/* begin isUnannotatableConstant: */
	argConstant = (((simStackEntry1->type)) == SSConstant)
	 && ((isImmediate((simStackEntry1->constant)))
	 || (!(	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate((simStackEntry1->constant)))
	 && ((oopisGreaterThan((simStackEntry1->constant), classTableRootObj()))
	 || (oopisLessThan((simStackEntry1->constant), nilObject()))))));
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		if (!(		/* begin isUnconditionalBranch */
			(isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC1 = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetBytecodePC = targetBytecodePC1;
	argNeedsReg = !argConstant;
	rcvrNeedsReg = !rcvrConstant;
	/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
	rNext = 0;
	rTop = 0;
	assert(argNeedsReg
	 || (rcvrNeedsReg));
	argReg1 = (rcvrReg1 = NoReg);
	if (argNeedsReg) {
		if (rcvrNeedsReg) {
			/* begin allocateRegForStackTopTwoEntriesInto: */
			topRegistersMask = 0;
			rTop1 = (rNext1 = NoReg);
			if ((registerOrNone(ssTop())) != NoReg) {
				rTop1 = registerOrNone(ssTop());
			}
			if ((registerOrNone(ssValue(1))) != NoReg) {
				reg = (rNext1 = registerOrNone(ssValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
			if (rTop1 == NoReg) {
				rTop1 = allocateRegNotConflictingWith(topRegistersMask);
			}
			if (rNext1 == NoReg) {
				rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
			}
			assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
			argReg1 = rTop1;
			rcvrReg1 = rNext1;
			popToReg(ssTop(), argReg1);
			popToReg(ssValue(1), rcvrReg1);
		}
		else {
			argReg1 = allocateRegForStackEntryAtnotConflictingWith(0, 0);
			popToReg(ssTop(), argReg1);
			if (((ssValue(1))->spilled)) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(AddCqR, BytesPerWord, SPReg);
			}
		}
	}
	else {
		assert(rcvrNeedsReg);
		assert(!((((ssTop())->spilled))));
		rcvrReg1 = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), rcvrReg1);
	}
	assert(!((argNeedsReg
 && (argReg1 == NoReg))));
	assert(!((rcvrNeedsReg
 && (rcvrReg1 == NoReg))));
	rcvrReg = rcvrReg1;
	argReg = argReg1;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argConstant, rcvrConstant, argReg, rcvrReg, orNot);
	}
	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= (simStackPtr - 2)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 2) + 1;
	}
	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (argConstant) {
		/* begin genCmpConstant:R: */
		if (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject())))) {
			annotateobjRef(genoperandoperand(CmpCwR, ((ssTop())->constant), rcvrReg), ((ssTop())->constant));
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, ((ssTop())->constant), rcvrReg);
		}
	}
	else {
		if (rcvrConstant) {
			/* begin genCmpConstant:R: */
			if (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssValue(1))->constant)))
			 && ((oopisGreaterThan(((ssValue(1))->constant), classTableRootObj()))
			 || (oopisLessThan(((ssValue(1))->constant), nilObject())))) {
				annotateobjRef(genoperandoperand(CmpCwR, ((ssValue(1))->constant), argReg), ((ssValue(1))->constant));
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(CmpCqR, ((ssValue(1))->constant), argReg);
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);
	if (notAFixup(fixupAt(nextPC))) {
		/* The next instruction is dead.  we can skip it. */
		deadCode = 1;
		ensureFixupAt(targetBytecodePC);
		ensureFixupAt(postBranchPC);
	}
	else {
		assert(!(deadCode));
	}
	if (orNot == ((branchDescriptor->isBranchTrue))) {
		/* a == b ifFalse: ... or a ~~ b ifTrue: ... jump on equal to post-branch pc */
		fixup = ensureNonMergeFixupAt(targetBytecodePC);
		jumpTarget = ensureNonMergeFixupAt(postBranchPC);
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
	}
	else {
		/* orNot is true for ~~ */
		/* a == b ifTrue: ... or a ~~ b ifFalse: ... jump on equal to target pc */
		fixup = ensureNonMergeFixupAt(postBranchPC);
		jumpTarget1 = ensureNonMergeFixupAt(targetBytecodePC);
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget1));
	}
	if (unforwardArg
	 && (unforwardRcvr)) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(argReg, TempReg, label, 0);
	}
	genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder((unforwardRcvr
		? rcvrReg
		: argReg), TempReg, label, fixup);
	if (!deadCode) {
		ssPushConstant(trueObject());
	}
	return 0;
}


/*	Generates the machine code for #== in the case where the instruction is
	not followed by a branch
 */

	/* StackToRegisterMappingCogit>>#genIdenticalNoBranchArgIsConstant:rcvrIsConstant:argReg:rcvrReg:orNotIf: */
static NoDbgRegParms sqInt
genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot)
{
    AbstractInstruction *jumpEqual;
    AbstractInstruction *jumpNotEqual;
    AbstractInstruction *label;
    sqInt resultReg;

	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrRegOrNone != NoReg));
	if (argIsConstant) {
		/* begin genCmpConstant:R: */
		if (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject())))) {
			annotateobjRef(genoperandoperand(CmpCwR, ((ssTop())->constant), rcvrRegOrNone), ((ssTop())->constant));
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, ((ssTop())->constant), rcvrRegOrNone);
		}
	}
	else {
		if (rcvrIsConstant) {
			/* begin genCmpConstant:R: */
			if (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssValue(1))->constant)))
			 && ((oopisGreaterThan(((ssValue(1))->constant), classTableRootObj()))
			 || (oopisLessThan(((ssValue(1))->constant), nilObject())))) {
				annotateobjRef(genoperandoperand(CmpCwR, ((ssValue(1))->constant), argReg), ((ssValue(1))->constant));
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(CmpCqR, ((ssValue(1))->constant), argReg);
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrRegOrNone);
		}
	}
	ssPop(2);
	resultReg = (rcvrRegOrNone == NoReg
		? argReg
		: rcvrRegOrNone);
	jumpEqual = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	if (!argIsConstant) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(argReg, TempReg, label, 0);
	}
	if (!rcvrIsConstant) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(rcvrRegOrNone, TempReg, label, 0);
	}
	if (orNot) {
		/* begin genMoveConstant:R: */
		genoperandoperand(MoveCqR, trueObject(), resultReg);
	}
	else {
		/* begin genMoveConstant:R: */
		genoperandoperand(MoveCqR, falseObject(), resultReg);
	}
	jumpNotEqual = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpEqual, (orNot
		? 
			/* begin genMoveConstant:R: */
genoperandoperand(MoveCqR, falseObject(), resultReg)
		: genoperandoperand(MoveCqR, trueObject(), resultReg)));
	jmpTarget(jumpNotEqual, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushRegister(resultReg);
	return 0;
}


/*	Decompose code generation for #== into a common constant-folding version,
	followed by a double dispatch through the objectRepresentation to a
	version that doesn't deal with forwarders and a version that does. */

	/* StackToRegisterMappingCogit>>#genInlinedIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genInlinedIdenticalOrNotIf(sqInt orNot)
{
    BytecodeDescriptor *primDescriptor;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	if ((isUnannotatableConstant(ssTop()))
	 && (isUnannotatableConstant(ssValue(1)))) {
		assert(!((primDescriptor->isMapped)));
		result = ((orNot
			? (((ssTop())->constant)) != (((ssValue(1))->constant))
			: (((ssTop())->constant)) == (((ssValue(1))->constant)))
			? trueObject()
			: falseObject());
		ssPop(2);
		return ssPushConstant(result);
	}
	return genForwardersInlinedIdenticalOrNotIf(orNot);
}

	/* StackToRegisterMappingCogit>>#genJumpBackTo: */
static NoDbgRegParms sqInt
genJumpBackTo(sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt i;
    void *jumpTarget;
    void *jumpTarget1;


	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= simStackPtr) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = simStackPtr + 1;
	}
	/* can't fall through */
	deadCode = 1;
	/* begin checkLiteral:forInstruction: */
	genoperandoperand(MoveAwR, stackLimitAddress(), TempReg);
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SPReg);
	/* begin fixupAt: */
	jumpTarget = fixupAtIndex(targetBytecodePC - initialPC);
	genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)jumpTarget));
	abstractInstruction = genoperand(Call, ceCheckForInterruptTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin annotateBytecode: */
	(abstractInstruction1->annotation = HasBytecodePC);
	/* begin fixupAt: */
	jumpTarget1 = fixupAtIndex(targetBytecodePC - initialPC);
	genoperand(Jump, ((sqInt)jumpTarget1));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genJumpIf:to: */
static NoDbgRegParms sqInt
genJumpIfto(sqInt boolean, sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    CogSimStackEntry *desc;
    sqInt eventualTarget;
    BytecodeFixup *fixup;
    sqInt i;
    void *jumpTarget;
    AbstractInstruction *ok;
    sqInt quickConstant;

	eventualTarget = eventualTargetOf(targetBytecodePC);
	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= (simStackPtr - 1)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 1)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 1))); i < simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 1) + 1;
	}
	desc = ssTop();
	ssPop(1);
	if (	/* begin stackEntryIsBoolean: */
		(((desc->type)) == SSConstant)
	 && ((((desc->constant)) == (trueObject()))
	 || (((desc->constant)) == (falseObject())))) {
		/* Must arrange there's a fixup at the target whether it is jumped to or
		   not so that the simStackPtr can be kept correct. */
		/* Must annotate the bytecode for correct pc mapping. */
		fixup = ensureFixupAt(eventualTarget);
		abstractInstruction = (((desc->constant)) == boolean
			? genoperand(Jump, ((sqInt)fixup))
			: (prevInstIsPCAnnotated()
					? gen(Nop)
					: genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		extA = 0;
		return 0;
	}
	popToReg(desc, TempReg);
	assert((objectAfter(falseObject())) == (trueObject()));
	/* begin genSubConstant:R: */
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(boolean))
	 && ((oopisGreaterThan(boolean, classTableRootObj()))
	 || (oopisLessThan(boolean, nilObject())))) {
		annotateobjRef(genoperandoperand(SubCwR, boolean, TempReg), TempReg);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, boolean, TempReg);
	}
	jumpTarget = ensureFixupAt(eventualTarget);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
	if (((extA & 1) != 0)) {
		extA = 0;
		abstractInstruction1 = lastOpcode();
		/* begin annotateBytecode: */
		(abstractInstruction1->annotation = HasBytecodePC);
		return 0;
	}
	extA = 0;
	/* begin CmpCq:R: */
	quickConstant = (boolean == (falseObject())
		? (trueObject()) - (falseObject())
		: (falseObject()) - (trueObject()));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, quickConstant, TempReg);
	ok = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genCallMustBeBooleanFor(boolean);
	jmpTarget(ok, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genJumpTo: */
static NoDbgRegParms sqInt
genJumpTo(sqInt targetBytecodePC)
{
    sqInt eventualTarget;
    BytecodeFixup *fixup;
    BytecodeDescriptor *generator;
    sqInt i;
    sqInt i1;

	eventualTarget = eventualTargetOf(targetBytecodePC);
	if ((eventualTarget > bytecodePC)
	 && ((	/* begin stackTopIsBoolean */
		(simStackPtr >= methodOrBlockNumArgs)
	 && (stackEntryIsBoolean(ssTop())))
	 && (isConditionalBranch(generator = generatorForPC(eventualTarget))))) {
		eventualTarget = (eventualTarget + ((generator->numBytes))) + ((((generator->isBranchTrue)) == ((((ssTop())->constant)) == (trueObject()))
	? ((generator->spanFunction))(generator, eventualTarget, 0, methodObj)
	: 0));
		ssPop(1);
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
		ssPop(-1);
	}
	else {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i1 <= simStackPtr; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
	}
	/* can't fall through */
	deadCode = 1;
	/* begin Jump: */
	genoperand(Jump, ((sqInt)fixup));
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeBinaryInlinePrimitive: */
static NoDbgRegParms sqInt
genLowcodeBinaryInlinePrimitive(sqInt prim)
{
    sqInt frResult;
    sqInt frResult1;
    sqInt i;
    sqInt i1;
    sqInt i11;
    sqInt i110;
    sqInt i111;
    sqInt i1110;
    sqInt i112;
    sqInt i113;
    sqInt i114;
    sqInt i115;
    sqInt i116;
    sqInt i117;
    sqInt i118;
    sqInt i119;
    sqInt i12;
    sqInt i120;
    sqInt i121;
    sqInt i122;
    sqInt i13;
    sqInt i14;
    sqInt i15;
    sqInt i16;
    sqInt i17;
    sqInt i18;
    sqInt i19;
    sqInt i2;
    sqInt i21;
    sqInt i22;
    sqInt i3;
    sqInt i4;
    sqInt i5;
    sqInt index1;
    sqInt index11;
    sqInt index110;
    sqInt index111;
    sqInt index1110;
    sqInt index112;
    sqInt index113;
    sqInt index114;
    sqInt index115;
    sqInt index116;
    sqInt index117;
    sqInt index118;
    sqInt index119;
    sqInt index12;
    sqInt index120;
    sqInt index121;
    sqInt index122;
    sqInt index13;
    sqInt index14;
    sqInt index15;
    sqInt index16;
    sqInt index17;
    sqInt index18;
    sqInt index19;
    sqInt object;
    sqInt object1;
    sqInt object10;
    sqInt object11;
    sqInt object12;
    sqInt object13;
    sqInt object14;
    sqInt object15;
    sqInt object16;
    sqInt object17;
    sqInt object18;
    sqInt object19;
    sqInt object2;
    sqInt object20;
    sqInt object21;
    sqInt object22;
    sqInt object23;
    sqInt object3;
    sqInt object4;
    sqInt object5;
    sqInt object6;
    sqInt object7;
    sqInt object8;
    sqInt objectValue;
    sqInt objectValue1;
    sqInt objectValue10;
    sqInt objectValue11;
    sqInt objectValue12;
    sqInt objectValue13;
    sqInt objectValue14;
    sqInt objectValue15;
    sqInt objectValue16;
    sqInt objectValue17;
    sqInt objectValue18;
    sqInt objectValue19;
    sqInt objectValue2;
    sqInt objectValue20;
    sqInt objectValue21;
    sqInt objectValue22;
    sqInt objectValue23;
    sqInt objectValue3;
    sqInt objectValue4;
    sqInt objectValue5;
    sqInt objectValue6;
    sqInt objectValue7;
    sqInt objectValue8;
    sqInt objectValue9;
    sqInt object9;
    sqInt pointer;
    sqInt pointerValue;
    sqInt rOopTop;
    sqInt rOopTop1;
    sqInt rOopTop10;
    sqInt rOopTop11;
    sqInt rOopTop110;
    sqInt rOopTop12;
    sqInt rOopTop13;
    sqInt rOopTop14;
    sqInt rOopTop15;
    sqInt rOopTop16;
    sqInt rOopTop17;
    sqInt rOopTop18;
    sqInt rOopTop19;
    sqInt rOopTop2;
    sqInt rOopTop20;
    sqInt rOopTop21;
    sqInt rOopTop22;
    sqInt rOopTop3;
    sqInt rOopTop4;
    sqInt rOopTop5;
    sqInt rOopTop6;
    sqInt rOopTop7;
    sqInt rOopTop8;
    sqInt rOopTop9;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult11;
    sqInt rResult12;
    sqInt rResult13;
    sqInt rResult14;
    sqInt rResult2;
    sqInt rResult3;
    sqInt rResult4;
    sqInt rResult5;
    sqInt rResult6;
    sqInt rResult7;
    sqInt rResult8;
    sqInt rResult9;
    sqInt value;
    sqInt value1;
    sqInt value10;
    sqInt value11;
    sqInt value12;
    sqInt value13;
    sqInt value14;
    sqInt value2;
    sqInt value3;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt value7;
    sqInt value8;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue10;
    sqInt valueValue11;
    sqInt valueValue12;
    sqInt valueValue13;
    sqInt valueValue14;
    sqInt valueValue2;
    sqInt valueValue3;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;
    sqInt valueValue7;
    sqInt valueValue8;
    sqInt valueValue9;
    sqInt value9;

	switch (prim) {
	case 0:
		/* begin genLowcodeByteSizeOf */
		objectValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop = NoReg;
		rResult = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index1 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i1 = index1; i1 <= (simStackPtr); i1 += 1) {
				if ((registerOrNone(simStackAt(index1))) == rOopTop) {
					goto l2;
				}
			}
	l2:;
			rOopTop = NoReg;
	l1:;
		}
		if (rOopTop == NoReg) {
			rOopTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult = allocateRegNotConflictingWith(((rOopTop < 0) ? (((usqInt)(1)) >> (-rOopTop)) : (1ULL << rOopTop)));
		assert(!(((rOopTop == NoReg)
 || (rResult == NoReg))));
		object = rOopTop;
		value = rResult;
		popToReg(ssTop(), object);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcByteSizeOfto(object, value);
		return 0;

	case 1:
		/* begin genLowcodeFirstFieldPointer */
		objectValue1 = 0;
		rOopTop1 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop1 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index11 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i11 = index11; i11 <= (simStackPtr); i11 += 1) {
				if ((registerOrNone(simStackAt(index11))) == rOopTop1) {
					goto l6;
				}
			}
	l6:;
			rOopTop1 = NoReg;
	l5:;
		}
		if (rOopTop1 == NoReg) {
			rOopTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop1 == NoReg)));
		object1 = rOopTop1;
		popToReg(ssTop(), object1);
		ssPop(1);
		genLcFirstFieldPointer(object1);
		return 0;

	case 2:
		/* begin genLowcodeFirstIndexableFieldPointer */
		objectValue2 = 0;
		rOopTop2 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop2 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index12 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i12 = index12; i12 <= (simStackPtr); i12 += 1) {
				if ((registerOrNone(simStackAt(index12))) == rOopTop2) {
					goto l9;
				}
			}
	l9:;
			rOopTop2 = NoReg;
	l8:;
		}
		if (rOopTop2 == NoReg) {
			rOopTop2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop2 == NoReg)));
		object2 = rOopTop2;
		popToReg(ssTop(), object2);
		ssPop(1);
		genLcFirstIndexableFieldPointer(object2);
		return 0;

	case 3:
		/* begin genLowcodeIsBytes */
		objectValue3 = 0;
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop3 = NoReg;
		rResult1 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop3 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index13 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i13 = index13; i13 <= (simStackPtr); i13 += 1) {
				if ((registerOrNone(simStackAt(index13))) == rOopTop3) {
					goto l12;
				}
			}
	l12:;
			rOopTop3 = NoReg;
	l11:;
		}
		if (rOopTop3 == NoReg) {
			rOopTop3 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rOopTop3 < 0) ? (((usqInt)(1)) >> (-rOopTop3)) : (1ULL << rOopTop3)));
		assert(!(((rOopTop3 == NoReg)
 || (rResult1 == NoReg))));
		object3 = rOopTop3;
		value1 = rResult1;
		popToReg(ssTop(), object3);
		ssPop(1);
		genLcIsBytesto(object3, value1);
		return 0;

	case 4:
		/* begin genLowcodeIsFloatObject */
		objectValue4 = 0;
		valueValue2 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop4 = NoReg;
		rResult2 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop4 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index14 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i14 = index14; i14 <= (simStackPtr); i14 += 1) {
				if ((registerOrNone(simStackAt(index14))) == rOopTop4) {
					goto l15;
				}
			}
	l15:;
			rOopTop4 = NoReg;
	l14:;
		}
		if (rOopTop4 == NoReg) {
			rOopTop4 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult2 = allocateRegNotConflictingWith(((rOopTop4 < 0) ? (((usqInt)(1)) >> (-rOopTop4)) : (1ULL << rOopTop4)));
		assert(!(((rOopTop4 == NoReg)
 || (rResult2 == NoReg))));
		object4 = rOopTop4;
		value2 = rResult2;
		popToReg(ssTop(), object4);
		ssPop(1);
		genLcIsFloatObjectto(object4, value2);
		return 0;

	case 5:
		/* begin genLowcodeIsIndexable */
		objectValue5 = 0;
		valueValue3 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop5 = NoReg;
		rResult3 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop5 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index15 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i15 = index15; i15 <= (simStackPtr); i15 += 1) {
				if ((registerOrNone(simStackAt(index15))) == rOopTop5) {
					goto l18;
				}
			}
	l18:;
			rOopTop5 = NoReg;
	l17:;
		}
		if (rOopTop5 == NoReg) {
			rOopTop5 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult3 = allocateRegNotConflictingWith(((rOopTop5 < 0) ? (((usqInt)(1)) >> (-rOopTop5)) : (1ULL << rOopTop5)));
		assert(!(((rOopTop5 == NoReg)
 || (rResult3 == NoReg))));
		object5 = rOopTop5;
		value3 = rResult3;
		popToReg(ssTop(), object5);
		ssPop(1);
		genLcIsIndexableto(object5, value3);
		return 0;

	case 6:
		/* begin genLowcodeIsIntegerObject */
		objectValue6 = 0;
		valueValue4 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop6 = NoReg;
		rResult4 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop6 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index16 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i16 = index16; i16 <= (simStackPtr); i16 += 1) {
				if ((registerOrNone(simStackAt(index16))) == rOopTop6) {
					goto l21;
				}
			}
	l21:;
			rOopTop6 = NoReg;
	l20:;
		}
		if (rOopTop6 == NoReg) {
			rOopTop6 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult4 = allocateRegNotConflictingWith(((rOopTop6 < 0) ? (((usqInt)(1)) >> (-rOopTop6)) : (1ULL << rOopTop6)));
		assert(!(((rOopTop6 == NoReg)
 || (rResult4 == NoReg))));
		object6 = rOopTop6;
		value4 = rResult4;
		popToReg(ssTop(), object6);
		ssPop(1);
		genLcIsIntegerObjectto(object6, value4);
		return 0;

	case 7:
		/* begin genLowcodeIsPointers */
		objectValue7 = 0;
		valueValue5 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop7 = NoReg;
		rResult5 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop7 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index17 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i17 = index17; i17 <= (simStackPtr); i17 += 1) {
				if ((registerOrNone(simStackAt(index17))) == rOopTop7) {
					goto l24;
				}
			}
	l24:;
			rOopTop7 = NoReg;
	l23:;
		}
		if (rOopTop7 == NoReg) {
			rOopTop7 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult5 = allocateRegNotConflictingWith(((rOopTop7 < 0) ? (((usqInt)(1)) >> (-rOopTop7)) : (1ULL << rOopTop7)));
		assert(!(((rOopTop7 == NoReg)
 || (rResult5 == NoReg))));
		object7 = rOopTop7;
		value5 = rResult5;
		popToReg(ssTop(), object7);
		ssPop(1);
		genLcIsPointersto(object7, value5);
		return 0;

	case 8:
		/* begin genLowcodeIsWords */
		objectValue8 = 0;
		valueValue6 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop8 = NoReg;
		rResult6 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop8 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index18 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i18 = index18; i18 <= (simStackPtr); i18 += 1) {
				if ((registerOrNone(simStackAt(index18))) == rOopTop8) {
					goto l27;
				}
			}
	l27:;
			rOopTop8 = NoReg;
	l26:;
		}
		if (rOopTop8 == NoReg) {
			rOopTop8 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult6 = allocateRegNotConflictingWith(((rOopTop8 < 0) ? (((usqInt)(1)) >> (-rOopTop8)) : (1ULL << rOopTop8)));
		assert(!(((rOopTop8 == NoReg)
 || (rResult6 == NoReg))));
		object8 = rOopTop8;
		value6 = rResult6;
		popToReg(ssTop(), object8);
		ssPop(1);
		genLcIsWordsto(object8, value6);
		return 0;

	case 9:
		/* begin genLowcodeIsWordsOrBytes */
		objectValue9 = 0;
		valueValue7 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop9 = NoReg;
		rResult7 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop9 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index19 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i19 = index19; i19 <= (simStackPtr); i19 += 1) {
				if ((registerOrNone(simStackAt(index19))) == rOopTop9) {
					goto l30;
				}
			}
	l30:;
			rOopTop9 = NoReg;
	l29:;
		}
		if (rOopTop9 == NoReg) {
			rOopTop9 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult7 = allocateRegNotConflictingWith(((rOopTop9 < 0) ? (((usqInt)(1)) >> (-rOopTop9)) : (1ULL << rOopTop9)));
		assert(!(((rOopTop9 == NoReg)
 || (rResult7 == NoReg))));
		object9 = rOopTop9;
		value7 = rResult7;
		popToReg(ssTop(), object9);
		ssPop(1);
		genLcIsWordsOrBytesto(object9, value7);
		return 0;

	case 10:
		/* begin genLowcodeOopSmallIntegerToInt32 */
		objectValue10 = 0;
		rOopTop10 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop10 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index110 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i110 = index110; i110 <= (simStackPtr); i110 += 1) {
				if ((registerOrNone(simStackAt(index110))) == rOopTop10) {
					goto l33;
				}
			}
	l33:;
			rOopTop10 = NoReg;
	l32:;
		}
		if (rOopTop10 == NoReg) {
			rOopTop10 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop10 == NoReg)));
		object10 = rOopTop10;
		popToReg(ssTop(), object10);
		ssPop(1);
		genConvertSmallIntegerToIntegerInReg(object10);
		ssPushNativeRegister(object10);
		return 0;

	case 11:
		/* begin genLowcodeOopSmallIntegerToInt64 */
		objectValue11 = 0;
		valueValue8 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop11 = NoReg;
		rResult11 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop11 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index111 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i111 = index111; i111 <= (simStackPtr); i111 += 1) {
				if ((registerOrNone(simStackAt(index111))) == rOopTop11) {
					goto l34;
				}
			}
	l34:;
			rOopTop11 = NoReg;
	l38:;
		}
		if (rOopTop11 == NoReg) {
			rOopTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult11 = allocateRegNotConflictingWith(((rOopTop11 < 0) ? (((usqInt)(1)) >> (-rOopTop11)) : (1ULL << rOopTop11)));
		assert(!(((rOopTop11 == NoReg)
 || (rResult11 == NoReg))));
		object11 = rOopTop11;
		value8 = rResult11;
		popToReg(ssTop(), object11);
		ssPop(1);
		genConvertSmallIntegerToIntegerInReg(object11);
		ssPushNativeRegister(object11);
		return 0;

	case 12:
		/* begin genLowcodeOopToBoolean32 */
		objectValue12 = 0;
		rOopTop12 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop12 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index112 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i112 = index112; i112 <= (simStackPtr); i112 += 1) {
				if ((registerOrNone(simStackAt(index112))) == rOopTop12) {
					goto l42;
				}
			}
	l42:;
			rOopTop12 = NoReg;
	l41:;
		}
		if (rOopTop12 == NoReg) {
			rOopTop12 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop12 == NoReg)));
		object12 = rOopTop12;
		popToReg(ssTop(), object12);
		ssPop(1);
		annotateobjRef(genoperandoperand(SubCwR, falseObject(), object12), falseObject());
		ssPushNativeRegister(object12);
		return 0;

	case 13:
		/* begin genLowcodeOopToBoolean64 */
		objectValue13 = 0;
		valueValue9 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop13 = NoReg;
		rResult12 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop13 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index113 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i113 = index113; i113 <= (simStackPtr); i113 += 1) {
				if ((registerOrNone(simStackAt(index113))) == rOopTop13) {
					goto l43;
				}
			}
	l43:;
			rOopTop13 = NoReg;
	l47:;
		}
		if (rOopTop13 == NoReg) {
			rOopTop13 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult12 = allocateRegNotConflictingWith(((rOopTop13 < 0) ? (((usqInt)(1)) >> (-rOopTop13)) : (1ULL << rOopTop13)));
		assert(!(((rOopTop13 == NoReg)
 || (rResult12 == NoReg))));
		object13 = rOopTop13;
		value9 = rResult12;
		popToReg(ssTop(), object13);
		ssPop(1);
		annotateobjRef(genoperandoperand(SubCwR, falseObject(), object13), falseObject());
		ssPushNativeRegister(object13);
		return 0;

	case 14:
		/* begin genLowcodeOopToFloat32 */
		objectValue14 = 0;
		valueValue10 = 0;
		/* begin allocateRegistersForLowcodeOopResultFloat: */
		rOopTop14 = NoReg;
		frResult = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop14 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index114 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i114 = index114; i114 <= (simStackPtr); i114 += 1) {
				if ((registerOrNone(simStackAt(index114))) == rOopTop14) {
					goto l51;
				}
			}
	l51:;
			rOopTop14 = NoReg;
	l50:;
		}
		if (rOopTop14 == NoReg) {
			rOopTop14 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rOopTop14 == NoReg)
 || (frResult == NoReg))));
		object14 = rOopTop14;
		value10 = frResult;
		popToReg(ssTop(), object14);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i2 <= simStackPtr; i2 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOoptoFloat32(object14, value10);
		return 0;

	case 15:
		/* begin genLowcodeOopToFloat64 */
		objectValue15 = 0;
		valueValue11 = 0;
		/* begin allocateRegistersForLowcodeOopResultFloat: */
		rOopTop15 = NoReg;
		frResult1 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop15 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index115 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i115 = index115; i115 <= (simStackPtr); i115 += 1) {
				if ((registerOrNone(simStackAt(index115))) == rOopTop15) {
					goto l54;
				}
			}
	l54:;
			rOopTop15 = NoReg;
	l53:;
		}
		if (rOopTop15 == NoReg) {
			rOopTop15 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rOopTop15 == NoReg)
 || (frResult1 == NoReg))));
		object15 = rOopTop15;
		value11 = frResult1;
		popToReg(ssTop(), object15);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i3 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i3 <= simStackPtr; i3 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i3), frameOffsetOfTemporary(i3 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOoptoFloat64(object15, value11);
		return 0;

	case 16:
		/* begin genLowcodeOopToInt32 */
		objectValue16 = 0;
		rOopTop16 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop16 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index116 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i116 = index116; i116 <= (simStackPtr); i116 += 1) {
				if ((registerOrNone(simStackAt(index116))) == rOopTop16) {
					goto l57;
				}
			}
	l57:;
			rOopTop16 = NoReg;
	l56:;
		}
		if (rOopTop16 == NoReg) {
			rOopTop16 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop16 == NoReg)));
		object16 = rOopTop16;
		popToReg(ssTop(), object16);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i4 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i4 <= simStackPtr; i4 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i4), frameOffsetOfTemporary(i4 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOopToInt32(object16);
		return 0;

	case 17:
		/* begin genLowcodeOopToInt64 */
		objectValue17 = 0;
		valueValue12 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop17 = NoReg;
		rResult13 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop17 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index117 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i117 = index117; i117 <= (simStackPtr); i117 += 1) {
				if ((registerOrNone(simStackAt(index117))) == rOopTop17) {
					goto l58;
				}
			}
	l58:;
			rOopTop17 = NoReg;
	l62:;
		}
		if (rOopTop17 == NoReg) {
			rOopTop17 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult13 = allocateRegNotConflictingWith(((rOopTop17 < 0) ? (((usqInt)(1)) >> (-rOopTop17)) : (1ULL << rOopTop17)));
		assert(!(((rOopTop17 == NoReg)
 || (rResult13 == NoReg))));
		object17 = rOopTop17;
		value12 = rResult13;
		popToReg(ssTop(), object17);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i21 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i21 <= simStackPtr; i21 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i21), frameOffsetOfTemporary(i21 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOopToInt64(object17);
		return 0;

	case 18:
		/* begin genLowcodeOopToPointer */
		objectValue18 = 0;
		rOopTop18 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop18 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index118 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i118 = index118; i118 <= (simStackPtr); i118 += 1) {
				if ((registerOrNone(simStackAt(index118))) == rOopTop18) {
					goto l66;
				}
			}
	l66:;
			rOopTop18 = NoReg;
	l65:;
		}
		if (rOopTop18 == NoReg) {
			rOopTop18 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop18 == NoReg)));
		object18 = rOopTop18;
		popToReg(ssTop(), object18);
		ssPop(1);
		genLcOopToPointer(object18);
		return 0;

	case 19:
		/* begin genLowcodeOopToPointerReinterpret */
		objectValue19 = 0;
		pointerValue = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop19 = NoReg;
		rResult8 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop19 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index119 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i119 = index119; i119 <= (simStackPtr); i119 += 1) {
				if ((registerOrNone(simStackAt(index119))) == rOopTop19) {
					goto l69;
				}
			}
	l69:;
			rOopTop19 = NoReg;
	l68:;
		}
		if (rOopTop19 == NoReg) {
			rOopTop19 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult8 = allocateRegNotConflictingWith(((rOopTop19 < 0) ? (((usqInt)(1)) >> (-rOopTop19)) : (1ULL << rOopTop19)));
		assert(!(((rOopTop19 == NoReg)
 || (rResult8 == NoReg))));
		object19 = rOopTop19;
		pointer = rResult8;
		popToReg(ssTop(), object19);
		ssPop(1);
		ssPushNativeRegister(object19);
		return 0;

	case 20:
		/* begin genLowcodeOopToUInt32 */
		objectValue20 = 0;
		valueValue13 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop20 = NoReg;
		rResult9 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop20 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index120 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i120 = index120; i120 <= (simStackPtr); i120 += 1) {
				if ((registerOrNone(simStackAt(index120))) == rOopTop20) {
					goto l72;
				}
			}
	l72:;
			rOopTop20 = NoReg;
	l71:;
		}
		if (rOopTop20 == NoReg) {
			rOopTop20 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult9 = allocateRegNotConflictingWith(((rOopTop20 < 0) ? (((usqInt)(1)) >> (-rOopTop20)) : (1ULL << rOopTop20)));
		assert(!(((rOopTop20 == NoReg)
 || (rResult9 == NoReg))));
		object20 = rOopTop20;
		value13 = rResult9;
		popToReg(ssTop(), object20);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i5 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i5 <= simStackPtr; i5 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i5), frameOffsetOfTemporary(i5 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOopToUInt32(object20);
		return 0;

	case 21:
		/* begin genLowcodeOopToUInt64 */
		objectValue21 = 0;
		valueValue14 = 0;
		/* begin allocateRegistersForLowcodeOopResultInteger: */
		rOopTop110 = NoReg;
		rResult14 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop110 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index1110 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i1110 = index1110; i1110 <= (simStackPtr); i1110 += 1) {
				if ((registerOrNone(simStackAt(index1110))) == rOopTop110) {
					goto l73;
				}
			}
	l73:;
			rOopTop110 = NoReg;
	l77:;
		}
		if (rOopTop110 == NoReg) {
			rOopTop110 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult14 = allocateRegNotConflictingWith(((rOopTop110 < 0) ? (((usqInt)(1)) >> (-rOopTop110)) : (1ULL << rOopTop110)));
		assert(!(((rOopTop110 == NoReg)
 || (rResult14 == NoReg))));
		object21 = rOopTop110;
		value14 = rResult14;
		popToReg(ssTop(), object21);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i22 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i22 <= simStackPtr; i22 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i22), frameOffsetOfTemporary(i22 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcOopToUInt64(object21);
		return 0;

	case 22:
		/* begin genLowcodePin */
		objectValue22 = 0;
		rOopTop21 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop21 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index121 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i121 = index121; i121 <= (simStackPtr); i121 += 1) {
				if ((registerOrNone(simStackAt(index121))) == rOopTop21) {
					goto l81;
				}
			}
	l81:;
			rOopTop21 = NoReg;
	l80:;
		}
		if (rOopTop21 == NoReg) {
			rOopTop21 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop21 == NoReg)));
		object22 = rOopTop21;
		popToReg(ssTop(), object22);
		ssPop(1);
		abort();
		return 0;

	case 23:
		/* begin genLowcodeUnpin */
		objectValue23 = 0;
		rOopTop22 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop22 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index122 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i122 = index122; i122 <= (simStackPtr); i122 += 1) {
				if ((registerOrNone(simStackAt(index122))) == rOopTop22) {
					goto l84;
				}
			}
	l84:;
			rOopTop22 = NoReg;
	l83:;
		}
		if (rOopTop22 == NoReg) {
			rOopTop22 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop22 == NoReg)));
		object23 = rOopTop22;
		popToReg(ssTop(), object23);
		ssPop(1);
		abort();
		return 0;

	default:
		return EncounteredUnknownBytecode;
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeNullaryInlinePrimitive: */
static NoDbgRegParms sqInt
genLowcodeNullaryInlinePrimitive(sqInt prim)
{
    AbstractInstruction *cont;
    AbstractInstruction *cont1;
    sqInt floatValue;
    sqInt floatValueValue;
    sqInt frTop;
    sqInt frTop1;
    sqInt i;
    sqInt i1;
    sqInt i11;
    sqInt i12;
    sqInt i2;
    sqInt i3;
    sqInt i4;
    AbstractInstruction *inst;
    AbstractInstruction *inst1;
    sqInt object;
    sqInt object1;
    sqInt object2;
    sqInt object3;
    sqInt object4;
    sqInt objectValue;
    sqInt objectValue1;
    sqInt objectValue2;
    sqInt objectValue3;
    sqInt objectValue4;
    sqInt pointer;
    sqInt pointer1;
    sqInt pointerClassLiteral;
    sqInt pointerValue;
    sqInt pointerValue1;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult11;
    sqInt rResult2;
    sqInt rResult3;
    sqInt rTop;
    sqInt rTop1;
    sqInt rTop11;
    sqInt rTop12;
    sqInt rTop2;
    sqInt rTop3;
    sqInt rTop4;
    sqInt rTop5;
    sqInt rTop6;
    sqInt singleFloatValue;
    sqInt singleFloatValueValue;
    AbstractInstruction *trueJump;
    AbstractInstruction *trueJump1;
    sqInt value;
    sqInt value1;
    sqInt value2;
    sqInt value3;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue2;
    sqInt valueValue3;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;

	switch (prim) {
	case 0:
		/* begin genLowcodeBoolean32ToOop */
		objectValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultOop: */
		rTop = NoReg;
		rResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult = allocateRegNotConflictingWith(((rTop < 0) ? (((usqInt)(1)) >> (-rTop)) : (1ULL << rTop)));
		assert(!(((rTop == NoReg)
 || (rResult == NoReg))));
		value = rTop;
		object = rResult;
		nativePopToReg(ssNativeTop(), value);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, value);
		/* False */
		trueJump = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		annotateobjRef(genoperandoperand(MoveCwR, falseObject(), value), falseObject());
		/* True */
		cont = genoperand(Jump, ((sqInt)0));
		inst = genoperandoperand(MoveCwR, trueObject(), value);
		jmpTarget(trueJump, inst);
		annotateobjRef(inst, trueObject());
		jmpTarget(cont, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushRegister(value);
		return 0;

	case 1:
		/* begin genLowcodeBoolean64ToOop */
		objectValue1 = 0;
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultOop: */
		rTop1 = NoReg;
		rResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
		assert(!(((rTop1 == NoReg)
 || (rResult1 == NoReg))));
		value1 = rTop1;
		object1 = rResult1;
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, 0, value1);
		/* False */
		trueJump1 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		annotateobjRef(genoperandoperand(MoveCwR, falseObject(), value1), falseObject());
		/* True */
		cont1 = genoperand(Jump, ((sqInt)0));
		inst1 = genoperandoperand(MoveCwR, trueObject(), value1);
		jmpTarget(trueJump1, inst1);
		annotateobjRef(inst1, trueObject());
		jmpTarget(cont1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushRegister(value1);
		return 0;

	case 2:
		/* begin genLowcodeFloat32ToOop */
		objectValue2 = 0;
		singleFloatValueValue = 0;
		/* begin allocateRegistersForLowcodeFloatResultOop: */
		frTop = NoReg;
		/* Float argument */
		rResult2 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop == NoReg) {
			frTop = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop == NoReg)
 || (rResult2 == NoReg))));
		singleFloatValue = frTop;
		object2 = rResult2;
		nativePopToReg(ssNativeTop(), singleFloatValue);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcFloat32toOop(singleFloatValue, object2);
		return 0;

	case 3:
		/* begin genLowcodeFloat64ToOop */
		floatValueValue = 0;
		objectValue3 = 0;
		/* begin allocateRegistersForLowcodeFloatResultOop: */
		frTop1 = NoReg;
		/* Float argument */
		rResult3 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop1 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop1 == NoReg) {
			frTop1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult3 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop1 == NoReg)
 || (rResult3 == NoReg))));
		floatValue = frTop1;
		object3 = rResult3;
		nativePopToReg(ssNativeTop(), floatValue);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i1 <= simStackPtr; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcFloat64toOop(floatValue, object3);
		return 0;

	case 4:
		/* begin genLowcodeInt32ToOop */
		valueValue2 = 0;
		rTop2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop2 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop2 == NoReg) {
			rTop2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop2 == NoReg)));
		value2 = rTop2;
		nativePopToReg(ssNativeTop(), value2);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i2 <= simStackPtr; i2 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcInt32ToOop(value2);
		return 0;

	case 5:
		/* begin genLowcodeInt64ToOop */
		valueValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop11 == NoReg)));
		value3 = rTop11;
		nativePopToReg(ssNativeTop(), value3);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i11 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i11 <= simStackPtr; i11 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i11), frameOffsetOfTemporary(i11 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcInt64ToOop(value3);
		return 0;

	case 6:
		/* begin genLowcodePointerToOop */
		pointerValue = 0;
		pointerClassLiteral = getLiteral(extA);
		/* begin allocateRegistersForLowcodeInteger: */
		rTop3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop3 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop3 == NoReg) {
			rTop3 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop3 == NoReg)));
		pointer = rTop3;
		nativePopToReg(ssNativeTop(), pointer);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i3 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i3 <= simStackPtr; i3 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i3), frameOffsetOfTemporary(i3 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcPointerToOopclass(pointer, pointerClassLiteral);
		extA = 0;
		return 0;

	case 7:
		/* begin genLowcodePointerToOopReinterprer */
		pointerValue1 = 0;
		rTop4 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop4 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop4 == NoReg) {
			rTop4 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop4 == NoReg)));
		pointer1 = rTop4;
		nativePopToReg(ssNativeTop(), pointer1);
		ssNativePop(1);
		ssPushRegister(pointer1);
		return 0;

	case 8:
		/* begin genLowcodeSmallInt32ToOop */
		valueValue4 = 0;
		rTop5 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop5 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop5 == NoReg) {
			rTop5 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop5 == NoReg)));
		value4 = rTop5;
		nativePopToReg(ssNativeTop(), value4);
		ssNativePop(1);
		genConvertIntegerToSmallIntegerInReg(value4);
		ssPushRegister(value4);
		return 0;

	case 9:
		/* begin genLowcodeUint32ToOop */
		valueValue5 = 0;
		rTop6 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop6 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop6 == NoReg) {
			rTop6 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop6 == NoReg)));
		value5 = rTop6;
		nativePopToReg(ssNativeTop(), value5);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i4 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i4 <= simStackPtr; i4 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i4), frameOffsetOfTemporary(i4 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcUInt32ToOop(value5);
		return 0;

	case 10:
		/* begin genLowcodeUint64ToOop */
		objectValue4 = 0;
		valueValue6 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultOop: */
		rTop12 = NoReg;
		rResult11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop12 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop12 == NoReg) {
			rTop12 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult11 = allocateRegNotConflictingWith(((rTop12 < 0) ? (((usqInt)(1)) >> (-rTop12)) : (1ULL << rTop12)));
		assert(!(((rTop12 == NoReg)
 || (rResult11 == NoReg))));
		value6 = rTop12;
		object4 = rResult11;
		nativePopToReg(ssNativeTop(), value6);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i12 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i12 <= simStackPtr; i12 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i12), frameOffsetOfTemporary(i12 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcUInt64ToOop(value6);
		return 0;

	default:
		return EncounteredUnknownBytecode;
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeTrinaryInlinePrimitive: */
static NoDbgRegParms sqInt
genLowcodeTrinaryInlinePrimitive(sqInt prim)
{
    AbstractInstruction *contJump;
    AbstractInstruction *contJump1;
    AbstractInstruction *falseJump;
    AbstractInstruction *falseJump1;
    sqInt fieldIndex;
    sqInt fieldIndex1;
    sqInt fieldIndexValue;
    sqInt first;
    sqInt first1;
    sqInt firstValue;
    sqInt firstValue1;
    sqInt i1;
    sqInt i11;
    sqInt i12;
    sqInt index1;
    sqInt index11;
    sqInt index12;
    sqInt object;
    sqInt object1;
    sqInt objectValue;
    sqInt objectValue1;
    sqInt oopTopRegisterMask;
    sqInt oopTopRegisterMask1;
    sqInt oopTopRegisterMask2;
    sqInt oopTopRegisterMask3;
    sqInt rOopNext;
    sqInt rOopNext1;
    sqInt rOopNext2;
    sqInt rOopNext3;
    sqInt rOopTop;
    sqInt rOopTop1;
    sqInt rOopTop2;
    sqInt rOopTop3;
    sqInt rResult;
    sqInt rTop;
    sqInt second;
    sqInt second1;
    sqInt secondValue;
    sqInt secondValue1;
    sqInt topRegisterMask;
    sqInt value;
    sqInt value1;
    sqInt value2;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue2;

	switch (prim) {
	case 0:
		/* begin genLowcodeOopEqual */
		firstValue = 0;
		secondValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeOop2ResultInteger: */
		rOopTop = (rOopNext = NoReg);
		rResult = NoReg;
		oopTopRegisterMask = 0;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index1 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i1 = index1; i1 <= (simStackPtr); i1 += 1) {
				if ((registerOrNone(simStackAt(index1))) == rOopTop) {
					goto l2;
				}
			}
	l2:;
			rOopTop = NoReg;
	l1:;
		}
		if ((registerOrNone(ssValue(1))) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopNext = registerOrNone(ssValue(1));
			/* begin isOopRegister:usedBefore: */
			index1 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i1 = index1; i1 < simStackPtr; i1 += 1) {
				if ((registerOrNone(simStackAt(index1))) == rOopNext) {
					goto l4;
				}
			}
	l4:;
			rOopNext = NoReg;
	l3:;
			if (rOopNext != NoReg) {
				oopTopRegisterMask = ((rOopNext < 0) ? (((usqInt)(1)) >> (-rOopNext)) : (1ULL << rOopNext));
			}
		}
		if (rOopTop == NoReg) {
			rOopTop = allocateRegNotConflictingWith(oopTopRegisterMask);
		}
		if (rOopNext == NoReg) {
			rOopNext = allocateRegNotConflictingWith(((rOopTop < 0) ? (((usqInt)(1)) >> (-rOopTop)) : (1ULL << rOopTop)));
		}
		rResult = allocateRegNotConflictingWith((1ULL << rOopTop) | (1ULL << rOopNext));
		assert(!(((rOopTop == NoReg)
 || ((rOopNext == NoReg)
 || (rResult == NoReg)))));
		second = rOopTop;
		first = rOopNext;
		value = rResult;
		popToReg(ssTop(), second);
		ssPop(1);
		popToReg(ssTop(), first);
		ssPop(1);
		/* begin CmpR:R: */
		assert(!((second == SPReg)));
		genoperandoperand(CmpRR, second, first);
		/* True result */
		falseJump = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first);
		/* False result */
		contJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first);
		jmpTarget(contJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first);
		return 0;

	case 1:
		/* begin genLowcodeOopNotEqual */
		firstValue1 = 0;
		secondValue1 = 0;
		/* begin allocateRegistersForLowcodeOop2: */
		rOopTop1 = (rOopNext1 = NoReg);
		oopTopRegisterMask1 = 0;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop1 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index11 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i11 = index11; i11 <= (simStackPtr); i11 += 1) {
				if ((registerOrNone(simStackAt(index11))) == rOopTop1) {
					goto l10;
				}
			}
	l10:;
			rOopTop1 = NoReg;
	l8:;
		}
		if ((registerOrNone(ssValue(1))) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopNext1 = registerOrNone(ssValue(1));
			/* begin isOopRegister:usedBefore: */
			index11 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i11 = index11; i11 < simStackPtr; i11 += 1) {
				if ((registerOrNone(simStackAt(index11))) == rOopNext1) {
					goto l9;
				}
			}
	l9:;
			rOopNext1 = NoReg;
	l7:;
			if (rOopNext1 != NoReg) {
				oopTopRegisterMask1 = ((rOopNext1 < 0) ? (((usqInt)(1)) >> (-rOopNext1)) : (1ULL << rOopNext1));
			}
		}
		if (rOopTop1 == NoReg) {
			rOopTop1 = allocateRegNotConflictingWith(oopTopRegisterMask1);
		}
		if (rOopNext1 == NoReg) {
			rOopNext1 = allocateRegNotConflictingWith(((rOopTop1 < 0) ? (((usqInt)(1)) >> (-rOopTop1)) : (1ULL << rOopTop1)));
		}
		assert(!(((rOopTop1 == NoReg)
 || (rOopNext1 == NoReg))));
		second1 = rOopTop1;
		first1 = rOopNext1;
		popToReg(ssTop(), second1);
		ssPop(1);
		popToReg(ssTop(), first1);
		ssPop(1);
		/* begin CmpR:R: */
		assert(!((second1 == SPReg)));
		genoperandoperand(CmpRR, second1, first1);
		/* True result */
		falseJump1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first1);
		/* False result */
		contJump1 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first1);
		jmpTarget(contJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first1);
		return 0;

	case 2:
		/* begin genLowcodeStoreObjectField */
		objectValue = 0;
		valueValue1 = 0;
		fieldIndex = extA;
		/* begin allocateRegistersForLowcodeOop2: */
		rOopTop2 = (rOopNext2 = NoReg);
		oopTopRegisterMask2 = 0;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop2 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index12 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i12 = index12; i12 <= (simStackPtr); i12 += 1) {
				if ((registerOrNone(simStackAt(index12))) == rOopTop2) {
					goto l15;
				}
			}
	l15:;
			rOopTop2 = NoReg;
	l13:;
		}
		if ((registerOrNone(ssValue(1))) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopNext2 = registerOrNone(ssValue(1));
			/* begin isOopRegister:usedBefore: */
			index12 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i12 = index12; i12 < simStackPtr; i12 += 1) {
				if ((registerOrNone(simStackAt(index12))) == rOopNext2) {
					goto l14;
				}
			}
	l14:;
			rOopNext2 = NoReg;
	l12:;
			if (rOopNext2 != NoReg) {
				oopTopRegisterMask2 = ((rOopNext2 < 0) ? (((usqInt)(1)) >> (-rOopNext2)) : (1ULL << rOopNext2));
			}
		}
		if (rOopTop2 == NoReg) {
			rOopTop2 = allocateRegNotConflictingWith(oopTopRegisterMask2);
		}
		if (rOopNext2 == NoReg) {
			rOopNext2 = allocateRegNotConflictingWith(((rOopTop2 < 0) ? (((usqInt)(1)) >> (-rOopTop2)) : (1ULL << rOopTop2)));
		}
		assert(!(((rOopTop2 == NoReg)
 || (rOopNext2 == NoReg))));
		value1 = rOopTop2;
		object = rOopNext2;
		popToReg(ssTop(), value1);
		ssPop(1);
		popToReg(ssTop(), object);
		ssPop(1);
		genLcStoreobjectfield(value1, object, fieldIndex);
		extA = 0;
		return 0;

	case 3:
		/* begin genLowcodeStoreObjectFieldAt */
		fieldIndexValue = 0;
		objectValue1 = 0;
		valueValue2 = 0;
		/* begin allocateRegistersForLowcodeIntegerOop2: */
		rTop = (rOopTop3 = (rOopNext3 = NoReg));
		oopTopRegisterMask3 = (topRegisterMask = 0);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
			oopTopRegisterMask3 = ((rTop < 0) ? (((usqInt)(1)) >> (-rTop)) : (1ULL << rTop));
		}
		if ((registerOrNone(ssTop())) != NoReg) {
			rOopTop3 = registerOrNone(ssTop());
			topRegisterMask = ((rOopTop3 < 0) ? (((usqInt)(1)) >> (-rOopTop3)) : (1ULL << rOopTop3));
		}
		if ((registerOrNone(ssValue(1))) != NoReg) {
			rOopNext3 = registerOrNone(ssValue(1));
			topRegisterMask = topRegisterMask | (((rOopNext3 < 0) ? (((usqInt)(1)) >> (-rOopNext3)) : (1ULL << rOopNext3)));
			oopTopRegisterMask3 = oopTopRegisterMask3 | (((rOopNext3 < 0) ? (((usqInt)(1)) >> (-rOopNext3)) : (1ULL << rOopNext3)));
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(topRegisterMask);
		}
		if (rOopTop3 == NoReg) {
			rOopTop3 = allocateRegNotConflictingWith(oopTopRegisterMask3);
		}
		if (rOopNext3 == NoReg) {
			rOopNext3 = allocateRegNotConflictingWith((1ULL << rTop) | (1ULL << rOopTop3));
		}
		assert(!(((rTop == NoReg)
 || ((rOopTop3 == NoReg)
 || (rOopNext3 == NoReg)))));
		fieldIndex1 = rTop;
		value2 = rOopTop3;
		object1 = rOopNext3;
		popToReg(ssTop(), value2);
		ssPop(1);
		nativePopToReg(ssNativeTop(), fieldIndex1);
		ssNativePop(1);
		popToReg(ssTop(), object1);
		ssPop(1);
		genLcStoreobjectat(value2, object1, fieldIndex1);
		return 0;

	default:
		return EncounteredUnknownBytecode;
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeUnaryInlinePrimitive2: */
static NoDbgRegParms sqInt
genLowcodeUnaryInlinePrimitive2(sqInt prim)
{
    AbstractInstruction *abstractInstruction;
    sqInt baseOffset;
    sqInt baseOffset1;
    sqInt baseOffset10;
    sqInt baseOffset11;
    sqInt baseOffset12;
    sqInt baseOffset13;
    sqInt baseOffset14;
    sqInt baseOffset15;
    sqInt baseOffset16;
    sqInt baseOffset17;
    sqInt baseOffset18;
    sqInt baseOffset19;
    sqInt baseOffset2;
    sqInt baseOffset20;
    sqInt baseOffset21;
    sqInt baseOffset22;
    sqInt baseOffset23;
    sqInt baseOffset3;
    sqInt baseOffset4;
    sqInt baseOffset5;
    sqInt baseOffset6;
    sqInt baseOffset7;
    sqInt baseOffset8;
    sqInt baseOffset9;
    sqInt classOop;
    sqInt classOop1;
    sqInt classOop2;
    sqInt classOopValue;
    sqInt classOopValue1;
    sqInt classOopValue2;
    AbstractInstruction *contJump;
    AbstractInstruction *contJump1;
    AbstractInstruction *contJump2;
    AbstractInstruction *contJump3;
    AbstractInstruction *contJump4;
    AbstractInstruction *contJump5;
    AbstractInstruction *contJump6;
    AbstractInstruction *contJump7;
    sqInt doubleValue;
    sqInt doubleValue1;
    sqInt doubleValueValue;
    sqInt doubleValueValue1;
    AbstractInstruction *falseJump;
    AbstractInstruction *falseJump1;
    AbstractInstruction *falseJump2;
    AbstractInstruction *falseJump3;
    AbstractInstruction *falseJump4;
    AbstractInstruction *falseJump5;
    AbstractInstruction *falseJump6;
    AbstractInstruction *falseJump7;
    sqInt fieldIndex;
    sqInt fieldIndexValue;
    sqInt first;
    sqInt first1;
    sqInt first10;
    sqInt first11;
    sqInt first2;
    sqInt first3;
    sqInt first4;
    sqInt first5;
    sqInt first6;
    sqInt first7;
    sqInt first8;
    sqInt firstValue;
    sqInt firstValue1;
    sqInt firstValue10;
    sqInt firstValue11;
    sqInt firstValue2;
    sqInt firstValue3;
    sqInt firstValue4;
    sqInt firstValue5;
    sqInt firstValue6;
    sqInt firstValue7;
    sqInt firstValue8;
    sqInt firstValue9;
    sqInt first9;
    sqInt floatValue;
    sqInt floatValue1;
    sqInt floatValue2;
    sqInt floatValue3;
    sqInt floatValue4;
    sqInt floatValue5;
    sqInt floatValue6;
    sqInt floatValueValue;
    sqInt floatValueValue1;
    sqInt floatValueValue2;
    sqInt floatValueValue3;
    sqInt floatValueValue4;
    sqInt floatValueValue5;
    sqInt floatValueValue6;
    sqInt frResult;
    sqInt frResult1;
    sqInt frResult2;
    sqInt frResult3;
    sqInt frResult4;
    sqInt frResult5;
    sqInt frResult6;
    sqInt frResult7;
    sqInt frResult8;
    sqInt frResult9;
    sqInt frTop;
    sqInt frTop1;
    sqInt frTop2;
    sqInt frTop3;
    sqInt frTop4;
    sqInt i;
    sqInt i1;
    sqInt i11;
    sqInt i12;
    sqInt i2;
    sqInt i3;
    sqInt index1;
    sqInt index11;
    sqInt indexableSize;
    sqInt indexableSize1;
    sqInt indexableSizeValue;
    sqInt int32Result;
    sqInt int32ResultValue;
    sqInt int64Result;
    sqInt int64Result1;
    sqInt int64Result2;
    sqInt int64ResultValue;
    sqInt int64ResultValue1;
    sqInt int64ResultValue2;
    sqInt object;
    sqInt object1;
    sqInt object2;
    sqInt objectValue;
    sqInt objectValue1;
    sqInt objectValue2;
    sqInt pointer;
    sqInt pointer1;
    sqInt pointer2;
    sqInt pointer3;
    sqInt pointer4;
    sqInt pointer5;
    sqInt pointer6;
    sqInt pointer7;
    sqInt pointer8;
    sqInt pointerResult;
    sqInt pointerResult1;
    sqInt pointerResultValue;
    sqInt pointerResultValue1;
    sqInt pointerValue;
    sqInt pointerValue1;
    sqInt pointerValue2;
    sqInt pointerValue3;
    sqInt pointerValue4;
    sqInt pointerValue5;
    sqInt pointerValue6;
    sqInt pointerValue7;
    sqInt pointerValue8;
    sqInt reg;
    sqInt reg1;
    sqInt reg10;
    sqInt reg11;
    sqInt reg12;
    sqInt reg13;
    sqInt reg2;
    sqInt reg3;
    sqInt reg4;
    sqInt reg5;
    sqInt reg6;
    sqInt reg7;
    sqInt reg8;
    sqInt reg9;
    sqInt result;
    sqInt result1;
    sqInt result2;
    sqInt result3;
    sqInt result4;
    sqInt resultValue;
    sqInt resultValue1;
    sqInt resultValue2;
    sqInt resultValue3;
    sqInt resultValue4;
    sqInt rNext;
    sqInt rNext1;
    sqInt rNext10;
    sqInt rNext11;
    sqInt rNext12;
    sqInt rNext13;
    sqInt rNext2;
    sqInt rNext3;
    sqInt rNext4;
    sqInt rNext5;
    sqInt rNext6;
    sqInt rNext7;
    sqInt rNext8;
    sqInt rNext9;
    sqInt rOopResult;
    sqInt rOopTop;
    sqInt rOopTop1;
    sqInt rOopTop2;
    sqInt rOopTop3;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult10;
    sqInt rResult11;
    sqInt rResult110;
    sqInt rResult111;
    sqInt rResult112;
    sqInt rResult12;
    sqInt rResult13;
    sqInt rResult14;
    sqInt rResult15;
    sqInt rResult16;
    sqInt rResult17;
    sqInt rResult18;
    sqInt rResult19;
    sqInt rResult2;
    sqInt rResult20;
    sqInt rResult21;
    sqInt rResult22;
    sqInt rResult23;
    sqInt rResult24;
    sqInt rResult25;
    sqInt rResult26;
    sqInt rResult27;
    sqInt rResult28;
    sqInt rResult29;
    sqInt rResult3;
    sqInt rResult30;
    sqInt rResult4;
    sqInt rResult5;
    sqInt rResult6;
    sqInt rResult7;
    sqInt rResult8;
    sqInt rResult9;
    sqInt rTop;
    sqInt rTop1;
    sqInt rTop10;
    sqInt rTop11;
    sqInt rTop110;
    sqInt rTop12;
    sqInt rTop13;
    sqInt rTop14;
    sqInt rTop15;
    sqInt rTop16;
    sqInt rTop17;
    sqInt rTop18;
    sqInt rTop19;
    sqInt rTop2;
    sqInt rTop20;
    sqInt rTop21;
    sqInt rTop22;
    sqInt rTop23;
    sqInt rTop24;
    sqInt rTop25;
    sqInt rTop26;
    sqInt rTop27;
    sqInt rTop3;
    sqInt rTop4;
    sqInt rTop5;
    sqInt rTop6;
    sqInt rTop7;
    sqInt rTop8;
    sqInt rTop9;
    sqInt second;
    sqInt second1;
    sqInt second10;
    sqInt second11;
    sqInt second2;
    sqInt second3;
    sqInt second4;
    sqInt second5;
    sqInt second6;
    sqInt second7;
    sqInt second8;
    sqInt secondValue;
    sqInt secondValue1;
    sqInt secondValue10;
    sqInt secondValue11;
    sqInt secondValue2;
    sqInt secondValue3;
    sqInt secondValue4;
    sqInt secondValue5;
    sqInt secondValue6;
    sqInt secondValue7;
    sqInt secondValue8;
    sqInt secondValue9;
    sqInt second9;
    sqInt shiftAmount;
    sqInt shiftAmount1;
    sqInt shiftAmountValue;
    sqInt shiftAmountValue1;
    sqInt topRegisterMask;
    sqInt topRegisterMask1;
    sqInt topRegistersMask;
    sqInt topRegistersMask1;
    sqInt topRegistersMask10;
    sqInt topRegistersMask11;
    sqInt topRegistersMask12;
    sqInt topRegistersMask13;
    sqInt topRegistersMask14;
    sqInt topRegistersMask2;
    sqInt topRegistersMask3;
    sqInt topRegistersMask4;
    sqInt topRegistersMask5;
    sqInt topRegistersMask6;
    sqInt topRegistersMask7;
    sqInt topRegistersMask8;
    sqInt topRegistersMask9;
    sqInt value;
    sqInt value1;
    sqInt value10;
    sqInt value11;
    sqInt value12;
    sqInt value13;
    sqInt value14;
    sqInt value15;
    sqInt value16;
    sqInt value17;
    sqInt value18;
    sqInt value19;
    sqInt value2;
    sqInt value20;
    sqInt value21;
    sqInt value22;
    sqInt value23;
    sqInt value24;
    sqInt value25;
    sqInt value26;
    sqInt value27;
    sqInt value28;
    sqInt value29;
    sqInt value3;
    sqInt value30;
    sqInt value31;
    sqInt value32;
    sqInt value33;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt value7;
    sqInt value8;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue10;
    sqInt valueValue11;
    sqInt valueValue12;
    sqInt valueValue13;
    sqInt valueValue14;
    sqInt valueValue15;
    sqInt valueValue16;
    sqInt valueValue17;
    sqInt valueValue18;
    sqInt valueValue19;
    sqInt valueValue2;
    sqInt valueValue20;
    sqInt valueValue21;
    sqInt valueValue22;
    sqInt valueValue23;
    sqInt valueValue24;
    sqInt valueValue25;
    sqInt valueValue26;
    sqInt valueValue27;
    sqInt valueValue28;
    sqInt valueValue29;
    sqInt valueValue3;
    sqInt valueValue30;
    sqInt valueValue31;
    sqInt valueValue32;
    sqInt valueValue33;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;
    sqInt valueValue7;
    sqInt valueValue8;
    sqInt valueValue9;
    sqInt value9;

	switch (prim) {
	case 60:
		/* begin genLowcodeFloat64ToFloat32 */
		floatValueValue = 0;
		topRegistersMask = 0;
		frTop = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop == NoReg) {
			frTop = allocateFloatRegNotConflictingWith(topRegistersMask);
		}
		assert(!((frTop == NoReg)));
		floatValue = frTop;
		nativePopToReg(ssNativeTop(), floatValue);
		ssNativePop(1);
		/* begin ConvertRd:Rs: */
		genoperandoperand(ConvertRdRs, floatValue, floatValue);
		ssPushNativeRegisterSingleFloat(floatValue);
		return 0;

	case 61:
		/* begin genLowcodeFloat64ToInt32 */
		floatValueValue1 = 0;
		int32ResultValue = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop1 = NoReg;
		/* Float argument */
		rResult = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop1 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop1 == NoReg) {
			frTop1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop1 == NoReg)
 || (rResult == NoReg))));
		floatValue1 = frTop1;
		int32Result = rResult;
		nativePopToReg(ssNativeTop(), floatValue1);
		ssNativePop(1);
		/* begin ConvertRd:R: */
		genoperandoperand(ConvertRdR, floatValue1, int32Result);
		ssPushNativeRegister(int32Result);
		return 0;

	case 0x3E:
		/* begin genLowcodeFloat64ToInt64 */
		floatValueValue2 = 0;
		int64ResultValue = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop2 = NoReg;
		/* Float argument */
		rResult1 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop2 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop2 == NoReg) {
			frTop2 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop2 == NoReg)
 || (rResult1 == NoReg))));
		floatValue2 = frTop2;
		int64Result = rResult1;
		nativePopToReg(ssNativeTop(), floatValue2);
		ssNativePop(1);
		abort();
		return 0;

	case 0x3F:
		/* begin genLowcodeFloat64ToUInt32 */
		floatValueValue3 = 0;
		int64ResultValue1 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop3 = NoReg;
		/* Float argument */
		rResult2 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop3 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop3 == NoReg) {
			frTop3 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop3 == NoReg)
 || (rResult2 == NoReg))));
		floatValue3 = frTop3;
		int64Result1 = rResult2;
		nativePopToReg(ssNativeTop(), floatValue3);
		ssNativePop(1);
		/* begin ConvertRd:R: */
		genoperandoperand(ConvertRdR, floatValue3, int64Result1);
		ssPushNativeRegister(int64Result1);
		return 0;

	case 64:
		/* begin genLowcodeFloat64ToUInt64 */
		floatValueValue4 = 0;
		int64ResultValue2 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop4 = NoReg;
		/* Float argument */
		rResult3 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop4 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop4 == NoReg) {
			frTop4 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult3 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop4 == NoReg)
 || (rResult3 == NoReg))));
		floatValue4 = frTop4;
		int64Result2 = rResult3;
		nativePopToReg(ssNativeTop(), floatValue4);
		ssNativePop(1);
		abort();
		return 0;

	case 65:
		/* begin genLowcodeFree */
		pointerValue = 0;
		rTop = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop == NoReg)));
		pointer = rTop;
		nativePopToReg(ssNativeTop(), pointer);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		if (pointer != ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, pointer, ReceiverResultReg);
		}
		abstractInstruction = genoperand(Call, ceFreeTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		return 0;

	case 66:
		/* begin genLowcodeInstantiateIndexable32Oop */
		classOopValue = 0;
		indexableSizeValue = 0;
		objectValue = 0;
		/* begin allocateRegistersForLowcodeIntegerOopResultOop: */
		rTop1 = (rOopTop = NoReg);
		rOopResult = NoReg;
		topRegisterMask = 0;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if ((registerOrNone(ssTop())) != NoReg) {
			rOopTop = registerOrNone(ssTop());
			topRegisterMask = ((rOopTop < 0) ? (((usqInt)(1)) >> (-rOopTop)) : (1ULL << rOopTop));
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(topRegisterMask);
		}
		if (rOopTop == NoReg) {
			rOopTop = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
		}
		rOopResult = allocateRegNotConflictingWith((1ULL << rTop1) | (1ULL << rOopTop));
		assert(!(((rTop1 == NoReg)
 || ((rOopTop == NoReg)
 || (rOopResult == NoReg)))));
		indexableSize = rTop1;
		classOop = rOopTop;
		object = rOopResult;
		nativePopToReg(ssNativeTop(), indexableSize);
		ssNativePop(1);
		popToReg(ssTop(), classOop);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i1 <= simStackPtr; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcInstantiateOopindexableSize(classOop, indexableSize);
		return 0;

	case 67:
		/* begin genLowcodeInstantiateIndexableOop */
		classOopValue1 = 0;
		objectValue1 = 0;
		indexableSize1 = extA;
		/* begin allocateRegistersForLowcodeOopResultOop: */
		rOopTop1 = NoReg;
		rResult4 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop1 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index1 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i11 = index1; i11 <= (simStackPtr); i11 += 1) {
				if ((registerOrNone(simStackAt(index1))) == rOopTop1) {
					goto l11;
				}
			}
	l11:;
			rOopTop1 = NoReg;
	l10:;
		}
		if (rOopTop1 == NoReg) {
			rOopTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult4 = allocateRegNotConflictingWith(((rOopTop1 < 0) ? (((usqInt)(1)) >> (-rOopTop1)) : (1ULL << rOopTop1)));
		assert(!(((rOopTop1 == NoReg)
 || (rResult4 == NoReg))));
		classOop1 = rOopTop1;
		object1 = rResult4;
		popToReg(ssTop(), classOop1);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i2 <= simStackPtr; i2 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcInstantiateOopconstantIndexableSize(classOop1, indexableSize1);
		extA = 0;
		return 0;

	case 68:
		/* begin genLowcodeInstantiateOop */
		classOopValue2 = 0;
		rOopTop2 = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop2 = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index11 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i12 = index11; i12 <= (simStackPtr); i12 += 1) {
				if ((registerOrNone(simStackAt(index11))) == rOopTop2) {
					goto l14;
				}
			}
	l14:;
			rOopTop2 = NoReg;
	l13:;
		}
		if (rOopTop2 == NoReg) {
			rOopTop2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop2 == NoReg)));
		classOop2 = rOopTop2;
		popToReg(ssTop(), classOop2);
		ssPop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i3 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i3 <= simStackPtr; i3 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i3), frameOffsetOfTemporary(i3 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genLcInstantiateOop(classOop2);
		return 0;

	case 69:
		/* begin genLowcodeInt32Equal */
		firstValue = 0;
		secondValue = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask1 = 0;
		rTop2 = (rNext = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop2 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg = (rNext = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask1 = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
		}
		if (rTop2 == NoReg) {
			rTop2 = allocateRegNotConflictingWith(topRegistersMask1);
		}
		if (rNext == NoReg) {
			rNext = allocateRegNotConflictingWith(((rTop2 < 0) ? (((usqInt)(1)) >> (-rTop2)) : (1ULL << rTop2)));
		}
		assert(!(((rTop2 == NoReg)
 || (rNext == NoReg))));
		second = rTop2;
		first = rNext;
		nativePopToReg(ssNativeTop(), second);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second == SPReg)));
		genoperandoperand(CmpRR, second, first);
		/* True result */
		falseJump = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first);
		/* False result */
		contJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first);
		jmpTarget(contJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first);
		return 0;

	case 70:
		/* begin genLowcodeInt32Great */
		firstValue1 = 0;
		secondValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask2 = 0;
		rTop3 = (rNext1 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop3 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext1 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg1 = (rNext1 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask2 = ((reg1 < 0) ? (((usqInt)(1)) >> (-reg1)) : (1ULL << reg1));
			}
		}
		if (rTop3 == NoReg) {
			rTop3 = allocateRegNotConflictingWith(topRegistersMask2);
		}
		if (rNext1 == NoReg) {
			rNext1 = allocateRegNotConflictingWith(((rTop3 < 0) ? (((usqInt)(1)) >> (-rTop3)) : (1ULL << rTop3)));
		}
		assert(!(((rTop3 == NoReg)
 || (rNext1 == NoReg))));
		second1 = rTop3;
		first1 = rNext1;
		nativePopToReg(ssNativeTop(), second1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first1);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second1 == SPReg)));
		genoperandoperand(CmpRR, second1, first1);
		/* True result */
		falseJump1 = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first1);
		/* False result */
		contJump1 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first1);
		jmpTarget(contJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first1);
		return 0;

	case 71:
		/* begin genLowcodeInt32GreatEqual */
		firstValue2 = 0;
		secondValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask3 = 0;
		rTop4 = (rNext2 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop4 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext2 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext2 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg2 = (rNext2 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask3 = ((reg2 < 0) ? (((usqInt)(1)) >> (-reg2)) : (1ULL << reg2));
			}
		}
		if (rTop4 == NoReg) {
			rTop4 = allocateRegNotConflictingWith(topRegistersMask3);
		}
		if (rNext2 == NoReg) {
			rNext2 = allocateRegNotConflictingWith(((rTop4 < 0) ? (((usqInt)(1)) >> (-rTop4)) : (1ULL << rTop4)));
		}
		assert(!(((rTop4 == NoReg)
 || (rNext2 == NoReg))));
		second2 = rTop4;
		first2 = rNext2;
		nativePopToReg(ssNativeTop(), second2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first2);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second2 == SPReg)));
		genoperandoperand(CmpRR, second2, first2);
		/* True result */
		falseJump2 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first2);
		/* False result */
		contJump2 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first2);
		jmpTarget(contJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first2);
		return 0;

	case 72:
		/* begin genLowcodeInt32Less */
		firstValue3 = 0;
		secondValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask4 = 0;
		rTop5 = (rNext3 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop5 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext3 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext3 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg3 = (rNext3 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask4 = ((reg3 < 0) ? (((usqInt)(1)) >> (-reg3)) : (1ULL << reg3));
			}
		}
		if (rTop5 == NoReg) {
			rTop5 = allocateRegNotConflictingWith(topRegistersMask4);
		}
		if (rNext3 == NoReg) {
			rNext3 = allocateRegNotConflictingWith(((rTop5 < 0) ? (((usqInt)(1)) >> (-rTop5)) : (1ULL << rTop5)));
		}
		assert(!(((rTop5 == NoReg)
 || (rNext3 == NoReg))));
		second3 = rTop5;
		first3 = rNext3;
		nativePopToReg(ssNativeTop(), second3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first3);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second3 == SPReg)));
		genoperandoperand(CmpRR, second3, first3);
		/* True result */
		falseJump3 = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first3);
		/* False result */
		contJump3 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first3);
		jmpTarget(contJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first3);
		return 0;

	case 73:
		/* begin genLowcodeInt32LessEqual */
		firstValue4 = 0;
		secondValue4 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask5 = 0;
		rTop6 = (rNext4 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop6 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext4 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext4 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg4 = (rNext4 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask5 = ((reg4 < 0) ? (((usqInt)(1)) >> (-reg4)) : (1ULL << reg4));
			}
		}
		if (rTop6 == NoReg) {
			rTop6 = allocateRegNotConflictingWith(topRegistersMask5);
		}
		if (rNext4 == NoReg) {
			rNext4 = allocateRegNotConflictingWith(((rTop6 < 0) ? (((usqInt)(1)) >> (-rTop6)) : (1ULL << rTop6)));
		}
		assert(!(((rTop6 == NoReg)
 || (rNext4 == NoReg))));
		second4 = rTop6;
		first4 = rNext4;
		nativePopToReg(ssNativeTop(), second4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first4);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second4 == SPReg)));
		genoperandoperand(CmpRR, second4, first4);
		/* True result */
		falseJump4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first4);
		/* False result */
		contJump4 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump4, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first4);
		jmpTarget(contJump4, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first4);
		return 0;

	case 74:
		/* begin genLowcodeInt32NotEqual */
		firstValue5 = 0;
		secondValue5 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask6 = 0;
		rTop7 = (rNext5 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop7 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext5 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext5 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg5 = (rNext5 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask6 = ((reg5 < 0) ? (((usqInt)(1)) >> (-reg5)) : (1ULL << reg5));
			}
		}
		if (rTop7 == NoReg) {
			rTop7 = allocateRegNotConflictingWith(topRegistersMask6);
		}
		if (rNext5 == NoReg) {
			rNext5 = allocateRegNotConflictingWith(((rTop7 < 0) ? (((usqInt)(1)) >> (-rTop7)) : (1ULL << rTop7)));
		}
		assert(!(((rTop7 == NoReg)
 || (rNext5 == NoReg))));
		second5 = rTop7;
		first5 = rNext5;
		nativePopToReg(ssNativeTop(), second5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first5);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second5 == SPReg)));
		genoperandoperand(CmpRR, second5, first5);
		/* True result */
		falseJump5 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first5);
		/* False result */
		contJump5 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump5, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first5);
		jmpTarget(contJump5, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first5);
		return 0;

	case 75:
		/* begin genLowcodeInt32ToFloat32 */
		resultValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop8 = NoReg;
		frResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop8 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop8 == NoReg) {
			rTop8 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop8 == NoReg)
 || (frResult == NoReg))));
		value = rTop8;
		result = frResult;
		nativePopToReg(ssNativeTop(), value);
		ssNativePop(1);
		/* begin ConvertR:Rs: */
		genoperandoperand(ConvertRRs, value, result);
		ssPushNativeRegisterSingleFloat(result);
		return 0;

	case 76:
		/* begin genLowcodeInt32ToFloat64 */
		resultValue1 = 0;
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop9 = NoReg;
		frResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop9 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop9 == NoReg) {
			rTop9 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop9 == NoReg)
 || (frResult1 == NoReg))));
		value1 = rTop9;
		result1 = frResult1;
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin ConvertR:Rd: */
		genoperandoperand(ConvertRRd, value1, result1);
		ssPushNativeRegisterDoubleFloat(result1);
		return 0;

	case 77:
		/* begin genLowcodeInt32ToPointer */
		valueValue2 = 0;
		rTop10 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop10 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop10 == NoReg) {
			rTop10 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop10 == NoReg)));
		value2 = rTop10;
		nativePopToReg(ssNativeTop(), value2);
		ssNativePop(1);
		ssPushNativeRegister(value2);
		return 0;

	case 78:
		/* begin genLowcodeInt64Equal */
		firstValue6 = 0;
		secondValue6 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask7 = 0;
		rTop11 = (rNext11 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext11 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext11 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg6 = (rNext11 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask7 = ((reg6 < 0) ? (((usqInt)(1)) >> (-reg6)) : (1ULL << reg6));
			}
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(topRegistersMask7);
		}
		if (rNext11 == NoReg) {
			rNext11 = allocateRegNotConflictingWith(((rTop11 < 0) ? (((usqInt)(1)) >> (-rTop11)) : (1ULL << rTop11)));
		}
		assert(!(((rTop11 == NoReg)
 || (rNext11 == NoReg))));
		second6 = rTop11;
		first6 = rNext11;
		nativePopToReg(ssNativeTop(), second6);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first6);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second6 == SPReg)));
		genoperandoperand(CmpRR, second6, first6);
		/* True result */
		falseJump6 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first6);
		/* False result */
		contJump6 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump6, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first6);
		jmpTarget(contJump6, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first6);
		return 0;

	case 79:
		/* begin genLowcodeInt64Great */
		firstValue7 = 0;
		secondValue7 = 0;
		valueValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask8 = 0;
		rTop12 = (rNext6 = NoReg);
		rResult5 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop12 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext6 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext6 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg7 = (rNext6 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask8 = ((reg7 < 0) ? (((usqInt)(1)) >> (-reg7)) : (1ULL << reg7));
			}
		}
		if (rTop12 == NoReg) {
			rTop12 = allocateRegNotConflictingWith(topRegistersMask8);
		}
		if (rNext6 == NoReg) {
			rNext6 = allocateRegNotConflictingWith(((rTop12 < 0) ? (((usqInt)(1)) >> (-rTop12)) : (1ULL << rTop12)));
		}
		assert(!(((rTop12 == NoReg)
 || (rNext6 == NoReg))));
		rResult5 = allocateFloatRegNotConflictingWith((1ULL << rTop12) | (1ULL << rNext6));
		assert(!((rResult5 == NoReg)));
		second7 = rTop12;
		first7 = rNext6;
		value3 = rResult5;
		nativePopToReg(ssNativeTop(), second7);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first7);
		ssNativePop(1);
		abort();
		return 0;

	case 80:
		/* begin genLowcodeInt64GreatEqual */
		firstValue8 = 0;
		secondValue8 = 0;
		valueValue4 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask9 = 0;
		rTop13 = (rNext7 = NoReg);
		rResult6 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop13 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext7 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext7 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg8 = (rNext7 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask9 = ((reg8 < 0) ? (((usqInt)(1)) >> (-reg8)) : (1ULL << reg8));
			}
		}
		if (rTop13 == NoReg) {
			rTop13 = allocateRegNotConflictingWith(topRegistersMask9);
		}
		if (rNext7 == NoReg) {
			rNext7 = allocateRegNotConflictingWith(((rTop13 < 0) ? (((usqInt)(1)) >> (-rTop13)) : (1ULL << rTop13)));
		}
		assert(!(((rTop13 == NoReg)
 || (rNext7 == NoReg))));
		rResult6 = allocateFloatRegNotConflictingWith((1ULL << rTop13) | (1ULL << rNext7));
		assert(!((rResult6 == NoReg)));
		second8 = rTop13;
		first8 = rNext7;
		value4 = rResult6;
		nativePopToReg(ssNativeTop(), second8);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first8);
		ssNativePop(1);
		abort();
		return 0;

	case 81:
		/* begin genLowcodeInt64Less */
		firstValue9 = 0;
		secondValue9 = 0;
		valueValue5 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask10 = 0;
		rTop14 = (rNext8 = NoReg);
		rResult7 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop14 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext8 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext8 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg9 = (rNext8 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask10 = ((reg9 < 0) ? (((usqInt)(1)) >> (-reg9)) : (1ULL << reg9));
			}
		}
		if (rTop14 == NoReg) {
			rTop14 = allocateRegNotConflictingWith(topRegistersMask10);
		}
		if (rNext8 == NoReg) {
			rNext8 = allocateRegNotConflictingWith(((rTop14 < 0) ? (((usqInt)(1)) >> (-rTop14)) : (1ULL << rTop14)));
		}
		assert(!(((rTop14 == NoReg)
 || (rNext8 == NoReg))));
		rResult7 = allocateFloatRegNotConflictingWith((1ULL << rTop14) | (1ULL << rNext8));
		assert(!((rResult7 == NoReg)));
		second9 = rTop14;
		first9 = rNext8;
		value5 = rResult7;
		nativePopToReg(ssNativeTop(), second9);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first9);
		ssNativePop(1);
		abort();
		return 0;

	case 82:
		/* begin genLowcodeInt64LessEqual */
		firstValue10 = 0;
		secondValue10 = 0;
		valueValue6 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask11 = 0;
		rTop15 = (rNext9 = NoReg);
		rResult8 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop15 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext9 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext9 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg10 = (rNext9 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask11 = ((reg10 < 0) ? (((usqInt)(1)) >> (-reg10)) : (1ULL << reg10));
			}
		}
		if (rTop15 == NoReg) {
			rTop15 = allocateRegNotConflictingWith(topRegistersMask11);
		}
		if (rNext9 == NoReg) {
			rNext9 = allocateRegNotConflictingWith(((rTop15 < 0) ? (((usqInt)(1)) >> (-rTop15)) : (1ULL << rTop15)));
		}
		assert(!(((rTop15 == NoReg)
 || (rNext9 == NoReg))));
		rResult8 = allocateFloatRegNotConflictingWith((1ULL << rTop15) | (1ULL << rNext9));
		assert(!((rResult8 == NoReg)));
		second10 = rTop15;
		first10 = rNext9;
		value6 = rResult8;
		nativePopToReg(ssNativeTop(), second10);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first10);
		ssNativePop(1);
		abort();
		return 0;

	case 83:
		/* begin genLowcodeInt64NotEqual */
		firstValue11 = 0;
		secondValue11 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask12 = 0;
		rTop16 = (rNext12 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop16 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext12 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext12 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg11 = (rNext12 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask12 = ((reg11 < 0) ? (((usqInt)(1)) >> (-reg11)) : (1ULL << reg11));
			}
		}
		if (rTop16 == NoReg) {
			rTop16 = allocateRegNotConflictingWith(topRegistersMask12);
		}
		if (rNext12 == NoReg) {
			rNext12 = allocateRegNotConflictingWith(((rTop16 < 0) ? (((usqInt)(1)) >> (-rTop16)) : (1ULL << rTop16)));
		}
		assert(!(((rTop16 == NoReg)
 || (rNext12 == NoReg))));
		second11 = rTop16;
		first11 = rNext12;
		nativePopToReg(ssNativeTop(), second11);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first11);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second11 == SPReg)));
		genoperandoperand(CmpRR, second11, first11);
		/* True result */
		falseJump7 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first11);
		/* False result */
		contJump7 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump7, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first11);
		jmpTarget(contJump7, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first11);
		return 0;

	case 84:
		/* begin genLowcodeInt64ToFloat32 */
		resultValue2 = 0;
		valueValue7 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop17 = NoReg;
		frResult2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop17 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop17 == NoReg) {
			rTop17 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult2 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop17 == NoReg)
 || (frResult2 == NoReg))));
		value7 = rTop17;
		result2 = frResult2;
		nativePopToReg(ssNativeTop(), value7);
		ssNativePop(1);
		abort();
		return 0;

	case 85:
		/* begin genLowcodeInt64ToFloat64 */
		resultValue3 = 0;
		valueValue8 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop18 = NoReg;
		frResult3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop18 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop18 == NoReg) {
			rTop18 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult3 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop18 == NoReg)
 || (frResult3 == NoReg))));
		value8 = rTop18;
		result3 = frResult3;
		nativePopToReg(ssNativeTop(), value8);
		ssNativePop(1);
		abort();
		return 0;

	case 86:
		/* begin genLowcodeInt64ToPointer */
		valueValue9 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop19 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop19 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop19 == NoReg) {
			rTop19 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop19 == NoReg)));
		value9 = rTop19;
		nativePopToReg(ssNativeTop(), value9);
		ssNativePop(1);
		ssPushNativeRegister(value9);
		return 0;

	case 87:
		/* begin genLowcodeLeftShift32 */
		shiftAmountValue = 0;
		valueValue10 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask13 = 0;
		rTop20 = (rNext10 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop20 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext10 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext10 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg12 = (rNext10 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask13 = ((reg12 < 0) ? (((usqInt)(1)) >> (-reg12)) : (1ULL << reg12));
			}
		}
		if (rTop20 == NoReg) {
			rTop20 = allocateRegNotConflictingWith(topRegistersMask13);
		}
		if (rNext10 == NoReg) {
			rNext10 = allocateRegNotConflictingWith(((rTop20 < 0) ? (((usqInt)(1)) >> (-rTop20)) : (1ULL << rTop20)));
		}
		assert(!(((rTop20 == NoReg)
 || (rNext10 == NoReg))));
		shiftAmount = rTop20;
		value10 = rNext10;
		nativePopToReg(ssNativeTop(), shiftAmount);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value10);
		ssNativePop(1);
		/* begin LogicalShiftLeftR:R: */
		genoperandoperand(LogicalShiftLeftRR, shiftAmount, value10);
		ssPushNativeRegister(value10);
		return 0;

	case 88:
		/* begin genLowcodeLeftShift64 */
		resultValue4 = 0;
		shiftAmountValue1 = 0;
		valueValue11 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask14 = 0;
		rTop21 = (rNext13 = NoReg);
		rResult9 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop21 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext13 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext13 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg13 = (rNext13 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask14 = ((reg13 < 0) ? (((usqInt)(1)) >> (-reg13)) : (1ULL << reg13));
			}
		}
		if (rTop21 == NoReg) {
			rTop21 = allocateRegNotConflictingWith(topRegistersMask14);
		}
		if (rNext13 == NoReg) {
			rNext13 = allocateRegNotConflictingWith(((rTop21 < 0) ? (((usqInt)(1)) >> (-rTop21)) : (1ULL << rTop21)));
		}
		assert(!(((rTop21 == NoReg)
 || (rNext13 == NoReg))));
		rResult9 = allocateFloatRegNotConflictingWith((1ULL << rTop21) | (1ULL << rNext13));
		assert(!((rResult9 == NoReg)));
		shiftAmount1 = rTop21;
		value11 = rNext13;
		result4 = rResult9;
		nativePopToReg(ssNativeTop(), shiftAmount1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value11);
		ssNativePop(1);
		abort();
		return 0;

	case 89:
		/* begin genLowcodeLoadArgumentAddress */
		pointerValue1 = 0;
		baseOffset = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult10 = NoReg;
		rResult10 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult10 == NoReg)));
		pointer1 = rResult10;
		loadNativeArgumentAddressto(baseOffset, pointer1);
		ssPushNativeRegister(pointer1);
		extA = 0;
		return 0;

	case 90:
		/* begin genLowcodeLoadArgumentFloat32 */
		floatValueValue5 = 0;
		baseOffset1 = extA;
		/* begin allocateRegistersForLowcodeResultFloat: */
		/* Float result */
		frResult4 = NoReg;
		frResult4 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((frResult4 == NoReg)));
		floatValue5 = frResult4;
		loadNativeArgumentAddressto(baseOffset1, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rRs, 0, TempReg, floatValue5);
		ssPushNativeRegisterSingleFloat(floatValue5);
		extA = 0;
		return 0;

	case 91:
		/* begin genLowcodeLoadArgumentFloat64 */
		doubleValueValue = 0;
		baseOffset2 = extA;
		/* begin allocateRegistersForLowcodeResultFloat: */
		/* Float result */
		frResult5 = NoReg;
		frResult5 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((frResult5 == NoReg)));
		doubleValue = frResult5;
		loadNativeArgumentAddressto(baseOffset2, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM64rRd, 0, TempReg, doubleValue);
		ssPushNativeRegisterDoubleFloat(doubleValue);
		extA = 0;
		return 0;

	case 92:
		/* begin genLowcodeLoadArgumentInt16 */
		valueValue12 = 0;
		baseOffset3 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult11 = NoReg;
		rResult11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult11 == NoReg)));
		value12 = rResult11;
		loadNativeArgumentAddressto(baseOffset3, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, TempReg, value12);
		genoperandoperand(SignExtend16RR, value12, value12);
		ssPushNativeRegister(value12);
		extA = 0;
		return 0;

	case 93:
		/* begin genLowcodeLoadArgumentInt32 */
		valueValue13 = 0;
		baseOffset4 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult12 = NoReg;
		rResult12 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult12 == NoReg)));
		value13 = rResult12;
		loadNativeArgumentAddressto(baseOffset4, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, TempReg, value13);
		ssPushNativeRegister(value13);
		extA = 0;
		return 0;

	case 94:
		/* begin genLowcodeLoadArgumentInt64 */
		valueValue14 = 0;
		baseOffset5 = extA;
				/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult13 = NoReg;
		rResult13 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult13 == NoReg)));
		value14 = rResult13;
		loadNativeArgumentAddressto(baseOffset5, TempReg);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, TempReg, value14);
		ssPushNativeRegister(value14);
		extA = 0;
		return 0;

	case 95:
		/* begin genLowcodeLoadArgumentInt8 */
		valueValue15 = 0;
		baseOffset6 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult14 = NoReg;
		rResult14 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult14 == NoReg)));
		value15 = rResult14;
		loadNativeArgumentAddressto(baseOffset6, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, TempReg, value15);
		genoperandoperand(SignExtend8RR, value15, value15);
		ssPushNativeRegister(value15);
		extA = 0;
		return 0;

	case 96:
		/* begin genLowcodeLoadArgumentPointer */
		pointerResultValue = 0;
		baseOffset7 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult15 = NoReg;
		rResult15 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult15 == NoReg)));
		pointerResult = rResult15;
		loadNativeArgumentAddressto(baseOffset7, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, TempReg, pointerResult);
		ssPushNativeRegister(pointerResult);
		extA = 0;
		return 0;

	case 97:
		/* begin genLowcodeLoadArgumentUInt16 */
		valueValue16 = 0;
		baseOffset8 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult16 = NoReg;
		rResult16 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult16 == NoReg)));
		value16 = rResult16;
		loadNativeArgumentAddressto(baseOffset8, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, TempReg, value16);
		ssPushNativeRegister(value16);
		extA = 0;
		return 0;

	case 98:
		/* begin genLowcodeLoadArgumentUInt32 */
		valueValue17 = 0;
		baseOffset9 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult17 = NoReg;
		rResult17 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult17 == NoReg)));
		value17 = rResult17;
		loadNativeArgumentAddressto(baseOffset9, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, TempReg, value17);
		ssPushNativeRegister(value17);
		extA = 0;
		return 0;

	case 99:
		/* begin genLowcodeLoadArgumentUInt64 */
		valueValue18 = 0;
		baseOffset10 = extA;
				/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult18 = NoReg;
		rResult18 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult18 == NoReg)));
		value18 = rResult18;
		loadNativeArgumentAddressto(baseOffset10, TempReg);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, TempReg, value18);
		ssPushNativeRegister(value18);
		extA = 0;
		return 0;

	case 100:
		/* begin genLowcodeLoadArgumentUInt8 */
		valueValue19 = 0;
		baseOffset11 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult19 = NoReg;
		rResult19 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult19 == NoReg)));
		value19 = rResult19;
		loadNativeArgumentAddressto(baseOffset11, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, TempReg, value19);
		ssPushNativeRegister(value19);
		extA = 0;
		return 0;

	case 101:
		/* begin genLowcodeLoadFloat32FromMemory */
		pointerValue2 = 0;
		valueValue20 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop22 = NoReg;
		frResult6 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop22 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop22 == NoReg) {
			rTop22 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult6 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop22 == NoReg)
 || (frResult6 == NoReg))));
		pointer2 = rTop22;
		value20 = frResult6;
		nativePopToReg(ssNativeTop(), pointer2);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rRs, 0, pointer2, value20);
		ssPushNativeRegisterSingleFloat(value20);
		return 0;

	case 102:
		/* begin genLowcodeLoadFloat64FromMemory */
		pointerValue3 = 0;
		valueValue21 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop23 = NoReg;
		frResult7 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop23 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop23 == NoReg) {
			rTop23 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult7 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop23 == NoReg)
 || (frResult7 == NoReg))));
		pointer3 = rTop23;
		value21 = frResult7;
		nativePopToReg(ssNativeTop(), pointer3);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM64rRd, 0, pointer3, value21);
		ssPushNativeRegisterDoubleFloat(value21);
		return 0;

	case 103:
		/* begin genLowcodeLoadInt16FromMemory */
		pointerValue4 = 0;
		valueValue22 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop24 = NoReg;
		rResult20 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop24 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop24 == NoReg) {
			rTop24 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult20 = allocateRegNotConflictingWith(((rTop24 < 0) ? (((usqInt)(1)) >> (-rTop24)) : (1ULL << rTop24)));
		assert(!(((rTop24 == NoReg)
 || (rResult20 == NoReg))));
		pointer4 = rTop24;
		value22 = rResult20;
		nativePopToReg(ssNativeTop(), pointer4);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, pointer4, value22);
		genoperandoperand(SignExtend16RR, value22, value22);
		ssPushNativeRegister(value22);
		return 0;

	case 104:
		/* begin genLowcodeLoadInt32FromMemory */
		pointerValue5 = 0;
		valueValue23 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop25 = NoReg;
		rResult21 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop25 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop25 == NoReg) {
			rTop25 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult21 = allocateRegNotConflictingWith(((rTop25 < 0) ? (((usqInt)(1)) >> (-rTop25)) : (1ULL << rTop25)));
		assert(!(((rTop25 == NoReg)
 || (rResult21 == NoReg))));
		pointer5 = rTop25;
		value23 = rResult21;
		nativePopToReg(ssNativeTop(), pointer5);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, pointer5, value23);
		ssPushNativeRegister(value23);
		return 0;

	case 105:
		/* begin genLowcodeLoadInt64FromMemory */
		pointerValue6 = 0;
		valueValue24 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop110 = NoReg;
		rResult110 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop110 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop110 == NoReg) {
			rTop110 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult110 = allocateRegNotConflictingWith(((rTop110 < 0) ? (((usqInt)(1)) >> (-rTop110)) : (1ULL << rTop110)));
		assert(!(((rTop110 == NoReg)
 || (rResult110 == NoReg))));
		pointer6 = rTop110;
		value24 = rResult110;
		nativePopToReg(ssNativeTop(), pointer6);
		ssNativePop(1);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, pointer6, value24);
		ssPushNativeRegister(value24);
		return 0;

	case 106:
		/* begin genLowcodeLoadInt8FromMemory */
		pointerValue7 = 0;
		valueValue25 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop26 = NoReg;
		rResult22 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop26 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop26 == NoReg) {
			rTop26 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult22 = allocateRegNotConflictingWith(((rTop26 < 0) ? (((usqInt)(1)) >> (-rTop26)) : (1ULL << rTop26)));
		assert(!(((rTop26 == NoReg)
 || (rResult22 == NoReg))));
		pointer7 = rTop26;
		value25 = rResult22;
		nativePopToReg(ssNativeTop(), pointer7);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, pointer7, value25);
		genoperandoperand(SignExtend8RR, value25, value25);
		ssPushNativeRegister(value25);
		return 0;

	case 107:
		/* begin genLowcodeLoadLocalAddress */
		pointerValue8 = 0;
		baseOffset12 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult23 = NoReg;
		rResult23 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult23 == NoReg)));
		pointer8 = rResult23;
		loadNativeLocalAddressto(baseOffset12, pointer8);
		ssPushNativeRegister(pointer8);
		extA = 0;
		return 0;

	case 108:
		/* begin genLowcodeLoadLocalFloat32 */
		floatValueValue6 = 0;
		baseOffset13 = extA;
		/* begin allocateRegistersForLowcodeResultFloat: */
		/* Float result */
		frResult8 = NoReg;
		frResult8 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((frResult8 == NoReg)));
		floatValue6 = frResult8;
		loadNativeLocalAddressto(baseOffset13, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rRs, 0, TempReg, floatValue6);
		ssPushNativeRegisterSingleFloat(floatValue6);
		extA = 0;
		return 0;

	case 109:
		/* begin genLowcodeLoadLocalFloat64 */
		doubleValueValue1 = 0;
		baseOffset14 = extA;
		/* begin allocateRegistersForLowcodeResultFloat: */
		/* Float result */
		frResult9 = NoReg;
		frResult9 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((frResult9 == NoReg)));
		doubleValue1 = frResult9;
		loadNativeLocalAddressto(baseOffset14, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM64rRd, 0, TempReg, doubleValue1);
		ssPushNativeRegisterDoubleFloat(doubleValue1);
		extA = 0;
		return 0;

	case 110:
		/* begin genLowcodeLoadLocalInt16 */
		valueValue26 = 0;
		baseOffset15 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult24 = NoReg;
		rResult24 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult24 == NoReg)));
		value26 = rResult24;
		loadNativeLocalAddressto(baseOffset15, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, TempReg, value26);
		genoperandoperand(SignExtend16RR, value26, value26);
		ssPushNativeRegister(value26);
		extA = 0;
		return 0;

	case 111:
		/* begin genLowcodeLoadLocalInt32 */
		valueValue27 = 0;
		baseOffset16 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult25 = NoReg;
		rResult25 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult25 == NoReg)));
		value27 = rResult25;
		loadNativeLocalAddressto(baseOffset16, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, TempReg, value27);
		ssPushNativeRegister(value27);
		extA = 0;
		return 0;

	case 112:
		/* begin genLowcodeLoadLocalInt64 */
		valueValue28 = 0;
		baseOffset17 = extA;
				/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult111 = NoReg;
		rResult111 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult111 == NoReg)));
		value28 = rResult111;
		loadNativeLocalAddressto(baseOffset17, TempReg);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, TempReg, value28);
		ssPushNativeRegister(value28);
		extA = 0;
		return 0;

	case 113:
		/* begin genLowcodeLoadLocalInt8 */
		valueValue29 = 0;
		baseOffset18 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult26 = NoReg;
		rResult26 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult26 == NoReg)));
		value29 = rResult26;
		loadNativeLocalAddressto(baseOffset18, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, TempReg, value29);
		genoperandoperand(SignExtend8RR, value29, value29);
		ssPushNativeRegister(value29);
		extA = 0;
		return 0;

	case 114:
		/* begin genLowcodeLoadLocalPointer */
		pointerResultValue1 = 0;
		baseOffset19 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult27 = NoReg;
		rResult27 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult27 == NoReg)));
		pointerResult1 = rResult27;
		loadNativeLocalAddressto(baseOffset19, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, TempReg, pointerResult1);
		ssPushNativeRegister(pointerResult1);
		extA = 0;
		return 0;

	case 115:
		/* begin genLowcodeLoadLocalUInt16 */
		valueValue30 = 0;
		baseOffset20 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult28 = NoReg;
		rResult28 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult28 == NoReg)));
		value30 = rResult28;
		loadNativeLocalAddressto(baseOffset20, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, TempReg, value30);
		ssPushNativeRegister(value30);
		extA = 0;
		return 0;

	case 116:
		/* begin genLowcodeLoadLocalUInt32 */
		valueValue31 = 0;
		baseOffset21 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult29 = NoReg;
		rResult29 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult29 == NoReg)));
		value31 = rResult29;
		loadNativeLocalAddressto(baseOffset21, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, TempReg, value31);
		ssPushNativeRegister(value31);
		extA = 0;
		return 0;

	case 117:
		/* begin genLowcodeLoadLocalUInt64 */
		valueValue32 = 0;
		baseOffset22 = extA;
				/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult112 = NoReg;
		rResult112 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult112 == NoReg)));
		value32 = rResult112;
		loadNativeLocalAddressto(baseOffset22, TempReg);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, TempReg, value32);
		ssPushNativeRegister(value32);
		extA = 0;
		return 0;

	case 118:
		/* begin genLowcodeLoadLocalUInt8 */
		valueValue33 = 0;
		baseOffset23 = extA;
		/* begin allocateRegistersForLowcodeResultInteger: */
		/* Float result */
		rResult30 = NoReg;
		rResult30 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!((rResult30 == NoReg)));
		value33 = rResult30;
		loadNativeLocalAddressto(baseOffset23, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, TempReg, value33);
		ssPushNativeRegister(value33);
		extA = 0;
		return 0;

	case 119:
		/* begin genLowcodeLoadObjectAt */
		fieldIndexValue = 0;
		objectValue2 = 0;
		/* begin allocateRegistersForLowcodeIntegerOop: */
		rTop27 = (rOopTop3 = NoReg);
		topRegisterMask1 = 0;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop27 = nativeRegisterOrNone(ssNativeTop());
		}
		if ((registerOrNone(ssTop())) != NoReg) {
			rOopTop3 = registerOrNone(ssTop());
			topRegisterMask1 = ((rOopTop3 < 0) ? (((usqInt)(1)) >> (-rOopTop3)) : (1ULL << rOopTop3));
		}
		if (rTop27 == NoReg) {
			rTop27 = allocateRegNotConflictingWith(topRegisterMask1);
		}
		if (rOopTop3 == NoReg) {
			rOopTop3 = allocateRegNotConflictingWith(((rTop27 < 0) ? (((usqInt)(1)) >> (-rTop27)) : (1ULL << rTop27)));
		}
		assert(!(((rTop27 == NoReg)
 || (rOopTop3 == NoReg))));
		fieldIndex = rTop27;
		object2 = rOopTop3;
		nativePopToReg(ssNativeTop(), fieldIndex);
		ssNativePop(1);
		popToReg(ssTop(), object2);
		ssPop(1);
		genLcLoadObjectat(object2, fieldIndex);
		return 0;

	default:
		return genLowcodeUnaryInlinePrimitive3(prim);
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeUnaryInlinePrimitive3: */
static NoDbgRegParms sqInt
genLowcodeUnaryInlinePrimitive3(sqInt prim)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction11;
    AbstractInstruction *abstractInstruction12;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    sqInt alignedSize;
    sqInt base;
    sqInt base1;
    sqInt base2;
    sqInt baseValue;
    sqInt baseValue1;
    sqInt baseValue2;
    sqInt constant;
    sqInt constant1;
    AbstractInstruction *contJump;
    AbstractInstruction *contJump1;
    sqInt dest;
    sqInt dest1;
    sqInt dest2;
    sqInt destValue;
    sqInt destValue1;
    sqInt destValue2;
    AbstractInstruction *falseJump;
    AbstractInstruction *falseJump1;
    sqInt fieldIndex;
    sqInt first;
    sqInt first1;
    sqInt first2;
    sqInt first3;
    sqInt first4;
    sqInt first5;
    sqInt firstValue;
    sqInt firstValue1;
    sqInt firstValue2;
    sqInt firstValue3;
    sqInt firstValue4;
    sqInt firstValue5;
    sqInt frTop;
    sqInt frTop1;
    sqInt i;
    sqInt i1;
    sqInt i11;
    sqInt i2;
    sqInt i3;
    sqInt i4;
    sqInt i5;
    sqInt index1;
    sqInt nativeValueIndex;
    sqInt nativeValueIndex1;
    sqInt nextRegisterMask;
    sqInt nextRegisterMask1;
    sqInt object;
    sqInt objectValue;
    sqInt offset;
    sqInt offset1;
    sqInt offset2;
    sqInt offset3;
    sqInt offset4;
    sqInt offset5;
    sqInt offset6;
    sqInt offsetValue;
    sqInt offsetValue1;
    sqInt pointer;
    sqInt pointer1;
    sqInt pointer2;
    sqInt pointer3;
    sqInt pointer4;
    sqInt pointer5;
    sqInt pointer6;
    sqInt pointer7;
    sqInt pointer8;
    sqInt pointerResult;
    sqInt pointerResultValue;
    sqInt pointerValue;
    sqInt pointerValue1;
    sqInt pointerValue2;
    sqInt pointerValue3;
    sqInt pointerValue4;
    sqInt pointerValue5;
    sqInt pointerValue6;
    sqInt pointerValue7;
    sqInt pointerValue8;
    sqInt pointerValueValue;
    sqInt pointerValue9;
    sqInt quickConstant;
    sqInt reg;
    sqInt reg1;
    sqInt reg2;
    sqInt reg3;
    sqInt reg4;
    sqInt reg5;
    sqInt reg6;
    sqInt reg7;
    sqInt reg8;
    sqInt registerID;
    sqInt registerID1;
    sqInt registerID2;
    sqInt registerID3;
    sqInt registerID4;
    sqInt result;
    sqInt result1;
    sqInt result2;
    sqInt resultValue;
    sqInt resultValue1;
    sqInt resultValue2;
    sqInt rNext;
    sqInt rNext1;
    sqInt rNext11;
    sqInt rNext12;
    sqInt rNext2;
    sqInt rNext3;
    sqInt rNext4;
    sqInt rNext5;
    sqInt rNext6;
    sqInt rNext7;
    sqInt rNext8;
    sqInt rNextNext;
    sqInt rNextNext1;
    sqInt rOopTop;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult11;
    sqInt rResult12;
    sqInt rResult2;
    sqInt rResult3;
    sqInt rResult4;
    sqInt rResult5;
    sqInt rResult6;
    sqInt rResult7;
    sqInt rTop;
    sqInt rTop1;
    sqInt rTop10;
    sqInt rTop11;
    sqInt rTop110;
    sqInt rTop12;
    sqInt rTop13;
    sqInt rTop14;
    sqInt rTop15;
    sqInt rTop16;
    sqInt rTop17;
    sqInt rTop18;
    sqInt rTop19;
    sqInt rTop2;
    sqInt rTop20;
    sqInt rTop21;
    sqInt rTop22;
    sqInt rTop23;
    sqInt rTop24;
    sqInt rTop25;
    sqInt rTop26;
    sqInt rTop3;
    sqInt rTop4;
    sqInt rTop5;
    sqInt rTop6;
    sqInt rTop7;
    sqInt rTop8;
    sqInt rTop9;
    sqInt second;
    sqInt second1;
    sqInt second2;
    sqInt second3;
    sqInt second4;
    sqInt second5;
    sqInt secondValue;
    sqInt secondValue1;
    sqInt secondValue2;
    sqInt secondValue3;
    sqInt secondValue4;
    sqInt secondValue5;
    sqInt size;
    sqInt size1;
    sqInt size2;
    sqInt size3;
    sqInt size4;
    sqInt size5;
    sqInt sizeLow;
    sqInt sizeValue;
    sqInt sizeValue1;
    sqInt sizeValue2;
    sqInt sizeValue3;
    sqInt source;
    sqInt source1;
    sqInt source2;
    sqInt sourceValue;
    sqInt sourceValue1;
    sqInt sourceValue2;
    sqInt topRegistersMask;
    sqInt topRegistersMask1;
    sqInt topRegistersMask10;
    sqInt topRegistersMask2;
    sqInt topRegistersMask3;
    sqInt topRegistersMask4;
    sqInt topRegistersMask5;
    sqInt topRegistersMask6;
    sqInt topRegistersMask7;
    sqInt topRegistersMask8;
    sqInt topRegistersMask9;
    sqInt value;
    sqInt value1;
    sqInt value10;
    sqInt value11;
    sqInt value2;
    sqInt value3;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt value7;
    sqInt value8;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue10;
    sqInt valueValue11;
    sqInt valueValue2;
    sqInt valueValue3;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;
    sqInt valueValue7;
    sqInt valueValue8;
    sqInt valueValue9;
    sqInt value9;

	switch (prim) {
	case 120:
		/* begin genLowcodeLoadObjectField */
		objectValue = 0;
		fieldIndex = extA;
		/* begin allocateRegistersForLowcodeOop: */
		rOopTop = NoReg;
		if ((registerOrNone(ssTop())) != NoReg) {
			/* Ensure we are not using a duplicated register. */
			rOopTop = registerOrNone(ssTop());
			/* begin isOopRegister:usedBefore: */
			index1 = ((simSpillBase < 0) ? 0 : simSpillBase);
			for (i1 = index1; i1 <= (simStackPtr); i1 += 1) {
				if ((registerOrNone(simStackAt(index1))) == rOopTop) {
					goto l17;
				}
			}
	l17:;
			rOopTop = NoReg;
	l16:;
		}
		if (rOopTop == NoReg) {
			rOopTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rOopTop == NoReg)));
		object = rOopTop;
		popToReg(ssTop(), object);
		ssPop(1);
		genLcLoadObjectfield(object, fieldIndex);
		extA = 0;
		return 0;

	case 121:
		/* begin genLowcodeLoadPointerFromMemory */
		pointerResultValue = 0;
		pointerValue2 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop = NoReg;
		rResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult = allocateRegNotConflictingWith(((rTop < 0) ? (((usqInt)(1)) >> (-rTop)) : (1ULL << rTop)));
		assert(!(((rTop == NoReg)
 || (rResult == NoReg))));
		pointer2 = rTop;
		pointerResult = rResult;
		nativePopToReg(ssNativeTop(), pointer2);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, 0, pointer2, pointerResult);
		ssPushNativeRegister(pointerResult);
		return 0;

	case 122:
		/* begin genLowcodeLoadUInt16FromMemory */
		pointerValue3 = 0;
		valueValue3 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop2 = NoReg;
		rResult2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop2 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop2 == NoReg) {
			rTop2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult2 = allocateRegNotConflictingWith(((rTop2 < 0) ? (((usqInt)(1)) >> (-rTop2)) : (1ULL << rTop2)));
		assert(!(((rTop2 == NoReg)
 || (rResult2 == NoReg))));
		pointer3 = rTop2;
		value3 = rResult2;
		nativePopToReg(ssNativeTop(), pointer3);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM16rR, 0, pointer3, value3);
		ssPushNativeRegister(value3);
		return 0;

	case 123:
		/* begin genLowcodeLoadUInt32FromMemory */
		pointerValue4 = 0;
		valueValue4 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop3 = NoReg;
		rResult3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop3 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop3 == NoReg) {
			rTop3 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult3 = allocateRegNotConflictingWith(((rTop3 < 0) ? (((usqInt)(1)) >> (-rTop3)) : (1ULL << rTop3)));
		assert(!(((rTop3 == NoReg)
 || (rResult3 == NoReg))));
		pointer4 = rTop3;
		value4 = rResult3;
		nativePopToReg(ssNativeTop(), pointer4);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM32rR, 0, pointer4, value4);
		ssPushNativeRegister(value4);
		return 0;

	case 0x7C:
		/* begin genLowcodeLoadUInt64FromMemory */
		pointerValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop1 = NoReg;
		rResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
		assert(!(((rTop1 == NoReg)
 || (rResult1 == NoReg))));
		pointer = rTop1;
		value = rResult1;
		nativePopToReg(ssNativeTop(), pointer);
		ssNativePop(1);
		/* begin MoveM64:r:R: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveMwrR, 0, pointer, value);
		ssPushNativeRegister(value);
		return 0;

	case 125:
		/* begin genLowcodeLoadUInt8FromMemory */
		pointerValue5 = 0;
		valueValue5 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop4 = NoReg;
		rResult4 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop4 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop4 == NoReg) {
			rTop4 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult4 = allocateRegNotConflictingWith(((rTop4 < 0) ? (((usqInt)(1)) >> (-rTop4)) : (1ULL << rTop4)));
		assert(!(((rTop4 == NoReg)
 || (rResult4 == NoReg))));
		pointer5 = rTop4;
		value5 = rResult4;
		nativePopToReg(ssNativeTop(), pointer5);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveM8rR, 0, pointer5, value5);
		ssPushNativeRegister(value5);
		return 0;

	case 0x7E:
		/* begin genLowcodeLocalFrameSize */
		size = extA;
		assert(needsFrame);
		/* Align the size to 16 bytes. */
		hasNativeFrame = 1;
		/* Mark the stack frame */
		alignedSize = (size + 15) & -16;
		annotateobjRef(gMoveCwR(splObj(LowcodeContextMark), TempReg), splObj(LowcodeContextMark));
		/* begin MoveR:Mw:r: */
		offset = frameOffsetOfNativeFrameMark();
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, TempReg, offset, FPReg);
		genoperandoperand(MoveAwR, nativeStackPointerAddress(), TempReg);
		genoperandoperand(AddCqR, 1, TempReg);
		offset1 = frameOffsetOfPreviousNativeStackPointer();
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, TempReg, offset1, FPReg);
		genoperandoperand(SubCqR, alignedSize, TempReg);
		offset2 = frameOffsetOfNativeFramePointer();
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, TempReg, offset2, FPReg);
		offset3 = frameOffsetOfNativeStackPointer();
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, TempReg, offset3, FPReg);
		quickConstant = 1 + (defaultNativeStackFrameSize());
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, quickConstant, TempReg);
		genoperandoperand(MoveRAw, TempReg, nativeStackPointerAddress());
		extA = 0;
		return 0;

	case 0x7F:
		/* begin genLowcodeLockRegisters */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i5 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i5 <= simStackPtr; i5 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i5), frameOffsetOfTemporary(i5 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		return 0;

	case 128:
		/* begin genLowcodeLockVM */
		abort();
		return 0;

	case 129:
		/* begin genLowcodeMalloc32 */
		pointerValue6 = 0;
		sizeValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop5 = NoReg;
		rResult5 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop5 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop5 == NoReg) {
			rTop5 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult5 = allocateRegNotConflictingWith(((rTop5 < 0) ? (((usqInt)(1)) >> (-rTop5)) : (1ULL << rTop5)));
		assert(!(((rTop5 == NoReg)
 || (rResult5 == NoReg))));
		size1 = rTop5;
		pointer6 = rResult5;
		nativePopToReg(ssNativeTop(), size1);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		if (size1 != ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, size1, ReceiverResultReg);
		}
		abstractInstruction3 = genoperand(Call, ceMallocTrampoline);
		(abstractInstruction3->annotation = IsRelativeCall);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, TempReg, pointer6);
		ssPushNativeRegister(pointer6);
		return 0;

	case 130:
		/* begin genLowcodeMalloc64 */
		pointerValue7 = 0;
		sizeValue1 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop14 = NoReg;
		rResult12 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop14 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop14 == NoReg) {
			rTop14 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult12 = allocateRegNotConflictingWith(((rTop14 < 0) ? (((usqInt)(1)) >> (-rTop14)) : (1ULL << rTop14)));
		assert(!(((rTop14 == NoReg)
 || (rResult12 == NoReg))));
		size2 = rTop14;
		pointer7 = rResult12;
		nativePopToReg(ssNativeTop(), size2);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i11 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i11 <= simStackPtr; i11 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i11), frameOffsetOfTemporary(i11 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		if (size2 != ReceiverResultReg) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, size2, ReceiverResultReg);
		}
		abstractInstruction12 = genoperand(Call, ceMallocTrampoline);
		(abstractInstruction12->annotation = IsRelativeCall);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, TempReg, pointer7);
		ssPushNativeRegister(pointer7);
		return 0;

	case 131:
		/* begin genLowcodeMemcpy32 */
		destValue = 0;
		sizeValue2 = 0;
		sourceValue = 0;
		/* begin allocateRegistersForLowcodeInteger3: */
		rTop6 = (rNext = (rNextNext = NoReg));
		nativeValueIndex = 1;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop6 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
				rNext = nativeRegisterOrNone(ssNativeValue(nativeValueIndex));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
					rNextNext = nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex));
				}
				nativeValueIndex += 1;
			}
		}
		if (rNextNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
				rNextNext = nativeRegisterOrNone(ssNativeValue(nativeValueIndex));
			}
		}
		if (rTop6 == NoReg) {
			nextRegisterMask = 0;
			if (rNext != NoReg) {
				nextRegisterMask = ((rNext < 0) ? (((usqInt)(1)) >> (-rNext)) : (1ULL << rNext));
			}
			if (rNextNext != NoReg) {
				nextRegisterMask = nextRegisterMask | (((rNextNext < 0) ? (((usqInt)(1)) >> (-rNextNext)) : (1ULL << rNextNext)));
			}
			rTop6 = allocateRegNotConflictingWith(nextRegisterMask);
		}
		if (rNext == NoReg) {
			nextRegisterMask = ((rTop6 < 0) ? (((usqInt)(1)) >> (-rTop6)) : (1ULL << rTop6));
			if (rNextNext != NoReg) {
				nextRegisterMask = nextRegisterMask | (((rNextNext < 0) ? (((usqInt)(1)) >> (-rNextNext)) : (1ULL << rNextNext)));
			}
			rNext = allocateRegNotConflictingWith(nextRegisterMask);
		}
		if (rNextNext == NoReg) {
			nextRegisterMask = (1ULL << rTop6) | (1ULL << rNext);
			rNextNext = allocateRegNotConflictingWith(nextRegisterMask);
		}
		assert(!(((rTop6 == NoReg)
 || ((rNext == NoReg)
 || (rNextNext == NoReg)))));
		size3 = rTop6;
		source = rNext;
		dest = rNextNext;
		nativePopToReg(ssNativeTop(), size3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), source);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), dest);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i2 <= simStackPtr; i2 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genMemCopytosize(backEnd, source, dest, size3);
		return 0;

	case 132:
		/* begin genLowcodeMemcpy64 */
		destValue1 = 0;
		sizeLow = 0;
		sizeValue3 = 0;
		sourceValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger3: */
		rTop7 = (rNext1 = (rNextNext1 = NoReg));
		nativeValueIndex1 = 1;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop7 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext1 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
				rNext1 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
					rNextNext1 = nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex1));
				}
				nativeValueIndex1 += 1;
			}
		}
		if (rNextNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
				rNextNext1 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
			}
		}
		if (rTop7 == NoReg) {
			nextRegisterMask1 = 0;
			if (rNext1 != NoReg) {
				nextRegisterMask1 = ((rNext1 < 0) ? (((usqInt)(1)) >> (-rNext1)) : (1ULL << rNext1));
			}
			if (rNextNext1 != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNext1)) : (1ULL << rNextNext1)));
			}
			rTop7 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		if (rNext1 == NoReg) {
			nextRegisterMask1 = ((rTop7 < 0) ? (((usqInt)(1)) >> (-rTop7)) : (1ULL << rTop7));
			if (rNextNext1 != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNext1)) : (1ULL << rNextNext1)));
			}
			rNext1 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		if (rNextNext1 == NoReg) {
			nextRegisterMask1 = (1ULL << rTop7) | (1ULL << rNext1);
			rNextNext1 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		assert(!(((rTop7 == NoReg)
 || ((rNext1 == NoReg)
 || (rNextNext1 == NoReg)))));
		size4 = rTop7;
		source1 = rNext1;
		dest1 = rNextNext1;
		nativePopToReg(ssNativeTop(), size4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), source1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), dest1);
		ssNativePop(1);
		/* begin ssFlushAll */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i3 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i3 <= simStackPtr; i3 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i3), frameOffsetOfTemporary(i3 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		voidReceiverResultRegContainsSelf();
		genMemCopytosize(backEnd, source1, dest1, sizeLow);
		return 0;

	case 133:
		/* begin genLowcodeMemcpyFixed */
		destValue2 = 0;
		sourceValue2 = 0;
		size5 = extA;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask = 0;
		rTop8 = (rNext2 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop8 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext2 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext2 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg = (rNext2 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
		}
		if (rTop8 == NoReg) {
			rTop8 = allocateRegNotConflictingWith(topRegistersMask);
		}
		if (rNext2 == NoReg) {
			rNext2 = allocateRegNotConflictingWith(((rTop8 < 0) ? (((usqInt)(1)) >> (-rTop8)) : (1ULL << rTop8)));
		}
		assert(!(((rTop8 == NoReg)
 || (rNext2 == NoReg))));
		source2 = rTop8;
		dest2 = rNext2;
		nativePopToReg(ssNativeTop(), source2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), dest2);
		ssNativePop(1);
		if (size5 == BytesPerWord) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveMwrR, 0, source2, TempReg);
			genoperandoperandoperand(MoveRMwr, TempReg, 0, dest2);
		}
		else {
			/* begin ssFlushAll */
			assert(tempsValidAndVolatileEntriesSpilled());
			ssNativeFlushTo(simNativeStackPtr);
			if (simSpillBase <= simStackPtr) {
				for (i4 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i4 <= simStackPtr; i4 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i4), frameOffsetOfTemporary(i4 - 1), FPReg);
				}
				simSpillBase = simStackPtr + 1;
			}
			voidReceiverResultRegContainsSelf();
			genMemCopytoconstantSize(backEnd, source2, dest2, size5);
		}
		extA = 0;
		return 0;

	case 134:
		/* begin genLowcodeMoveFloat32ToPhysical */
		nativeStackPopToReg(ssNativeTop(), extA);
		ssNativePop(1);
		currentCallCleanUpSize += BytesPerWord;
		extA = 0;
		return 0;

	case 135:
		/* begin genLowcodeMoveFloat64ToPhysical */
		nativeStackPopToReg(ssNativeTop(), extA);
		ssNativePop(1);
		currentCallCleanUpSize += 8;
		extA = 0;
		return 0;

	case 136:
		/* begin genLowcodeMoveInt32ToPhysical */
		nativeStackPopToReg(ssNativeTop(), extA);
		ssNativePop(1);
		currentCallCleanUpSize += BytesPerWord;
		extA = 0;
		return 0;

	case 137:
		/* begin genLowcodeMoveInt64ToPhysical */
		nativeStackPopToReg(ssNativeTop(), extA);
		ssNativePop(1);
		currentCallCleanUpSize += 8;
		extA = 0;
		return 0;

	case 138:
		/* begin genLowcodeMovePointerToPhysical */
		nativeStackPopToReg(ssNativeTop(), extA);
		ssNativePop(1);
		currentCallCleanUpSize += BytesPerWord;
		extA = 0;
		return 0;

	case 139:
		/* begin genLowcodeMul32 */
		firstValue = 0;
		secondValue = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask1 = 0;
		rTop9 = (rNext3 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop9 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext3 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext3 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg1 = (rNext3 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask1 = ((reg1 < 0) ? (((usqInt)(1)) >> (-reg1)) : (1ULL << reg1));
			}
		}
		if (rTop9 == NoReg) {
			rTop9 = allocateRegNotConflictingWith(topRegistersMask1);
		}
		if (rNext3 == NoReg) {
			rNext3 = allocateRegNotConflictingWith(((rTop9 < 0) ? (((usqInt)(1)) >> (-rTop9)) : (1ULL << rTop9)));
		}
		assert(!(((rTop9 == NoReg)
 || (rNext3 == NoReg))));
		second = rTop9;
		first = rNext3;
		nativePopToReg(ssNativeTop(), second);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first);
		ssNativePop(1);
		/* begin MulR:R: */
		genMulRR(backEnd, second, first);
		ssPushNativeRegister(first);
		return 0;

	case 140:
		/* begin genLowcodeMul64 */
		firstValue1 = 0;
		resultValue1 = 0;
		secondValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask2 = 0;
		rTop10 = (rNext4 = NoReg);
		rResult6 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop10 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext4 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext4 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg2 = (rNext4 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask2 = ((reg2 < 0) ? (((usqInt)(1)) >> (-reg2)) : (1ULL << reg2));
			}
		}
		if (rTop10 == NoReg) {
			rTop10 = allocateRegNotConflictingWith(topRegistersMask2);
		}
		if (rNext4 == NoReg) {
			rNext4 = allocateRegNotConflictingWith(((rTop10 < 0) ? (((usqInt)(1)) >> (-rTop10)) : (1ULL << rTop10)));
		}
		assert(!(((rTop10 == NoReg)
 || (rNext4 == NoReg))));
		rResult6 = allocateFloatRegNotConflictingWith((1ULL << rTop10) | (1ULL << rNext4));
		assert(!((rResult6 == NoReg)));
		second1 = rTop10;
		first1 = rNext4;
		result1 = rResult6;
		nativePopToReg(ssNativeTop(), second1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first1);
		ssNativePop(1);
		abort();
		return 0;

	case 141:
		/* begin genLowcodeNeg32 */
		valueValue6 = 0;
		rTop15 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop15 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop15 == NoReg) {
			rTop15 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop15 == NoReg)));
		value6 = rTop15;
		nativePopToReg(ssNativeTop(), value6);
		ssNativePop(1);
		/* begin NegateR: */
		genoperand(NegateR, value6);
		ssPushNativeRegister(value6);
		return 0;

	case 142:
		/* begin genLowcodeNeg64 */
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop11 == NoReg)));
		value1 = rTop11;
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin NegateR: */
		genoperand(NegateR, value1);
		ssPushNativeRegister(value1);
		return 0;

	case 143:
		/* begin genLowcodeNot32 */
		valueValue7 = 0;
		rTop16 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop16 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop16 == NoReg) {
			rTop16 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop16 == NoReg)));
		value7 = rTop16;
		nativePopToReg(ssNativeTop(), value7);
		ssNativePop(1);
		/* begin NotR: */
		genoperand(NotR, value7);
		ssPushNativeRegister(value7);
		return 0;

	case 144:
		/* begin genLowcodeNot64 */
		valueValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop12 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop12 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop12 == NoReg) {
			rTop12 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop12 == NoReg)));
		value2 = rTop12;
		nativePopToReg(ssNativeTop(), value2);
		ssNativePop(1);
		/* begin NotR: */
		genoperand(NotR, value2);
		ssPushNativeRegister(value2);
		return 0;

	case 145:
		/* begin genLowcodeOr32 */
		firstValue2 = 0;
		resultValue2 = 0;
		secondValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask3 = 0;
		rTop17 = (rNext5 = NoReg);
		rResult7 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop17 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext5 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext5 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg3 = (rNext5 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask3 = ((reg3 < 0) ? (((usqInt)(1)) >> (-reg3)) : (1ULL << reg3));
			}
		}
		if (rTop17 == NoReg) {
			rTop17 = allocateRegNotConflictingWith(topRegistersMask3);
		}
		if (rNext5 == NoReg) {
			rNext5 = allocateRegNotConflictingWith(((rTop17 < 0) ? (((usqInt)(1)) >> (-rTop17)) : (1ULL << rTop17)));
		}
		assert(!(((rTop17 == NoReg)
 || (rNext5 == NoReg))));
		rResult7 = allocateFloatRegNotConflictingWith((1ULL << rTop17) | (1ULL << rNext5));
		assert(!((rResult7 == NoReg)));
		second2 = rTop17;
		first2 = rNext5;
		result2 = rResult7;
		nativePopToReg(ssNativeTop(), second2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first2);
		ssNativePop(1);
		/* begin OrR:R: */
		genoperandoperand(OrRR, second2, first2);
		ssPushNativeRegister(first2);
		return 0;

	case 146:
		/* begin genLowcodeOr64 */
		firstValue3 = 0;
		secondValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask4 = 0;
		rTop18 = (rNext11 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop18 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext11 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext11 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg4 = (rNext11 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask4 = ((reg4 < 0) ? (((usqInt)(1)) >> (-reg4)) : (1ULL << reg4));
			}
		}
		if (rTop18 == NoReg) {
			rTop18 = allocateRegNotConflictingWith(topRegistersMask4);
		}
		if (rNext11 == NoReg) {
			rNext11 = allocateRegNotConflictingWith(((rTop18 < 0) ? (((usqInt)(1)) >> (-rTop18)) : (1ULL << rTop18)));
		}
		assert(!(((rTop18 == NoReg)
 || (rNext11 == NoReg))));
		second3 = rTop18;
		first3 = rNext11;
		nativePopToReg(ssNativeTop(), second3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first3);
		ssNativePop(1);
		/* begin OrR:R: */
		genoperandoperand(OrRR, second3, first3);
		ssPushNativeRegister(first3);
		return 0;

	case 147:
		/* begin genLowcodePerformCallout */
		callSwitchToCStack();
		genoperandoperand(MoveCwR, extA, TempReg);
		abstractInstruction = genoperand(Call, ceFFICalloutTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction1->annotation = HasBytecodePC);
		extA = 0;
		return 0;

	case 148:
		/* begin genLowcodePerformCalloutIndirect */
		nativeStackPopToReg(ssNativeTop(), TempReg);
		ssNativePop(1);
		callSwitchToCStack();
		/* begin CallRT: */
		abstractInstruction2 = genoperand(Call, ceFFICalloutTrampoline);
		(abstractInstruction2->annotation = IsRelativeCall);
		abstractInstruction11 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction11->annotation = HasBytecodePC);
		return 0;

	case 149:
		/* begin genLowcodePushCalloutResultFloat32 */
		cFloatResultToRs(backEnd, DPFPReg0);
		ssPushNativeRegisterSingleFloat(DPFPReg0);
		return 0;

	case 150:
		/* begin genLowcodePushCalloutResultFloat64 */
		cFloatResultToRd(backEnd, DPFPReg0);
		ssPushNativeRegisterDoubleFloat(DPFPReg0);
		return 0;

	case 151:
		/* begin genLowcodePushCalloutResultInt32 */
		genoperandoperand(MoveRR, ABIResultReg, ReceiverResultReg);
		ssPushNativeRegister(ReceiverResultReg);
		return 0;

	case 152:
		/* begin genLowcodePushCalloutResultInt64 */
		genoperandoperand(MoveRR, ABIResultReg, ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ABIResultReg, ReceiverResultReg);
		ssPushNativeRegister(ReceiverResultReg);
		return 0;

	case 153:
		/* begin genLowcodePushCalloutResultPointer */
		genoperandoperand(MoveRR, ABIResultReg, ReceiverResultReg);
		ssPushNativeRegister(ReceiverResultReg);
		return 0;

	case 161:
		/* begin genLowcodePlaftormCode */
		abort();
		return 0;

	case 162:
		/* begin genLowcodePointerAddConstantOffset */
		baseValue = 0;
		offset4 = extB;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop19 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop19 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop19 == NoReg) {
			rTop19 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop19 == NoReg)));
		base = rTop19;
		nativePopToReg(ssNativeTop(), base);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, offset4, base);
		ssPushNativeRegister(base);
		extB = 0;
		numExtB = 0;
		return 0;

	case 163:
		/* begin genLowcodePointerAddOffset32 */
		baseValue1 = 0;
		offsetValue = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask5 = 0;
		rTop20 = (rNext6 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop20 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext6 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext6 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg5 = (rNext6 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask5 = ((reg5 < 0) ? (((usqInt)(1)) >> (-reg5)) : (1ULL << reg5));
			}
		}
		if (rTop20 == NoReg) {
			rTop20 = allocateRegNotConflictingWith(topRegistersMask5);
		}
		if (rNext6 == NoReg) {
			rNext6 = allocateRegNotConflictingWith(((rTop20 < 0) ? (((usqInt)(1)) >> (-rTop20)) : (1ULL << rTop20)));
		}
		assert(!(((rTop20 == NoReg)
 || (rNext6 == NoReg))));
		offset5 = rTop20;
		base1 = rNext6;
		nativePopToReg(ssNativeTop(), offset5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), base1);
		ssNativePop(1);
		/* begin AddR:R: */
		genoperandoperand(AddRR, offset5, base1);
		ssPushNativeRegister(base1);
		return 0;

	case 164:
		/* begin genLowcodePointerAddOffset64 */
		baseValue2 = 0;
		offsetValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask6 = 0;
		rTop110 = (rNext12 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop110 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext12 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext12 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg6 = (rNext12 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask6 = ((reg6 < 0) ? (((usqInt)(1)) >> (-reg6)) : (1ULL << reg6));
			}
		}
		if (rTop110 == NoReg) {
			rTop110 = allocateRegNotConflictingWith(topRegistersMask6);
		}
		if (rNext12 == NoReg) {
			rNext12 = allocateRegNotConflictingWith(((rTop110 < 0) ? (((usqInt)(1)) >> (-rTop110)) : (1ULL << rTop110)));
		}
		assert(!(((rTop110 == NoReg)
 || (rNext12 == NoReg))));
		offset6 = rTop110;
		base2 = rNext12;
		nativePopToReg(ssNativeTop(), offset6);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), base2);
		ssNativePop(1);
		/* begin AddR:R: */
		genoperandoperand(AddRR, offset6, base2);
		ssPushNativeRegister(base2);
		return 0;

	case 165:
		/* begin genLowcodePointerEqual */
		firstValue4 = 0;
		secondValue4 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask7 = 0;
		rTop21 = (rNext7 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop21 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext7 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext7 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg7 = (rNext7 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask7 = ((reg7 < 0) ? (((usqInt)(1)) >> (-reg7)) : (1ULL << reg7));
			}
		}
		if (rTop21 == NoReg) {
			rTop21 = allocateRegNotConflictingWith(topRegistersMask7);
		}
		if (rNext7 == NoReg) {
			rNext7 = allocateRegNotConflictingWith(((rTop21 < 0) ? (((usqInt)(1)) >> (-rTop21)) : (1ULL << rTop21)));
		}
		assert(!(((rTop21 == NoReg)
 || (rNext7 == NoReg))));
		second4 = rTop21;
		first4 = rNext7;
		nativePopToReg(ssNativeTop(), second4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first4);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second4 == SPReg)));
		genoperandoperand(CmpRR, second4, first4);
		/* True result */
		falseJump = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first4);
		/* False result */
		contJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first4);
		jmpTarget(contJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first4);
		return 0;

	case 166:
		/* begin genLowcodePointerNotEqual */
		firstValue5 = 0;
		secondValue5 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask8 = 0;
		rTop22 = (rNext8 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop22 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext8 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext8 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg8 = (rNext8 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask8 = ((reg8 < 0) ? (((usqInt)(1)) >> (-reg8)) : (1ULL << reg8));
			}
		}
		if (rTop22 == NoReg) {
			rTop22 = allocateRegNotConflictingWith(topRegistersMask8);
		}
		if (rNext8 == NoReg) {
			rNext8 = allocateRegNotConflictingWith(((rTop22 < 0) ? (((usqInt)(1)) >> (-rTop22)) : (1ULL << rTop22)));
		}
		assert(!(((rTop22 == NoReg)
 || (rNext8 == NoReg))));
		second5 = rTop22;
		first5 = rNext8;
		nativePopToReg(ssNativeTop(), second5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first5);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second5 == SPReg)));
		genoperandoperand(CmpRR, second5, first5);
		/* True result */
		falseJump1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first5);
		/* False result */
		contJump1 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first5);
		jmpTarget(contJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first5);
		return 0;

	case 167:
		/* begin genLowcodePointerToInt32 */
		pointerValue8 = 0;
		rTop23 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop23 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop23 == NoReg) {
			rTop23 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop23 == NoReg)));
		pointer8 = rTop23;
		nativePopToReg(ssNativeTop(), pointer8);
		ssNativePop(1);
		ssPushNativeRegister(pointer8);
		return 0;

	case 168:
		/* begin genLowcodePointerToInt64 */
		pointerValue1 = 0;
		resultValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop13 = NoReg;
		rResult11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop13 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop13 == NoReg) {
			rTop13 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult11 = allocateRegNotConflictingWith(((rTop13 < 0) ? (((usqInt)(1)) >> (-rTop13)) : (1ULL << rTop13)));
		assert(!(((rTop13 == NoReg)
 || (rResult11 == NoReg))));
		pointer1 = rTop13;
		result = rResult11;
		nativePopToReg(ssNativeTop(), pointer1);
		ssNativePop(1);
		ssPushNativeRegister(pointer1);
		return 0;

	case 169:
		/* begin genLowcodePopFloat32 */
		valueValue8 = 0;
		topRegistersMask9 = 0;
		frTop = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop == NoReg) {
			frTop = allocateFloatRegNotConflictingWith(topRegistersMask9);
		}
		assert(!((frTop == NoReg)));
		value8 = frTop;
		nativePopToReg(ssNativeTop(), value8);
		ssNativePop(1);
		return 0;

	case 170:
		/* begin genLowcodePopFloat64 */
		valueValue9 = 0;
		topRegistersMask10 = 0;
		frTop1 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop1 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop1 == NoReg) {
			frTop1 = allocateFloatRegNotConflictingWith(topRegistersMask10);
		}
		assert(!((frTop1 == NoReg)));
		value9 = frTop1;
		nativePopToReg(ssNativeTop(), value9);
		ssNativePop(1);
		return 0;

	case 171:
		/* begin genLowcodePopInt32 */
		valueValue10 = 0;
		rTop24 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop24 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop24 == NoReg) {
			rTop24 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop24 == NoReg)));
		value10 = rTop24;
		nativePopToReg(ssNativeTop(), value10);
		ssNativePop(1);
		return 0;

	case 172:
		/* begin genLowcodePopInt64 */
		valueValue11 = 0;
		rTop25 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop25 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop25 == NoReg) {
			rTop25 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop25 == NoReg)));
		value11 = rTop25;
		nativePopToReg(ssNativeTop(), value11);
		ssNativePop(1);
		return 0;

	case 173:
		/* begin genLowcodePopMultipleNative */
		ssPopNativeSize(extA);
		extA = 0;
		return 0;

	case 174:
		/* begin genLowcodePopPointer */
		pointerValueValue = 0;
		rTop26 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop26 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop26 == NoReg) {
			rTop26 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop26 == NoReg)));
		pointerValue9 = rTop26;
		nativePopToReg(ssNativeTop(), pointerValue9);
		ssNativePop(1);
		return 0;

	case 175:
		/* begin genLowcodePushConstantUInt32 */
		constant = extA;
		ssPushNativeConstantInt32(constant);
		extA = 0;
		return 0;

	case 176:
		/* begin genLowcodePushConstantUInt64 */
		constant1 = extA;
		ssPushNativeConstantInt64(constant1);
		extA = 0;
		return 0;

	case 177:
		/* begin genLowcodePushNullPointer */
		ssPushNativeConstantPointer(0);
		return 0;

	case 178:
		/* begin genLowcodePushOne32 */
		ssPushNativeConstantInt32(1);
		return 0;

	case 179:
		/* begin genLowcodePushOne64 */
		ssPushNativeConstantInt64(1);
		return 0;

	case 180:
		/* begin genLowcodePushOneFloat32 */
		ssPushNativeConstantFloat32(1.0);
		return 0;

	case 181:
		/* begin genLowcodePushOneFloat64 */
		ssPushNativeConstantFloat64(1.0);
		return 0;

	case 182:
		/* begin genLowcodePushPhysicalFloat32 */
		registerID = extA;
		abort();
		extA = 0;
		return 0;

	case 183:
		/* begin genLowcodePushPhysicalFloat64 */
		registerID1 = extA;
		abort();
		extA = 0;
		return 0;

	case 184:
		/* begin genLowcodePushPhysicalInt32 */
		registerID2 = extA;
		abort();
		extA = 0;
		return 0;

	case 185:
		/* begin genLowcodePushPhysicalInt64 */
		registerID3 = extA;
		abort();
		extA = 0;
		return 0;

	case 186:
		/* begin genLowcodePushPhysicalPointer */
		registerID4 = extA;
		abort();
		extA = 0;
		return 0;

	default:
		return genLowcodeUnaryInlinePrimitive4(prim);
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeUnaryInlinePrimitive4: */
static NoDbgRegParms sqInt
genLowcodeUnaryInlinePrimitive4(sqInt prim)
{
    sqInt baseOffset;
    sqInt baseOffset1;
    sqInt baseOffset2;
    sqInt baseOffset3;
    sqInt baseOffset4;
    sqInt baseOffset5;
    sqInt baseOffset6;
    AbstractInstruction *contJump;
    AbstractInstruction *contJump1;
    AbstractInstruction *contJump2;
    AbstractInstruction *contJump3;
    sqInt doubleValue;
    sqInt doubleValueValue;
    AbstractInstruction *falseJump;
    AbstractInstruction *falseJump1;
    AbstractInstruction *falseJump2;
    AbstractInstruction *falseJump3;
    sqInt first;
    sqInt first1;
    sqInt first10;
    sqInt first11;
    sqInt first12;
    sqInt first13;
    sqInt first14;
    sqInt first15;
    sqInt first16;
    sqInt first17;
    sqInt first18;
    sqInt first19;
    sqInt first2;
    sqInt first3;
    sqInt first4;
    sqInt first5;
    sqInt first6;
    sqInt first7;
    sqInt first8;
    sqInt firstValue;
    sqInt firstValue1;
    sqInt firstValue10;
    sqInt firstValue11;
    sqInt firstValue12;
    sqInt firstValue13;
    sqInt firstValue14;
    sqInt firstValue15;
    sqInt firstValue16;
    sqInt firstValue17;
    sqInt firstValue18;
    sqInt firstValue19;
    sqInt firstValue2;
    sqInt firstValue3;
    sqInt firstValue4;
    sqInt firstValue5;
    sqInt firstValue6;
    sqInt firstValue7;
    sqInt firstValue8;
    sqInt firstValue9;
    sqInt first9;
    sqInt floatValue;
    sqInt floatValueValue;
    sqInt frResult;
    sqInt frResult1;
    sqInt frResult2;
    sqInt frResult3;
    sqInt frTop;
    sqInt frTop1;
    sqInt frTop2;
    sqInt frTop3;
    sqInt memoryPointer;
    sqInt memoryPointerValue;
    sqInt pointer;
    sqInt pointer1;
    sqInt pointer2;
    sqInt pointer3;
    sqInt pointer4;
    sqInt pointer5;
    sqInt pointerValue;
    sqInt pointerValue1;
    sqInt pointerValue2;
    sqInt pointerValue3;
    sqInt pointerValue4;
    sqInt pointerValue5;
    sqInt pointerValue6;
    sqInt pointerValue7;
    sqInt pointerValueValue;
    sqInt pointerValueValue1;
    sqInt reg;
    sqInt reg1;
    sqInt reg10;
    sqInt reg11;
    sqInt reg12;
    sqInt reg13;
    sqInt reg14;
    sqInt reg15;
    sqInt reg16;
    sqInt reg17;
    sqInt reg18;
    sqInt reg19;
    sqInt reg2;
    sqInt reg20;
    sqInt reg21;
    sqInt reg22;
    sqInt reg23;
    sqInt reg24;
    sqInt reg25;
    sqInt reg26;
    sqInt reg3;
    sqInt reg4;
    sqInt reg5;
    sqInt reg6;
    sqInt reg7;
    sqInt reg8;
    sqInt reg9;
    sqInt result;
    sqInt result1;
    sqInt result10;
    sqInt result11;
    sqInt result2;
    sqInt result3;
    sqInt result4;
    sqInt result5;
    sqInt result6;
    sqInt result7;
    sqInt result8;
    sqInt resultValue;
    sqInt resultValue1;
    sqInt resultValue10;
    sqInt resultValue11;
    sqInt resultValue2;
    sqInt resultValue3;
    sqInt resultValue4;
    sqInt resultValue5;
    sqInt resultValue6;
    sqInt resultValue7;
    sqInt resultValue8;
    sqInt resultValue9;
    sqInt result9;
    sqInt rNext;
    sqInt rNext1;
    sqInt rNext10;
    sqInt rNext11;
    sqInt rNext110;
    sqInt rNext12;
    sqInt rNext13;
    sqInt rNext14;
    sqInt rNext15;
    sqInt rNext16;
    sqInt rNext17;
    sqInt rNext18;
    sqInt rNext19;
    sqInt rNext2;
    sqInt rNext20;
    sqInt rNext21;
    sqInt rNext22;
    sqInt rNext23;
    sqInt rNext24;
    sqInt rNext25;
    sqInt rNext3;
    sqInt rNext4;
    sqInt rNext5;
    sqInt rNext6;
    sqInt rNext7;
    sqInt rNext8;
    sqInt rNext9;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult11;
    sqInt rResult12;
    sqInt rResult2;
    sqInt rResult3;
    sqInt rResult4;
    sqInt rResult5;
    sqInt rResult6;
    sqInt rResult7;
    sqInt rResult8;
    sqInt rResult9;
    sqInt rTop;
    sqInt rTop1;
    sqInt rTop10;
    sqInt rTop11;
    sqInt rTop110;
    sqInt rTop111;
    sqInt rTop112;
    sqInt rTop12;
    sqInt rTop13;
    sqInt rTop14;
    sqInt rTop15;
    sqInt rTop16;
    sqInt rTop17;
    sqInt rTop18;
    sqInt rTop19;
    sqInt rTop2;
    sqInt rTop20;
    sqInt rTop21;
    sqInt rTop22;
    sqInt rTop23;
    sqInt rTop24;
    sqInt rTop25;
    sqInt rTop26;
    sqInt rTop27;
    sqInt rTop28;
    sqInt rTop29;
    sqInt rTop3;
    sqInt rTop30;
    sqInt rTop31;
    sqInt rTop32;
    sqInt rTop33;
    sqInt rTop34;
    sqInt rTop35;
    sqInt rTop36;
    sqInt rTop37;
    sqInt rTop38;
    sqInt rTop39;
    sqInt rTop4;
    sqInt rTop40;
    sqInt rTop41;
    sqInt rTop42;
    sqInt rTop43;
    sqInt rTop44;
    sqInt rTop45;
    sqInt rTop46;
    sqInt rTop47;
    sqInt rTop5;
    sqInt rTop6;
    sqInt rTop7;
    sqInt rTop8;
    sqInt rTop9;
    sqInt second;
    sqInt second1;
    sqInt second10;
    sqInt second11;
    sqInt second12;
    sqInt second13;
    sqInt second14;
    sqInt second15;
    sqInt second16;
    sqInt second17;
    sqInt second18;
    sqInt second19;
    sqInt second2;
    sqInt second3;
    sqInt second4;
    sqInt second5;
    sqInt second6;
    sqInt second7;
    sqInt second8;
    sqInt secondValue;
    sqInt secondValue1;
    sqInt secondValue10;
    sqInt secondValue11;
    sqInt secondValue12;
    sqInt secondValue13;
    sqInt secondValue14;
    sqInt secondValue15;
    sqInt secondValue16;
    sqInt secondValue17;
    sqInt secondValue18;
    sqInt secondValue19;
    sqInt secondValue2;
    sqInt secondValue3;
    sqInt secondValue4;
    sqInt secondValue5;
    sqInt secondValue6;
    sqInt secondValue7;
    sqInt secondValue8;
    sqInt secondValue9;
    sqInt second9;
    sqInt shiftAmount;
    sqInt shiftAmount1;
    sqInt shiftAmountValue;
    sqInt shiftAmountValue1;
    sqInt topRegistersMask;
    sqInt topRegistersMask1;
    sqInt topRegistersMask10;
    sqInt topRegistersMask11;
    sqInt topRegistersMask12;
    sqInt topRegistersMask13;
    sqInt topRegistersMask14;
    sqInt topRegistersMask15;
    sqInt topRegistersMask16;
    sqInt topRegistersMask17;
    sqInt topRegistersMask18;
    sqInt topRegistersMask19;
    sqInt topRegistersMask2;
    sqInt topRegistersMask20;
    sqInt topRegistersMask21;
    sqInt topRegistersMask22;
    sqInt topRegistersMask23;
    sqInt topRegistersMask24;
    sqInt topRegistersMask25;
    sqInt topRegistersMask26;
    sqInt topRegistersMask27;
    sqInt topRegistersMask28;
    sqInt topRegistersMask3;
    sqInt topRegistersMask4;
    sqInt topRegistersMask5;
    sqInt topRegistersMask6;
    sqInt topRegistersMask7;
    sqInt topRegistersMask8;
    sqInt topRegistersMask9;
    sqInt value;
    sqInt value1;
    sqInt value10;
    sqInt value11;
    sqInt value12;
    sqInt value13;
    sqInt value14;
    sqInt value15;
    sqInt value16;
    sqInt value17;
    sqInt value18;
    sqInt value19;
    sqInt value2;
    sqInt value20;
    sqInt value21;
    sqInt value22;
    sqInt value23;
    sqInt value24;
    sqInt value25;
    sqInt value26;
    sqInt value27;
    sqInt value28;
    sqInt value29;
    sqInt value3;
    sqInt value30;
    sqInt value31;
    sqInt value32;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt value7;
    sqInt value8;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue10;
    sqInt valueValue11;
    sqInt valueValue12;
    sqInt valueValue13;
    sqInt valueValue14;
    sqInt valueValue15;
    sqInt valueValue16;
    sqInt valueValue17;
    sqInt valueValue18;
    sqInt valueValue19;
    sqInt valueValue2;
    sqInt valueValue20;
    sqInt valueValue21;
    sqInt valueValue22;
    sqInt valueValue23;
    sqInt valueValue24;
    sqInt valueValue25;
    sqInt valueValue26;
    sqInt valueValue27;
    sqInt valueValue28;
    sqInt valueValue29;
    sqInt valueValue3;
    sqInt valueValue30;
    sqInt valueValue31;
    sqInt valueValue32;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;
    sqInt valueValue7;
    sqInt valueValue8;
    sqInt valueValue9;
    sqInt value9;

	switch (prim) {
	case 187:
		/* begin genLowcodePushSessionIdentifier */
		ssPushNativeConstantInt32(getThisSessionID());
		return 0;

	case 188:
		/* begin genLowcodePushZero32 */
		ssPushNativeConstantInt32(0);
		return 0;

	case 189:
		/* begin genLowcodePushZero64 */
		ssPushNativeConstantInt64(0);
		return 0;

	case 190:
		/* begin genLowcodePushZeroFloat32 */
		ssPushNativeConstantFloat32(0.0);
		return 0;

	case 191:
		/* begin genLowcodePushZeroFloat64 */
		ssPushNativeConstantFloat64(0.0);
		return 0;

	case 192:
		/* begin genLowcodeRem32 */
		firstValue = 0;
		secondValue = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask = 0;
		rTop3 = (rNext = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop3 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg = (rNext = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
		}
		if (rTop3 == NoReg) {
			rTop3 = allocateRegNotConflictingWith(topRegistersMask);
		}
		if (rNext == NoReg) {
			rNext = allocateRegNotConflictingWith(((rTop3 < 0) ? (((usqInt)(1)) >> (-rTop3)) : (1ULL << rTop3)));
		}
		assert(!(((rTop3 == NoReg)
 || (rNext == NoReg))));
		second = rTop3;
		first = rNext;
		nativePopToReg(ssNativeTop(), second);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first);
		ssNativePop(1);
		gDivRRQuoRem(second, first, second, first);
		ssPushNativeRegister(first);
		return 0;

	case 193:
		/* begin genLowcodeRem64 */
		firstValue1 = 0;
		resultValue3 = 0;
		secondValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask1 = 0;
		rTop4 = (rNext1 = NoReg);
		rResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop4 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext1 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg1 = (rNext1 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask1 = ((reg1 < 0) ? (((usqInt)(1)) >> (-reg1)) : (1ULL << reg1));
			}
		}
		if (rTop4 == NoReg) {
			rTop4 = allocateRegNotConflictingWith(topRegistersMask1);
		}
		if (rNext1 == NoReg) {
			rNext1 = allocateRegNotConflictingWith(((rTop4 < 0) ? (((usqInt)(1)) >> (-rTop4)) : (1ULL << rTop4)));
		}
		assert(!(((rTop4 == NoReg)
 || (rNext1 == NoReg))));
		rResult = allocateFloatRegNotConflictingWith((1ULL << rTop4) | (1ULL << rNext1));
		assert(!((rResult == NoReg)));
		second1 = rTop4;
		first1 = rNext1;
		result3 = rResult;
		nativePopToReg(ssNativeTop(), second1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first1);
		ssNativePop(1);
		abort();
		return 0;

	case 194:
		/* begin genLowcodeRightShift32 */
		shiftAmountValue = 0;
		valueValue10 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask2 = 0;
		rTop5 = (rNext2 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop5 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext2 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext2 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg2 = (rNext2 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask2 = ((reg2 < 0) ? (((usqInt)(1)) >> (-reg2)) : (1ULL << reg2));
			}
		}
		if (rTop5 == NoReg) {
			rTop5 = allocateRegNotConflictingWith(topRegistersMask2);
		}
		if (rNext2 == NoReg) {
			rNext2 = allocateRegNotConflictingWith(((rTop5 < 0) ? (((usqInt)(1)) >> (-rTop5)) : (1ULL << rTop5)));
		}
		assert(!(((rTop5 == NoReg)
 || (rNext2 == NoReg))));
		shiftAmount = rTop5;
		value10 = rNext2;
		nativePopToReg(ssNativeTop(), shiftAmount);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value10);
		ssNativePop(1);
		/* begin LogicalShiftRightR:R: */
		genoperandoperand(LogicalShiftRightRR, shiftAmount, value10);
		ssPushNativeRegister(value10);
		return 0;

	case 195:
		/* begin genLowcodeRightShift64 */
		resultValue4 = 0;
		shiftAmountValue1 = 0;
		valueValue11 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask3 = 0;
		rTop6 = (rNext3 = NoReg);
		rResult2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop6 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext3 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext3 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg3 = (rNext3 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask3 = ((reg3 < 0) ? (((usqInt)(1)) >> (-reg3)) : (1ULL << reg3));
			}
		}
		if (rTop6 == NoReg) {
			rTop6 = allocateRegNotConflictingWith(topRegistersMask3);
		}
		if (rNext3 == NoReg) {
			rNext3 = allocateRegNotConflictingWith(((rTop6 < 0) ? (((usqInt)(1)) >> (-rTop6)) : (1ULL << rTop6)));
		}
		assert(!(((rTop6 == NoReg)
 || (rNext3 == NoReg))));
		rResult2 = allocateFloatRegNotConflictingWith((1ULL << rTop6) | (1ULL << rNext3));
		assert(!((rResult2 == NoReg)));
		shiftAmount1 = rTop6;
		value11 = rNext3;
		result4 = rResult2;
		nativePopToReg(ssNativeTop(), shiftAmount1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value11);
		ssNativePop(1);
		abort();
		return 0;

	case 196:
		/* begin genLowcodeSignExtend32From16 */
		valueValue12 = 0;
		rTop7 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop7 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop7 == NoReg) {
			rTop7 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop7 == NoReg)));
		value12 = rTop7;
		nativePopToReg(ssNativeTop(), value12);
		ssNativePop(1);
		/* begin SignExtend16R:R: */
		genoperandoperand(SignExtend16RR, value12, value12);
		ssPushNativeRegister(value12);
		return 0;

	case 197:
		/* begin genLowcodeSignExtend32From8 */
		valueValue13 = 0;
		rTop8 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop8 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop8 == NoReg) {
			rTop8 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop8 == NoReg)));
		value13 = rTop8;
		nativePopToReg(ssNativeTop(), value13);
		ssNativePop(1);
		/* begin SignExtend8R:R: */
		genoperandoperand(SignExtend8RR, value13, value13);
		ssPushNativeRegister(value13);
		return 0;

	case 198:
		/* begin genLowcodeSignExtend64From16 */
		valueValue = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop1 == NoReg)));
		value = rTop1;
		nativePopToReg(ssNativeTop(), value);
		ssNativePop(1);
		/* begin SignExtend16R:R: */
		genoperandoperand(SignExtend16RR, value, value);
		ssPushNativeRegister(value);
		return 0;

	case 199:
		/* begin genLowcodeSignExtend64From32 */
		resultValue = 0;
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop11 = NoReg;
		rResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rTop11 < 0) ? (((usqInt)(1)) >> (-rTop11)) : (1ULL << rTop11)));
		assert(!(((rTop11 == NoReg)
 || (rResult1 == NoReg))));
		value1 = rTop11;
		result = rResult1;
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin SignExtend32R:R: */
		genoperandoperand(SignExtend32RR, value1, value1);
		ssPushNativeRegister(value1);
		return 0;

	case 200:
		/* begin genLowcodeSignExtend64From8 */
		resultValue1 = 0;
		valueValue2 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop12 = NoReg;
		rResult11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop12 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop12 == NoReg) {
			rTop12 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult11 = allocateRegNotConflictingWith(((rTop12 < 0) ? (((usqInt)(1)) >> (-rTop12)) : (1ULL << rTop12)));
		assert(!(((rTop12 == NoReg)
 || (rResult11 == NoReg))));
		value2 = rTop12;
		result1 = rResult11;
		nativePopToReg(ssNativeTop(), value2);
		ssNativePop(1);
		/* begin ZeroExtend16R:R: */
		genoperandoperand(ZeroExtend16RR, value2, value2);
		ssPushNativeRegister(value2);
		return 0;

	case 201:
		/* begin genLowcodeStoreFloat32ToMemory */
		floatValueValue = 0;
		pointerValue = 0;
		/* begin allocateRegistersForLowcodeFloatInteger: */
		/* Integer registers */
		frTop = (rTop9 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop9 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop9 == NoReg) {
			rTop9 = allocateRegNotConflictingWith(0);
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			frTop = nativeFloatRegisterOrNone(ssNativeValue(1));
		}
		if (frTop == NoReg) {
			frTop = allocateFloatRegNotConflictingWith(0);
		}
		assert(!(((frTop == NoReg)
 || (rTop9 == NoReg))));
		floatValue = frTop;
		pointer = rTop9;
		nativePopToReg(ssNativeTop(), pointer);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), floatValue);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRsM32r, floatValue, 0, pointer);
		return 0;

	case 202:
		/* begin genLowcodeStoreFloat64ToMemory */
		doubleValueValue = 0;
		pointerValue1 = 0;
		/* begin allocateRegistersForLowcodeFloatInteger: */
		/* Integer registers */
		frTop1 = (rTop10 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop10 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop10 == NoReg) {
			rTop10 = allocateRegNotConflictingWith(0);
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			frTop1 = nativeFloatRegisterOrNone(ssNativeValue(1));
		}
		if (frTop1 == NoReg) {
			frTop1 = allocateFloatRegNotConflictingWith(0);
		}
		assert(!(((frTop1 == NoReg)
 || (rTop10 == NoReg))));
		doubleValue = frTop1;
		pointer1 = rTop10;
		nativePopToReg(ssNativeTop(), pointer1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), doubleValue);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRdM64r, doubleValue, 0, pointer1);
		return 0;

	case 203:
		/* begin genLowcodeStoreInt16ToMemory */
		pointerValue2 = 0;
		valueValue14 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask4 = 0;
		rTop18 = (rNext4 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop18 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext4 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext4 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg4 = (rNext4 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask4 = ((reg4 < 0) ? (((usqInt)(1)) >> (-reg4)) : (1ULL << reg4));
			}
		}
		if (rTop18 == NoReg) {
			rTop18 = allocateRegNotConflictingWith(topRegistersMask4);
		}
		if (rNext4 == NoReg) {
			rNext4 = allocateRegNotConflictingWith(((rTop18 < 0) ? (((usqInt)(1)) >> (-rTop18)) : (1ULL << rTop18)));
		}
		assert(!(((rTop18 == NoReg)
 || (rNext4 == NoReg))));
		pointer2 = rTop18;
		value14 = rNext4;
		nativePopToReg(ssNativeTop(), pointer2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value14);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value14, TempReg);
		genoperandoperandoperand(MoveRM16r, TempReg, 0, pointer2);
		return 0;

	case 204:
		/* begin genLowcodeStoreInt32ToMemory */
		pointerValue3 = 0;
		valueValue15 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask5 = 0;
		rTop19 = (rNext5 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop19 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext5 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext5 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg5 = (rNext5 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask5 = ((reg5 < 0) ? (((usqInt)(1)) >> (-reg5)) : (1ULL << reg5));
			}
		}
		if (rTop19 == NoReg) {
			rTop19 = allocateRegNotConflictingWith(topRegistersMask5);
		}
		if (rNext5 == NoReg) {
			rNext5 = allocateRegNotConflictingWith(((rTop19 < 0) ? (((usqInt)(1)) >> (-rTop19)) : (1ULL << rTop19)));
		}
		assert(!(((rTop19 == NoReg)
 || (rNext5 == NoReg))));
		pointer3 = rTop19;
		value15 = rNext5;
		nativePopToReg(ssNativeTop(), pointer3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value15);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRM32r, value15, 0, pointer3);
		return 0;

	case 205:
		/* begin genLowcodeStoreInt64ToMemory */
		pointerValue4 = 0;
		valueValue16 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask6 = 0;
		rTop110 = (rNext11 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop110 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext11 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext11 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg6 = (rNext11 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask6 = ((reg6 < 0) ? (((usqInt)(1)) >> (-reg6)) : (1ULL << reg6));
			}
		}
		if (rTop110 == NoReg) {
			rTop110 = allocateRegNotConflictingWith(topRegistersMask6);
		}
		if (rNext11 == NoReg) {
			rNext11 = allocateRegNotConflictingWith(((rTop110 < 0) ? (((usqInt)(1)) >> (-rTop110)) : (1ULL << rTop110)));
		}
		assert(!(((rTop110 == NoReg)
 || (rNext11 == NoReg))));
		pointer4 = rTop110;
		value16 = rNext11;
		nativePopToReg(ssNativeTop(), pointer4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value16);
		ssNativePop(1);
		/* begin MoveR:M64:r: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveRMwr, value16, 0, pointer4);
		return 0;

	case 206:
		/* begin genLowcodeStoreInt8ToMemory */
		pointerValue5 = 0;
		valueValue17 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask7 = 0;
		rTop20 = (rNext6 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop20 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext6 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext6 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg7 = (rNext6 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask7 = ((reg7 < 0) ? (((usqInt)(1)) >> (-reg7)) : (1ULL << reg7));
			}
		}
		if (rTop20 == NoReg) {
			rTop20 = allocateRegNotConflictingWith(topRegistersMask7);
		}
		if (rNext6 == NoReg) {
			rNext6 = allocateRegNotConflictingWith(((rTop20 < 0) ? (((usqInt)(1)) >> (-rTop20)) : (1ULL << rTop20)));
		}
		assert(!(((rTop20 == NoReg)
 || (rNext6 == NoReg))));
		pointer5 = rTop20;
		value17 = rNext6;
		nativePopToReg(ssNativeTop(), pointer5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value17);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value17, TempReg);
		genoperandoperandoperand(MoveRM8r, TempReg, 0, pointer5);
		return 0;

	case 207:
		/* begin genLowcodeStoreLocalFloat32 */
		valueValue18 = 0;
		baseOffset3 = extA;
		/* begin allocateRegistersForLowcodeFloat: */
		topRegistersMask8 = 0;
		frTop2 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop2 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop2 == NoReg) {
			frTop2 = allocateFloatRegNotConflictingWith(topRegistersMask8);
		}
		assert(!((frTop2 == NoReg)));
		value18 = frTop2;
		nativePopToReg(ssNativeTop(), value18);
		ssNativePop(1);
		loadNativeLocalAddressto(baseOffset3, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRsM32r, value18, 0, TempReg);
		extA = 0;
		return 0;

	case 208:
		/* begin genLowcodeStoreLocalFloat64 */
		valueValue19 = 0;
		baseOffset4 = extA;
		/* begin allocateRegistersForLowcodeFloat: */
		topRegistersMask9 = 0;
		frTop3 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop3 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop3 == NoReg) {
			frTop3 = allocateFloatRegNotConflictingWith(topRegistersMask9);
		}
		assert(!((frTop3 == NoReg)));
		value19 = frTop3;
		nativePopToReg(ssNativeTop(), value19);
		ssNativePop(1);
		loadNativeLocalAddressto(baseOffset4, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRdM64r, value19, 0, TempReg);
		extA = 0;
		return 0;

	case 209:
		/* begin genLowcodeStoreLocalInt16 */
		valueValue3 = 0;
		baseOffset = extA;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop == NoReg)));
		value3 = rTop;
		nativePopToReg(ssNativeTop(), value3);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value3, TempReg);
		loadNativeLocalAddressto(baseOffset, value3);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRM16r, TempReg, 0, value3);
		extA = 0;
		return 0;

	case 210:
		/* begin genLowcodeStoreLocalInt32 */
		valueValue20 = 0;
		baseOffset5 = extA;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop21 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop21 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop21 == NoReg) {
			rTop21 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop21 == NoReg)));
		value20 = rTop21;
		nativePopToReg(ssNativeTop(), value20);
		ssNativePop(1);
		loadNativeLocalAddressto(baseOffset5, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRM32r, value20, 0, TempReg);
		extA = 0;
		return 0;

	case 211:
		/* begin genLowcodeStoreLocalInt64 */
		valueValue4 = 0;
		baseOffset1 = extA;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop13 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop13 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop13 == NoReg) {
			rTop13 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop13 == NoReg)));
		value4 = rTop13;
		nativePopToReg(ssNativeTop(), value4);
		ssNativePop(1);
		/* begin MoveR:M64:r: */
		assert(BytesPerWord == 8);
		genoperandoperandoperand(MoveRMwr, value4, 0, TempReg);
		extA = 0;
		return 0;

	case 212:
		/* begin genLowcodeStoreLocalInt8 */
		valueValue5 = 0;
		baseOffset2 = extA;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop2 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop2 == NoReg) {
			rTop2 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop2 == NoReg)));
		value5 = rTop2;
		nativePopToReg(ssNativeTop(), value5);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value5, TempReg);
		loadNativeLocalAddressto(baseOffset2, value5);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRM8r, TempReg, 0, value5);
		extA = 0;
		return 0;

	case 213:
		/* begin genLowcodeStoreLocalPointer */
		pointerValueValue = 0;
		baseOffset6 = extA;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop22 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop22 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop22 == NoReg) {
			rTop22 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop22 == NoReg)));
		pointerValue6 = rTop22;
		nativePopToReg(ssNativeTop(), pointerValue6);
		ssNativePop(1);
		loadNativeLocalAddressto(baseOffset6, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, pointerValue6, 0, TempReg);
		extA = 0;
		return 0;

	case 214:
		/* begin genLowcodeStorePointerToMemory */
		memoryPointerValue = 0;
		pointerValueValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask10 = 0;
		rTop23 = (rNext7 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop23 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext7 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext7 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg8 = (rNext7 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask10 = ((reg8 < 0) ? (((usqInt)(1)) >> (-reg8)) : (1ULL << reg8));
			}
		}
		if (rTop23 == NoReg) {
			rTop23 = allocateRegNotConflictingWith(topRegistersMask10);
		}
		if (rNext7 == NoReg) {
			rNext7 = allocateRegNotConflictingWith(((rTop23 < 0) ? (((usqInt)(1)) >> (-rTop23)) : (1ULL << rTop23)));
		}
		assert(!(((rTop23 == NoReg)
 || (rNext7 == NoReg))));
		memoryPointer = rTop23;
		pointerValue7 = rNext7;
		nativePopToReg(ssNativeTop(), memoryPointer);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), pointerValue7);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRMwr, pointerValue7, 0, memoryPointer);
		return 0;

	case 215:
		/* begin genLowcodeSub32 */
		firstValue2 = 0;
		secondValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask11 = 0;
		rTop24 = (rNext8 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop24 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext8 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext8 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg9 = (rNext8 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask11 = ((reg9 < 0) ? (((usqInt)(1)) >> (-reg9)) : (1ULL << reg9));
			}
		}
		if (rTop24 == NoReg) {
			rTop24 = allocateRegNotConflictingWith(topRegistersMask11);
		}
		if (rNext8 == NoReg) {
			rNext8 = allocateRegNotConflictingWith(((rTop24 < 0) ? (((usqInt)(1)) >> (-rTop24)) : (1ULL << rTop24)));
		}
		assert(!(((rTop24 == NoReg)
 || (rNext8 == NoReg))));
		second2 = rTop24;
		first2 = rNext8;
		nativePopToReg(ssNativeTop(), second2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first2);
		ssNativePop(1);
		/* begin SubR:R: */
		genoperandoperand(SubRR, second2, first2);
		ssPushNativeRegister(first2);
		return 0;

	case 216:
		/* begin genLowcodeSub64 */
		firstValue3 = 0;
		secondValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask12 = 0;
		rTop111 = (rNext12 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop111 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext12 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext12 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg10 = (rNext12 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask12 = ((reg10 < 0) ? (((usqInt)(1)) >> (-reg10)) : (1ULL << reg10));
			}
		}
		if (rTop111 == NoReg) {
			rTop111 = allocateRegNotConflictingWith(topRegistersMask12);
		}
		if (rNext12 == NoReg) {
			rNext12 = allocateRegNotConflictingWith(((rTop111 < 0) ? (((usqInt)(1)) >> (-rTop111)) : (1ULL << rTop111)));
		}
		assert(!(((rTop111 == NoReg)
 || (rNext12 == NoReg))));
		second3 = rTop111;
		first3 = rNext12;
		nativePopToReg(ssNativeTop(), second3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first3);
		ssNativePop(1);
		/* begin SubR:R: */
		genoperandoperand(SubRR, second3, first3);
		ssPushNativeRegister(first3);
		return 0;

	case 217:
		/* begin genLowcodeTruncate32To16 */
		valueValue21 = 0;
		rTop25 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop25 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop25 == NoReg) {
			rTop25 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop25 == NoReg)));
		value21 = rTop25;
		nativePopToReg(ssNativeTop(), value21);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, 0xFFFF, value21);
		ssPushNativeRegister(value21);
		return 0;

	case 218:
		/* begin genLowcodeTruncate32To8 */
		valueValue22 = 0;
		rTop26 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop26 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop26 == NoReg) {
			rTop26 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop26 == NoReg)));
		value22 = rTop26;
		nativePopToReg(ssNativeTop(), value22);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, 0xFF, value22);
		ssPushNativeRegister(value22);
		return 0;

	case 219:
		/* begin genLowcodeTruncate64To16 */
		valueValue6 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop14 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop14 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop14 == NoReg) {
			rTop14 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop14 == NoReg)));
		value6 = rTop14;
		nativePopToReg(ssNativeTop(), value6);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, 0xFFFF, value6);
		ssPushNativeRegister(value6);
		return 0;

	case 220:
		/* begin genLowcodeTruncate64To32 */
		valueValue7 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop15 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop15 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop15 == NoReg) {
			rTop15 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop15 == NoReg)));
		value7 = rTop15;
		nativePopToReg(ssNativeTop(), value7);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, 0xFFFFFFFFU, value7);
		ssPushNativeRegister(value7);
		return 0;

	case 221:
		/* begin genLowcodeTruncate64To8 */
		resultValue2 = 0;
		valueValue8 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop16 = NoReg;
		rResult12 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop16 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop16 == NoReg) {
			rTop16 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult12 = allocateRegNotConflictingWith(((rTop16 < 0) ? (((usqInt)(1)) >> (-rTop16)) : (1ULL << rTop16)));
		assert(!(((rTop16 == NoReg)
 || (rResult12 == NoReg))));
		value8 = rTop16;
		result2 = rResult12;
		nativePopToReg(ssNativeTop(), value8);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AndCqR, 0xFF, value8);
		ssPushNativeRegister(value8);
		return 0;

	case 222:
		/* begin genLowcodeUdiv32 */
		firstValue4 = 0;
		secondValue4 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask13 = 0;
		rTop27 = (rNext9 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop27 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext9 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext9 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg11 = (rNext9 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask13 = ((reg11 < 0) ? (((usqInt)(1)) >> (-reg11)) : (1ULL << reg11));
			}
		}
		if (rTop27 == NoReg) {
			rTop27 = allocateRegNotConflictingWith(topRegistersMask13);
		}
		if (rNext9 == NoReg) {
			rNext9 = allocateRegNotConflictingWith(((rTop27 < 0) ? (((usqInt)(1)) >> (-rTop27)) : (1ULL << rTop27)));
		}
		assert(!(((rTop27 == NoReg)
 || (rNext9 == NoReg))));
		second4 = rTop27;
		first4 = rNext9;
		nativePopToReg(ssNativeTop(), second4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first4);
		ssNativePop(1);
		gDivRRQuoRem(second4, first4, first4, second4);
		ssPushNativeRegister(first4);
		return 0;

	case 223:
		/* begin genLowcodeUdiv64 */
		firstValue5 = 0;
		resultValue5 = 0;
		secondValue5 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask14 = 0;
		rTop28 = (rNext10 = NoReg);
		rResult3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop28 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext10 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext10 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg12 = (rNext10 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask14 = ((reg12 < 0) ? (((usqInt)(1)) >> (-reg12)) : (1ULL << reg12));
			}
		}
		if (rTop28 == NoReg) {
			rTop28 = allocateRegNotConflictingWith(topRegistersMask14);
		}
		if (rNext10 == NoReg) {
			rNext10 = allocateRegNotConflictingWith(((rTop28 < 0) ? (((usqInt)(1)) >> (-rTop28)) : (1ULL << rTop28)));
		}
		assert(!(((rTop28 == NoReg)
 || (rNext10 == NoReg))));
		rResult3 = allocateFloatRegNotConflictingWith((1ULL << rTop28) | (1ULL << rNext10));
		assert(!((rResult3 == NoReg)));
		second5 = rTop28;
		first5 = rNext10;
		result5 = rResult3;
		nativePopToReg(ssNativeTop(), second5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first5);
		ssNativePop(1);
		abort();
		return 0;

	case 224:
		/* begin genLowcodeUint32Great */
		firstValue6 = 0;
		secondValue6 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask15 = 0;
		rTop29 = (rNext13 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop29 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext13 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext13 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg13 = (rNext13 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask15 = ((reg13 < 0) ? (((usqInt)(1)) >> (-reg13)) : (1ULL << reg13));
			}
		}
		if (rTop29 == NoReg) {
			rTop29 = allocateRegNotConflictingWith(topRegistersMask15);
		}
		if (rNext13 == NoReg) {
			rNext13 = allocateRegNotConflictingWith(((rTop29 < 0) ? (((usqInt)(1)) >> (-rTop29)) : (1ULL << rTop29)));
		}
		assert(!(((rTop29 == NoReg)
 || (rNext13 == NoReg))));
		second6 = rTop29;
		first6 = rNext13;
		nativePopToReg(ssNativeTop(), second6);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first6);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second6 == SPReg)));
		genoperandoperand(CmpRR, second6, first6);
		/* True result */
		falseJump = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first6);
		/* False result */
		contJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first6);
		jmpTarget(contJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first6);
		return 0;

	case 225:
		/* begin genLowcodeUint32GreatEqual */
		firstValue7 = 0;
		secondValue7 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask16 = 0;
		rTop30 = (rNext14 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop30 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext14 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext14 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg14 = (rNext14 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask16 = ((reg14 < 0) ? (((usqInt)(1)) >> (-reg14)) : (1ULL << reg14));
			}
		}
		if (rTop30 == NoReg) {
			rTop30 = allocateRegNotConflictingWith(topRegistersMask16);
		}
		if (rNext14 == NoReg) {
			rNext14 = allocateRegNotConflictingWith(((rTop30 < 0) ? (((usqInt)(1)) >> (-rTop30)) : (1ULL << rTop30)));
		}
		assert(!(((rTop30 == NoReg)
 || (rNext14 == NoReg))));
		second7 = rTop30;
		first7 = rNext14;
		nativePopToReg(ssNativeTop(), second7);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first7);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second7 == SPReg)));
		genoperandoperand(CmpRR, second7, first7);
		/* True result */
		falseJump1 = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first7);
		/* False result */
		contJump1 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first7);
		jmpTarget(contJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first7);
		return 0;

	case 226:
		/* begin genLowcodeUint32Less */
		firstValue8 = 0;
		secondValue8 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask17 = 0;
		rTop31 = (rNext15 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop31 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext15 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext15 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg15 = (rNext15 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask17 = ((reg15 < 0) ? (((usqInt)(1)) >> (-reg15)) : (1ULL << reg15));
			}
		}
		if (rTop31 == NoReg) {
			rTop31 = allocateRegNotConflictingWith(topRegistersMask17);
		}
		if (rNext15 == NoReg) {
			rNext15 = allocateRegNotConflictingWith(((rTop31 < 0) ? (((usqInt)(1)) >> (-rTop31)) : (1ULL << rTop31)));
		}
		assert(!(((rTop31 == NoReg)
 || (rNext15 == NoReg))));
		second8 = rTop31;
		first8 = rNext15;
		nativePopToReg(ssNativeTop(), second8);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first8);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second8 == SPReg)));
		genoperandoperand(CmpRR, second8, first8);
		/* True result */
		falseJump2 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first8);
		/* False result */
		contJump2 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first8);
		jmpTarget(contJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first8);
		return 0;

	case 227:
		/* begin genLowcodeUint32LessEqual */
		firstValue9 = 0;
		secondValue9 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask18 = 0;
		rTop32 = (rNext16 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop32 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext16 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext16 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg16 = (rNext16 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask18 = ((reg16 < 0) ? (((usqInt)(1)) >> (-reg16)) : (1ULL << reg16));
			}
		}
		if (rTop32 == NoReg) {
			rTop32 = allocateRegNotConflictingWith(topRegistersMask18);
		}
		if (rNext16 == NoReg) {
			rNext16 = allocateRegNotConflictingWith(((rTop32 < 0) ? (((usqInt)(1)) >> (-rTop32)) : (1ULL << rTop32)));
		}
		assert(!(((rTop32 == NoReg)
 || (rNext16 == NoReg))));
		second9 = rTop32;
		first9 = rNext16;
		nativePopToReg(ssNativeTop(), second9);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first9);
		ssNativePop(1);
		/* begin CmpR:R: */
		assert(!((second9 == SPReg)));
		genoperandoperand(CmpRR, second9, first9);
		/* True result */
		falseJump3 = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 1, first9);
		/* False result */
		contJump3 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, first9);
		jmpTarget(contJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(first9);
		return 0;

	case 228:
		/* begin genLowcodeUint32ToFloat32 */
		resultValue6 = 0;
		valueValue23 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop33 = NoReg;
		frResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop33 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop33 == NoReg) {
			rTop33 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop33 == NoReg)
 || (frResult == NoReg))));
		value23 = rTop33;
		result6 = frResult;
		nativePopToReg(ssNativeTop(), value23);
		ssNativePop(1);
		/* begin ConvertR:Rs: */
		genoperandoperand(ConvertRRs, value23, result6);
		ssPushNativeRegisterSingleFloat(result6);
		return 0;

	case 229:
		/* begin genLowcodeUint32ToFloat64 */
		resultValue7 = 0;
		valueValue24 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop34 = NoReg;
		frResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop34 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop34 == NoReg) {
			rTop34 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop34 == NoReg)
 || (frResult1 == NoReg))));
		value24 = rTop34;
		result7 = frResult1;
		nativePopToReg(ssNativeTop(), value24);
		ssNativePop(1);
		/* begin ConvertR:Rd: */
		genoperandoperand(ConvertRRd, value24, result7);
		ssPushNativeRegisterDoubleFloat(result7);
		return 0;

	case 230:
		/* begin genLowcodeUint64Great */
		firstValue10 = 0;
		secondValue10 = 0;
		valueValue25 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask19 = 0;
		rTop35 = (rNext17 = NoReg);
		rResult4 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop35 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext17 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext17 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg17 = (rNext17 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask19 = ((reg17 < 0) ? (((usqInt)(1)) >> (-reg17)) : (1ULL << reg17));
			}
		}
		if (rTop35 == NoReg) {
			rTop35 = allocateRegNotConflictingWith(topRegistersMask19);
		}
		if (rNext17 == NoReg) {
			rNext17 = allocateRegNotConflictingWith(((rTop35 < 0) ? (((usqInt)(1)) >> (-rTop35)) : (1ULL << rTop35)));
		}
		assert(!(((rTop35 == NoReg)
 || (rNext17 == NoReg))));
		rResult4 = allocateFloatRegNotConflictingWith((1ULL << rTop35) | (1ULL << rNext17));
		assert(!((rResult4 == NoReg)));
		second10 = rTop35;
		first10 = rNext17;
		value25 = rResult4;
		nativePopToReg(ssNativeTop(), second10);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first10);
		ssNativePop(1);
		abort();
		return 0;

	case 231:
		/* begin genLowcodeUint64GreatEqual */
		firstValue11 = 0;
		secondValue11 = 0;
		valueValue26 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask20 = 0;
		rTop36 = (rNext18 = NoReg);
		rResult5 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop36 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext18 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext18 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg18 = (rNext18 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask20 = ((reg18 < 0) ? (((usqInt)(1)) >> (-reg18)) : (1ULL << reg18));
			}
		}
		if (rTop36 == NoReg) {
			rTop36 = allocateRegNotConflictingWith(topRegistersMask20);
		}
		if (rNext18 == NoReg) {
			rNext18 = allocateRegNotConflictingWith(((rTop36 < 0) ? (((usqInt)(1)) >> (-rTop36)) : (1ULL << rTop36)));
		}
		assert(!(((rTop36 == NoReg)
 || (rNext18 == NoReg))));
		rResult5 = allocateFloatRegNotConflictingWith((1ULL << rTop36) | (1ULL << rNext18));
		assert(!((rResult5 == NoReg)));
		second11 = rTop36;
		first11 = rNext18;
		value26 = rResult5;
		nativePopToReg(ssNativeTop(), second11);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first11);
		ssNativePop(1);
		abort();
		return 0;

	case 232:
		/* begin genLowcodeUint64Less */
		firstValue12 = 0;
		secondValue12 = 0;
		valueValue27 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask21 = 0;
		rTop37 = (rNext19 = NoReg);
		rResult6 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop37 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext19 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext19 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg19 = (rNext19 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask21 = ((reg19 < 0) ? (((usqInt)(1)) >> (-reg19)) : (1ULL << reg19));
			}
		}
		if (rTop37 == NoReg) {
			rTop37 = allocateRegNotConflictingWith(topRegistersMask21);
		}
		if (rNext19 == NoReg) {
			rNext19 = allocateRegNotConflictingWith(((rTop37 < 0) ? (((usqInt)(1)) >> (-rTop37)) : (1ULL << rTop37)));
		}
		assert(!(((rTop37 == NoReg)
 || (rNext19 == NoReg))));
		rResult6 = allocateFloatRegNotConflictingWith((1ULL << rTop37) | (1ULL << rNext19));
		assert(!((rResult6 == NoReg)));
		second12 = rTop37;
		first12 = rNext19;
		value27 = rResult6;
		nativePopToReg(ssNativeTop(), second12);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first12);
		ssNativePop(1);
		abort();
		return 0;

	case 233:
		/* begin genLowcodeUint64LessEqual */
		firstValue13 = 0;
		secondValue13 = 0;
		valueValue28 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask22 = 0;
		rTop38 = (rNext20 = NoReg);
		rResult7 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop38 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext20 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext20 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg20 = (rNext20 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask22 = ((reg20 < 0) ? (((usqInt)(1)) >> (-reg20)) : (1ULL << reg20));
			}
		}
		if (rTop38 == NoReg) {
			rTop38 = allocateRegNotConflictingWith(topRegistersMask22);
		}
		if (rNext20 == NoReg) {
			rNext20 = allocateRegNotConflictingWith(((rTop38 < 0) ? (((usqInt)(1)) >> (-rTop38)) : (1ULL << rTop38)));
		}
		assert(!(((rTop38 == NoReg)
 || (rNext20 == NoReg))));
		rResult7 = allocateFloatRegNotConflictingWith((1ULL << rTop38) | (1ULL << rNext20));
		assert(!((rResult7 == NoReg)));
		second13 = rTop38;
		first13 = rNext20;
		value28 = rResult7;
		nativePopToReg(ssNativeTop(), second13);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first13);
		ssNativePop(1);
		abort();
		return 0;

	case 234:
		/* begin genLowcodeUint64ToFloat32 */
		resultValue8 = 0;
		valueValue29 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop39 = NoReg;
		frResult2 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop39 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop39 == NoReg) {
			rTop39 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult2 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop39 == NoReg)
 || (frResult2 == NoReg))));
		value29 = rTop39;
		result8 = frResult2;
		nativePopToReg(ssNativeTop(), value29);
		ssNativePop(1);
		abort();
		return 0;

	case 235:
		/* begin genLowcodeUint64ToFloat64 */
		resultValue9 = 0;
		valueValue30 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultFloat: */
		rTop40 = NoReg;
		frResult3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop40 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop40 == NoReg) {
			rTop40 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult3 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((rTop40 == NoReg)
 || (frResult3 == NoReg))));
		value30 = rTop40;
		result9 = frResult3;
		nativePopToReg(ssNativeTop(), value30);
		ssNativePop(1);
		abort();
		return 0;

	case 236:
		/* begin genLowcodeUmul32 */
		firstValue14 = 0;
		secondValue14 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask23 = 0;
		rTop41 = (rNext21 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop41 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext21 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext21 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg21 = (rNext21 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask23 = ((reg21 < 0) ? (((usqInt)(1)) >> (-reg21)) : (1ULL << reg21));
			}
		}
		if (rTop41 == NoReg) {
			rTop41 = allocateRegNotConflictingWith(topRegistersMask23);
		}
		if (rNext21 == NoReg) {
			rNext21 = allocateRegNotConflictingWith(((rTop41 < 0) ? (((usqInt)(1)) >> (-rTop41)) : (1ULL << rTop41)));
		}
		assert(!(((rTop41 == NoReg)
 || (rNext21 == NoReg))));
		second14 = rTop41;
		first14 = rNext21;
		nativePopToReg(ssNativeTop(), second14);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first14);
		ssNativePop(1);
		/* begin MulR:R: */
		genMulRR(backEnd, second14, first14);
		ssPushNativeRegister(first14);
		return 0;

	case 237:
		/* begin genLowcodeUmul64 */
		firstValue15 = 0;
		resultValue10 = 0;
		secondValue15 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask24 = 0;
		rTop42 = (rNext22 = NoReg);
		rResult8 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop42 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext22 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext22 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg22 = (rNext22 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask24 = ((reg22 < 0) ? (((usqInt)(1)) >> (-reg22)) : (1ULL << reg22));
			}
		}
		if (rTop42 == NoReg) {
			rTop42 = allocateRegNotConflictingWith(topRegistersMask24);
		}
		if (rNext22 == NoReg) {
			rNext22 = allocateRegNotConflictingWith(((rTop42 < 0) ? (((usqInt)(1)) >> (-rTop42)) : (1ULL << rTop42)));
		}
		assert(!(((rTop42 == NoReg)
 || (rNext22 == NoReg))));
		rResult8 = allocateFloatRegNotConflictingWith((1ULL << rTop42) | (1ULL << rNext22));
		assert(!((rResult8 == NoReg)));
		second15 = rTop42;
		first15 = rNext22;
		result10 = rResult8;
		nativePopToReg(ssNativeTop(), second15);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first15);
		ssNativePop(1);
		abort();
		return 0;

	case 238:
		return 0 /* begin genLowcodeUnlockRegisters */;

	case 239:
		/* begin genLowcodeUnlockVM */
		abort();
		return 0;

	case 240:
		/* begin genLowcodeUrem32 */
		firstValue16 = 0;
		secondValue16 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask25 = 0;
		rTop43 = (rNext23 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop43 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext23 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext23 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg23 = (rNext23 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask25 = ((reg23 < 0) ? (((usqInt)(1)) >> (-reg23)) : (1ULL << reg23));
			}
		}
		if (rTop43 == NoReg) {
			rTop43 = allocateRegNotConflictingWith(topRegistersMask25);
		}
		if (rNext23 == NoReg) {
			rNext23 = allocateRegNotConflictingWith(((rTop43 < 0) ? (((usqInt)(1)) >> (-rTop43)) : (1ULL << rTop43)));
		}
		assert(!(((rTop43 == NoReg)
 || (rNext23 == NoReg))));
		second16 = rTop43;
		first16 = rNext23;
		nativePopToReg(ssNativeTop(), second16);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first16);
		ssNativePop(1);
		gDivRRQuoRem(second16, first16, second16, first16);
		ssPushNativeRegister(first16);
		return 0;

	case 241:
		/* begin genLowcodeUrem64 */
		firstValue17 = 0;
		resultValue11 = 0;
		secondValue17 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask26 = 0;
		rTop44 = (rNext24 = NoReg);
		rResult9 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop44 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext24 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext24 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg24 = (rNext24 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask26 = ((reg24 < 0) ? (((usqInt)(1)) >> (-reg24)) : (1ULL << reg24));
			}
		}
		if (rTop44 == NoReg) {
			rTop44 = allocateRegNotConflictingWith(topRegistersMask26);
		}
		if (rNext24 == NoReg) {
			rNext24 = allocateRegNotConflictingWith(((rTop44 < 0) ? (((usqInt)(1)) >> (-rTop44)) : (1ULL << rTop44)));
		}
		assert(!(((rTop44 == NoReg)
 || (rNext24 == NoReg))));
		rResult9 = allocateFloatRegNotConflictingWith((1ULL << rTop44) | (1ULL << rNext24));
		assert(!((rResult9 == NoReg)));
		second17 = rTop44;
		first17 = rNext24;
		result11 = rResult9;
		nativePopToReg(ssNativeTop(), second17);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first17);
		ssNativePop(1);
		abort();
		return 0;

	case 242:
		/* begin genLowcodeXor32 */
		firstValue18 = 0;
		secondValue18 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask27 = 0;
		rTop45 = (rNext25 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop45 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext25 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext25 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg25 = (rNext25 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask27 = ((reg25 < 0) ? (((usqInt)(1)) >> (-reg25)) : (1ULL << reg25));
			}
		}
		if (rTop45 == NoReg) {
			rTop45 = allocateRegNotConflictingWith(topRegistersMask27);
		}
		if (rNext25 == NoReg) {
			rNext25 = allocateRegNotConflictingWith(((rTop45 < 0) ? (((usqInt)(1)) >> (-rTop45)) : (1ULL << rTop45)));
		}
		assert(!(((rTop45 == NoReg)
 || (rNext25 == NoReg))));
		second18 = rTop45;
		first18 = rNext25;
		nativePopToReg(ssNativeTop(), second18);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first18);
		ssNativePop(1);
		/* begin XorR:R: */
		genoperandoperand(XorRR, second18, first18);
		ssPushNativeRegister(first18);
		return 0;

	case 243:
		/* begin genLowcodeXor64 */
		firstValue19 = 0;
		secondValue19 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask28 = 0;
		rTop112 = (rNext110 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop112 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext110 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext110 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg26 = (rNext110 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask28 = ((reg26 < 0) ? (((usqInt)(1)) >> (-reg26)) : (1ULL << reg26));
			}
		}
		if (rTop112 == NoReg) {
			rTop112 = allocateRegNotConflictingWith(topRegistersMask28);
		}
		if (rNext110 == NoReg) {
			rNext110 = allocateRegNotConflictingWith(((rTop112 < 0) ? (((usqInt)(1)) >> (-rTop112)) : (1ULL << rTop112)));
		}
		assert(!(((rTop112 == NoReg)
 || (rNext110 == NoReg))));
		second19 = rTop112;
		first19 = rNext110;
		nativePopToReg(ssNativeTop(), second19);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first19);
		ssNativePop(1);
		/* begin XorR:R: */
		genoperandoperand(XorRR, second19, first19);
		ssPushNativeRegister(first19);
		return 0;

	case 244:
		/* begin genLowcodeZeroExtend32From16 */
		valueValue31 = 0;
		rTop46 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop46 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop46 == NoReg) {
			rTop46 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop46 == NoReg)));
		value31 = rTop46;
		nativePopToReg(ssNativeTop(), value31);
		ssNativePop(1);
		/* begin ZeroExtend16R:R: */
		genoperandoperand(ZeroExtend16RR, value31, value31);
		ssPushNativeRegister(value31);
		return 0;

	case 245:
		/* begin genLowcodeZeroExtend32From8 */
		valueValue32 = 0;
		rTop47 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop47 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop47 == NoReg) {
			rTop47 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop47 == NoReg)));
		value32 = rTop47;
		nativePopToReg(ssNativeTop(), value32);
		ssNativePop(1);
		/* begin ZeroExtend8R:R: */
		genoperandoperand(ZeroExtend8RR, value32, value32);
		ssPushNativeRegister(value32);
		return 0;

	case 246:
		/* begin genLowcodeZeroExtend64From16 */
		valueValue9 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop17 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop17 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop17 == NoReg) {
			rTop17 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop17 == NoReg)));
		value9 = rTop17;
		nativePopToReg(ssNativeTop(), value9);
		ssNativePop(1);
		/* begin ZeroExtend16R:R: */
		genoperandoperand(ZeroExtend16RR, value9, value9);
		ssPushNativeRegister(value9);
		return 0;

	default:
		return genLowcodeUnaryInlinePrimitive5(prim);
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeUnaryInlinePrimitive5: */
static NoDbgRegParms sqInt
genLowcodeUnaryInlinePrimitive5(sqInt prim)
{
    sqInt result;
    sqInt resultValue;
    sqInt rResult1;
    sqInt rTop1;
    sqInt rTop11;
    sqInt value;
    sqInt value1;
    sqInt valueValue;
    sqInt valueValue1;

	switch (prim) {
	case 247:
		/* begin genLowcodeZeroExtend64From32 */
		resultValue = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop1 = NoReg;
		rResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
		assert(!(((rTop1 == NoReg)
 || (rResult1 == NoReg))));
		value = rTop1;
		result = rResult1;
		nativePopToReg(ssNativeTop(), value);
		ssNativePop(1);
		/* begin ZeroExtend32R:R: */
		genoperandoperand(ZeroExtend32RR, value, value);
		ssPushNativeRegister(value);
		return 0;

	case 0xF8:
		/* begin genLowcodeZeroExtend64From8 */
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop11 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop11 == NoReg)));
		value1 = rTop11;
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin ZeroExtend8R:R: */
		genoperandoperand(ZeroExtend8RR, value1, value1);
		ssPushNativeRegister(value1);
		return 0;

	default:
		return EncounteredUnknownBytecode;
	}
	return 0;
}


/*	Lowcode instruction generator dispatch */

	/* StackToRegisterMappingCogit>>#genLowcodeUnaryInlinePrimitive: */
static NoDbgRegParms sqInt
genLowcodeUnaryInlinePrimitive(sqInt prim)
{
    AbstractInstruction *abstractInstruction;
    sqInt alignment;
    sqInt base;
    sqInt base1;
    sqInt baseValue;
    sqInt baseValue1;
    sqInt check;
    sqInt checkValue;
    AbstractInstruction *contJump;
    AbstractInstruction *contJump1;
    AbstractInstruction *contJump10;
    AbstractInstruction *contJump11;
    AbstractInstruction *contJump2;
    AbstractInstruction *contJump3;
    AbstractInstruction *contJump4;
    AbstractInstruction *contJump5;
    AbstractInstruction *contJump6;
    AbstractInstruction *contJump7;
    AbstractInstruction *contJump8;
    AbstractInstruction *contJump9;
    sqInt dup2;
    sqInt dup21;
    sqInt dup22;
    sqInt dup23;
    sqInt dup24;
    sqInt dup2Value;
    sqInt dup2Value1;
    sqInt dup2Value2;
    sqInt dup2Value3;
    sqInt dup2Value4;
    sqInt expectedSession;
    AbstractInstruction *falseJump;
    AbstractInstruction *falseJump1;
    AbstractInstruction *falseJump10;
    AbstractInstruction *falseJump11;
    AbstractInstruction *falseJump2;
    AbstractInstruction *falseJump3;
    AbstractInstruction *falseJump4;
    AbstractInstruction *falseJump5;
    AbstractInstruction *falseJump6;
    AbstractInstruction *falseJump7;
    AbstractInstruction *falseJump8;
    AbstractInstruction *falseJump9;
    sqInt first;
    sqInt first1;
    sqInt first10;
    sqInt first11;
    sqInt first12;
    sqInt first13;
    sqInt first14;
    sqInt first15;
    sqInt first16;
    sqInt first17;
    sqInt first18;
    sqInt first19;
    sqInt first2;
    sqInt first20;
    sqInt first21;
    sqInt first22;
    sqInt first23;
    sqInt first24;
    sqInt first25;
    sqInt first3;
    sqInt first4;
    sqInt first5;
    sqInt first6;
    sqInt first7;
    sqInt first8;
    sqInt firstValue;
    sqInt firstValue1;
    sqInt firstValue10;
    sqInt firstValue11;
    sqInt firstValue12;
    sqInt firstValue13;
    sqInt firstValue14;
    sqInt firstValue15;
    sqInt firstValue16;
    sqInt firstValue17;
    sqInt firstValue18;
    sqInt firstValue19;
    sqInt firstValue2;
    sqInt firstValue20;
    sqInt firstValue21;
    sqInt firstValue22;
    sqInt firstValue23;
    sqInt firstValue24;
    sqInt firstValue25;
    sqInt firstValue3;
    sqInt firstValue4;
    sqInt firstValue5;
    sqInt firstValue6;
    sqInt firstValue7;
    sqInt firstValue8;
    sqInt firstValue9;
    sqInt first9;
    sqInt frNext;
    sqInt frNext1;
    sqInt frNext10;
    sqInt frNext11;
    sqInt frNext2;
    sqInt frNext3;
    sqInt frNext4;
    sqInt frNext5;
    sqInt frNext6;
    sqInt frNext7;
    sqInt frNext8;
    sqInt frNext9;
    sqInt frResult;
    sqInt frResult1;
    sqInt frResult2;
    sqInt frResult3;
    sqInt frTop;
    sqInt frTop1;
    sqInt frTop10;
    sqInt frTop11;
    sqInt frTop12;
    sqInt frTop13;
    sqInt frTop14;
    sqInt frTop15;
    sqInt frTop16;
    sqInt frTop17;
    sqInt frTop18;
    sqInt frTop19;
    sqInt frTop2;
    sqInt frTop20;
    sqInt frTop21;
    sqInt frTop22;
    sqInt frTop3;
    sqInt frTop4;
    sqInt frTop5;
    sqInt frTop6;
    sqInt frTop7;
    sqInt frTop8;
    sqInt frTop9;
    sqInt function;
    sqInt index;
    sqInt index1;
    sqInt indexValue;
    sqInt indexValue1;
    sqInt nativeValueIndex;
    sqInt nativeValueIndex1;
    sqInt nativeValueIndex2;
    sqInt newValue;
    sqInt newValueValue;
    sqInt nextRegisterMask;
    sqInt nextRegisterMask1;
    sqInt nextRegisterMask2;
    sqInt offset;
    sqInt offset1;
    sqInt offsetValue;
    sqInt offsetValue1;
    sqInt oldValue;
    sqInt oldValueValue;
    sqInt pointerValue;
    sqInt pointerValueValue;
    sqInt reg;
    sqInt reg1;
    sqInt reg10;
    sqInt reg11;
    sqInt reg12;
    sqInt reg13;
    sqInt reg14;
    sqInt reg15;
    sqInt reg16;
    sqInt reg17;
    sqInt reg18;
    sqInt reg19;
    sqInt reg2;
    sqInt reg20;
    sqInt reg21;
    sqInt reg22;
    sqInt reg23;
    sqInt reg24;
    sqInt reg25;
    sqInt reg26;
    sqInt reg27;
    sqInt reg3;
    sqInt reg4;
    sqInt reg5;
    sqInt reg6;
    sqInt reg7;
    sqInt reg8;
    sqInt registerID;
    sqInt reg9;
    sqInt result;
    sqInt result1;
    sqInt result2;
    sqInt result3;
    sqInt result4;
    sqInt result5;
    sqInt result6;
    sqInt result7;
    sqInt result8;
    sqInt resultValue;
    sqInt resultValue1;
    sqInt resultValue2;
    sqInt resultValue3;
    sqInt resultValue4;
    sqInt resultValue5;
    sqInt resultValue6;
    sqInt resultValue7;
    sqInt resultValue8;
    sqInt rNext;
    sqInt rNext1;
    sqInt rNext10;
    sqInt rNext11;
    sqInt rNext12;
    sqInt rNext13;
    sqInt rNext14;
    sqInt rNext15;
    sqInt rNext16;
    sqInt rNext17;
    sqInt rNext18;
    sqInt rNext2;
    sqInt rNext3;
    sqInt rNext4;
    sqInt rNext5;
    sqInt rNext6;
    sqInt rNext7;
    sqInt rNext8;
    sqInt rNextNext;
    sqInt rNextNext1;
    sqInt rNextNext2;
    sqInt rNextNextNext;
    sqInt rNextNextNext1;
    sqInt rNext9;
    sqInt rResult;
    sqInt rResult1;
    sqInt rResult10;
    sqInt rResult11;
    sqInt rResult12;
    sqInt rResult13;
    sqInt rResult14;
    sqInt rResult15;
    sqInt rResult16;
    sqInt rResult17;
    sqInt rResult18;
    sqInt rResult19;
    sqInt rResult2;
    sqInt rResult20;
    sqInt rResult21;
    sqInt rResult22;
    sqInt rResult3;
    sqInt rResult4;
    sqInt rResult5;
    sqInt rResult6;
    sqInt rResult7;
    sqInt rResult8;
    sqInt rResult9;
    sqInt rTop;
    sqInt rTop1;
    sqInt rTop10;
    sqInt rTop11;
    sqInt rTop12;
    sqInt rTop13;
    sqInt rTop14;
    sqInt rTop15;
    sqInt rTop16;
    sqInt rTop17;
    sqInt rTop18;
    sqInt rTop19;
    sqInt rTop2;
    sqInt rTop20;
    sqInt rTop21;
    sqInt rTop22;
    sqInt rTop23;
    sqInt rTop3;
    sqInt rTop4;
    sqInt rTop5;
    sqInt rTop6;
    sqInt rTop7;
    sqInt rTop8;
    sqInt rTop9;
    sqInt scale;
    sqInt scale1;
    sqInt scaleValue;
    sqInt scaleValue1;
    sqInt second;
    sqInt second1;
    sqInt second10;
    sqInt second11;
    sqInt second12;
    sqInt second13;
    sqInt second14;
    sqInt second15;
    sqInt second16;
    sqInt second17;
    sqInt second18;
    sqInt second19;
    sqInt second2;
    sqInt second20;
    sqInt second21;
    sqInt second22;
    sqInt second23;
    sqInt second24;
    sqInt second25;
    sqInt second3;
    sqInt second4;
    sqInt second5;
    sqInt second6;
    sqInt second7;
    sqInt second8;
    sqInt secondValue;
    sqInt secondValue1;
    sqInt secondValue10;
    sqInt secondValue11;
    sqInt secondValue12;
    sqInt secondValue13;
    sqInt secondValue14;
    sqInt secondValue15;
    sqInt secondValue16;
    sqInt secondValue17;
    sqInt secondValue18;
    sqInt secondValue19;
    sqInt secondValue2;
    sqInt secondValue20;
    sqInt secondValue21;
    sqInt secondValue22;
    sqInt secondValue23;
    sqInt secondValue24;
    sqInt secondValue25;
    sqInt secondValue3;
    sqInt secondValue4;
    sqInt secondValue5;
    sqInt secondValue6;
    sqInt secondValue7;
    sqInt secondValue8;
    sqInt secondValue9;
    sqInt second9;
    sqInt shiftAmount;
    sqInt shiftAmount1;
    sqInt shiftAmountValue;
    sqInt shiftAmountValue1;
    sqInt singleFloatValue;
    sqInt singleFloatValueValue;
    sqInt size;
    sqInt size1;
    sqInt sizeValue;
    sqInt sizeValue1;
    sqInt topRegistersMask;
    sqInt topRegistersMask1;
    sqInt topRegistersMask10;
    sqInt topRegistersMask11;
    sqInt topRegistersMask12;
    sqInt topRegistersMask13;
    sqInt topRegistersMask14;
    sqInt topRegistersMask15;
    sqInt topRegistersMask16;
    sqInt topRegistersMask17;
    sqInt topRegistersMask18;
    sqInt topRegistersMask19;
    sqInt topRegistersMask2;
    sqInt topRegistersMask20;
    sqInt topRegistersMask21;
    sqInt topRegistersMask22;
    sqInt topRegistersMask23;
    sqInt topRegistersMask24;
    sqInt topRegistersMask25;
    sqInt topRegistersMask26;
    sqInt topRegistersMask27;
    sqInt topRegistersMask28;
    sqInt topRegistersMask29;
    sqInt topRegistersMask3;
    sqInt topRegistersMask30;
    sqInt topRegistersMask4;
    sqInt topRegistersMask5;
    sqInt topRegistersMask6;
    sqInt topRegistersMask7;
    sqInt topRegistersMask8;
    sqInt topRegistersMask9;
    sqInt value;
    sqInt value1;
    sqInt value10;
    sqInt value11;
    sqInt value12;
    sqInt value13;
    sqInt value14;
    sqInt value15;
    sqInt value16;
    sqInt value17;
    sqInt value18;
    sqInt value19;
    sqInt value2;
    sqInt value20;
    sqInt value21;
    sqInt value22;
    sqInt value23;
    sqInt value24;
    sqInt value25;
    sqInt value26;
    sqInt value3;
    sqInt value4;
    sqInt value5;
    sqInt value6;
    sqInt value7;
    sqInt value8;
    sqInt valueValue;
    sqInt valueValue1;
    sqInt valueValue10;
    sqInt valueValue11;
    sqInt valueValue12;
    sqInt valueValue13;
    sqInt valueValue14;
    sqInt valueValue15;
    sqInt valueValue16;
    sqInt valueValue17;
    sqInt valueValue18;
    sqInt valueValue19;
    sqInt valueValue2;
    sqInt valueValue20;
    sqInt valueValue21;
    sqInt valueValue22;
    sqInt valueValue23;
    sqInt valueValue24;
    sqInt valueValue25;
    sqInt valueValue26;
    sqInt valueValue3;
    sqInt valueValue4;
    sqInt valueValue5;
    sqInt valueValue6;
    sqInt valueValue7;
    sqInt valueValue8;
    sqInt valueValue9;
    sqInt value9;

	switch (prim) {
	case 0:
		/* begin genLowcodeAdd32 */
		firstValue = 0;
		secondValue = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask = 0;
		rTop2 = (rNext = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop2 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg = (rNext = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
		}
		if (rTop2 == NoReg) {
			rTop2 = allocateRegNotConflictingWith(topRegistersMask);
		}
		if (rNext == NoReg) {
			rNext = allocateRegNotConflictingWith(((rTop2 < 0) ? (((usqInt)(1)) >> (-rTop2)) : (1ULL << rTop2)));
		}
		assert(!(((rTop2 == NoReg)
 || (rNext == NoReg))));
		second = rTop2;
		first = rNext;
		nativePopToReg(ssNativeTop(), second);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first);
		ssNativePop(1);
		/* begin AddR:R: */
		genoperandoperand(AddRR, second, first);
		ssPushNativeRegister(first);
		return 0;

	case 1:
		/* begin genLowcodeAdd64 */
		firstValue1 = 0;
		secondValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask1 = 0;
		rTop12 = (rNext1 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop12 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext1 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg1 = (rNext1 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask1 = ((reg1 < 0) ? (((usqInt)(1)) >> (-reg1)) : (1ULL << reg1));
			}
		}
		if (rTop12 == NoReg) {
			rTop12 = allocateRegNotConflictingWith(topRegistersMask1);
		}
		if (rNext1 == NoReg) {
			rNext1 = allocateRegNotConflictingWith(((rTop12 < 0) ? (((usqInt)(1)) >> (-rTop12)) : (1ULL << rTop12)));
		}
		assert(!(((rTop12 == NoReg)
 || (rNext1 == NoReg))));
		second1 = rTop12;
		first1 = rNext1;
		nativePopToReg(ssNativeTop(), second1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first1);
		ssNativePop(1);
		/* begin AddR:R: */
		genoperandoperand(AddRR, second1, first1);
		ssPushNativeRegister(first1);
		return 0;

	case 2:
		/* begin genLowcodeAlloca32 */
		sizeValue = 0;
		rTop = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop == NoReg) {
			rTop = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop == NoReg)));
		size = rTop;
		nativePopToReg(ssNativeTop(), size);
		ssNativePop(1);
		/* begin checkLiteral:forInstruction: */
		genoperandoperand(MoveAwR, nativeStackPointerAddress(), TempReg);
		genoperandoperand(SubRR, size, TempReg);
		genoperandoperand(AndCqR, -16, TempReg);
		genoperandoperand(MoveRR, TempReg, size);
		genoperandoperand(MoveRAw, size, nativeStackPointerAddress());
		ssPushNativeRegister(size);
		return 0;

	case 3:
		/* begin genLowcodeAlloca64 */
		sizeValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger: */
		rTop1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop1 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop1 == NoReg) {
			rTop1 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		assert(!((rTop1 == NoReg)));
		size1 = rTop1;
		nativePopToReg(ssNativeTop(), size1);
		ssNativePop(1);
		/* begin SubR:R: */
		genoperandoperand(SubRR, size1, SPReg);
		genoperandoperand(MoveRR, SPReg, size1);
		ssPushNativeRegister(size1);
		return 0;

	case 4:
		/* begin genLowcodeAnd32 */
		firstValue2 = 0;
		secondValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask2 = 0;
		rTop3 = (rNext2 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop3 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext2 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext2 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg2 = (rNext2 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask2 = ((reg2 < 0) ? (((usqInt)(1)) >> (-reg2)) : (1ULL << reg2));
			}
		}
		if (rTop3 == NoReg) {
			rTop3 = allocateRegNotConflictingWith(topRegistersMask2);
		}
		if (rNext2 == NoReg) {
			rNext2 = allocateRegNotConflictingWith(((rTop3 < 0) ? (((usqInt)(1)) >> (-rTop3)) : (1ULL << rTop3)));
		}
		assert(!(((rTop3 == NoReg)
 || (rNext2 == NoReg))));
		second2 = rTop3;
		first2 = rNext2;
		nativePopToReg(ssNativeTop(), second2);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first2);
		ssNativePop(1);
		/* begin AndR:R: */
		genoperandoperand(AndRR, second2, first2);
		ssPushNativeRegister(first2);
		return 0;

	case 5:
		/* begin genLowcodeAnd64 */
		firstValue3 = 0;
		secondValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask3 = 0;
		rTop13 = (rNext11 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop13 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext11 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext11 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg3 = (rNext11 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask3 = ((reg3 < 0) ? (((usqInt)(1)) >> (-reg3)) : (1ULL << reg3));
			}
		}
		if (rTop13 == NoReg) {
			rTop13 = allocateRegNotConflictingWith(topRegistersMask3);
		}
		if (rNext11 == NoReg) {
			rNext11 = allocateRegNotConflictingWith(((rTop13 < 0) ? (((usqInt)(1)) >> (-rTop13)) : (1ULL << rTop13)));
		}
		assert(!(((rTop13 == NoReg)
 || (rNext11 == NoReg))));
		second3 = rTop13;
		first3 = rNext11;
		nativePopToReg(ssNativeTop(), second3);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first3);
		ssNativePop(1);
		/* begin AndR:R: */
		genoperandoperand(AndRR, second3, first3);
		ssPushNativeRegister(first3);
		return 0;

	case 6:
		/* begin genLowcodeArithmeticRightShift32 */
		shiftAmountValue = 0;
		valueValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask4 = 0;
		rTop4 = (rNext3 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop4 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext3 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext3 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg4 = (rNext3 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask4 = ((reg4 < 0) ? (((usqInt)(1)) >> (-reg4)) : (1ULL << reg4));
			}
		}
		if (rTop4 == NoReg) {
			rTop4 = allocateRegNotConflictingWith(topRegistersMask4);
		}
		if (rNext3 == NoReg) {
			rNext3 = allocateRegNotConflictingWith(((rTop4 < 0) ? (((usqInt)(1)) >> (-rTop4)) : (1ULL << rTop4)));
		}
		assert(!(((rTop4 == NoReg)
 || (rNext3 == NoReg))));
		shiftAmount = rTop4;
		value1 = rNext3;
		nativePopToReg(ssNativeTop(), shiftAmount);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value1);
		ssNativePop(1);
		/* begin ArithmeticShiftRightR:R: */
		genoperandoperand(ArithmeticShiftRightRR, shiftAmount, value1);
		ssPushNativeRegister(value1);
		return 0;

	case 7:
		/* begin genLowcodeArithmeticRightShift64 */
		resultValue = 0;
		shiftAmountValue1 = 0;
		valueValue2 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask5 = 0;
		rTop5 = (rNext4 = NoReg);
		rResult = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop5 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext4 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext4 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg5 = (rNext4 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask5 = ((reg5 < 0) ? (((usqInt)(1)) >> (-reg5)) : (1ULL << reg5));
			}
		}
		if (rTop5 == NoReg) {
			rTop5 = allocateRegNotConflictingWith(topRegistersMask5);
		}
		if (rNext4 == NoReg) {
			rNext4 = allocateRegNotConflictingWith(((rTop5 < 0) ? (((usqInt)(1)) >> (-rTop5)) : (1ULL << rTop5)));
		}
		assert(!(((rTop5 == NoReg)
 || (rNext4 == NoReg))));
		rResult = allocateFloatRegNotConflictingWith((1ULL << rTop5) | (1ULL << rNext4));
		assert(!((rResult == NoReg)));
		shiftAmount1 = rTop5;
		value2 = rNext4;
		result = rResult;
		nativePopToReg(ssNativeTop(), shiftAmount1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), value2);
		ssNativePop(1);
		abort();
		return 0;

	case 8:
		/* begin genLowcodeBeginCall */
		alignment = extA;
		beginHighLevelCall(alignment);
		extA = 0;
		return 0;

	case 9:
		/* begin genLowcodeCallArgumentFloat32 */
		nativeStackPopToReg(ssNativeTop(), DPFPReg0);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRsM32r, DPFPReg0, -BytesPerWord, SPReg);
		genoperandoperand(SubCqR, BytesPerWord, SPReg);
		currentCallCleanUpSize += BytesPerWord;
		return 0;

	case 10:
		/* begin genLowcodeCallArgumentFloat64 */
		nativeStackPopToReg(ssNativeTop(), DPFPReg0);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveRdM64r, DPFPReg0, -8, SPReg);
		genoperandoperand(SubCqR, 8, SPReg);
		currentCallCleanUpSize += 8;
		return 0;

	case 11:
		/* begin genLowcodeCallArgumentInt32 */
		nativeStackPopToReg(ssNativeTop(), TempReg);
		ssNativePop(1);
		/* begin PushR: */
		genoperand(PushR, TempReg);
		currentCallCleanUpSize += BytesPerWord;
		return 0;

	case 12:
		/* begin genLowcodeCallArgumentInt64 */
				nativeStackPopToReg(ssNativeTop(), TempReg);
		ssNativePop(1);
		/* begin PushR: */
		genoperand(PushR, TempReg);
		currentCallCleanUpSize += BytesPerWord;
		return 0;

	case 13:
		/* begin genLowcodeCallArgumentPointer */
		nativeStackPopToReg(ssNativeTop(), TempReg);
		ssNativePop(1);
		/* begin PushR: */
		genoperand(PushR, TempReg);
		currentCallCleanUpSize += BytesPerWord;
		return 0;

	case 14:
		/* begin genLowcodeCallArgumentSpace */
		genoperandoperand(SubCqR, extA, SPReg);
		currentCallCleanUpSize += extA;
		extA = 0;
		return 0;

	case 15:
		/* begin genLowcodeCallArgumentStructure */
		nativeStackPopToReg(ssNativeTop(), TempReg);
		ssNativePop(1);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, extA, SPReg);
		/* Copy the structure */
		currentCallCleanUpSize += extA;
		genMemCopytoconstantSize(backEnd, TempReg, SPReg, extA);
		extA = 0;
		return 0;

	case 16:
		/* begin genLowcodeCallInstruction */
		function = extA;
		abstractInstruction = genoperand(Call, function);
		(abstractInstruction->annotation = IsRelativeCall);
		extA = 0;
		return 0;

	case 17:
		/* begin genLowcodeCallPhysical */
		registerID = extA;
		genoperand(CallR, registerID);
		extA = 0;
		return 0;

	case 18:
		/* begin genLowcodeCheckSessionIdentifier */
		expectedSession = extA;
		ssPushNativeConstantInt32((expectedSession == (getThisSessionID())
			? 1
			: 0));
		extA = 0;
		return 0;

	case 19:
		/* begin genLowcodeCompareAndSwap32 */
		checkValue = 0;
		newValueValue = 0;
		oldValueValue = 0;
		valueValue3 = 0;
		/* begin allocateRegistersForLowcodeInteger3ResultInteger: */
		rTop6 = (rNext5 = (rNextNext = NoReg));
		nativeValueIndex = 1;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop6 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext5 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext5 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
				rNext5 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
					rNextNext = nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex));
				}
				nativeValueIndex += 1;
			}
		}
		if (rNextNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex))) != NoReg) {
				rNextNext = nativeRegisterOrNone(ssNativeValue(nativeValueIndex));
			}
		}
		if (rTop6 == NoReg) {
			nextRegisterMask = 0;
			if (rNext5 != NoReg) {
				nextRegisterMask = ((rNext5 < 0) ? (((usqInt)(1)) >> (-rNext5)) : (1ULL << rNext5));
			}
			if (rNextNext != NoReg) {
				nextRegisterMask = nextRegisterMask | (((rNextNext < 0) ? (((usqInt)(1)) >> (-rNextNext)) : (1ULL << rNextNext)));
			}
			rTop6 = allocateRegNotConflictingWith(nextRegisterMask);
		}
		if (rNext5 == NoReg) {
			nextRegisterMask = ((rTop6 < 0) ? (((usqInt)(1)) >> (-rTop6)) : (1ULL << rTop6));
			if (rNextNext != NoReg) {
				nextRegisterMask = nextRegisterMask | (((rNextNext < 0) ? (((usqInt)(1)) >> (-rNextNext)) : (1ULL << rNextNext)));
			}
			rNext5 = allocateRegNotConflictingWith(nextRegisterMask);
		}
		if (rNextNext == NoReg) {
			nextRegisterMask = (1ULL << rTop6) | (1ULL << rNext5);
			rNextNext = allocateRegNotConflictingWith(nextRegisterMask);
		}
		assert(!(((rTop6 == NoReg)
 || ((rNext5 == NoReg)
 || (rNextNext == NoReg)))));
		rResult2 = allocateRegNotConflictingWith(((1ULL << rTop6) | (1ULL << rNext5)) | (1ULL << rNextNext));
		assert(!((rResult2 == NoReg)));
		newValue = rTop6;
		oldValue = rNext5;
		check = rNextNext;
		value3 = rResult2;
		nativePopToReg(ssNativeTop(), newValue);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), oldValue);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), check);
		ssNativePop(1);
		abort();
		return 0;

	case 20:
		/* begin genLowcodeDiv32 */
		firstValue4 = 0;
		secondValue4 = 0;
		/* begin allocateRegistersForLowcodeInteger2: */
		topRegistersMask6 = 0;
		rTop7 = (rNext6 = NoReg);
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop7 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext6 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext6 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg6 = (rNext6 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask6 = ((reg6 < 0) ? (((usqInt)(1)) >> (-reg6)) : (1ULL << reg6));
			}
		}
		if (rTop7 == NoReg) {
			rTop7 = allocateRegNotConflictingWith(topRegistersMask6);
		}
		if (rNext6 == NoReg) {
			rNext6 = allocateRegNotConflictingWith(((rTop7 < 0) ? (((usqInt)(1)) >> (-rTop7)) : (1ULL << rTop7)));
		}
		assert(!(((rTop7 == NoReg)
 || (rNext6 == NoReg))));
		second4 = rTop7;
		first4 = rNext6;
		nativePopToReg(ssNativeTop(), second4);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first4);
		ssNativePop(1);
		gDivRRQuoRem(second4, first4, first4, second4);
		ssPushNativeRegister(first4);
		return 0;

	case 21:
		/* begin genLowcodeDiv64 */
		firstValue5 = 0;
		resultValue1 = 0;
		secondValue5 = 0;
		/* begin allocateRegistersForLowcodeInteger2ResultInteger: */
		topRegistersMask7 = 0;
		rTop8 = (rNext7 = NoReg);
		rResult3 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop8 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext7 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext7 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(1))) != NoReg) {
				reg7 = (rNext7 = nativeRegisterOrNone(ssNativeValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask7 = ((reg7 < 0) ? (((usqInt)(1)) >> (-reg7)) : (1ULL << reg7));
			}
		}
		if (rTop8 == NoReg) {
			rTop8 = allocateRegNotConflictingWith(topRegistersMask7);
		}
		if (rNext7 == NoReg) {
			rNext7 = allocateRegNotConflictingWith(((rTop8 < 0) ? (((usqInt)(1)) >> (-rTop8)) : (1ULL << rTop8)));
		}
		assert(!(((rTop8 == NoReg)
 || (rNext7 == NoReg))));
		rResult3 = allocateFloatRegNotConflictingWith((1ULL << rTop8) | (1ULL << rNext7));
		assert(!((rResult3 == NoReg)));
		second5 = rTop8;
		first5 = rNext7;
		result1 = rResult3;
		nativePopToReg(ssNativeTop(), second5);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first5);
		ssNativePop(1);
		abort();
		return 0;

	case 22:
		/* begin genLowcodeDuplicateFloat32 */
		dup2Value1 = 0;
		valueValue4 = 0;
		/* begin allocateRegistersForLowcodeFloatResultFloat: */
		frTop = NoReg;
		/* Float argument */
		frResult = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop == NoReg) {
			frTop = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult = allocateFloatRegNotConflictingWith(((frTop < 0) ? (((usqInt)(1)) >> (-frTop)) : (1ULL << frTop)));
		assert(!(((frTop == NoReg)
 || (frResult == NoReg))));
		value4 = frTop;
		dup21 = frResult;
		nativePopToReg(ssNativeTop(), value4);
		ssNativePop(1);
		/* begin MoveRs:Rs: */
		genoperandoperand(MoveRsRs, value4, dup21);
		ssPushNativeRegisterSingleFloat(value4);
		ssPushNativeRegisterSingleFloat(dup21);
		return 0;

	case 23:
		/* begin genLowcodeDuplicateFloat64 */
		dup2Value2 = 0;
		valueValue5 = 0;
		/* begin allocateRegistersForLowcodeFloatResultFloat: */
		frTop1 = NoReg;
		/* Float argument */
		frResult1 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop1 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop1 == NoReg) {
			frTop1 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult1 = allocateFloatRegNotConflictingWith(((frTop1 < 0) ? (((usqInt)(1)) >> (-frTop1)) : (1ULL << frTop1)));
		assert(!(((frTop1 == NoReg)
 || (frResult1 == NoReg))));
		value5 = frTop1;
		dup22 = frResult1;
		nativePopToReg(ssNativeTop(), value5);
		ssNativePop(1);
		/* begin MoveRd:Rd: */
		genoperandoperand(MoveRdRd, value5, dup22);
		ssPushNativeRegisterDoubleFloat(value5);
		ssPushNativeRegisterDoubleFloat(dup22);
		return 0;

	case 24:
		/* begin genLowcodeDuplicateInt32 */
		dup2Value3 = 0;
		valueValue6 = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop9 = NoReg;
		rResult4 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop9 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop9 == NoReg) {
			rTop9 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult4 = allocateRegNotConflictingWith(((rTop9 < 0) ? (((usqInt)(1)) >> (-rTop9)) : (1ULL << rTop9)));
		assert(!(((rTop9 == NoReg)
 || (rResult4 == NoReg))));
		value6 = rTop9;
		dup23 = rResult4;
		nativePopToReg(ssNativeTop(), value6);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value6, dup23);
		ssPushNativeRegister(value6);
		ssPushNativeRegister(dup23);
		return 0;

	case 25:
		/* begin genLowcodeDuplicateInt64 */
		dup2Value = 0;
		valueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop11 = NoReg;
		rResult1 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop11 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop11 == NoReg) {
			rTop11 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult1 = allocateRegNotConflictingWith(((rTop11 < 0) ? (((usqInt)(1)) >> (-rTop11)) : (1ULL << rTop11)));
		assert(!(((rTop11 == NoReg)
 || (rResult1 == NoReg))));
		value = rTop11;
		dup2 = rResult1;
		nativePopToReg(ssNativeTop(), value);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, value, dup2);
		ssPushNativeRegister(value);
		ssPushNativeRegister(dup2);
		return 0;

	case 26:
		/* begin genLowcodeDuplicatePointer */
		dup2Value4 = 0;
		pointerValueValue = 0;
		/* begin allocateRegistersForLowcodeIntegerResultInteger: */
		rTop10 = NoReg;
		rResult5 = NoReg;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop10 = nativeRegisterOrNone(ssNativeTop());
		}
		if (rTop10 == NoReg) {
			rTop10 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult5 = allocateRegNotConflictingWith(((rTop10 < 0) ? (((usqInt)(1)) >> (-rTop10)) : (1ULL << rTop10)));
		assert(!(((rTop10 == NoReg)
 || (rResult5 == NoReg))));
		pointerValue = rTop10;
		dup24 = rResult5;
		nativePopToReg(ssNativeTop(), pointerValue);
		ssNativePop(1);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, pointerValue, dup24);
		ssPushNativeRegister(pointerValue);
		ssPushNativeRegister(dup24);
		return 0;

	case 27:
		/* begin genLowcodeEffectiveAddress32 */
		baseValue = 0;
		indexValue = 0;
		offsetValue = 0;
		scaleValue = 0;
		/* begin allocateRegistersForLowcodeInteger4: */
		rTop14 = (rNext8 = (rNextNext1 = (rNextNextNext = NoReg)));
		nativeValueIndex1 = 1;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop14 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext8 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext8 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
				rNext8 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
					rNextNext1 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
				}
				nativeValueIndex1 += 1;
			}
		}
		if (rNextNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
				rNextNext1 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
					rNextNextNext = nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex1));
				}
				nativeValueIndex1 += 1;
			}
		}
		if (rNextNextNext == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex1))) != NoReg) {
				rNextNextNext = nativeRegisterOrNone(ssNativeValue(nativeValueIndex1));
				nativeValueIndex1 += 1;
			}
		}
		if (rTop14 == NoReg) {
			nextRegisterMask1 = 0;
			if (rNext8 != NoReg) {
				nextRegisterMask1 = ((rNext8 < 0) ? (((usqInt)(1)) >> (-rNext8)) : (1ULL << rNext8));
			}
			if (rNextNext1 != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNext1)) : (1ULL << rNextNext1)));
			}
			if (rNextNextNext != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNextNext < 0) ? (((usqInt)(1)) >> (-rNextNextNext)) : (1ULL << rNextNextNext)));
			}
			rTop14 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		if (rNext8 == NoReg) {
			nextRegisterMask1 = ((rTop14 < 0) ? (((usqInt)(1)) >> (-rTop14)) : (1ULL << rTop14));
			if (rNextNext1 != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNext1)) : (1ULL << rNextNext1)));
			}
			if (rNextNextNext != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNextNext < 0) ? (((usqInt)(1)) >> (-rNextNextNext)) : (1ULL << rNextNextNext)));
			}
			rNext8 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		if (rNextNext1 == NoReg) {
			nextRegisterMask1 = (1ULL << rTop14) | (1ULL << rNext8);
			if (rNextNextNext != NoReg) {
				nextRegisterMask1 = nextRegisterMask1 | (((rNextNextNext < 0) ? (((usqInt)(1)) >> (-rNextNextNext)) : (1ULL << rNextNextNext)));
			}
			rNextNext1 = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		if (rNextNextNext == NoReg) {
			nextRegisterMask1 = ((1ULL << rTop14) | (1ULL << rNext8)) | (1ULL << rNextNext1);
			rNextNextNext = allocateRegNotConflictingWith(nextRegisterMask1);
		}
		assert(!(((rTop14 == NoReg)
 || ((rNext8 == NoReg)
 || ((rNextNext1 == NoReg)
 || (rNextNextNext == NoReg))))));
		offset = rTop14;
		scale = rNext8;
		index = rNextNext1;
		base = rNextNextNext;
		nativePopToReg(ssNativeTop(), offset);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), scale);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), index);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), base);
		ssNativePop(1);
		/* begin MulR:R: */
		genMulRR(backEnd, scale, index);
		genoperandoperand(AddRR, index, base);
		genoperandoperand(AddRR, offset, base);
		ssPushNativeRegister(base);
		return 0;

	case 28:
		/* begin genLowcodeEffectiveAddress64 */
		baseValue1 = 0;
		indexValue1 = 0;
		offsetValue1 = 0;
		resultValue2 = 0;
		scaleValue1 = 0;
		/* begin allocateRegistersForLowcodeInteger4ResultInteger: */
		rTop15 = (rNext9 = (rNextNext2 = (rNextNextNext1 = NoReg)));
		rResult6 = NoReg;
		nativeValueIndex2 = 1;
		if ((nativeRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop15 = nativeRegisterOrNone(ssNativeTop());
			if ((nativeRegisterSecondOrNone(ssNativeTop())) != NoReg) {
				rNext9 = nativeRegisterSecondOrNone(ssNativeTop());
			}
		}
		if (rNext9 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex2))) != NoReg) {
				rNext9 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex2));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex2))) != NoReg) {
					rNextNext2 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex2));
				}
				nativeValueIndex2 += 1;
			}
		}
		if (rNextNext2 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex2))) != NoReg) {
				rNextNext2 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex2));
				if ((nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex2))) != NoReg) {
					rNextNextNext1 = nativeRegisterSecondOrNone(ssNativeValue(nativeValueIndex2));
				}
				nativeValueIndex2 += 1;
			}
		}
		if (rNextNextNext1 == NoReg) {
			if ((nativeRegisterOrNone(ssNativeValue(nativeValueIndex2))) != NoReg) {
				rNextNextNext1 = nativeRegisterOrNone(ssNativeValue(nativeValueIndex2));
				nativeValueIndex2 += 1;
			}
		}
		if (rTop15 == NoReg) {
			nextRegisterMask2 = 0;
			if (rNext9 != NoReg) {
				nextRegisterMask2 = ((rNext9 < 0) ? (((usqInt)(1)) >> (-rNext9)) : (1ULL << rNext9));
			}
			if (rNextNext2 != NoReg) {
				nextRegisterMask2 = nextRegisterMask2 | (((rNextNext2 < 0) ? (((usqInt)(1)) >> (-rNextNext2)) : (1ULL << rNextNext2)));
			}
			if (rNextNextNext1 != NoReg) {
				nextRegisterMask2 = nextRegisterMask2 | (((rNextNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNextNext1)) : (1ULL << rNextNextNext1)));
			}
			rTop15 = allocateRegNotConflictingWith(nextRegisterMask2);
		}
		if (rNext9 == NoReg) {
			nextRegisterMask2 = ((rTop15 < 0) ? (((usqInt)(1)) >> (-rTop15)) : (1ULL << rTop15));
			if (rNextNext2 != NoReg) {
				nextRegisterMask2 = nextRegisterMask2 | (((rNextNext2 < 0) ? (((usqInt)(1)) >> (-rNextNext2)) : (1ULL << rNextNext2)));
			}
			if (rNextNextNext1 != NoReg) {
				nextRegisterMask2 = nextRegisterMask2 | (((rNextNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNextNext1)) : (1ULL << rNextNextNext1)));
			}
			rNext9 = allocateRegNotConflictingWith(nextRegisterMask2);
		}
		if (rNextNext2 == NoReg) {
			nextRegisterMask2 = (1ULL << rTop15) | (1ULL << rNext9);
			if (rNextNextNext1 != NoReg) {
				nextRegisterMask2 = nextRegisterMask2 | (((rNextNextNext1 < 0) ? (((usqInt)(1)) >> (-rNextNextNext1)) : (1ULL << rNextNextNext1)));
			}
			rNextNext2 = allocateRegNotConflictingWith(nextRegisterMask2);
		}
		if (rNextNextNext1 == NoReg) {
			nextRegisterMask2 = ((1ULL << rTop15) | (1ULL << rNext9)) | (1ULL << rNextNext2);
			rNextNextNext1 = allocateRegNotConflictingWith(nextRegisterMask2);
		}
		assert(!(((rTop15 == NoReg)
 || ((rNext9 == NoReg)
 || ((rNextNext2 == NoReg)
 || (rNextNextNext1 == NoReg))))));
		rResult6 = allocateRegNotConflictingWith((((1ULL << rTop15) | (1ULL << rNext9)) | (1ULL << rNextNext2)) | (1ULL << rNextNextNext1));
		offset1 = rTop15;
		scale1 = rNext9;
		index1 = rNextNext2;
		base1 = rNextNextNext1;
		result2 = rResult6;
		nativePopToReg(ssNativeTop(), offset1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), scale1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), index1);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), base1);
		ssNativePop(1);
		abort();
		return 0;

	case 29:
		/* begin genLowcodeEndCall */
		endHighLevelCallWithCleanup();
		return 0;

	case 30:
		/* begin genLowcodeEndCallNoCleanup */
		endHighLevelCallWithoutCleanup();
		return 0;

	case 0x1F:
		/* begin genLowcodeFloat32Add */
		firstValue6 = 0;
		secondValue6 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask8 = 0;
		rTop16 = (rNext10 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop16 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg8 = (rNext10 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask8 = ((reg8 < 0) ? (((usqInt)(1)) >> (-reg8)) : (1ULL << reg8));
		}
		if (rTop16 == NoReg) {
			rTop16 = allocateFloatRegNotConflictingWith(topRegistersMask8);
		}
		if (rNext10 == NoReg) {
			rNext10 = allocateFloatRegNotConflictingWith(((rTop16 < 0) ? (((usqInt)(1)) >> (-rTop16)) : (1ULL << rTop16)));
		}
		assert(!(((rTop16 == NoReg)
 || (rNext10 == NoReg))));
		second6 = rTop16;
		first6 = rNext10;
		nativePopToReg(ssNativeTop(), second6);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first6);
		ssNativePop(1);
		/* begin AddRs:Rs: */
		genoperandoperand(AddRsRs, second6, first6);
		ssPushNativeRegisterSingleFloat(first6);
		return 0;

	case 32:
		/* begin genLowcodeFloat32Div */
		firstValue7 = 0;
		secondValue7 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask9 = 0;
		rTop17 = (rNext12 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop17 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg9 = (rNext12 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask9 = ((reg9 < 0) ? (((usqInt)(1)) >> (-reg9)) : (1ULL << reg9));
		}
		if (rTop17 == NoReg) {
			rTop17 = allocateFloatRegNotConflictingWith(topRegistersMask9);
		}
		if (rNext12 == NoReg) {
			rNext12 = allocateFloatRegNotConflictingWith(((rTop17 < 0) ? (((usqInt)(1)) >> (-rTop17)) : (1ULL << rTop17)));
		}
		assert(!(((rTop17 == NoReg)
 || (rNext12 == NoReg))));
		second7 = rTop17;
		first7 = rNext12;
		nativePopToReg(ssNativeTop(), second7);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first7);
		ssNativePop(1);
		/* begin DivRs:Rs: */
		genoperandoperand(DivRsRs, second7, first7);
		ssPushNativeRegisterSingleFloat(first7);
		return 0;

	case 33:
		/* begin genLowcodeFloat32Equal */
		firstValue8 = 0;
		secondValue8 = 0;
		valueValue7 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask10 = 0;
		frTop2 = (frNext = NoReg);
		rResult7 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop2 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg10 = (frNext = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask10 = ((reg10 < 0) ? (((usqInt)(1)) >> (-reg10)) : (1ULL << reg10));
		}
		if (frTop2 == NoReg) {
			frTop2 = allocateFloatRegNotConflictingWith(topRegistersMask10);
		}
		if (frNext == NoReg) {
			frNext = allocateFloatRegNotConflictingWith(((frTop2 < 0) ? (((usqInt)(1)) >> (-frTop2)) : (1ULL << frTop2)));
		}
		rResult7 = allocateRegNotConflictingWith(0);
		assert(!(((frTop2 == NoReg)
 || ((frNext == NoReg)
 || (rResult7 == NoReg)))));
		second8 = frTop2;
		first8 = frNext;
		value7 = rResult7;
		nativePopToReg(ssNativeTop(), second8);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first8);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second8, first8);
		/* True result */
		falseJump = gJumpFPNotEqual(0);
		genoperandoperand(MoveCqR, 1, value7);
		/* False result */
		contJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value7);
		jmpTarget(contJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value7);
		return 0;

	case 34:
		/* begin genLowcodeFloat32Great */
		firstValue9 = 0;
		secondValue9 = 0;
		valueValue8 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask11 = 0;
		frTop3 = (frNext1 = NoReg);
		rResult8 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop3 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg11 = (frNext1 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask11 = ((reg11 < 0) ? (((usqInt)(1)) >> (-reg11)) : (1ULL << reg11));
		}
		if (frTop3 == NoReg) {
			frTop3 = allocateFloatRegNotConflictingWith(topRegistersMask11);
		}
		if (frNext1 == NoReg) {
			frNext1 = allocateFloatRegNotConflictingWith(((frTop3 < 0) ? (((usqInt)(1)) >> (-frTop3)) : (1ULL << frTop3)));
		}
		rResult8 = allocateRegNotConflictingWith(0);
		assert(!(((frTop3 == NoReg)
 || ((frNext1 == NoReg)
 || (rResult8 == NoReg)))));
		second9 = frTop3;
		first9 = frNext1;
		value8 = rResult8;
		nativePopToReg(ssNativeTop(), second9);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first9);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second9, first9);
		/* True result */
		falseJump1 = gJumpFPLessOrEqual(0);
		genoperandoperand(MoveCqR, 1, value8);
		/* False result */
		contJump1 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value8);
		jmpTarget(contJump1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value8);
		return 0;

	case 35:
		/* begin genLowcodeFloat32GreatEqual */
		firstValue10 = 0;
		secondValue10 = 0;
		valueValue9 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask12 = 0;
		frTop4 = (frNext2 = NoReg);
		rResult9 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop4 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg12 = (frNext2 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask12 = ((reg12 < 0) ? (((usqInt)(1)) >> (-reg12)) : (1ULL << reg12));
		}
		if (frTop4 == NoReg) {
			frTop4 = allocateFloatRegNotConflictingWith(topRegistersMask12);
		}
		if (frNext2 == NoReg) {
			frNext2 = allocateFloatRegNotConflictingWith(((frTop4 < 0) ? (((usqInt)(1)) >> (-frTop4)) : (1ULL << frTop4)));
		}
		rResult9 = allocateRegNotConflictingWith(0);
		assert(!(((frTop4 == NoReg)
 || ((frNext2 == NoReg)
 || (rResult9 == NoReg)))));
		second10 = frTop4;
		first10 = frNext2;
		value9 = rResult9;
		nativePopToReg(ssNativeTop(), second10);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first10);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second10, first10);
		/* True result */
		falseJump2 = gJumpFPLess(0);
		genoperandoperand(MoveCqR, 1, value9);
		/* False result */
		contJump2 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value9);
		jmpTarget(contJump2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value9);
		return 0;

	case 36:
		/* begin genLowcodeFloat32Less */
		firstValue11 = 0;
		secondValue11 = 0;
		valueValue10 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask13 = 0;
		frTop5 = (frNext3 = NoReg);
		rResult10 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop5 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg13 = (frNext3 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask13 = ((reg13 < 0) ? (((usqInt)(1)) >> (-reg13)) : (1ULL << reg13));
		}
		if (frTop5 == NoReg) {
			frTop5 = allocateFloatRegNotConflictingWith(topRegistersMask13);
		}
		if (frNext3 == NoReg) {
			frNext3 = allocateFloatRegNotConflictingWith(((frTop5 < 0) ? (((usqInt)(1)) >> (-frTop5)) : (1ULL << frTop5)));
		}
		rResult10 = allocateRegNotConflictingWith(0);
		assert(!(((frTop5 == NoReg)
 || ((frNext3 == NoReg)
 || (rResult10 == NoReg)))));
		second11 = frTop5;
		first11 = frNext3;
		value10 = rResult10;
		nativePopToReg(ssNativeTop(), second11);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first11);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second11, first11);
		/* True result */
		falseJump3 = gJumpFPGreaterOrEqual(0);
		genoperandoperand(MoveCqR, 1, value10);
		/* False result */
		contJump3 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value10);
		jmpTarget(contJump3, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value10);
		return 0;

	case 37:
		/* begin genLowcodeFloat32LessEqual */
		firstValue12 = 0;
		secondValue12 = 0;
		valueValue11 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask14 = 0;
		frTop6 = (frNext4 = NoReg);
		rResult11 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop6 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg14 = (frNext4 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask14 = ((reg14 < 0) ? (((usqInt)(1)) >> (-reg14)) : (1ULL << reg14));
		}
		if (frTop6 == NoReg) {
			frTop6 = allocateFloatRegNotConflictingWith(topRegistersMask14);
		}
		if (frNext4 == NoReg) {
			frNext4 = allocateFloatRegNotConflictingWith(((frTop6 < 0) ? (((usqInt)(1)) >> (-frTop6)) : (1ULL << frTop6)));
		}
		rResult11 = allocateRegNotConflictingWith(0);
		assert(!(((frTop6 == NoReg)
 || ((frNext4 == NoReg)
 || (rResult11 == NoReg)))));
		second12 = frTop6;
		first12 = frNext4;
		value11 = rResult11;
		nativePopToReg(ssNativeTop(), second12);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first12);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second12, first12);
		/* True result */
		falseJump4 = gJumpFPGreater(0);
		genoperandoperand(MoveCqR, 1, value11);
		/* False result */
		contJump4 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump4, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value11);
		jmpTarget(contJump4, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value11);
		return 0;

	case 38:
		/* begin genLowcodeFloat32Mul */
		firstValue13 = 0;
		secondValue13 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask15 = 0;
		rTop18 = (rNext13 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop18 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg15 = (rNext13 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask15 = ((reg15 < 0) ? (((usqInt)(1)) >> (-reg15)) : (1ULL << reg15));
		}
		if (rTop18 == NoReg) {
			rTop18 = allocateFloatRegNotConflictingWith(topRegistersMask15);
		}
		if (rNext13 == NoReg) {
			rNext13 = allocateFloatRegNotConflictingWith(((rTop18 < 0) ? (((usqInt)(1)) >> (-rTop18)) : (1ULL << rTop18)));
		}
		assert(!(((rTop18 == NoReg)
 || (rNext13 == NoReg))));
		second13 = rTop18;
		first13 = rNext13;
		nativePopToReg(ssNativeTop(), second13);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first13);
		ssNativePop(1);
		/* begin MulRs:Rs: */
		genoperandoperand(MulRsRs, second13, first13);
		ssPushNativeRegisterSingleFloat(first13);
		return 0;

	case 39:
		/* begin genLowcodeFloat32Neg */
		resultValue3 = 0;
		valueValue12 = 0;
		/* begin allocateRegistersForLowcodeFloatResultFloat: */
		frTop7 = NoReg;
		/* Float argument */
		frResult2 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop7 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop7 == NoReg) {
			frTop7 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult2 = allocateFloatRegNotConflictingWith(((frTop7 < 0) ? (((usqInt)(1)) >> (-frTop7)) : (1ULL << frTop7)));
		assert(!(((frTop7 == NoReg)
 || (frResult2 == NoReg))));
		value12 = frTop7;
		result3 = frResult2;
		nativePopToReg(ssNativeTop(), value12);
		ssNativePop(1);
		/* begin XorRs:Rs: */
		genoperandoperand(XorRsRs, result3, result3);
		genoperandoperand(SubRsRs, value12, result3);
		ssPushNativeRegisterSingleFloat(result3);
		return 0;

	case 40:
		/* begin genLowcodeFloat32NotEqual */
		firstValue14 = 0;
		secondValue14 = 0;
		valueValue13 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask16 = 0;
		frTop8 = (frNext5 = NoReg);
		rResult12 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop8 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg16 = (frNext5 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask16 = ((reg16 < 0) ? (((usqInt)(1)) >> (-reg16)) : (1ULL << reg16));
		}
		if (frTop8 == NoReg) {
			frTop8 = allocateFloatRegNotConflictingWith(topRegistersMask16);
		}
		if (frNext5 == NoReg) {
			frNext5 = allocateFloatRegNotConflictingWith(((frTop8 < 0) ? (((usqInt)(1)) >> (-frTop8)) : (1ULL << frTop8)));
		}
		rResult12 = allocateRegNotConflictingWith(0);
		assert(!(((frTop8 == NoReg)
 || ((frNext5 == NoReg)
 || (rResult12 == NoReg)))));
		second14 = frTop8;
		first14 = frNext5;
		value13 = rResult12;
		nativePopToReg(ssNativeTop(), second14);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first14);
		ssNativePop(1);
		/* begin CmpRs:Rs: */
		genoperandoperand(CmpRsRs, second14, first14);
		/* True result */
		falseJump5 = gJumpFPEqual(0);
		genoperandoperand(MoveCqR, 1, value13);
		/* False result */
		contJump5 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump5, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value13);
		jmpTarget(contJump5, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value13);
		return 0;

	case 41:
		/* begin genLowcodeFloat32Sqrt */
		valueValue14 = 0;
		topRegistersMask17 = 0;
		frTop9 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop9 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop9 == NoReg) {
			frTop9 = allocateFloatRegNotConflictingWith(topRegistersMask17);
		}
		assert(!((frTop9 == NoReg)));
		value14 = frTop9;
		nativePopToReg(ssNativeTop(), value14);
		ssNativePop(1);
		/* begin SqrtRs: */
		genoperand(SqrtRs, value14);
		ssPushNativeRegisterSingleFloat(value14);
		return 0;

	case 42:
		/* begin genLowcodeFloat32Sub */
		firstValue15 = 0;
		secondValue15 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask18 = 0;
		rTop19 = (rNext14 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop19 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg17 = (rNext14 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask18 = ((reg17 < 0) ? (((usqInt)(1)) >> (-reg17)) : (1ULL << reg17));
		}
		if (rTop19 == NoReg) {
			rTop19 = allocateFloatRegNotConflictingWith(topRegistersMask18);
		}
		if (rNext14 == NoReg) {
			rNext14 = allocateFloatRegNotConflictingWith(((rTop19 < 0) ? (((usqInt)(1)) >> (-rTop19)) : (1ULL << rTop19)));
		}
		assert(!(((rTop19 == NoReg)
 || (rNext14 == NoReg))));
		second15 = rTop19;
		first15 = rNext14;
		nativePopToReg(ssNativeTop(), second15);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first15);
		ssNativePop(1);
		/* begin SubRs:Rs: */
		genoperandoperand(SubRsRs, second15, first15);
		ssPushNativeRegisterSingleFloat(first15);
		return 0;

	case 43:
		/* begin genLowcodeFloat32ToFloat64 */
		singleFloatValueValue = 0;
		topRegistersMask19 = 0;
		frTop10 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop10 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop10 == NoReg) {
			frTop10 = allocateFloatRegNotConflictingWith(topRegistersMask19);
		}
		assert(!((frTop10 == NoReg)));
		singleFloatValue = frTop10;
		nativePopToReg(ssNativeTop(), singleFloatValue);
		ssNativePop(1);
		/* begin ConvertRs:Rd: */
		genoperandoperand(ConvertRsRd, singleFloatValue, singleFloatValue);
		ssPushNativeRegisterDoubleFloat(singleFloatValue);
		return 0;

	case 44:
		/* begin genLowcodeFloat32ToInt32 */
		resultValue4 = 0;
		valueValue15 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop11 = NoReg;
		/* Float argument */
		rResult13 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop11 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop11 == NoReg) {
			frTop11 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult13 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop11 == NoReg)
 || (rResult13 == NoReg))));
		value15 = frTop11;
		result4 = rResult13;
		nativePopToReg(ssNativeTop(), value15);
		ssNativePop(1);
		/* begin ConvertRs:R: */
		genoperandoperand(ConvertRsR, value15, result4);
		ssPushNativeRegister(result4);
		return 0;

	case 45:
		/* begin genLowcodeFloat32ToInt64 */
		resultValue5 = 0;
		valueValue16 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop12 = NoReg;
		/* Float argument */
		rResult14 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop12 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop12 == NoReg) {
			frTop12 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult14 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop12 == NoReg)
 || (rResult14 == NoReg))));
		value16 = frTop12;
		result5 = rResult14;
		nativePopToReg(ssNativeTop(), value16);
		ssNativePop(1);
		abort();
		return 0;

	case 46:
		/* begin genLowcodeFloat32ToUInt32 */
		resultValue6 = 0;
		valueValue17 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop13 = NoReg;
		/* Float argument */
		rResult15 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop13 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop13 == NoReg) {
			frTop13 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult15 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop13 == NoReg)
 || (rResult15 == NoReg))));
		value17 = frTop13;
		result6 = rResult15;
		nativePopToReg(ssNativeTop(), value17);
		ssNativePop(1);
		/* begin ConvertRs:R: */
		genoperandoperand(ConvertRsR, value17, result6);
		ssPushNativeRegister(result6);
		return 0;

	case 47:
		/* begin genLowcodeFloat32ToUInt64 */
		resultValue7 = 0;
		valueValue18 = 0;
		/* begin allocateRegistersForLowcodeFloatResultInteger: */
		frTop14 = NoReg;
		/* Float argument */
		rResult16 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop14 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop14 == NoReg) {
			frTop14 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		rResult16 = allocateRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		assert(!(((frTop14 == NoReg)
 || (rResult16 == NoReg))));
		value18 = frTop14;
		result7 = rResult16;
		nativePopToReg(ssNativeTop(), value18);
		ssNativePop(1);
		abort();
		return 0;

	case 48:
		/* begin genLowcodeFloat64Add */
		firstValue16 = 0;
		secondValue16 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask20 = 0;
		rTop20 = (rNext15 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop20 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg18 = (rNext15 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask20 = ((reg18 < 0) ? (((usqInt)(1)) >> (-reg18)) : (1ULL << reg18));
		}
		if (rTop20 == NoReg) {
			rTop20 = allocateFloatRegNotConflictingWith(topRegistersMask20);
		}
		if (rNext15 == NoReg) {
			rNext15 = allocateFloatRegNotConflictingWith(((rTop20 < 0) ? (((usqInt)(1)) >> (-rTop20)) : (1ULL << rTop20)));
		}
		assert(!(((rTop20 == NoReg)
 || (rNext15 == NoReg))));
		second16 = rTop20;
		first16 = rNext15;
		nativePopToReg(ssNativeTop(), second16);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first16);
		ssNativePop(1);
		/* begin AddRd:Rd: */
		genoperandoperand(AddRdRd, second16, first16);
		ssPushNativeRegisterDoubleFloat(first16);
		return 0;

	case 49:
		/* begin genLowcodeFloat64Div */
		firstValue17 = 0;
		secondValue17 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask21 = 0;
		rTop21 = (rNext16 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop21 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg19 = (rNext16 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask21 = ((reg19 < 0) ? (((usqInt)(1)) >> (-reg19)) : (1ULL << reg19));
		}
		if (rTop21 == NoReg) {
			rTop21 = allocateFloatRegNotConflictingWith(topRegistersMask21);
		}
		if (rNext16 == NoReg) {
			rNext16 = allocateFloatRegNotConflictingWith(((rTop21 < 0) ? (((usqInt)(1)) >> (-rTop21)) : (1ULL << rTop21)));
		}
		assert(!(((rTop21 == NoReg)
 || (rNext16 == NoReg))));
		second17 = rTop21;
		first17 = rNext16;
		nativePopToReg(ssNativeTop(), second17);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first17);
		ssNativePop(1);
		/* begin DivRd:Rd: */
		genoperandoperand(DivRdRd, second17, first17);
		ssPushNativeRegisterDoubleFloat(first17);
		return 0;

	case 50:
		/* begin genLowcodeFloat64Equal */
		firstValue18 = 0;
		secondValue18 = 0;
		valueValue19 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask22 = 0;
		frTop15 = (frNext6 = NoReg);
		rResult17 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop15 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg20 = (frNext6 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask22 = ((reg20 < 0) ? (((usqInt)(1)) >> (-reg20)) : (1ULL << reg20));
		}
		if (frTop15 == NoReg) {
			frTop15 = allocateFloatRegNotConflictingWith(topRegistersMask22);
		}
		if (frNext6 == NoReg) {
			frNext6 = allocateFloatRegNotConflictingWith(((frTop15 < 0) ? (((usqInt)(1)) >> (-frTop15)) : (1ULL << frTop15)));
		}
		rResult17 = allocateRegNotConflictingWith(0);
		assert(!(((frTop15 == NoReg)
 || ((frNext6 == NoReg)
 || (rResult17 == NoReg)))));
		second18 = frTop15;
		first18 = frNext6;
		value19 = rResult17;
		nativePopToReg(ssNativeTop(), second18);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first18);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second18, first18);
		/* True result */
		falseJump6 = gJumpFPNotEqual(0);
		genoperandoperand(MoveCqR, 1, value19);
		/* False result */
		contJump6 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump6, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value19);
		jmpTarget(contJump6, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value19);
		return 0;

	case 51:
		/* begin genLowcodeFloat64Great */
		firstValue19 = 0;
		secondValue19 = 0;
		valueValue20 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask23 = 0;
		frTop16 = (frNext7 = NoReg);
		rResult18 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop16 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg21 = (frNext7 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask23 = ((reg21 < 0) ? (((usqInt)(1)) >> (-reg21)) : (1ULL << reg21));
		}
		if (frTop16 == NoReg) {
			frTop16 = allocateFloatRegNotConflictingWith(topRegistersMask23);
		}
		if (frNext7 == NoReg) {
			frNext7 = allocateFloatRegNotConflictingWith(((frTop16 < 0) ? (((usqInt)(1)) >> (-frTop16)) : (1ULL << frTop16)));
		}
		rResult18 = allocateRegNotConflictingWith(0);
		assert(!(((frTop16 == NoReg)
 || ((frNext7 == NoReg)
 || (rResult18 == NoReg)))));
		second19 = frTop16;
		first19 = frNext7;
		value20 = rResult18;
		nativePopToReg(ssNativeTop(), second19);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first19);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second19, first19);
		/* True result */
		falseJump7 = gJumpFPLessOrEqual(0);
		genoperandoperand(MoveCqR, 1, value20);
		/* False result */
		contJump7 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump7, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value20);
		jmpTarget(contJump7, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value20);
		return 0;

	case 52:
		/* begin genLowcodeFloat64GreatEqual */
		firstValue20 = 0;
		secondValue20 = 0;
		valueValue21 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask24 = 0;
		frTop17 = (frNext8 = NoReg);
		rResult19 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop17 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg22 = (frNext8 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask24 = ((reg22 < 0) ? (((usqInt)(1)) >> (-reg22)) : (1ULL << reg22));
		}
		if (frTop17 == NoReg) {
			frTop17 = allocateFloatRegNotConflictingWith(topRegistersMask24);
		}
		if (frNext8 == NoReg) {
			frNext8 = allocateFloatRegNotConflictingWith(((frTop17 < 0) ? (((usqInt)(1)) >> (-frTop17)) : (1ULL << frTop17)));
		}
		rResult19 = allocateRegNotConflictingWith(0);
		assert(!(((frTop17 == NoReg)
 || ((frNext8 == NoReg)
 || (rResult19 == NoReg)))));
		second20 = frTop17;
		first20 = frNext8;
		value21 = rResult19;
		nativePopToReg(ssNativeTop(), second20);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first20);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second20, first20);
		/* True result */
		falseJump8 = gJumpFPLess(0);
		genoperandoperand(MoveCqR, 1, value21);
		/* False result */
		contJump8 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump8, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value21);
		jmpTarget(contJump8, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value21);
		return 0;

	case 53:
		/* begin genLowcodeFloat64Less */
		firstValue21 = 0;
		secondValue21 = 0;
		valueValue22 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask25 = 0;
		frTop18 = (frNext9 = NoReg);
		rResult20 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop18 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg23 = (frNext9 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask25 = ((reg23 < 0) ? (((usqInt)(1)) >> (-reg23)) : (1ULL << reg23));
		}
		if (frTop18 == NoReg) {
			frTop18 = allocateFloatRegNotConflictingWith(topRegistersMask25);
		}
		if (frNext9 == NoReg) {
			frNext9 = allocateFloatRegNotConflictingWith(((frTop18 < 0) ? (((usqInt)(1)) >> (-frTop18)) : (1ULL << frTop18)));
		}
		rResult20 = allocateRegNotConflictingWith(0);
		assert(!(((frTop18 == NoReg)
 || ((frNext9 == NoReg)
 || (rResult20 == NoReg)))));
		second21 = frTop18;
		first21 = frNext9;
		value22 = rResult20;
		nativePopToReg(ssNativeTop(), second21);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first21);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second21, first21);
		/* True result */
		falseJump9 = gJumpFPGreaterOrEqual(0);
		genoperandoperand(MoveCqR, 1, value22);
		/* False result */
		contJump9 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump9, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value22);
		jmpTarget(contJump9, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value22);
		return 0;

	case 54:
		/* begin genLowcodeFloat64LessEqual */
		firstValue22 = 0;
		secondValue22 = 0;
		valueValue23 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask26 = 0;
		frTop19 = (frNext10 = NoReg);
		rResult21 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop19 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg24 = (frNext10 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask26 = ((reg24 < 0) ? (((usqInt)(1)) >> (-reg24)) : (1ULL << reg24));
		}
		if (frTop19 == NoReg) {
			frTop19 = allocateFloatRegNotConflictingWith(topRegistersMask26);
		}
		if (frNext10 == NoReg) {
			frNext10 = allocateFloatRegNotConflictingWith(((frTop19 < 0) ? (((usqInt)(1)) >> (-frTop19)) : (1ULL << frTop19)));
		}
		rResult21 = allocateRegNotConflictingWith(0);
		assert(!(((frTop19 == NoReg)
 || ((frNext10 == NoReg)
 || (rResult21 == NoReg)))));
		second22 = frTop19;
		first22 = frNext10;
		value23 = rResult21;
		nativePopToReg(ssNativeTop(), second22);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first22);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second22, first22);
		/* True result */
		falseJump10 = gJumpFPGreater(0);
		genoperandoperand(MoveCqR, 1, value23);
		/* False result */
		contJump10 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump10, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value23);
		jmpTarget(contJump10, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value23);
		return 0;

	case 55:
		/* begin genLowcodeFloat64Mul */
		firstValue23 = 0;
		secondValue23 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask27 = 0;
		rTop22 = (rNext17 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop22 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg25 = (rNext17 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask27 = ((reg25 < 0) ? (((usqInt)(1)) >> (-reg25)) : (1ULL << reg25));
		}
		if (rTop22 == NoReg) {
			rTop22 = allocateFloatRegNotConflictingWith(topRegistersMask27);
		}
		if (rNext17 == NoReg) {
			rNext17 = allocateFloatRegNotConflictingWith(((rTop22 < 0) ? (((usqInt)(1)) >> (-rTop22)) : (1ULL << rTop22)));
		}
		assert(!(((rTop22 == NoReg)
 || (rNext17 == NoReg))));
		second23 = rTop22;
		first23 = rNext17;
		nativePopToReg(ssNativeTop(), second23);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first23);
		ssNativePop(1);
		/* begin MulRd:Rd: */
		genoperandoperand(MulRdRd, second23, first23);
		ssPushNativeRegisterDoubleFloat(first23);
		return 0;

	case 56:
		/* begin genLowcodeFloat64Neg */
		resultValue8 = 0;
		valueValue24 = 0;
		/* begin allocateRegistersForLowcodeFloatResultFloat: */
		frTop20 = NoReg;
		/* Float argument */
		frResult3 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop20 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop20 == NoReg) {
			frTop20 = allocateFloatRegNotConflictingWith(0 /* begin emptyRegisterMask */);
		}
		frResult3 = allocateFloatRegNotConflictingWith(((frTop20 < 0) ? (((usqInt)(1)) >> (-frTop20)) : (1ULL << frTop20)));
		assert(!(((frTop20 == NoReg)
 || (frResult3 == NoReg))));
		value24 = frTop20;
		result8 = frResult3;
		nativePopToReg(ssNativeTop(), value24);
		ssNativePop(1);
		/* begin XorRd:Rd: */
		genoperandoperand(XorRdRd, result8, result8);
		genoperandoperand(SubRdRd, value24, result8);
		ssPushNativeRegisterDoubleFloat(result8);
		return 0;

	case 57:
		/* begin genLowcodeFloat64NotEqual */
		firstValue24 = 0;
		secondValue24 = 0;
		valueValue25 = 0;
		/* begin allocateRegistersForLowcodeFloat2ResultInteger: */
		topRegistersMask28 = 0;
		frTop21 = (frNext11 = NoReg);
		rResult22 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop21 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg26 = (frNext11 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask28 = ((reg26 < 0) ? (((usqInt)(1)) >> (-reg26)) : (1ULL << reg26));
		}
		if (frTop21 == NoReg) {
			frTop21 = allocateFloatRegNotConflictingWith(topRegistersMask28);
		}
		if (frNext11 == NoReg) {
			frNext11 = allocateFloatRegNotConflictingWith(((frTop21 < 0) ? (((usqInt)(1)) >> (-frTop21)) : (1ULL << frTop21)));
		}
		rResult22 = allocateRegNotConflictingWith(0);
		assert(!(((frTop21 == NoReg)
 || ((frNext11 == NoReg)
 || (rResult22 == NoReg)))));
		second24 = frTop21;
		first24 = frNext11;
		value25 = rResult22;
		nativePopToReg(ssNativeTop(), second24);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first24);
		ssNativePop(1);
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, second24, first24);
		/* True result */
		falseJump11 = gJumpFPEqual(0);
		genoperandoperand(MoveCqR, 1, value25);
		/* False result */
		contJump11 = genoperand(Jump, ((sqInt)0));
		jmpTarget(falseJump11, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, 0, value25);
		jmpTarget(contJump11, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushNativeRegister(value25);
		return 0;

	case 58:
		/* begin genLowcodeFloat64Sqrt */
		valueValue26 = 0;
		topRegistersMask29 = 0;
		frTop22 = NoReg;
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			frTop22 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if (frTop22 == NoReg) {
			frTop22 = allocateFloatRegNotConflictingWith(topRegistersMask29);
		}
		assert(!((frTop22 == NoReg)));
		value26 = frTop22;
		nativePopToReg(ssNativeTop(), value26);
		ssNativePop(1);
		/* begin SqrtRd: */
		genoperand(SqrtRd, value26);
		ssPushNativeRegisterDoubleFloat(value26);
		return 0;

	case 59:
		/* begin genLowcodeFloat64Sub */
		firstValue25 = 0;
		secondValue25 = 0;
		/* begin allocateRegistersForLowcodeFloat2: */
		topRegistersMask30 = 0;
		rTop23 = (rNext18 = NoReg);
		if ((nativeFloatRegisterOrNone(ssNativeTop())) != NoReg) {
			rTop23 = nativeFloatRegisterOrNone(ssNativeTop());
		}
		if ((nativeFloatRegisterOrNone(ssNativeValue(1))) != NoReg) {
			reg27 = (rNext18 = nativeFloatRegisterOrNone(ssNativeValue(1)));
			/* begin registerMaskFor: */
			topRegistersMask30 = ((reg27 < 0) ? (((usqInt)(1)) >> (-reg27)) : (1ULL << reg27));
		}
		if (rTop23 == NoReg) {
			rTop23 = allocateFloatRegNotConflictingWith(topRegistersMask30);
		}
		if (rNext18 == NoReg) {
			rNext18 = allocateFloatRegNotConflictingWith(((rTop23 < 0) ? (((usqInt)(1)) >> (-rTop23)) : (1ULL << rTop23)));
		}
		assert(!(((rTop23 == NoReg)
 || (rNext18 == NoReg))));
		second25 = rTop23;
		first25 = rNext18;
		nativePopToReg(ssNativeTop(), second25);
		ssNativePop(1);
		nativePopToReg(ssNativeTop(), first25);
		ssNativePop(1);
		/* begin SubRd:Rd: */
		genoperandoperand(SubRdRd, second25, first25);
		ssPushNativeRegisterDoubleFloat(first25);
		return 0;

	default:
		return genLowcodeUnaryInlinePrimitive2(prim);
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#genMarshalledSend:numArgs:sendTable: */
static NoDbgRegParms sqInt
genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable)
{
    sqInt annotation;
    AbstractInstruction *instruction;

	assert(needsFrame);
	/* begin annotationForSendTable: */
	if (sendTable == ordinarySendTrampolines) {
		annotation = IsSendCall;
		goto l1;
	}
	if (sendTable == directedSuperSendTrampolines) {
		annotation = IsDirectedSuperSend;
		goto l1;
	}
	if (sendTable == directedSuperBindingSendTrampolines) {
		annotation = IsDirectedSuperBindingSend;
		goto l1;
	}
	assert(sendTable == superSendTrampolines);
	annotation = IsSuperSend;
	l1:	/* end annotationForSendTable: */;
	if (	/* begin annotationIsForUncheckedEntryPoint: */
		(annotation == IsSuperSend)
	 || (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend)))) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg: */
		instruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(ReceiverResultReg, TempReg, instruction, 0);
	}
	if (numArgs >= (NumSendTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, numArgs, SendNumArgsReg);
	}
	if (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend))) {
		/* begin genMoveConstant:R: */
		if (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(tempOop))
		 && ((oopisGreaterThan(tempOop, classTableRootObj()))
		 || (oopisLessThan(tempOop, nilObject())))) {
			annotateobjRef(genoperandoperand(MoveCwR, tempOop, TempReg), tempOop);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, tempOop, TempReg);
		}
	}
	genLoadInlineCacheWithSelector(selectorIndex);
	((genoperand(Call, sendTable[((numArgs < (NumSendTrampolines - 1)) ? numArgs : (NumSendTrampolines - 1))]))->annotation = annotation);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	return ssPushRegister(ReceiverResultReg);
}


/*	Generate the abort for a method. This abort performs either a call of
	ceSICMiss: to handle a single-in-line cache miss or a call of
	ceStackOverflow: to handle a
	stack overflow. It distinguishes the two by testing ResultReceiverReg. If
	the register is zero then this is a stack-overflow because a) the receiver
	has already
	been pushed and so can be set to zero before calling the abort, and b) the
	receiver must always contain an object (and hence be non-zero) on SIC
	miss.  */

	/* StackToRegisterMappingCogit>>#genMethodAbortTrampolineFor: */
static NoDbgRegParms usqInt
genMethodAbortTrampolineFor(sqInt numArgs)
{
    AbstractInstruction *jumpSICMiss;

	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, 0, ReceiverResultReg);
	/* The abort sequence has pushed the LinkReg a second time - because a stack
	   overflow can only happen after building a frame, which pushes LinkReg anyway, and
	   we still need to push LinkReg in case we get to this routine from a sendMissAbort.
	   (On ARM there is a simpler way; use two separate abort calls since all instructions are 32-bits
	   but on x86 the zero receiver reg, call methodAbort sequence is smaller; we may fix this one day).
	   Overwrite that duplicate with the right one - the return address for the call to the abort trampoline.
	   The only reason it matters is an assert in ceStackOverflow: uses it */
	jumpSICMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceStackOverflow, 1, SendNumArgsReg, null, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpSICMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSICMiss, trampolineNamenumRegArgs("ceMethodAbort", numArgs), 1, ReceiverResultReg, null, null, null, 0 /* begin emptyRegisterMask */, 0, NoReg, 1);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged
	target or a call of ceMNUFromPICMNUMethod:receiver: to handle an
	MNU dispatch in a closed PIC. It distinguishes the two by testing
	ClassReg. If the register is zero then this is an MNU. */

	/* StackToRegisterMappingCogit>>#genPICAbortTrampolineFor: */
static NoDbgRegParms usqInt
genPICAbortTrampolineFor(sqInt numArgs)
{
	zeroOpcodeIndex();
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genInnerPICAbortTrampoline(trampolineNamenumRegArgs("cePICAbort", numArgs));
}

	/* StackToRegisterMappingCogit>>#genPICMissTrampolineFor: */
static NoDbgRegParms usqInt
genPICMissTrampolineFor(sqInt numArgs)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCPICMissreceiver, trampolineNamenumRegArgs("cePICMiss", numArgs), 2, ClassReg, ReceiverResultReg, null, null, 0 /* begin emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genPopStackBytecode */
static sqInt
genPopStackBytecode(void)
{
	if (((ssTop())->spilled)) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(AddCqR, BytesPerWord, SPReg);
	}
	ssPop(1);
	return 0;
}


/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive. */
/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive.
	Override to push the register args first. */

	/* StackToRegisterMappingCogit>>#genPrimitiveClosureValue */
static sqInt
genPrimitiveClosureValue(void)
{
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpFail1;
    AbstractInstruction *jumpFail2;
    AbstractInstruction *jumpFail3;
    AbstractInstruction *jumpFail4;
    AbstractInstruction *jumpFailNArgs;
    sqInt offset;
    void (*primitiveRoutine)(void);
    sqInt result;

	genPushRegisterArgs();
	genLoadSlotsourceRegdestReg(ClosureNumArgsIndex, ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)methodOrBlockNumArgs << 3) | 1), TempReg);
	jumpFailNArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ClassReg);
	jumpFail1 = genJumpImmediate(ClassReg);
	genGetCompactClassIndexNonImmOfinto(ClassReg, TempReg);
	genCmpClassMethodContextCompactIndexR(TempReg);
	/* We defer unforwarding the receiver to the prologue; scanning blocks
	   for inst var refs and only unforwarding if the block refers to inst vars. */
	jumpFail2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(MethodIndex, ClassReg, SendNumArgsReg);
	jumpFail3 = genJumpImmediate(SendNumArgsReg);
	genGetFormatOfinto(SendNumArgsReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	jumpFail4 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	/* begin MoveM16:r:R: */
	offset = offsetof(CogMethod, blockEntryOffset);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveM16rR, offset, ClassReg, TempReg);
	genoperandoperand(AddRR, ClassReg, TempReg);
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, null);
	if (primitiveRoutine == primitiveClosureValueNoContextSwitch) {
		if (blockNoContextSwitchOffset == null) {
			return NotFullyInitialized;
		}
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(SubCqR, blockNoContextSwitchOffset, TempReg);
	}
	genoperand(JumpR, TempReg);
	jmpTarget(jumpBCMethod, jmpTarget(jumpFail1, jmpTarget(jumpFail2, jmpTarget(jumpFail3, jmpTarget(jumpFail4, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	if (((result = compileInterpreterPrimitiveflags(primitiveRoutine, primitivePropertyFlagsnumArgs(primitiveIndex, methodOrBlockNumArgs)))) < 0) {
		return result;
	}
	jmpTarget(jumpFailNArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive. */
/*	Override to push the register args first. */

	/* StackToRegisterMappingCogit>>#genPrimitiveFullClosureValue */
static sqInt
genPrimitiveFullClosureValue(void)
{
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpFail4;
    AbstractInstruction *jumpFailImmediateMethod;
    AbstractInstruction *jumpFailNArgs;
    void (*primitiveRoutine)(void);
    sqInt quickConstant;
    sqInt result;

	genPushRegisterArgs();
	genLoadSlotsourceRegdestReg(ClosureNumArgsIndex, ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, (((usqInt)methodOrBlockNumArgs << 3) | 1), TempReg);
	/* We defer unforwarding the receiver to the prologue; scanning blocks
	   for inst var refs and only unforwarding if the block refers to inst vars. */
	jumpFailNArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(FullClosureCompiledBlockIndex, ReceiverResultReg, SendNumArgsReg);
	jumpFailImmediateMethod = genJumpImmediate(SendNumArgsReg);
	genGetFormatOfinto(SendNumArgsReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	jumpFail4 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, null);
	/* begin AddCq:R: */
	quickConstant = (primitiveRoutine == primitiveFullClosureValueNoContextSwitch
		? fullBlockNoContextSwitchEntryOffset()
		: fullBlockEntryOffset());
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, quickConstant, ClassReg);
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpBCMethod, jmpTarget(jumpFailImmediateMethod, jmpTarget(jumpFail4, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (((result = compileInterpreterPrimitiveflags(primitiveRoutine, primitivePropertyFlagsnumArgs(primitiveIndex, methodOrBlockNumArgs)))) < 0) {
		return result;
	}
	jmpTarget(jumpFailNArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Generate an in-line perform primitive. The lookup code requires the
	selector to be in Arg0Reg.
	adjustArgumentsForPerform: adjusts the arguments once
	genLookupForPerformNumArgs: has generated the code for the lookup. */

	/* StackToRegisterMappingCogit>>#genPrimitivePerform */
static sqInt
genPrimitivePerform(void)
{
    sqInt offset;

	if (methodOrBlockNumArgs > (numRegArgs())) {
		/* begin MoveMw:r:R: */
		offset = (methodOrBlockNumArgs) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperandoperand(MoveMwrR, offset, SPReg, Arg0Reg);
	}
	return genLookupForPerformNumArgs(methodOrBlockNumArgs);
}


/*	Generate an in-line perform:withArguments: primitive. The lookup code
	requires the selector to be in Arg0Reg
	and the array to be in Arg1Reg. The primitive will only handle cases 0 to
	numRegArgs. Is it worth it you ask?
	Here are arguemnt count requencies for a short run of Croquet/Virtend
	which show that even for V3, with only
	one rtegister arg, it very much is (but we wimp out on V3 cuz of the
	complexity of checking the Array):
	[0] = 253743		52.5%
	[1] = 116117		24%/76.5%
	[2] = 99876		20.7%/97.2%
	[3] = 12837		2.7%/99.9%
	[4] = 209
	[5] = 84
	[6] = 2
	[7] = 313
	[8] = 0
	[9] = 0
	[10] = 0
	[11] = 1
	[12] = 0
	[13] = 0
	[14] = 0
	[15] = 0 */

	/* StackToRegisterMappingCogit>>#genPrimitivePerformWithArguments */
static sqInt
genPrimitivePerformWithArguments(void)
{
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBadNumArgs1;
    AbstractInstruction *jumpBadNumArgs2;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpImmArray;
    AbstractInstruction *jumpInterpret;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt quickConstant;
    sqInt reg;


	/* begin genLookupForPerformWithArguments */
	jumpImmArray = genJumpImmediate(Arg1Reg);
	genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, SendNumArgsReg, 0);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	if (!(		/* begin addressIsInCodeZone: */
			((((usqInt)(methodCacheAddress()))) >= codeBase)
		 && ((((usqInt)(methodCacheAddress()))) < limitAddress))) {
		/* begin MoveCq:R: */
		quickConstant = methodCacheAddress();
		reg = (cacheBaseReg = Extra0Reg);
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, quickConstant, reg);
	}
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 0, cacheBaseReg);
	/* Fetch the method, and check if it is cogged. */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin MoveMw:r:R: */
	itsAHit = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	/* check the argument count; if it's wrong fall back on the interpreter primitive. */
	jumpInterpret = genJumpImmediate(ClassReg);
	/* begin genLoadcmNumArgsOf:into: */
	genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	genoperandoperandoperand(MoveMbrR, BytesPerWord, ClassReg, SendNumArgsReg);
	genGetRawSlotSizeOfNonImminto(Arg1Reg, TempReg);
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SendNumArgsReg);
	jumpBadNumArgs1 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(CmpCqR, numRegArgs(), SendNumArgsReg);
	/* Fetch arguments and jump to the method's unchecked entry-point. */
	jumpBadNumArgs2 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	genFetchRegArgsForPerformWithArguments(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpImmArray, jmpTarget(jumpSelectorMiss, jmpTarget(jumpInterpret, jmpTarget(jumpBadNumArgs1, jmpTarget(jumpBadNumArgs2, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genPushActiveContextBytecode */
static sqInt
genPushActiveContextBytecode(void)
{
	assert(needsFrame);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genGetActiveContextNumArgslargeinBlock(methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	return ssPushRegister(ReceiverResultReg);
}


/*	Block compilation. At this point in the method create the block. Note its
	start and defer generating code for it until after the method and any
	other preceding
	blocks. The block's actual code will be compiled later. */
/*	143 10001111 llllkkkk jjjjjjjj iiiiiiii	Push Closure Num Copied llll Num
	Args kkkk BlockSize jjjjjjjjiiiiiiii */

	/* StackToRegisterMappingCogit>>#genPushClosureCopyCopiedValuesBytecode */
static sqInt
genPushClosureCopyCopiedValuesBytecode(void)
{
    sqInt i;
    sqInt numArgs;
    sqInt numCopied;
    sqInt reg;
    sqInt startpc;

	assert(needsFrame);
	startpc = bytecodePC + (((generatorAt(byte0))->numBytes));
	addBlockStartAtnumArgsnumCopiedspan(startpc, (numArgs = byte1 & 15), (numCopied = ((usqInt)(byte1)) >> 4), (((sqInt)((usqInt)(byte2) << 8))) + byte3);
	/* begin genInlineClosure:numArgs:numCopied: */
	assert(getActiveContextAllocatesInMachineCode());
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(startpc + 1, numArgs, numCopied, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (ClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	<SmallInteger> */
/*	Override to avoid the BytecodeSetHasDirectedSuperSend check, which is
	unnecessary here given the simulation stack. */

	/* StackToRegisterMappingCogit>>#genPushLiteralIndex: */
static NoDbgRegParms sqInt
genPushLiteralIndex(sqInt literalIndex)
{
    sqInt literal;

	literal = getLiteral(literalIndex);
	return ssPushConstant(literal);
}

	/* StackToRegisterMappingCogit>>#genPushLiteralVariable: */
static NoDbgRegParms sqInt
genPushLiteralVariable(sqInt literalIndex)
{
    sqInt association;
    sqInt bcpc;
    sqInt descriptor;
    BytecodeDescriptor *descriptor1;
    sqInt eA;
    sqInt eB;
    sqInt extb;
    sqInt freeReg;
    sqInt savedB0;
    sqInt savedB1;
    sqInt savedB2;
    sqInt savedB3;
    sqInt savedEA;
    sqInt savedEB;
    sqInt savedNEB;

	descriptor = 0;
	extb = 0;
	/* If followed by a directed super send bytecode, avoid generating any code yet.
	   The association will be passed to the directed send trampoline in a register
	   and fully dereferenced only when first linked.  It will be ignored in later sends. */
	association = getLiteral(literalIndex);
	assert(!(directedSendUsesBinding));
	/* begin nextDescriptorExtensionsAndNextPCInto: */
	descriptor1 = generatorAt(byte0);
	savedB0 = byte0;
	savedB1 = byte1;
	savedB2 = byte2;
	savedB3 = byte3;
	savedEA = extA;
	savedEB = extB;
	savedNEB = numExtB;
	bcpc = bytecodePC + ((descriptor1->numBytes));
	do {
		if (bcpc > endPC) {
			goto l1;
		}
		byte0 = (fetchByteofObject(bcpc, methodObj)) + bytecodeSetOffset;
		descriptor1 = generatorAt(byte0);
		loadSubsequentBytesForDescriptorat(descriptor1, bcpc);
		if (!((descriptor1->isExtension))) {
			eA = extA;
			eB = extB;
			extA = savedEA;
			extB = savedEB;
			numExtB = savedNEB;
			byte0 = savedB0;
			byte1 = savedB1;
			byte2 = savedB2;
			byte3 = savedB3;
			if (			/* begin isDirectedSuper:extA:extB: */
				(descriptor1)
			 && ((((descriptor1->generator)) == genExtSendSuperBytecode)
			 && (eB >= 64))) {
				ssPushConstant(association);
				directedSendUsesBinding = 1;
				return 0;
			}
			goto l1;
		}
		((descriptor1->generator))();
		bcpc += (descriptor1->numBytes);
	} while(1);
	l1:	/* end nextDescriptorExtensionsAndNextPCInto: */;
	/* N.B. Do _not_ use ReceiverResultReg to avoid overwriting receiver in assignment in frameless methods. */
	/* So far descriptors are not rich enough to describe the entire dereference so generate the register
	   load but don't push the result.  There is an order-of-evaluation issue if we defer the dereference. */
	freeReg = allocateRegNotConflictingWith(0);
	/* begin genMoveConstant:R: */
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(association))
	 && ((oopisGreaterThan(association, classTableRootObj()))
	 || (oopisLessThan(association, nilObject())))) {
		annotateobjRef(genoperandoperand(MoveCwR, association, TempReg), association);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, association, TempReg);
	}
	genLoadSlotsourceRegdestReg(ValueIndex, TempReg, freeReg);
	ssPushRegister(freeReg);
	return 0;
}

	/* StackToRegisterMappingCogit>>#genPushMaybeContextReceiverVariable: */
static NoDbgRegParms sqInt
genPushMaybeContextReceiverVariable(sqInt slotIndex)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *jmpDone;
    AbstractInstruction *jmpSingle;


	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ensureReceiverResultRegContainsSelf();
	/* begin genPushMaybeContextSlotIndex: */
	assert(needsFrame);
	if (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {
		/* We have no way of reloading ReceiverResultReg since we need the inst var value as the result. */
		voidReceiverResultRegContainsSelf();
	}
	if (slotIndex == InstructionPointerIndex) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
		abstractInstruction = genoperand(Call, ceFetchContextInstVarTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		return ssPushRegister(SendNumArgsReg);
	}
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	jmpSingle = genJumpNotSmallInteger(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	abstractInstruction1 = genoperand(Call, ceFetchContextInstVarTrampoline);
	(abstractInstruction1->annotation = IsRelativeCall);
	jmpDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genLoadSlotsourceRegdestReg(slotIndex, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jmpDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return ssPushRegister(SendNumArgsReg);
}

	/* StackToRegisterMappingCogit>>#genPushNewArrayBytecode */
static sqInt
genPushNewArrayBytecode(void)
{
    sqInt i;
    sqInt i1;
    int popValues;
    sqInt size;

	assert(needsFrame);
	voidReceiverResultRegContainsSelf();
	if ((popValues = byte1 > 0x7F)) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i1 <= simStackPtr; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
	}
	else {
		/* begin ssAllocateCallReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << SendNumArgsReg)) | ((1U << ReceiverResultReg))), simStackPtr, simNativeStackPtr);
	}
	size = byte1 & 0x7F;
	if (!popValues) {
		if (tryCollapseTempVectorInitializationOfSize(size)) {
			return 0;
		}
	}
	genNewArrayOfSizeinitialized(size, !popValues);
	if (popValues) {
		for (i = (size - 1); i >= 0; i += -1) {
			/* begin PopR: */
			genoperand(PopR, TempReg);
			genStoreSourceRegslotIndexintoNewObjectInDestReg(TempReg, i, ReceiverResultReg);
		}
		ssPop(size);
	}
	return ssPushRegister(ReceiverResultReg);
}

	/* StackToRegisterMappingCogit>>#genPushReceiverBytecode */
static sqInt
genPushReceiverBytecode(void)
{
	if ((((simSelf())->liveRegister)) == ReceiverResultReg) {
		return ssPushRegister(ReceiverResultReg);
	}
	return ssPushDesc(ssSelfDescriptor());
}

	/* StackToRegisterMappingCogit>>#genPushReceiverVariable: */
static NoDbgRegParms sqInt
genPushReceiverVariable(sqInt index)
{
	ensureReceiverResultRegContainsSelf();
	return ssPushBaseoffset(ReceiverResultReg, slotOffsetOfInstVarIndex(index));
}


/*	Ensure that the register args are pushed before the retpc for methods with
	arity <= self numRegArgs.
 */
/*	This isn't as clumsy on a RISC. But putting the receiver and
	args above the return address means the CoInterpreter has a
	single machine-code frame format which saves us a lot of work. */

	/* StackToRegisterMappingCogit>>#genPushRegisterArgs */
static void
genPushRegisterArgs(void)
{
	if (!(regArgsHaveBeenPushed
		 || (methodOrBlockNumArgs > (numRegArgs())))) {
		genPushRegisterArgsForNumArgsscratchReg(backEnd, methodOrBlockNumArgs, SendNumArgsReg);
		regArgsHaveBeenPushed = 1;
	}
}

	/* StackToRegisterMappingCogit>>#genPushRemoteTempLongBytecode */
static sqInt
genPushRemoteTempLongBytecode(void)
{
    sqInt offset;
    sqInt remoteTempReg;
    sqInt tempVectReg;

	tempVectReg = allocateRegNotConflictingWith(0);
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfTemporary(byte2);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, tempVectReg);
	remoteTempReg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | (((tempVectReg < 0) ? (((usqInt)(1)) >> (-tempVectReg)) : (1ULL << tempVectReg))));
	if (remoteTempReg == NoReg) {
		remoteTempReg = tempVectReg;
	}
	genLoadSlotsourceRegdestReg(byte1, tempVectReg, remoteTempReg);
	return ssPushRegister(remoteTempReg);
}


/*	If a frameless method (not a block), only argument temps can be accessed.
	This is assured by the use of needsFrameIfMod16GENumArgs: in pushTemp. */

	/* StackToRegisterMappingCogit>>#genPushTemporaryVariable: */
static NoDbgRegParms sqInt
genPushTemporaryVariable(sqInt index)
{
	assert((inBlock > 0)
	 || (needsFrame
	 || (index < methodOrBlockNumArgs)));
	return ssPushDesc(simStack[index + 1]);
}


/*	In a frameless method ReceiverResultReg already contains self.
	In a frameful method, ReceiverResultReg /may/ contain self. */

	/* StackToRegisterMappingCogit>>#genReturnReceiver */
static sqInt
genReturnReceiver(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin putSelfInReceiverResultReg */
			storeToReg(simSelf(), ReceiverResultReg);
		}
	}
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromBlock */
static sqInt
genReturnTopFromBlock(void)
{
	assert(inBlock > 0);
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genBlockReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromMethod */
static sqInt
genReturnTopFromMethod(void)
{
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genSendDirectedSuper:numArgs: */
static NoDbgRegParms sqInt
genSendDirectedSupernumArgs(sqInt selectorIndex, sqInt numArgs)
{
    sqInt result;

	assert((((ssTop())->type)) == SSConstant);
	tempOop = ((ssTop())->constant);
	ssPop(1);
	marshallSendArguments(numArgs);
	result = genMarshalledSendnumArgssendTable(selectorIndex, numArgs, (directedSendUsesBinding
		? directedSuperBindingSendTrampolines
		: directedSuperSendTrampolines));
	directedSendUsesBinding = 0;
	return result;
}

	/* StackToRegisterMappingCogit>>#genSendSuper:numArgs: */
static NoDbgRegParms sqInt
genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, superSendTrampolines);
}


/*	Generate a trampoline with four arguments.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* StackToRegisterMappingCogit>>#genSendTrampolineFor:numArgs:called:arg:arg:arg:arg: */
static NoDbgRegParms usqInt
genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3)
{
    sqInt routine;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	routine = ceDereferenceSelectorIndex;
	if (!(routine == null)) {
		/* Explicitly save LinkReg via ExtraReg2; it's presumably faster than pushing/popping */
		/* begin Call: */
		genoperand(Call, routine);
	}
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(aRoutine, aString, 4, regOrConst0, regOrConst1, regOrConst2, regOrConst3, 0 /* begin emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genSend:numArgs: */
static NoDbgRegParms sqInt
genSendnumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, ordinarySendTrampolines);
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorArithmetic */
static sqInt
genSpecialSelectorArithmetic(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt argInt;
    int argIsConst;
    sqInt argIsInt;
    sqInt i;
    sqInt index;
    AbstractInstruction *jumpContinue;
    AbstractInstruction *jumpNotSmallInts;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	argIsInt = ((argIsConst = (((ssTop())->type)) == SSConstant))
	 && ((((((argInt = ((ssTop())->constant)))) & 7) == 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
	 && ((((((rcvrInt = ((ssValue(1))->constant)))) & 7) == 1)))
	 || ((mclassIsSmallInteger())
	 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsInt
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		rcvrInt = (rcvrInt >> 3);
		argInt = (argInt >> 3);
		switch ((primDescriptor->opcode)) {
		case AddRR:
			result = rcvrInt + argInt;
			break;
		case SubRR:
			result = rcvrInt - argInt;
			break;
		case AndRR:
			result = rcvrInt & argInt;
			break;
		case OrRR:
			result = rcvrInt | argInt;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
		if (isIntegerValue(result)) {
			/* Must annotate the bytecode for correct pc mapping. */
			ssPop(2);
			return ssPushAnnotatedConstant((((usqInt)result << 3) | 1));
		}
		return genSpecialSelectorSend();
	}
	if ((rcvrIsConst
	 && (!rcvrIsInt))
	 || (argIsConst
	 && (!argIsInt))) {
		return genSpecialSelectorSend();
	}
	if (!(argIsInt
		 || (rcvrIsInt))) {
		return genSpecialSelectorSend();
	}
	if (argIsInt) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= (simStackPtr - 2)) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = (simStackPtr - 2) + 1;
		}
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsInt))
		? (argIsInt
				? genJumpNotSmallInteger(ReceiverResultReg)
				: (rcvrIsInt
						? genJumpNotSmallInteger(Arg0Reg)
						: genJumpNotSmallIntegersInandscratch(ReceiverResultReg, Arg0Reg, TempReg)))
		: 0);
	switch ((primDescriptor->opcode)) {
	case AddRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);
			/* begin AddR:R: */
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			if (rcvrIsInt
			 && (rcvrIsConst)) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(MoveCqR, rcvrInt, ReceiverResultReg);
			}
			else {
				/* begin SubR:R: */
				genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);
				genSetSmallIntegerTagsIn(ReceiverResultReg);
			}
		}
		break;
	case SubRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);
			/* begin SubR:R: */
			genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);
			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
			genSetSmallIntegerTagsIn(Arg0Reg);
		}
		break;
	case AndRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(AndCqR, argInt, ReceiverResultReg);
		}
		else {
			/* begin AndR:R: */
			genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (!(jumpNotSmallInts == null)
			? genoperand(Jump, ((sqInt)0))
			: 0);
		break;
	case OrRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(OrCqR, argInt, ReceiverResultReg);
		}
		else {
			/* begin OrR:R: */
			genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (!(jumpNotSmallInts == null)
			? genoperand(Jump, ((sqInt)0))
			: 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	if (jumpNotSmallInts == null) {
		if (!jumpContinue) {
			/* overflow cannot happen */
			/* begin annotateInstructionForBytecode */
			abstractInstruction = (prevInstIsPCAnnotated()
				? gen(Nop)
				: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
			/* begin annotateBytecode: */
			(abstractInstruction->annotation = HasBytecodePC);
			ssPushRegister(ReceiverResultReg);
			return 0;
		}
	}
	else {
		jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	if (argIsInt) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, argInt, Arg0Reg);
	}
	index = byte0 - (/* begin firstSpecialSelectorBytecodeOffset */
	(bytecodeSetOffset == 0x100
	? AltFirstSpecialSelector + 0x100
	: FirstSpecialSelector));
	genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
	jmpTarget(jumpContinue, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorClass */
static sqInt
genSpecialSelectorClass(void)
{
    sqInt requiredReg1;
    sqInt topReg;

	topReg = registerOrNone(ssTop());
	ssPop(1);
	if ((topReg == NoReg)
	 || (topReg == ClassReg)) {
		requiredReg1 = (topReg = SendNumArgsReg);
		/* begin ssAllocateRequiredReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((((requiredReg1 < 0) ? (((usqInt)(1)) >> (-requiredReg1)) : (1ULL << requiredReg1))) | ((1U << ClassReg)), simStackPtr, simNativeStackPtr);
	}
	else {
		/* begin ssAllocateRequiredReg: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr, simNativeStackPtr);
	}
	ssPush(1);
	popToReg(ssTop(), topReg);
	genGetClassObjectOfintoscratchRegmayBeAForwarder(topReg, ClassReg, TempReg, mayBeAForwarder(ssTop()));
	ssPop(1);
	return ssPushRegister(ClassReg);
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorComparison */
static sqInt
genSpecialSelectorComparison(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt argInt;
    sqInt argIsIntConst;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt descr;
    sqInt i;
    sqInt index;
    sqInt inlineCAB;
    AbstractInstruction *jumpNotSmallInts;
    void *jumpTarget;
    sqInt nExts;
    sqInt next;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranch;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    BytecodeDescriptor *primDescriptor1;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt target;
    sqInt targetBytecodePC;
    sqInt targetPC;

	descr = 0;
	next = 0;
	postBranch = 0;
	target = 0;
	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= (simStackPtr - 2)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 2) + 1;
	}
	primDescriptor = generatorAt(byte0);
	argIsIntConst = ((((ssTop())->type)) == SSConstant)
	 && ((((((argInt = ((ssTop())->constant)))) & 7) == 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
	 && (((((((ssValue(1))->constant))) & 7) == 1)))
	 || ((mclassIsSmallInteger())
	 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsIntConst
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		return genStaticallyResolvedSpecialSelectorComparison();
	}
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor1 = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		if (!(		/* begin isUnconditionalBranch */
			(isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetPC = targetBytecodePC;
	/* Further, only interested in inlining = and ~= if there's a SmallInteger constant involved.
	   The relational operators successfully statically predict SmallIntegers; the equality operators do not. */
	inlineCAB = ((branchDescriptor->isBranchTrue))
	 || ((branchDescriptor->isBranchFalse));
	if (inlineCAB
	 && ((((primDescriptor->opcode)) == JumpZero)
	 || (((primDescriptor->opcode)) == JumpNonZero))) {
		inlineCAB = argIsIntConst
		 || (rcvrIsInt);
	}
	if (!inlineCAB) {
		return genSpecialSelectorSend();
	}
	if (argIsIntConst) {
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsIntConst))
		? (argIsIntConst
				? genJumpNotSmallInteger(ReceiverResultReg)
				: (rcvrIsInt
						? genJumpNotSmallInteger(Arg0Reg)
						: genJumpNotSmallIntegersInandscratch(ReceiverResultReg, Arg0Reg, TempReg)))
		: 0);
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(CmpCqR, argInt, ReceiverResultReg);
	}
	else {
		/* begin CmpR:R: */
		assert(!((Arg0Reg == SPReg)));
		genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	}
	genConditionalBranchoperand(((branchDescriptor->isBranchTrue)
		? (primDescriptor->opcode)
		: inverseBranchFor((primDescriptor->opcode))), ((usqInt)(ensureNonMergeFixupAt(targetPC))));
	jumpTarget = ensureNonMergeFixupAt(postBranchPC);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)jumpTarget));
	if (!jumpNotSmallInts) {
		/* begin annotateInstructionForBytecode */
		abstractInstruction = (prevInstIsPCAnnotated()
			? gen(Nop)
			: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		ensureFixupAt(postBranchPC);
		ensureFixupAt(targetPC);
		deadCode = 1;
		return 0;
	}
	jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, argInt, Arg0Reg);
	}
	index = byte0 - (/* begin firstSpecialSelectorBytecodeOffset */
	(bytecodeSetOffset == 0x100
	? AltFirstSpecialSelector + 0x100
	: FirstSpecialSelector));
	return genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
}


/*	Assumes both operands are ints */

	/* StackToRegisterMappingCogit>>#genStaticallyResolvedSpecialSelectorComparison */
static sqInt
genStaticallyResolvedSpecialSelectorComparison(void)
{
    sqInt argInt;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int result;

	primDescriptor = generatorAt(byte0);
	argInt = ((ssTop())->constant);
	rcvrInt = ((ssValue(1))->constant);
	switch ((primDescriptor->opcode)) {
	case JumpLess:
		result = rcvrInt < argInt;
		break;
	case JumpLessOrEqual:
		result = rcvrInt <= argInt;
		break;
	case JumpGreater:
		result = rcvrInt > argInt;
		break;
	case JumpGreaterOrEqual:
		result = rcvrInt >= argInt;
		break;
	case JumpZero:
		result = rcvrInt == argInt;
		break;
	case JumpNonZero:
		result = rcvrInt != argInt;
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	ssPop(2);
	return ssPushAnnotatedConstant((result
		? trueObject()
		: falseObject()));
}


/*	We need a frame because the association has to be in ReceiverResultReg for
	the various trampolines
	and ReceiverResultReg holds only the receiver in frameless methods.
 */

	/* StackToRegisterMappingCogit>>#genStorePop:LiteralVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    sqInt association;
    sqInt i;
    sqInt topReg;

	assert(needsFrame);
	/* begin genLoadLiteralVariable:in: */
	association = getLiteral(litVarIndex);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	if (	/* begin shouldAnnotateObjectReference: */
		(isNonImmediate(association))
	 && ((oopisGreaterThan(association, classTableRootObj()))
	 || (oopisLessThan(association, nilObject())))) {
		annotateobjRef(genoperandoperand(MoveCwR, association, ReceiverResultReg), association);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		genoperandoperand(MoveCqR, association, ReceiverResultReg);
	}
#  if IMMUTABILITY
	if (needsImmCheck) {
		/* begin ssAllocateRequiredReg:upThrough: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr - 1, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, ValueIndex, ReceiverResultReg, TempReg, needsStoreCheck, 0);
	}
#  endif // IMMUTABILITY

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, ValueIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}


/*	The reason we need a frame here is that assigning to an inst var of a
	context may
	involve wholesale reorganization of stack pages, and the only way to
	preserve the
	execution state of an activation in that case is if it has a frame. */

	/* StackToRegisterMappingCogit>>#genStorePop:MaybeContextReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    sqInt i;
    AbstractInstruction *immutabilityFailure;
    AbstractInstruction *mutableJump;

	assert(needsFrame);
	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();
	/* begin genGenericStorePop:MaybeContextSlotIndex:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
	immutabilityFailure = ((AbstractInstruction *) 0);
	assert(needsFrame);
#  if IMMUTABILITY
	if (needsImmCheck) {
		mutableJump = genJumpMutablescratchReg(ReceiverResultReg, TempReg);
		/* begin genStoreTrampolineCall: */
		assert(IMMUTABILITY);
		if (slotIndex >= (NumStoreTrampolines - 1)) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(MoveCqR, slotIndex, TempReg);
			abstractInstruction = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
			(abstractInstruction->annotation = IsRelativeCall);
		}
		else {
			/* begin CallRT: */
			abstractInstruction1 = genoperand(Call, ceStoreTrampolines[slotIndex]);
			(abstractInstruction1->annotation = IsRelativeCall);
		}
		abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction2->annotation = HasBytecodePC);
		storeToReg(simSelf(), ReceiverResultReg);
		immutabilityFailure = genoperand(Jump, ((sqInt)0));
		jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif // IMMUTABILITY

	ssPop(1);
	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ClassReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ssPush(1);
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= simStackPtr) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = simStackPtr + 1;
	}
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	abstractInstruction3 = genoperand(Call, ceStoreContextInstVarTrampoline);
	(abstractInstruction3->annotation = IsRelativeCall);
#  if IMMUTABILITY
	if (needsImmCheck) {
		jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif

	return 0;
}

	/* StackToRegisterMappingCogit>>#genStorePop:ReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    sqInt i;
    sqInt needsImmCheck1;
    sqInt needsStoreCheck1;
    sqInt topReg;

	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();
	needsStoreCheck1 = (!useTwoPaths)
	 && (needsStoreCheck);
	needsImmCheck1 = needsImmCheck
	 && (!useTwoPaths);
	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#  if IMMUTABILITY
	if (needsImmCheck1) {
		/* begin ssAllocateRequiredReg:upThrough: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr - 1, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, slotIndex, ReceiverResultReg, TempReg, needsStoreCheck1, 1);
	}
#  endif // IMMUTABILITY

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck1);
}


/*	The only reason we assert needsFrame here is that in a frameless method
	ReceiverResultReg must and does contain only self, but the ceStoreCheck
	trampoline expects the target of the store to be in ReceiverResultReg. So
	in a frameless method we would have a conflict between the receiver and
	the temote temp store, unless we we smart enough to realise that
	ReceiverResultReg was unused after the literal variable store, unlikely
	given that methods return self by default. */

	/* StackToRegisterMappingCogit>>#genStorePop:RemoteTemp:At:needsStoreCheck: */
static NoDbgRegParms sqInt
genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck)
{
    sqInt offset;
    sqInt topReg;

	assert(needsFrame);
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	voidReceiverResultRegContainsSelf();
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfTemporary(remoteTempIndex);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, ReceiverResultReg);
	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}

	/* StackToRegisterMappingCogit>>#genStorePop:TemporaryVariable: */
static NoDbgRegParms sqInt
genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex)
{
    sqInt offset;
    sqInt reg;

	ssFlushUpThroughTemporaryVariable(tempIndex);
	reg = ssStorePoptoPreferredReg(popBoolean, TempReg);
	/* begin MoveR:Mw:r: */
	offset = frameOffsetOfTemporary(tempIndex);
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveRMwr, reg, offset, FPReg);
	((simStackAt(tempIndex + 1))->bcptr = bytecodePC);
	return 0;
}


/*	Generate a method return from within a method or a block.
	Frameless method activation looks like
	CISCs (x86):
	receiver
	args
	sp->	ret pc.
	RISCs (ARM):
	receiver
	args
	ret pc in LR.
	A fully framed activation is described in CoInterpreter
	class>initializeFrameIndices. Return pops receiver and arguments off the
	stack. Callee pushes the result. */

	/* StackToRegisterMappingCogit>>#genUpArrowReturn */
static sqInt
genUpArrowReturn(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt i;
    sqInt offset;


	/* can't fall through */
	deadCode = 1;
	if (inBlock > 0) {
		assert(needsFrame);
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceNonLocalReturnTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin annotateBytecode: */
		(abstractInstruction1->annotation = HasBytecodePC);
		return 0;
	}
	if (
#  if IMMUTABILITY
		needsFrame
			 && (!useTwoPaths)
#  else
		needsFrame
#  endif
		) {
		if (hasNativeFrame) {
			leaveNativeFrame();
		}
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);
		genoperand(PopR, FPReg);
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	else {
		offset = ((methodOrBlockNumArgs > (numRegArgs()))
		 || (regArgsHaveBeenPushed)
			? (methodOrBlockNumArgs + 1) * BytesPerWord
			: 0);
		/* begin RetN: */
		genoperand(RetN, offset);
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#genVanillaInlinedIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genVanillaInlinedIdenticalOrNotIf(sqInt orNot)
{
    sqInt arg;
    int argIsConstant;
    sqInt argNeedsReg;
    sqInt argReg;
    sqInt argReg1;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt descr;
    sqInt i;
    void *jumpTarget;
    void *jumpTarget1;
    void *jumpTarget2;
    void *jumpTarget3;
    sqInt nExts;
    sqInt next;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranch;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvr;
    sqInt rcvrIsConstant;
    sqInt rcvrNeedsReg;
    sqInt rcvrReg;
    sqInt rcvrReg1;
    sqInt reg;
    sqInt rNext;
    sqInt rNext1;
    sqInt rTop;
    sqInt rTop1;
    sqInt target;
    sqInt targetBytecodePC;
    sqInt targetPC;
    sqInt topRegistersMask;

	arg = 0;
	descr = 0;
	next = 0;
	postBranch = 0;
	rcvr = 0;
	target = 0;
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		if (!(		/* begin isUnconditionalBranch */
			(isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetPC = targetBytecodePC;
	/* They can't be both constants to use correct machine opcodes.
	   However annotable constants can't be resolved statically, hence we need to careful. */
	argIsConstant = (((ssTop())->type)) == SSConstant;
	rcvrIsConstant = (!argIsConstant)
	 && ((((ssValue(1))->type)) == SSConstant);
	argNeedsReg = !argIsConstant;
	rcvrNeedsReg = !rcvrIsConstant;
	/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
	rNext = 0;
	rTop = 0;
	assert(argNeedsReg
	 || (rcvrNeedsReg));
	argReg1 = (rcvrReg1 = NoReg);
	if (argNeedsReg) {
		if (rcvrNeedsReg) {
			/* begin allocateRegForStackTopTwoEntriesInto: */
			topRegistersMask = 0;
			rTop1 = (rNext1 = NoReg);
			if ((registerOrNone(ssTop())) != NoReg) {
				rTop1 = registerOrNone(ssTop());
			}
			if ((registerOrNone(ssValue(1))) != NoReg) {
				reg = (rNext1 = registerOrNone(ssValue(1)));
				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
			if (rTop1 == NoReg) {
				rTop1 = allocateRegNotConflictingWith(topRegistersMask);
			}
			if (rNext1 == NoReg) {
				rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
			}
			assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
			argReg1 = rTop1;
			rcvrReg1 = rNext1;
			popToReg(ssTop(), argReg1);
			popToReg(ssValue(1), rcvrReg1);
		}
		else {
			argReg1 = allocateRegForStackEntryAtnotConflictingWith(0, 0);
			popToReg(ssTop(), argReg1);
			if (((ssValue(1))->spilled)) {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(AddCqR, BytesPerWord, SPReg);
			}
		}
	}
	else {
		assert(rcvrNeedsReg);
		assert(!((((ssTop())->spilled))));
		rcvrReg1 = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), rcvrReg1);
	}
	assert(!((argNeedsReg
 && (argReg1 == NoReg))));
	assert(!((rcvrNeedsReg
 && (rcvrReg1 == NoReg))));
	rcvrReg = rcvrReg1;
	argReg = argReg1;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argIsConstant, rcvrIsConstant, argReg, rcvrReg, orNot);
	}
	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= (simStackPtr - 2)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 2) + 1;
	}
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (argIsConstant) {
		/* begin genCmpConstant:R: */
		if (		/* begin shouldAnnotateObjectReference: */
			(isNonImmediate(((ssTop())->constant)))
		 && ((oopisGreaterThan(((ssTop())->constant), classTableRootObj()))
		 || (oopisLessThan(((ssTop())->constant), nilObject())))) {
			annotateobjRef(genoperandoperand(CmpCwR, ((ssTop())->constant), rcvrReg), ((ssTop())->constant));
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(CmpCqR, ((ssTop())->constant), rcvrReg);
		}
	}
	else {
		if (rcvrIsConstant) {
			/* begin genCmpConstant:R: */
			if (			/* begin shouldAnnotateObjectReference: */
				(isNonImmediate(((ssValue(1))->constant)))
			 && ((oopisGreaterThan(((ssValue(1))->constant), classTableRootObj()))
			 || (oopisLessThan(((ssValue(1))->constant), nilObject())))) {
				annotateobjRef(genoperandoperand(CmpCwR, ((ssValue(1))->constant), argReg), ((ssValue(1))->constant));
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperand(CmpCqR, ((ssValue(1))->constant), argReg);
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);
	if (notAFixup(fixupAt(nextPC))) {
		/* The next instruction is dead.  we can skip it. */
		deadCode = 1;
		ensureFixupAt(targetPC);
		ensureFixupAt(postBranchPC);
	}
	else {
		assert(!(deadCode));
	}
	if (orNot == ((branchDescriptor->isBranchTrue))) {
		/* a == b ifFalse: ... or a ~~ b ifTrue: ... jump on equal to post-branch pc */
		ensureNonMergeFixupAt(targetPC);
		jumpTarget = ensureNonMergeFixupAt(postBranchPC);
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		jumpTarget1 = ensureNonMergeFixupAt(targetPC);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)jumpTarget1));
	}
	else {
		/* orNot is true for ~~ */
		/* a == b ifTrue: ... or a ~~ b ifFalse: ... jump on equal to target pc */
		jumpTarget2 = ensureNonMergeFixupAt(targetPC);
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget2));
		jumpTarget3 = ensureNonMergeFixupAt(postBranchPC);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)jumpTarget3));
	}
	if (!deadCode) {
		ssPushConstant(trueObject());
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#initSimStackForFramefulMethod: */
static NoDbgRegParms void
initSimStackForFramefulMethod(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;


	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
	simNativeSpillBase = (simNativeStackPtr = -1);
	simNativeStackSize = 0;
	cascade0 = simSelf();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 1);
	(cascade0->registerr = FPReg);
	(cascade0->offset = FoxMFReceiver);
	(cascade0->liveRegister = NoReg);
	for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxCallerSavedIP + (((methodOrBlockNumArgs - i) + 1) * BytesPerWord));
		(desc->bcptr = startpc);
	}
	for (i = (methodOrBlockNumArgs + 1); i <= simStackPtr; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxMFReceiver - ((i - methodOrBlockNumArgs) * BytesPerWord));
		(desc->bcptr = startpc);
	}
}


/*	The register receiver (the closure itself) and args are pushed by the
	closure value primitive(s)
	and hence a frameless block has all arguments and copied values pushed to
	the stack. However,
	the method receiver (self) is put in the ReceiverResultReg by the block
	entry. 
 */

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessBlock: */
static NoDbgRegParms void
initSimStackForFramelessBlock(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps >= methodOrBlockNumArgs);
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = SPReg);
		(desc->offset = ((methodOrBlockNumArgs + 1) - i) * BytesPerWord);
		(desc->bcptr = startpc);
	}
	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
	simNativeSpillBase = (simNativeStackPtr = -1);
	simNativeStackSize = 0;
}

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessMethod: */
static NoDbgRegParms void
initSimStackForFramelessMethod(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps == methodOrBlockNumArgs);
	assert((numRegArgs()) <= 2);
	if (((methodOrBlockNumArgs >= 1) && (methodOrBlockNumArgs <= (numRegArgs())))) {
		desc = simStackAt(1);
		(desc->type = SSRegister);
		(desc->spilled = 0);
		(desc->registerr = Arg0Reg);
		(desc->bcptr = startpc);
		if (methodOrBlockNumArgs > 1) {
			desc = simStackAt(2);
			(desc->type = SSRegister);
			(desc->spilled = 0);
			(desc->registerr = Arg1Reg);
			(desc->bcptr = startpc);
		}
	}
	else {
		for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
			desc = simStackAt(i);
			(desc->type = SSBaseOffset);
			(desc->registerr = SPReg);
			(desc->spilled = 1);
			(desc->offset = ((methodOrBlockNumArgs + 1) - i) * BytesPerWord);
			(desc->bcptr = startpc);
		}
	}
	simStackPtr = methodOrBlockNumArgs;
	simSpillBase = methodOrBlockNumArgs + 1;
	simNativeSpillBase = (simNativeStackPtr = -1);
	simNativeStackSize = 0;
}


/*	Do not inline (inBlock access) */

	/* StackToRegisterMappingCogit>>#isNonForwarderReceiver: */
static NoDbgRegParms sqInt
isNonForwarderReceiver(sqInt reg)
{
	return ((((simSelf())->liveRegister)) == ReceiverResultReg)
	 && ((inBlock == 0)
	 && (reg == ReceiverResultReg));
}

	/* StackToRegisterMappingCogit>>#leaveNativeFrame */
static void
leaveNativeFrame(void)
{
    sqInt offset;

	assert(needsFrame);
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfPreviousNativeStackPointer();
	/* begin checkQuickConstant:forInstruction: */
	genoperandoperandoperand(MoveMwrR, offset, FPReg, TempReg);
	genoperandoperand(SubCqR, 1, TempReg);
	genoperandoperand(MoveRAw, TempReg, nativeStackPointerAddress());
}

	/* StackToRegisterMappingCogit>>#liveFloatRegisters */
static sqInt
liveFloatRegisters(void)
{
    sqInt i;
    sqInt regsSet;

	regsSet = 0;
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= simStackPtr; i += 1) {
		regsSet = regsSet | (floatRegisterMask(simStackAt(i)));
	}
	for (i = (((simNativeSpillBase < 0) ? 0 : simNativeSpillBase)); i <= simNativeStackPtr; i += 1) {
		regsSet = regsSet | (nativeFloatRegisterMask(simNativeStackAt(i)));
	}
	return regsSet;
}

	/* StackToRegisterMappingCogit>>#liveRegisters */
static sqInt
liveRegisters(void)
{
    sqInt i;
    sqInt regsSet;

	if (needsFrame) {
		regsSet = 0;
	}
	else {
		regsSet = (1U << ReceiverResultReg);
		if ((methodOrBlockNumArgs <= (numRegArgs()))
		 && (methodOrBlockNumArgs > 0)) {
			regsSet = regsSet | ((1U << Arg0Reg));
			if (methodOrBlockNumArgs > 1) {
				regsSet = regsSet | ((1U << Arg1Reg));
			}
		}
	}
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= simStackPtr; i += 1) {
		regsSet = regsSet | (registerMask(simStackAt(i)));
	}
	for (i = (((simNativeSpillBase < 0) ? 0 : simNativeSpillBase)); i <= simNativeStackPtr; i += 1) {
		regsSet = regsSet | (nativeRegisterMask(simNativeStackAt(i)));
	}
	return regsSet;
}


/*	insert nops for dead code that is mapped so that bc 
	to mc mapping is not many to one */

	/* StackToRegisterMappingCogit>>#mapDeadDescriptorIfNeeded: */
static NoDbgRegParms sqInt
mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor)
{
    AbstractInstruction *abstractInstruction;

	flag("annotateInstruction");
	if (((descriptor->isMapped))
	 || ((inBlock > 0)
	 && ((descriptor->isMappedInBlock)))) {
		abstractInstruction = gen(Nop);
		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
	}
	return 0;
}


/*	Spill everything on the simulated stack that needs spilling (that below
	receiver and arguments).
	Marshall receiver and arguments to stack and/or registers depending on arg
	count. If the args don't fit in registers push receiver and args (spill
	everything), but still assign
	the receiver to ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#marshallSendArguments: */
static NoDbgRegParms void
marshallSendArguments(sqInt numArgs)
{
    sqInt anyRefs;
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt i1;
    sqInt i2;
    sqInt numSpilled;


	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	ssNativeFlushTo(simNativeStackPtr);
	if (simSpillBase <= ((simStackPtr - numArgs) - 1)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < ((simStackPtr - numArgs) - 1)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : ((simStackPtr - numArgs) - 1))); i < (simStackPtr - numArgs); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = ((simStackPtr - numArgs) - 1) + 1;
	}
	if (numArgs > (numRegArgs())) {
		/* If there are no spills and no references to ReceiverResultReg
		   the fetch of ReceiverResultReg from the stack can be avoided
		   by assigning directly to ReceiverResultReg and pushing it. */
		numSpilled = numberOfSpillsInTopNItems(numArgs + 1);
		anyRefs = anyReferencesToRegisterinTopNItems(ReceiverResultReg, numArgs + 1);
		if ((numSpilled > 0)
		 || (anyRefs)) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			ssNativeFlushTo(simNativeStackPtr);
			if (simSpillBase <= simStackPtr) {
				for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i1 <= simStackPtr; i1 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
				}
				simSpillBase = simStackPtr + 1;
			}
			storeToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
		}
		else {
			cascade0 = simStackAt(simStackPtr - numArgs);
			storeToReg(cascade0, ReceiverResultReg);
			(cascade0->type = SSRegister);
			(cascade0->registerr = ReceiverResultReg);
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			ssNativeFlushTo(simNativeStackPtr);
			if (simSpillBase <= simStackPtr) {
				for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i2 <= simStackPtr; i2 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
				}
				simSpillBase = simStackPtr + 1;
			}
		}
	}
	else {
		/* Move the args to the register arguments, being careful to do
		   so last to first so e.g. previous contents don't get overwritten.
		   Also check for any arg registers in use by other args. */
		if (numArgs > 0) {
			if (numArgs > 1) {
				/* begin ssAllocateRequiredReg:upThrough: */
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg0Reg), simStackPtr - 2, simNativeStackPtr);
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg1Reg), simStackPtr - 1, simNativeStackPtr);
			}
			else {
				/* begin ssAllocateRequiredReg:upThrough: */
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg0Reg), simStackPtr - 1, simNativeStackPtr);
			}
		}
		if (numArgs > 1) {
			popToReg(simStackAt(simStackPtr), Arg1Reg);
		}
		if (numArgs > 0) {
			popToReg(simStackAt((simStackPtr - numArgs) + 1), Arg0Reg);
		}
		popToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
	}
	ssPop(numArgs + 1);
}


/*	For assert checking; or rather for avoiding assert fails when dealing with
	the hack for block temps in the SqueakV3PlusClosures bytecode set.
 */

	/* StackToRegisterMappingCogit>>#maybeCompilingFirstPassOfBlockWithInitialPushNil */
static sqInt
maybeCompilingFirstPassOfBlockWithInitialPushNil(void)
{
	return (inBlock == InVanillaBlock)
	 && ((methodOrBlockNumTemps > methodOrBlockNumArgs)
	 && (compilationPass == 1));
}


/*	If this bytecode has a fixup, some kind of merge needs to be done. There
	are 4 cases:
	1) the bytecode has no fixup (fixup isNotAFixup)
	do nothing
	2) the bytecode has a non merge fixup
	the fixup has needsNonMergeFixup.
	The code generating non merge fixup (currently only special selector code)
	is responsible
	for the merge so no need to do it.
	We set deadCode to false as the instruction can be reached from jumps.
	3) the bytecode has a merge fixup, but execution flow *cannot* fall
	through to the merge point.
	the fixup has needsMergeFixup and deadCode = true.
	ignores the current simStack as it does not mean anything 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	4) the bytecode has a merge fixup and execution flow *can* fall through to
	the merge point.
	the fixup has needsMergeFixup and deadCode = false.
	flushes the stack to the stack pointer so the fall through execution path
	simStack is 
	in the state the merge point expects it to be. 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	
	In addition, if this is a backjump merge point, we patch the fixup to hold
	the current simStackPtr 
	for later assertions. */

	/* StackToRegisterMappingCogit>>#mergeWithFixupIfRequired: */
static NoDbgRegParms sqInt
mergeWithFixupIfRequired(BytecodeFixup *fixup)
{
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt i1;


	/* begin assertCorrectSimStackPtr */
	assert((simSpillBase >= methodOrBlockNumTemps)
	 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
	 && (simSpillBase > methodOrBlockNumArgs)));
	if (needsFrame
	 && (simSpillBase > 0)) {
		assert(((((simStackAt(simSpillBase - 1))->spilled)) == 1)
		 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
		 && (simSpillBase > methodOrBlockNumArgs)));
		assert((simSpillBase > simStackPtr)
		 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	}
	if (((fixup->targetInstruction)) == 0) {
		return 0;
	}
	if ((((usqInt)((fixup->targetInstruction)))) == NeedsNonMergeFixupFlag) {
		deadCode = 0;
		return 0;
	}
	assert(isMergeFixup(fixup));
	traceMerge(fixup);
	if (deadCode) {
		/* case 3 */
		/* Would like to assert fixup simStackPtr >= methodOrBlockNumTemps
		   but can't because of the initialNils hack. */
		assert((((fixup->simStackPtr)) >= methodOrBlockNumTemps)
		 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
		simStackPtr = (fixup->simStackPtr);
		simNativeStackPtr = (fixup->simNativeStackPtr);
		simNativeStackSize = (fixup->simNativeStackSize);
	}
	else {
		/* case 4 */
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
	}
	deadCode = 0;
	if ((fixup->isTargetOfBackwardBranch)) {
		(fixup->simStackPtr = simStackPtr);
		(fixup->simNativeStackPtr = simNativeStackPtr);
		(fixup->simNativeStackSize = simNativeStackSize);
	}
	(fixup->targetInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	assert(simStackPtr == ((fixup->simStackPtr)));
	assert(simNativeStackPtr == ((fixup->simNativeStackPtr)));
	assert(simNativeStackSize == ((fixup->simNativeStackSize)));
	/* begin restoreSimStackAtMergePoint: */
	((simSelf())->liveRegister = NoReg);
	for (i1 = (methodOrBlockNumTemps + 1); i1 <= simStackPtr; i1 += 1) {
		cascade0 = simStackAt(i1);
		(cascade0->type = SSSpill);
		(cascade0->offset = FoxMFReceiver - ((i1 - methodOrBlockNumArgs) * BytesPerOop));
		(cascade0->registerr = FPReg);
		(cascade0->spilled = 1);
	}
	simSpillBase = simStackPtr + 1;
	for (i1 = 0; i1 <= simNativeStackPtr; i1 += 1) {
		ensureIsMarkedAsSpilled(simNativeStackAt(i1));
	}
	simNativeSpillBase = simNativeStackPtr + 1;
	return 0;
}

	/* StackToRegisterMappingCogit>>#methodAbortTrampolineFor: */
static NoDbgRegParms sqInt
methodAbortTrampolineFor(sqInt numArgs)
{
	return methodAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}


/*	This is a hook for subclasses to filter out methods they can't deal with. */
/*	Frameless methods with local temporaries cause problems,
	mostly in asserts, and yet they matter not at all for performance.
	Shun them. */

	/* StackToRegisterMappingCogit>>#methodFoundInvalidPostScan */
static sqInt
methodFoundInvalidPostScan(void)
{
	if (!needsFrame) {
		return methodOrBlockNumTemps > methodOrBlockNumArgs;
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#needsFrameIfMod16GENumArgs: */
static NoDbgRegParms int
needsFrameIfMod16GENumArgs(sqInt stackDelta)
{
	return (byte0 % 16) >= methodOrBlockNumArgs;
}


/*	As of August 2013, the code generator can't deal with spills in frameless
	methods (the
	issue is to do with the stack offset to get at an argument, which is
	changed when there's a spill).
	In e.g. TextColor>>#dominates: other ^other class == self class the second
	send of class
	needs also rto allocate a register that the first one used, but the first
	one's register can't be
	spilled. So avoid this by only allowing class to be sent if the stack
	contains a single element. */

	/* StackToRegisterMappingCogit>>#needsFrameIfStackGreaterThanOne: */
static NoDbgRegParms int
needsFrameIfStackGreaterThanOne(sqInt stackDelta)
{
	return stackDelta > 1;
}

	/* StackToRegisterMappingCogit>>#numberOfSpillsInTopNItems: */
static NoDbgRegParms sqInt
numberOfSpillsInTopNItems(sqInt n)
{
    sqInt i;

	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((simStackAt(i))->type)) == SSSpill) {
			return n - (simStackPtr - i);
		}
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#picAbortTrampolineFor: */
static NoDbgRegParms sqInt
picAbortTrampolineFor(sqInt numArgs)
{
	return picAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}

	/* StackToRegisterMappingCogit>>#prevInstIsPCAnnotated */
static sqInt
prevInstIsPCAnnotated(void)
{
    sqInt prevIndex;
    AbstractInstruction *prevInst;

	if (!(opcodeIndex > 0)) {
		return 0;
	}
	prevIndex = opcodeIndex - 1;
	while (1) {
		if (prevIndex <= 0) {
			return 0;
		}
		prevInst = abstractInstructionAt(prevIndex);
		if (((!((prevInst->annotation))
			? 0
			: (prevInst->annotation))) >= HasBytecodePC) {
			return 1;
		}
		if (!(((prevInst->opcode)) == Label)) break;
		prevIndex -= 1;
	}
	return 0;
}


/*	Used to mark ReceiverResultReg as dead or not containing simSelf.
	Used when the simStack has already been flushed, e.g. for sends. */

	/* StackToRegisterMappingCogit>>#receiverIsInReceiverResultReg */
static int
receiverIsInReceiverResultReg(void)
{
	return (((simSelf())->liveRegister)) == ReceiverResultReg;
}


/*	When a block must be recompiled due to overestimating the
	numInitialNils fixups must be restored, which means rescannning
	since backward branches need their targets initialized. */

	/* StackToRegisterMappingCogit>>#reinitializeFixupsFrom:through: */
static NoDbgRegParms void
reinitializeFixupsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt pc;
    BytecodeFixup *self_in_CogSSBytecodeFixup;
    sqInt targetPC;

	pc = start;
	nExts = 0;
	while (pc <= end) {
		/* begin fixupAt: */
		self_in_CogSSBytecodeFixup = fixupAtIndex(pc - initialPC);
		(self_in_CogSSBytecodeFixup->targetInstruction) = 0;
		(self_in_CogSSBytecodeFixup->simStackPtr) = 0;
		(self_in_CogSSBytecodeFixup->simNativeStackPtr) = ((self_in_CogSSBytecodeFixup->simNativeStackSize) = 0);
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((isBranch(descriptor))
		 && ((		/* begin isBackwardBranch:at:exts:in: */
			assert(((descriptor->spanFunction))),
		(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0))) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			/* begin initializeFixupAt: */
			fixup = fixupAtIndex(targetPC - initialPC);
			(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
			/* begin setIsBackwardBranchFixup */
			(fixup->isTargetOfBackwardBranch) = 1;
		}
		if ((descriptor->isBlockCreation)) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			pc = (pc + ((descriptor->numBytes))) + distance;
		}
		else {
			pc += (descriptor->numBytes);
		}
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
	}
}


/*	Scan the block to determine if the block needs a frame or not */

	/* StackToRegisterMappingCogit>>#scanBlock: */
static NoDbgRegParms sqInt
scanBlock(BlockStart *blockStart)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt framelessStackDelta;
    sqInt nExts;
    sqInt numPushNils;
    sqInt (* const numPushNilsFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt) = squeakV3orSistaV1NumPushNils;
    sqInt pc;
    sqInt pushingNils;

	needsFrame = 0;
	hasNativeFrame = 0;
	prevBCDescriptor = null;
	methodOrBlockNumArgs = (blockStart->numArgs);
	inBlock = InVanillaBlock;
	pc = (blockStart->startpc);
	end = ((blockStart->startpc)) + ((blockStart->span));
	framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0))));
	pushingNils = 1;
	while (pc < end) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (!needsFrame) {
			if ((!((descriptor->needsFrameFunction)))
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {
				needsFrame = 1;
			}
			else {
				framelessStackDelta += (descriptor->stackDelta);
			}
		}
		/* begin maybeNoteDescriptor:blockStart: */
		if ((descriptor->isInstVarRef)) {
			(blockStart->hasInstVarRef = 1);
		}
		if (pushingNils
		 && (!((descriptor->isExtension)))) {
			/* Count the initial number of pushed nils acting as temp initializers.  We can't tell
			   whether an initial pushNil is an operand reference or a temp initializer, except
			   when the pushNil is a jump target (has a fixup), which never happens:
			   self systemNavigation browseAllSelect:
			   [:m| | ebc |
			   (ebc := m embeddedBlockClosures
			   select: [:ea| ea decompile statements first isMessage]
			   thenCollect: [:ea| ea decompile statements first selector]) notEmpty
			   and: [(#(whileTrue whileFalse whileTrue: whileFalse:) intersection: ebc) notEmpty]]
			   or if the bytecode set has a push multiple nils bytecode.  We simply count initial nils.
			   Rarely we may end up over-estimating.  We will correct by checking the stack depth
			   at the end of the block in compileBlockBodies. */
			if (((numPushNils = numPushNilsFunction(descriptor, pc, nExts, methodObj))) > 0) {
				assert(((descriptor->numBytes)) == 1);
				(blockStart->numInitialNils = ((blockStart->numInitialNils)) + numPushNils);
			}
			else {
				pushingNils = 0;
			}
		}
		pc = (pc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)
	: 0));
		if ((descriptor->isExtension)) {
			nExts += 1;
		}
		else {
			nExts = (extA = (numExtB = (extB = 0)));
		}
		prevBCDescriptor = descriptor;
	}
	if (!needsFrame) {
		assert((framelessStackDelta >= 0)
		 && (((blockStart->numInitialNils)) >= framelessStackDelta));
		(blockStart->numInitialNils = ((blockStart->numInitialNils)) - framelessStackDelta);
	}
	return 0;
}


/*	Scan the method (and all embedded blocks) to determine
	- what the last bytecode is; extra bytes at the end of a method are used
	to encode things like source pointers or temp names
	- if the method needs a frame or not
	- what are the targets of any backward branches.
	- how many blocks it creates
	Answer the block count or on error a negative error code */

	/* StackToRegisterMappingCogit>>#scanMethod */
static sqInt
scanMethod(void)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt framelessStackDelta;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt numBlocks;
    sqInt pc;
    sqInt seenInstVarStore;
    sqInt targetPC;

	needsFrame = (useTwoPaths = (seenInstVarStore = 0));
	hasNativeFrame = 0;
	prevBCDescriptor = null;
	if ((primitiveIndex > 0)
	 && (isQuickPrimitiveIndex(primitiveIndex))) {
		return 0;
	}
	pc = (latestContinuation = initialPC);
	numBlocks = (framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0)))));
	while (pc <= endPC) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			if (((descriptor->opcode)) == Nop) {
				/* unknown bytecode tag; see Cogit class>>#generatorTableFrom: */
				return EncounteredUnknownBytecode;
			}
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			endPC = pc;
		}
		if (!needsFrame) {
			if ((!((descriptor->needsFrameFunction)))
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {
				/* With immutability we win simply by avoiding a frame build if the receiver is young and not immutable. */
#        if IMMUTABILITY
				if ((descriptor->is1ByteInstVarStore)) {
					useTwoPaths = 1;
				}
				else {
					needsFrame = 1;
					useTwoPaths = 0;
				}
#        else // IMMUTABILITY
				needsFrame = 1;
				useTwoPaths = 0;
#        endif

			}
			else {
				/* Without immutability we win if there are two or more stores and the receiver is new. */
				framelessStackDelta += (descriptor->stackDelta);
#        if IMMUTABILITY
#        else
				if ((descriptor->is1ByteInstVarStore)) {
					if (seenInstVarStore) {
						useTwoPaths = 1;
					}
					else {
						seenInstVarStore = 1;
					}
				}
#        endif // IMMUTABILITY

			}
		}
		if (isBranch(descriptor)) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			if ((			/* begin isBackwardBranch:at:exts:in: */
				assert(((descriptor->spanFunction))),
			(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0)) {
				/* begin initializeFixupAt: */
				fixup = fixupAtIndex(targetPC - initialPC);
				(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
				/* begin setIsBackwardBranchFixup */
				(fixup->isTargetOfBackwardBranch) = 1;
			}
			else {
				latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
			}
		}
		if ((descriptor->isBlockCreation)) {
			numBlocks += 1;
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
		}
		pc += (descriptor->numBytes);
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: (extA = (numExtB = (extB = 0))));
		prevBCDescriptor = descriptor;
	}
	return numBlocks;
}

	/* StackToRegisterMappingCogit>>#squeakV3orSistaV1PushNilSize:numInitialNils: */
static NoDbgRegParms sqInt
squeakV3orSistaV1PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils)
{
	return (methodUsesAlternateBytecodeSet(aMethodObj),
	numInitialNils);
}

	/* StackToRegisterMappingCogit>>#squeakV3orSistaV1:Num:Push:Nils: */
static NoDbgRegParms sqInt
squeakV3orSistaV1NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	return (	/* begin v3:Num:Push:Nils: */
		(((descriptor->generator)) == genPushConstantNilBytecode
		? 1
		: 0));
}

	/* StackToRegisterMappingCogit>>#ssAllocateRequiredFloatRegMask:upThrough:upThroughNative: */
static NoDbgRegParms void
ssAllocateRequiredFloatRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr)
{
    sqInt i;
    sqInt i1;
    sqInt lastRequired;
    sqInt lastRequiredNative;
    sqInt liveRegs;

	lastRequired = -1;
	/* compute live regs while noting the last occurrence of required regs.
	   If these are not free we must spill from simSpillBase to last occurrence.
	   Note we are conservative here; we could allocate FPReg in frameless methods. */
	lastRequiredNative = -1;
	liveRegs = NoReg;
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= stackPtr; i += 1) {
		liveRegs = liveRegs | (registerMask(simStackAt(i)));
		if ((0) != 0) {
			lastRequired = i;
		}
	}
	assert(lastRequiredNative == simNativeStackPtr);
	for (i = (((simNativeSpillBase < 0) ? 0 : simNativeSpillBase)); i <= nativeStackPtr; i += 1) {
		liveRegs = liveRegs | (nativeRegisterMask(simNativeStackAt(i)));
		if ((0) != 0) {
			lastRequiredNative = i;
		}
	}
	if (!((liveRegs & requiredRegsMask) == 0)) {
		/* Some live, must spill */
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= lastRequired) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < lastRequired) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : lastRequired)); i1 <= lastRequired; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = lastRequired + 1;
		}
		assert(((liveFloatRegisters()) & requiredRegsMask) == 0);
	}
}

	/* StackToRegisterMappingCogit>>#ssAllocateRequiredFloatReg: */
static NoDbgRegParms void
ssAllocateRequiredFloatReg(sqInt requiredReg)
{
	ssAllocateRequiredFloatRegMaskupThroughupThroughNative(((requiredReg < 0) ? (((usqInt)(1)) >> (-requiredReg)) : (1ULL << requiredReg)), simStackPtr, simNativeStackPtr);
}

	/* StackToRegisterMappingCogit>>#ssAllocateRequiredRegMask:upThrough:upThroughNative: */
static NoDbgRegParms void
ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr)
{
    sqInt i;
    sqInt i1;
    sqInt lastRequired;
    sqInt lastRequiredNative;
    sqInt liveRegs;

	lastRequired = -1;
	/* compute live regs while noting the last occurrence of required regs.
	   If these are not free we must spill from simSpillBase to last occurrence.
	   Note we are conservative here; we could allocate FPReg in frameless methods. */
	lastRequiredNative = -1;
	liveRegs = (1U << FPReg) | (1U << SPReg);
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= stackPtr; i += 1) {
		liveRegs = liveRegs | (registerMask(simStackAt(i)));
		if ((((registerMask(simStackAt(i))) & requiredRegsMask) != 0)) {
			lastRequired = i;
		}
	}
	assert(nativeStackPtr == simNativeStackPtr);
	for (i = (((simNativeSpillBase < 0) ? 0 : simNativeSpillBase)); i <= nativeStackPtr; i += 1) {
		liveRegs = liveRegs | (nativeRegisterMask(simNativeStackAt(i)));
		if ((((nativeRegisterMask(simNativeStackAt(i))) & requiredRegsMask) != 0)) {
			lastRequiredNative = i;
		}
	}
	if (((liveRegs & requiredRegsMask) != 0)) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= lastRequired) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < lastRequired) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : lastRequired)); i1 <= lastRequired; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = lastRequired + 1;
		}
		assert(!(((((liveRegisters()) & requiredRegsMask) != 0))));
	}
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughReceiverVariable: */
static NoDbgRegParms void
ssFlushUpThroughReceiverVariable(sqInt slotIndex)
{
    CogSimStackEntry *desc;
    CogSimStackEntry *desc1;
    sqInt i;
    sqInt index;

	desc = ((CogSimStackEntry *) 0);
	ssNativeFlushTo(simNativeStackPtr);
	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((desc1 = simStackAt(index)),
		(((desc1->type)) == SSBaseOffset)
			 && ((((desc1->registerr)) == ReceiverResultReg)
			 && (((desc1->offset)) == (slotOffsetOfInstVarIndex(slotIndex)))))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			ssNativeFlushTo(simNativeStackPtr);
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
	l1:	/* end ssFlushUpThrough: */;
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughTemporaryVariable: */
static NoDbgRegParms void
ssFlushUpThroughTemporaryVariable(sqInt tempIndex)
{
    CogSimStackEntry *desc;
    CogSimStackEntry *desc1;
    sqInt i;
    sqInt index;
    sqInt offset;

	desc = ((CogSimStackEntry *) 0);
	ssNativeFlushTo(simNativeStackPtr);
	offset = ((simStackAt(tempIndex + 1))->offset);
	assert(offset == (frameOffsetOfTemporary(tempIndex)));
	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((desc1 = simStackAt(index)),
		(((desc1->type)) == SSBaseOffset)
			 && ((((desc1->registerr)) == FPReg)
			 && (((desc1->offset)) == offset)))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			ssNativeFlushTo(simNativeStackPtr);
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
	l1:	/* end ssFlushUpThrough: */;
}

	/* StackToRegisterMappingCogit>>#ssNativeFlushTo: */
static NoDbgRegParms void
ssNativeFlushTo(sqInt index)
{
    sqInt allocatedScratchRegister;
    sqInt i;
    sqInt loadedPointer;
    sqInt offset;
    sqInt offset1;

	if (simNativeSpillBase <= index) {
		loadedPointer = 0;
		allocatedScratchRegister = 0;
		for (i = (((simNativeSpillBase < 0) ? 0 : simNativeSpillBase)); i <= index; i += 1) {
			if (!loadedPointer) {
				/* begin MoveMw:r:R: */
				offset = frameOffsetOfNativeFramePointer();
				/* begin checkQuickConstant:forInstruction: */
				genoperandoperandoperand(MoveMwrR, offset, FPReg, TempReg);
				loadedPointer = 1;
			}
			if ((spillingNeedsScratchRegister(simNativeStackAt(i)))
			 && (!allocatedScratchRegister)) {
				/* begin PushR: */
				genoperand(PushR, FPReg);
				allocatedScratchRegister = 1;
			}
			ensureSpilledSPscratchRegister(simNativeStackAt(i), TempReg, FPReg);
		}
		simNativeSpillBase = index + 1;
		if (allocatedScratchRegister) {
			/* begin PopR: */
			genoperand(PopR, FPReg);
		}
		if (loadedPointer) {
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperand(SubCqR, simNativeStackSize, TempReg);
			offset1 = frameOffsetOfNativeStackPointer();
			/* begin checkQuickConstant:forInstruction: */
			genoperandoperandoperand(MoveRMwr, TempReg, offset1, FPReg);
		}
	}
}

	/* StackToRegisterMappingCogit>>#ssNativePop: */
static NoDbgRegParms void
ssNativePop(sqInt n)
{
	assert((simNativeStackPtr - n) >= -1);
	simNativeStackPtr -= n;
	if (simNativeStackPtr >= 0) {
		simNativeStackSize = ((ssNativeTop())->offset);
	}
	else {
		simNativeStackSize = 0;
	}
}

	/* StackToRegisterMappingCogit>>#ssNativePush: */
static NoDbgRegParms void
ssNativePush(sqInt n)
{
	simNativeStackPtr += n;
}

	/* StackToRegisterMappingCogit>>#ssNativeTop */
static CogSimStackNativeEntry *
ssNativeTop(void)
{
	assert(simNativeStackPtr >= 0);
	return simNativeStackAt(simNativeStackPtr);
}

	/* StackToRegisterMappingCogit>>#ssNativeValue: */
static NoDbgRegParms CogSimStackNativeEntry *
ssNativeValue(sqInt n)
{
	return simNativeStackAt(simNativeStackPtr - n);
}

	/* StackToRegisterMappingCogit>>#ssPopNativeSize: */
static NoDbgRegParms void
ssPopNativeSize(sqInt popSize)
{
    sqInt popCount;
    sqInt poppingSize;
    sqInt stackPosition;

	poppingSize = 0;
	stackPosition = simNativeStackPtr;
	while ((poppingSize < popSize)
	 && (stackPosition >= 0)) {
		poppingSize += stackSpillSize(simNativeStackAt(stackPosition));
		stackPosition -= 1;
	}
	assert(poppingSize == popSize);
	popCount = simNativeStackPtr - stackPosition;
	ssNativePop(popCount);
}

	/* StackToRegisterMappingCogit>>#ssPop: */
static NoDbgRegParms void
ssPop(sqInt n)
{
    sqInt i;

	assert(((simStackPtr - n) >= methodOrBlockNumTemps)
	 || (((!needsFrame)
	 && ((simStackPtr - n) >= 0))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil())));
	simStackPtr -= n;
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
}

	/* StackToRegisterMappingCogit>>#ssPushAnnotatedConstant: */
static NoDbgRegParms sqInt
ssPushAnnotatedConstant(sqInt literal)
{
    AbstractInstruction *abstractInstruction;

	ssPushConstant(literal);
	/* begin annotateInstructionForBytecode */
	abstractInstruction = (prevInstIsPCAnnotated()
		? gen(Nop)
		: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushBase:offset: */
static NoDbgRegParms sqInt
ssPushBaseoffset(sqInt reg, sqInt offset)
{
    CogSimStackEntry *cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->offset = offset);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushConstant: */
static NoDbgRegParms sqInt
ssPushConstant(sqInt literal)
{
    CogSimStackEntry *cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSConstant);
	(cascade0->spilled = 0);
	(cascade0->constant = literal);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushDesc: */
static NoDbgRegParms sqInt
ssPushDesc(SimStackEntry simStackEntry)
{
    sqInt i;

	if (((simStackEntry.type)) == SSSpill) {
		(simStackEntry.type = SSBaseOffset);
	}
	(simStackEntry.spilled = 0);
	(simStackEntry.bcptr = bytecodePC);
	simStack[(simStackPtr += 1)] = simStackEntry;
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeConstantFloat32: */
static NoDbgRegParms sqInt
ssPushNativeConstantFloat32(float aFloat32)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += BytesPerWord;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSConstantFloat32);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->constantFloat32 = aFloat32);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeConstantFloat64: */
static NoDbgRegParms sqInt
ssPushNativeConstantFloat64(double aFloat64)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += 8;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSConstantFloat64);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->constantFloat64 = aFloat64);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeConstantInt32: */
static NoDbgRegParms sqInt
ssPushNativeConstantInt32(sqInt anInt32)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += BytesPerWord;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSConstantInt32);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->constantInt32 = anInt32);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeConstantInt64: */
static NoDbgRegParms sqInt
ssPushNativeConstantInt64(sqLong anInt64)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += 8;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSConstantInt64);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->constantInt64 = anInt64);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeConstantPointer: */
static NoDbgRegParms sqInt
ssPushNativeConstantPointer(sqInt aNativePointer)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += BytesPerWord;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSConstantNativePointer);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->constantNativePointer = aNativePointer);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeRegisterDoubleFloat: */
static NoDbgRegParms sqInt
ssPushNativeRegisterDoubleFloat(sqInt reg)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += 8;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSRegisterDoubleFloat);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeRegisterSingleFloat: */
static NoDbgRegParms sqInt
ssPushNativeRegisterSingleFloat(sqInt reg)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += BytesPerWord;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSRegisterSingleFloat);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeRegister: */
static NoDbgRegParms sqInt
ssPushNativeRegister(sqInt reg)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += BytesPerWord;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSNativeRegister);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushNativeRegister:secondRegister: */
static NoDbgRegParms sqInt
ssPushNativeRegistersecondRegister(sqInt reg, sqInt secondReg)
{
    CogSimStackNativeEntry *cascade0;

	ssNativePush(1);
	if (simNativeSpillBase > simNativeStackPtr) {
		simNativeSpillBase = ((simNativeStackPtr < 0) ? 0 : simNativeStackPtr);
	}
	simNativeStackSize += 8;
	/* begin ssNativeTop */
	assert(simNativeStackPtr >= 0);
	cascade0 = simNativeStackAt(simNativeStackPtr);
	(cascade0->type = SSRegisterPair);
	(cascade0->spilled = 0);
	(cascade0->offset = simNativeStackSize);
	(cascade0->registerr = reg);
	(cascade0->registerSecond = secondReg);
	(cascade0->bcptr = bytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushRegister: */
static NoDbgRegParms sqInt
ssPushRegister(sqInt reg)
{
    CogSimStackEntry *cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPush: */
static NoDbgRegParms void
ssPush(sqInt n)
{
	simStackPtr += n;
}

	/* StackToRegisterMappingCogit>>#ssSelfDescriptor */
static SimStackEntry
ssSelfDescriptor(void)
{
	return simStack[0];
}


/*	In addition to ssStorePop:toReg:, if this is a store and not
	a popInto I change the simulated stack to use the register 
	for the top value */

	/* StackToRegisterMappingCogit>>#ssStoreAndReplacePop:toReg: */
static NoDbgRegParms void
ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg)
{
    char topSpilled;

	topSpilled = ((ssTop())->spilled);
	ssStorePoptoReg(popBoolean
	 || (topSpilled), reg);
	if (!popBoolean) {
		if (!topSpilled) {
			ssPop(1);
		}
		ssPushRegister(reg);
	}
}


/*	Store or pop the top simulated stack entry to a register.
	Use preferredReg if the entry is not itself a register.
	Answer the actual register the result ends up in. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toPreferredReg: */
static NoDbgRegParms sqInt
ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg)
{
    sqInt actualReg;

	actualReg = preferredReg;
	if ((((ssTop())->type)) == SSRegister) {
		assert(!(((ssTop())->spilled)));
		actualReg = ((ssTop())->registerr);
	}
	ssStorePoptoReg(popBoolean, actualReg);
	return actualReg;
}


/*	Store or pop the top simulated stack entry to a register.
	N.B.: popToReg: and storeToReg: does not generate anything if 
	it moves a register to the same register. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toReg: */
static NoDbgRegParms void
ssStorePoptoReg(sqInt popBoolean, sqInt reg)
{
	if (popBoolean) {
		popToReg(ssTop(), reg);
		ssPop(1);
	}
	else {
		storeToReg(ssTop(), reg);
	}
}

	/* StackToRegisterMappingCogit>>#ssTop */
static CogSimStackEntry *
ssTop(void)
{
	return simStackAt(simStackPtr);
}

	/* StackToRegisterMappingCogit>>#ssValue: */
static NoDbgRegParms CogSimStackEntry *
ssValue(sqInt n)
{
	return simStackAt(simStackPtr - n);
}

	/* StackToRegisterMappingCogit>>#stackEntryIsBoolean: */
static NoDbgRegParms sqInt
stackEntryIsBoolean(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((((simStackEntry->constant)) == (trueObject()))
	 || (((simStackEntry->constant)) == (falseObject())));
}


/*	Answer if the stack is valid up to, but not including, simSpillBase. */

	/* StackToRegisterMappingCogit>>#tempsValidAndVolatileEntriesSpilled */
static sqInt
tempsValidAndVolatileEntriesSpilled(void)
{
    sqInt culprit;
    sqInt i;

	culprit = 0;
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		if (!(((((simStackAt(i))->type)) == SSBaseOffset)
			 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i < simSpillBase; i += 1) {
		if (!(((simStackAt(i))->spilled))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	return 1;
}


/*	If the sequence of bytecodes is
	push: (Array new: 1)
	popIntoTemp: tempIndex
	pushConstant: const or pushTemp: n
	popIntoTemp: 0 inVectorAt: tempIndex
	collapse this into
	tempAt: tempIndex put: {const or temp}
	and answer true, otherwise answer false.
	One might think that we should look for a sequence of more than
	one pushes and pops but this is extremely rare.
	Exclude pushRcvr: n to avoid potential complications with context inst
	vars.  */

	/* StackToRegisterMappingCogit>>#tryCollapseTempVectorInitializationOfSize: */
static NoDbgRegParms sqInt
tryCollapseTempVectorInitializationOfSize(sqInt slots)
{
    sqInt pc;
    sqInt pc1;
    sqInt pc2;
    BytecodeDescriptor *pushArrayDesc;
    BytecodeDescriptor *pushValueDesc;
    sqInt reg;
    sqInt remoteTempIndex;
    BytecodeDescriptor *storeArrayDesc;
    BytecodeDescriptor *storeValueDesc;
    sqInt tempIndex;

	if (slots != 1) {
		return 0;
	}
	/* begin generatorForPC: */
	pushArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(bytecodePC, methodObj)));
	assert(((pushArrayDesc->generator)) == genPushNewArrayBytecode);
	pc = bytecodePC + ((pushArrayDesc->numBytes));
	/* begin generatorForPC: */
	storeArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
	if (((storeArrayDesc->generator)) == genStoreAndPopTemporaryVariableBytecode) {
		tempIndex = (fetchByteofObject(bytecodePC + ((pushArrayDesc->numBytes)), methodObj)) & 7;
	}
	else {
		if (!(((storeArrayDesc->generator)) == genLongStoreAndPopTemporaryVariableBytecode)) {
			return 0;
		}
		tempIndex = fetchByteofObject((bytecodePC + ((pushArrayDesc->numBytes))) + 1, methodObj);
	}
	pc1 = (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes));
	/* begin generatorForPC: */
	pushValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc1, methodObj)));
	if (!((((pushValueDesc->generator)) == genPushLiteralConstantBytecode)
		 || ((((pushValueDesc->generator)) == genPushQuickIntegerConstantBytecode)
		 || (((pushValueDesc->generator)) == genPushTemporaryVariableBytecode)))) {
		return 0;
	}
	pc2 = ((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes));
	/* begin generatorForPC: */
	storeValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc2, methodObj)));
	remoteTempIndex = fetchByteofObject((((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + 2, methodObj);
	if (!((((storeValueDesc->generator)) == genStoreAndPopRemoteTempLongBytecode)
		 && (tempIndex == remoteTempIndex))) {
		return 0;
	}
	genNewArrayOfSizeinitialized(1, 0);
	evaluateat(pushValueDesc, (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes)));
	reg = ssStorePoptoPreferredReg(1, TempReg);
	genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, 0, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
	evaluateat(storeArrayDesc, bytecodePC + ((pushArrayDesc->numBytes)));
	/* + pushArrayDesc numBytes this gets added by nextBytecodePCFor:at:exts:in: */
	bytecodePC = ((bytecodePC + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + ((storeValueDesc->numBytes));
	return 1;
}

	/* StackToRegisterMappingCogit>>#violatesEnsureSpilledSpillAssert */
static sqInt
violatesEnsureSpilledSpillAssert(void)
{
	return 1;
}


/*	Used when ReceiverResultReg is allocated for other than simSelf, and
	there may be references to ReceiverResultReg which need to be spilled. */

	/* StackToRegisterMappingCogit>>#voidReceiverResultRegContainsSelf */
static void
voidReceiverResultRegContainsSelf(void)
{
    sqInt i;
    sqInt i1;
    sqInt spillIndex;


	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	spillIndex = 0;
	for (i = ((((methodOrBlockNumTemps + 1) < simSpillBase) ? simSpillBase : (methodOrBlockNumTemps + 1))); i <= simStackPtr; i += 1) {
		if ((registerOrNone(simStackAt(i))) == ReceiverResultReg) {
			spillIndex = i;
		}
	}
	if (spillIndex > 0) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		ssNativeFlushTo(simNativeStackPtr);
		if (simSpillBase <= spillIndex) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < spillIndex) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : spillIndex)); i1 <= spillIndex; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = spillIndex + 1;
		}
	}
}
