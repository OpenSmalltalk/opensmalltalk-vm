/* Automatically generated by
	CCodeGenerator VMMaker.oscog-eem.3456 uuid: e316760f-1758-4b6b-aa08-f84bc7c44ef3
   from
	StackToRegisterMappingCogit VMMaker.oscog-eem.3456 uuid: e316760f-1758-4b6b-aa08-f84bc7c44ef3
 */
static char __buildInfo[] = "StackToRegisterMappingCogit VMMaker.oscog-eem.3456 uuid: e316760f-1758-4b6b-aa08-f84bc7c44ef3 " __DATE__ ;
char *__cogitBuildInfo = __buildInfo;



#include "sqConfig.h"
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "sqPlatformSpecific.h"
#include "sqMemoryAccess.h"
#include "sqCogStackAlignment.h"
#include "dispdbg.h"
#include "cogmethod.h"
#if COGMTVM
#include "cointerpmt.h"
#else
#include "cointerp.h"
#endif
#include "cogit.h"


/*** Constants ***/
#define ABICallerSavedRegisterMask 0x120F
#define ABIResultReg 0
#define AddCqR 106
#define AddCqRR 123
#define AddCwR 114
#define AddOpcode 4
#define AddRdRd 132
#define AddRR 100
#define AddRRR 126
#define AL 14
#define AlignmentNops 3
#define AllButTypeMask 0xFFFFFFFCU
#define AndCqR 108
#define AndCqRR 124
#define AndCwR 116
#define AndOpcode 0
#define AndRR 102
#define AnnotationShift 5
#define Arg0Reg 3
#define Arg1Reg 4
#define ArithmeticShiftRightCqR 91
#define ArithmeticShiftRightCqRR 128
#define ArithmeticShiftRightRR 92
#define BadRegisterSet 1
#define BicOpcode 14
#define BlockCreationBytecodeSize 4
#define BytecodeSetHasDirectedSuperSend 0
#define CArg0Reg 0
#define CArg1Reg 1
#define CArg2Reg 2
#define CArg3Reg 3
#define Call 6
#define CallerSavedRegisterMask 0xC
#define CallFull 7
#define CC 3
#define ClassArray 7
#define ClassFloatCompactIndex 6
#define ClassLargePositiveIntegerCompactIndex 5
#define ClassMethodContextCompactIndex 14
#define ClassReg 2
#define ClosureFirstCopiedValueIndex 3
#define ClosureNumArgsIndex 2
#define ClosureOuterContextIndex 0
#define ClosureStartPCIndex 1
#define ClzRR 157
#define CMBlock 4
#define CMClosedPIC 2
#define CMFree 1
#define CMMaxUsageCount 7
#define CMMethod 5
#define CMMethodFlaggedForBecome 6
#define CMOpenPIC 3
#define CMPSMULL 167
#define CmpCqR 105
#define CmpCwR 112
#define CmpNotOpcode 11
#define CmpOpcode 10
#define CmpRdRd 131
#define CmpRR 99
#define CompactClasses 28
#define CompletePrimitive 4
#define ConcreteIPReg 12
#define ConcreteVarBaseReg 10
#define ConstZero 1
#define ConvertRRd 145
#define CS 2
#if !defined(Debug) /* Allow this to be overridden on the compiler command line */
# define Debug DEBUGVM
#endif
#define DisplacementMask 0x1F
#define DisplacementX2N 0
#define DivRdRd 135
#define DPFPReg0 0
#define DPFPReg1 1
#define DPFPReg2 2
#define DPFPReg3 3
#define DPFPReg4 4
#define DPFPReg5 5
#define DPFPReg6 6
#define DPFPReg7 7
#define EncounteredUnknownBytecode -6
#define EQ 0
#define Extra0Reg 7
#define Extra1Reg 8
#define Extra2Reg 9
#define Fill32 4
#define FirstAnnotation 64
#define FirstJump 12
#define FirstSpecialSelector 176
#define FoxCallerSavedIP 4
#define FoxIFSavedIP -16
#define FoxMethod -4
#define FoxMFReceiver -12
#define FoxThisContext -8
#define FPReg 11
#define GCModeBecome 8
#define GCModeFull 1
#define GCModeNewSpace 2
#define GE 10
#define GT 12
#define HasBytecodePC 5
#define HashBitsOffset 17
#define HashMaskUnshifted 0xFFF
#define HashMultiplyConstant 1664525
#define HashMultiplyMask 0xFFFFFFF
#define HeaderIndex 0
#define HeaderTypeShort 3
#define HeaderTypeSizeAndClass 0
#define HI 8
#if !defined(IMMUTABILITY) /* Allow this to be overridden on the compiler command line */
# define IMMUTABILITY 0
#endif
#define InstanceSpecificationIndex 2
#define InstructionPointerIndex 1
#define InsufficientCodeSpace -2
#define InVanillaBlock 1
#define IsAbsPCReference 3
#define IsAnnotationExtension 1
#define IsDirectedSuperBindingSend null
#define IsDirectedSuperSend null
#define IsDisplacementX2N 0
#define IsNSSendCall null
#define IsObjectReference 2
#define IsRelativeCall 4
#define IsSendCall 7
#define IsSuperSend 8
#define Jump 16
#define JumpAbove 33
#define JumpAboveOrEqual 32
#define JumpBelow 31
#define JumpBelowOrEqual 34
#define JumpCarry 25
#define JumpFPEqual 35
#define JumpFPGreater 39
#define JumpFPGreaterOrEqual 40
#define JumpFPLess 37
#define JumpFPLessOrEqual 38
#define JumpFPNotEqual 36
#define JumpFPOrdered 41
#define JumpFPUnordered 42
#define JumpFull 12
#define JumpGreater 29
#define JumpGreaterOrEqual 28
#define JumpLess 27
#define JumpLessOrEqual 30
#define JumpLong 13
#define JumpLongNonZero 15
#define JumpLongZero 14
#define JumpNegative 19
#define JumpNoCarry 26
#define JumpNonNegative 20
#define JumpNonZero 18
#define JumpNoOverflow 22
#define JumpOverflow 21
#define JumpR 10
#define JumpZero 17
#define Label 1
#define LastJump 42
#define LE 13
#define LinkReg 14
#define Literal 2
#define LoadEffectiveAddressMwrR 88
#define LogicalShiftLeftCqR 95
#define LogicalShiftLeftCqRR 129
#define LogicalShiftLeftRR 96
#define LogicalShiftRightCqR 93
#define LogicalShiftRightCqRR 130
#define LogicalShiftRightRR 94
#define LongSizeMask 0xFFFFFFFCU
#define LR 14
#define LS 9
#define LT 11
#define MapEnd 0
#define MaxCompiledPrimitiveIndex 582
#define MaxCPICCases 6
#define MaxMethodSize 65535
#define MaxNegativeErrorCode -8
#define MaxStackAllocSize 1572864
#define MaxStackCheckOffset 0xFFF
#define MaxX2NDisplacement 992
#define MethodCacheClass 2
#define MethodCacheMask 0xFFC
#define MethodCacheMethod 3
#define MethodCacheSelector 1
#define MethodIndex 3
#define MethodTooBig -4
#define MFMethodFlagHasContextFlag 1
#define MFMethodFlagIsBlockFlag 2
#define MI 4
#define MoveAbR 48
#define MoveAwR 44
#define MoveCqR 69
#define MoveCwR 70
#define MoveM16rR 57
#define MoveM64rRd 75
#define MoveMbrR 65
#define MoveMwrR 50
#define MoveNotOpcode 15
#define MoveOpcode 13
#define MoveRAb 49
#define MoveRAw 46
#define MoveRdM64r 76
#define MoveRdRd 74
#define MoveRM16r 58
#define MoveRMbr 66
#define MoveRMwr 51
#define MoveRR 43
#define MoveRXbrR 68
#define MoveRXwrR 53
#define MoveXbrRR 67
#define MoveXwrRR 52
#define MSR 161
#define MulRdRd 134
#define NativeSPReg 13
#define NE 1
#define NeedsMergeFixupFlag 2
#define NeedsNonMergeFixupFlag 1
#define NegateR 89
#define Nop 5
#define NoReg -1
#define NotFullyInitialized -1
#define NumObjRefsInRuntime 0
#define NumSendTrampolines 4
#define NumSpecialSelectors 32
#define NumTrampolines (54 + (IMMUTABILITY ? 0 : 0))
#define OrCqR 109
#define OrCqRR 125
#define OrCwR 117
#define OrOpcode 12
#define OrRR 103
#define PC 15
#define PCReg 15
#define PL 5
#define PopLDM 163
#define PopR 80
#define PrefetchAw 87
#define PrimCallCollectsProfileSamples 16
#define PrimCallMayEndureCodeCompaction 8
#define PrimCallNeedsNewMethod 4
#define PrimNumberExternalCall 117
#define PrimNumberFFICall 120
#define PushCq 82
#define PushCw 83
#define PushR 81
#define PushSTM 164
#define ReceiverIndex 5
#define ReceiverResultReg 5
#define RetN 9
#define RISCTempReg 12
#define RootBit 0x40000000
#define RootBitDigitLength 4
#define RsbOpcode 3
#define SelectorCannotInterpret 34
#define SelectorDoesNotUnderstand 20
#define SenderIndex 0
#define SendNumArgsReg 6
#define ShouldNotJIT -8
#define Size4Bit 0
#define SizeMask 0xFC
#define SMULL 160
#define SP 13
#define SPReg 13
#define SqrtRd 136
#define SSBaseOffset 1
#define SSConstant 2
#define SSRegister 3
#define SSSpill 4
#define Stop 11
#define SubCqR 107
#define SubCwR 115
#define SubOpcode 2
#define SubRdRd 133
#define SubRR 101
#define SubRRR 127
#define TempReg 0
#define TstCqR 110
#define TstOpcode 8
#define TypeMask 0x3
#define UnfailingPrimitive 3
#define UnimplementedPrimitive -7
#define ValueIndex 1
#define VarBaseReg 10
#define VC 7
#define VS 6
#define XorCqR 111
#define XorCwR 118
#define XorOpcode 1
#define XorRR 104
#define YoungSelectorInPIC -5

typedef struct _AbstractInstruction {
	unsigned char	opcode;
	unsigned char	machineCodeSize;
	unsigned char	maxSize;
	unsigned char	annotation;
	unsigned char	conditionOrNil;
	unsigned int		machineCode[2];
	usqInt		operands[3];
	usqInt	address;
	struct _AbstractInstruction *dependent;
 } AbstractInstruction;

#define CogOutOfLineLiteralsARMCompiler AbstractInstruction
#define CogARMCompiler AbstractInstruction
#define CogAbstractInstruction AbstractInstruction


typedef struct {
	AbstractInstruction *fakeHeader;
	AbstractInstruction *fillInstruction;
	sqInt	numArgs;
	sqInt	numCopied;
	sqInt	numInitialNils;
	sqInt	startpc;
	AbstractInstruction *entryLabel;
	AbstractInstruction *stackCheckLabel;
	sqInt	span;
	sqInt	hasInstVarRef;
 } BlockStart;

#define CogBlockStart BlockStart


typedef struct _BytecodeDescriptor {
	sqInt (*generator)(void);
	sqInt NoDbgRegParms (*spanFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt);
	sqInt NoDbgRegParms (*needsFrameFunction)(sqInt);
	signed char	stackDelta;
	unsigned char	opcode;
	unsigned char	numBytes;
	unsigned		isBranchTrue : 1;
	unsigned		isBranchFalse : 1;
	unsigned		isReturn : 1;
	unsigned		isBlockCreation : 1;
	unsigned		isMapped : 1;
	unsigned		isMappedInBlock : 1;
	unsigned		isExtension : 1;
	unsigned		isInstVarRef : 1;
	unsigned		is1ByteInstVarStore : 1;
	unsigned		hasUnsafeJump : 1;
 } BytecodeDescriptor;

#define CogBytecodeDescriptor BytecodeDescriptor


typedef struct {
	sqInt (*primitiveGenerator)(void);
	sqInt	primNumArgs;
 } PrimitiveDescriptor;

#define CogPrimitiveDescriptor PrimitiveDescriptor


typedef struct {
	char	type;
	char	spilled;
	signed char	liveRegister;
	signed char	registerr;
	sqInt	offset;
	sqInt	constant;
	sqInt	bcptr;
 } SimStackEntry;

#define CogSimStackEntry SimStackEntry


typedef struct {
	AbstractInstruction *targetInstruction;
	unsigned char	simStackPtr;
	char	isTargetOfBackwardBranch;
	unsigned short	instructionIndex;
 } BytecodeFixup;

#define CogSSBytecodeFixup BytecodeFixup
#define CogBytecodeFixup BytecodeFixup


typedef struct {
	sqInt	isReceiverResultRegLive;
	CogSimStackEntry *ssEntry;
 } CogSSOptStatus;



/*** Function Prototypes ***/


#if !PRODUCTION && defined(PlatformNoDbgRegParms)
# define NoDbgRegParms PlatformNoDbgRegParms
#endif

#if !defined(NoDbgRegParms)
# define NoDbgRegParms /*empty*/
#endif



#if !defined(NeverInline)
# define NeverInline /*empty*/
#endif

static NoDbgRegParms AbstractInstruction * addDependent(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anInstruction);
static NoDbgRegParms sqInt availableFloatRegisterOrNoneFor(AbstractInstruction *self_in_CogAbstractInstruction, sqInt liveRegsMask);
static NoDbgRegParms AbstractInstruction * cloneLiteralFrom(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *existingLiteral);
static NoDbgRegParms sqInt genLoadCStackPointer(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadCStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadStackPointerForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg);
static NoDbgRegParms sqInt genLoadStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt genLoadStackPointersForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg);
static NoDbgRegParms sqInt genSaveStackPointers(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms AbstractInstruction * genSwapRRScratch(AbstractInstruction *self_in_CogAbstractInstruction, sqInt regA, sqInt regB, sqInt regTmp);
static NoDbgRegParms AbstractInstruction * genWriteCResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister);
static NoDbgRegParms AbstractInstruction * initializeSharableLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal);
static NoDbgRegParms AbstractInstruction * initializeUniqueLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal);
static NoDbgRegParms sqInt isAnInstruction(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *addressOrInstruction);
static NoDbgRegParms int isJump(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms AbstractInstruction * jmpTarget(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anAbstractInstruction);
static NoDbgRegParms sqInt literal32BeforeFollowingAddress(AbstractInstruction *self_in_CogAbstractInstruction, sqInt followingAddress);
static NoDbgRegParms AbstractInstruction * relocateJumpLongBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta);
static NoDbgRegParms AbstractInstruction * relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta);
static NoDbgRegParms AbstractInstruction * resolveJumpTarget(AbstractInstruction *self_in_CogAbstractInstruction);
static NoDbgRegParms sqInt rewriteConditionalJumpLongAttarget(AbstractInstruction *self_in_CogAbstractInstruction, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms usqInt addsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt addrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt addrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt addReg);
static NoDbgRegParms usqInt aeabiDivModFunctionAddr(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms usqInt andsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt andrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms sqInt availableRegisterOrNoneFor(AbstractInstruction *self_in_CogARMCompiler, sqInt liveRegsMask);
static NoDbgRegParms usqInt bicsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt bl(AbstractInstruction *self_in_CogARMCompiler, sqInt offset);
static NoDbgRegParms usqInt b(AbstractInstruction *self_in_CogARMCompiler, sqInt offset);
static NoDbgRegParms sqInt callInstructionByteSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt callTargetFromReturnAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt computeMaximumSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt concretizeAt(AbstractInstruction *self_in_CogARMCompiler, sqInt actualAddress);
static NoDbgRegParms sqInt concretizeCMPSMULL(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms unsigned char concretizeConditionalInstruction(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt concretizeFill32(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt concretizeMSR(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt concretizePushOrPopMultipleRegisters(AbstractInstruction *self_in_CogARMCompiler, sqInt doPush);
static NoDbgRegParms sqInt concretizeSMULL(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms int conditionIsNotNever(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms usqInt dataOpTyperdrnrmlsr(AbstractInstruction *self_in_CogARMCompiler, sqInt armOpcode, sqInt destReg, sqInt srcReg, sqInt addReg, sqInt shft);
static NoDbgRegParms sqInt dispatchConcretize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms unsigned int fmsrFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt regA, sqInt regB);
static NoDbgRegParms unsigned int fsitodFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt regA, sqInt regB);
static NoDbgRegParms sqInt fullCallsAreRelative(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms AbstractInstruction * genDivRRQuoRem(AbstractInstruction *self_in_CogARMCompiler, sqInt abstractRegDivisor, sqInt abstractRegDividend, sqInt abstractRegQuotient, sqInt abstractRegRemainder);
static NoDbgRegParms AbstractInstruction * genMarshallNArgsargargargarg(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3);
static NoDbgRegParms AbstractInstruction * genMulRR(AbstractInstruction *self_in_CogARMCompiler, sqInt regSource, sqInt regDest);
static NoDbgRegParms AbstractInstruction * genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs);
static NoDbgRegParms AbstractInstruction * genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs, sqInt ignored);
static NoDbgRegParms sqInt genRemoveNArgsFromStack(AbstractInstruction *self_in_CogARMCompiler, sqInt n);
static NoDbgRegParms AbstractInstruction * genRestoreRegs(AbstractInstruction *self_in_CogARMCompiler, sqInt regMask);
static NoDbgRegParms AbstractInstruction * genSaveRegs(AbstractInstruction *self_in_CogARMCompiler, sqInt regMask);
static NoDbgRegParms AbstractInstruction * genSubstituteReturnAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt retpc);
static NoDbgRegParms sqInt hasVarBaseRegister(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt instructionBeforeAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt followingAddress);
static NoDbgRegParms sqInt instructionIsBLX(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsBL(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsBX(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsB(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsCMP(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsLDR(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsOR(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionIsPush(AbstractInstruction *self_in_CogARMCompiler, sqInt instr);
static NoDbgRegParms sqInt instructionSizeAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc);
static NoDbgRegParms sqInt inverseOpcodeFor(AbstractInstruction *self_in_CogARMCompiler, sqInt armOpcode);
static NoDbgRegParms sqInt isCallPrecedingReturnPC(AbstractInstruction *self_in_CogARMCompiler, sqInt mcpc);
static NoDbgRegParms int isInImmediateJumpRange(AbstractInstruction *self_in_CogARMCompiler, usqIntptr_t operand);
static NoDbgRegParms sqInt isJumpAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc);
static NoDbgRegParms sqInt isPCRelativeValueLoad(AbstractInstruction *self_in_CogARMCompiler, unsigned int instr);
static NoDbgRegParms int isWithinCallRange(AbstractInstruction *self_in_CogARMCompiler, sqInt anAddress);
static NoDbgRegParms sqInt jumpLongByteSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt jumpLongConditionalByteSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt jumpLongTargetBeforeFollowingAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt mcpc);
static NoDbgRegParms usqInt jumpTargetPCAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc);
static NoDbgRegParms usqInt ldrbrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue);
static NoDbgRegParms usqInt ldrbrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms usqInt ldrhrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate8bitValue);
static NoDbgRegParms sqInt ldrhrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms usqInt ldrrnplusImm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt immediate12bitValue);
static NoDbgRegParms usqInt ldrrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue);
static NoDbgRegParms usqInt ldrrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms sqInt loadCwInto(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg);
static NoDbgRegParms sqInt loadPICLiteralByteSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt machineCodeWords(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms usqInt movsrn(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg);
static NoDbgRegParms usqInt movimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot);
static NoDbgRegParms usqInt movrn(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg);
static NoDbgRegParms usqInt mvnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot);
static NoDbgRegParms sqInt numIntRegArgs(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms usqInt orrimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot);
static NoDbgRegParms AbstractInstruction * padIfPossibleWithStopsFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt startAddr, sqInt endAddr);
static NoDbgRegParms sqInt popR(AbstractInstruction *self_in_CogARMCompiler, sqInt dstReg);
static NoDbgRegParms sqInt pushLinkRegisterByteSize(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms sqInt pushR(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg);
static NoDbgRegParms AbstractInstruction * relocateCallBeforeReturnPCby(AbstractInstruction *self_in_CogARMCompiler, sqInt retpc, sqInt delta);
static NoDbgRegParms sqInt rewriteCallAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress);
static NoDbgRegParms sqInt rewriteCallFullAttarget(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms sqInt rewriteJumpFullAttarget(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static NoDbgRegParms sqInt rewriteJumpLongAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress);
static NoDbgRegParms sqInt rewriteTransferAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress);
static NoDbgRegParms sqInt setsConditionCodesFor(AbstractInstruction *self_in_CogARMCompiler, sqInt aConditionalJumpOpcode);
static NoDbgRegParms sqInt shiftSetsConditionCodesFor(AbstractInstruction *self_in_CogARMCompiler, sqInt aConditionalJumpOpcode);
static NoDbgRegParms sqInt stackPageInterruptHeadroomBytes(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms AbstractInstruction * stopsFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt startAddr, sqInt endAddr);
static NoDbgRegParms usqInt strbrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue);
static NoDbgRegParms usqInt strbrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms usqInt strhrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate8bitValue);
static NoDbgRegParms sqInt strhrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms usqInt strrnplusImm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt immediate12bitValue);
static NoDbgRegParms usqInt strrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue);
static NoDbgRegParms usqInt strrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg);
static NoDbgRegParms usqInt subsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt subrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms usqInt tstrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt ignored, sqInt srcReg, sqInt immediate, sqInt rot);
static NoDbgRegParms sqInt zoneCallsAreRelative(AbstractInstruction *self_in_CogARMCompiler);
static NoDbgRegParms CogMethod * cmHomeMethod(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMBlock(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMClosedPIC(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMFree(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMMethodEtAl(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms int isCMOpenPIC(CogBlockMethod *self_in_CogBlockMethod);
static NoDbgRegParms sqInt isBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor);
static NoDbgRegParms sqInt isConditionalBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor);
static NoDbgRegParms AbstractInstruction * gAndCqR(sqInt quickConstant, sqInt reg);
static NoDbgRegParms AbstractInstruction * gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
extern sqInt abortOffset(void);
static void addCleanBlockStarts(void);
extern void addCogMethodsToHeapMap(void);
static NoDbgRegParms sqInt addressIsInCurrentCompilation(sqInt address);
static NoDbgRegParms sqInt addressIsInFixups(BytecodeFixup *address);
static NoDbgRegParms sqInt addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC);
static void alignMethodZoneBase(void);
static NoDbgRegParms sqInt alignUptoRoutineBoundary(sqInt anAddress);
static sqInt allMachineCodeObjectReferencesValid(void);
static sqInt allMethodsHaveCorrectHeader(void);
static NoDbgRegParms AbstractInstruction * annotateAbsolutePCRef(AbstractInstruction *abstractInstruction);
static NoDbgRegParms AbstractInstruction * annotateBytecode(AbstractInstruction *abstractInstruction);
static NoDbgRegParms AbstractInstruction * annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop);
static NoDbgRegParms void assertSaneJumpTarget(AbstractInstruction *jumpTarget);
static NoDbgRegParms sqInt availableRegisterOrNoneIn(sqInt liveRegsMask);
static NoDbgRegParms sqInt blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg);
extern sqInt bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static NoDbgRegParms AbstractInstruction * CallFullRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved);
static NoDbgRegParms AbstractInstruction * CallRT(sqInt callTarget);
static void callCogCodePopReceiver(void);
static void callCogCodePopReceiverAndClassRegs(void);
static NoDbgRegParms sqInt ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver);
static NoDbgRegParms void ceFree(void *pointer);
static NoDbgRegParms void* ceMalloc(size_t size);
static NoDbgRegParms sqInt ceSICMiss(sqInt receiver);
static NoDbgRegParms sqInt checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
static NoDbgRegParms sqInt checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern sqInt checkIntegrityOfObjectReferencesInCode(sqInt gcModes);
static NoDbgRegParms sqInt checkMaybeObjRefInClosedPIC(sqInt maybeObject);
static NoDbgRegParms sqInt checkValidObjectReferencesInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms NeverInline sqInt cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg);
static NoDbgRegParms sqInt closedPICRefersToUnmarkedObject(CogMethod *cPIC);
extern char * codeEntryFor(char *address);
extern char * codeEntryNameFor(char *address);
extern sqInt cogCodeBase(void);
extern sqInt cogCodeConstituents(sqInt withDetails);
static NoDbgRegParms void cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase);
extern void cogitPostGCAction(sqInt gcMode);
static NoDbgRegParms sqInt cogMethodDoesntLookKosher(CogMethod *cogMethod);
extern CogMethod * cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs);
static NoDbgRegParms CogMethod * cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs);
static NoDbgRegParms CogMethod * cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase);
extern CogMethod * cogselector(sqInt aMethodObj, sqInt aSelectorOop);
static NoDbgRegParms sqInt collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static NoDbgRegParms sqInt collectCogMethodConstituent(CogMethod *cogMethod);
extern void compactCogCompiledCode(void);
static void compactPICsWithFreedTargets(void);
static AbstractInstruction * compileAbort(void);
static NoDbgRegParms sqInt compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex);
static NoDbgRegParms void compileBlockEntry(BlockStart *blockStart);
static NoDbgRegParms void compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask);
static AbstractInstruction * compileCPICEntry(void);
static void compileEntry(void);
static sqInt compileMethodBody(void);
static NoDbgRegParms sqInt compilePICAbort(sqInt numArgs);
static NoDbgRegParms AbstractInstruction * compileStackOverflowCheck(sqInt canContextSwitch);
static NoDbgRegParms void compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone);
static void computeEntryOffsets(void);
static void computeFullBlockEntryOffsets(void);
static usqInt computeGoodVarBaseAddress(void);
static void computeMaximumSizes(void);
static NoDbgRegParms sqInt configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta);
static NoDbgRegParms sqInt configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta);
static NoDbgRegParms sqInt cPICCompactAndIsNowEmpty(CogMethod *cPIC);
static NoDbgRegParms sqInt cPICHasFreedTargets(CogMethod *cPIC);
static sqInt cPICPrototypeCaseOffset(void);
static NoDbgRegParms sqInt cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod);
static NoDbgRegParms sqInt createCPICData(CogMethod *cPIC);
static NoDbgRegParms AbstractInstruction * gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder);
extern int defaultCogCodeSize(void);
static NoDbgRegParms sqInt deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader);
static NoDbgRegParms sqInt endPCOf(sqInt aMethod);
static void enterCogCodePopReceiver(void);
static NoDbgRegParms sqInt entryPointTagIsSelector(sqInt entryPoint);
static NoDbgRegParms sqInt expectedClosedPICPrototype(CogMethod *cPIC);
static NoDbgRegParms sqInt fillInBlockHeadersAt(sqInt startAddress);
static NoDbgRegParms void fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector);
static NoDbgRegParms sqInt findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc);
static NoDbgRegParms usqInt findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc);
static NoDbgRegParms sqInt findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod);
extern CogBlockMethod * findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod);
static NoDbgRegParms sqInt findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc);
static NoDbgRegParms sqInt firstMappedPCFor(CogMethod *cogMethod);
static sqInt firstPrototypeMethodOop(void);
static NoDbgRegParms BytecodeFixup * fixupAt(sqInt fixupPC);
extern void flagCogMethodForBecome(CogMethod *cogMethod);
extern void freeBecomeFlaggedMethods(void);
extern void freeCogMethod(CogMethod *cogMethod);
static NoDbgRegParms AbstractInstruction * genCallMustBeBooleanFor(sqInt boolean);
static NoDbgRegParms AbstractInstruction * genConditionalBranchoperand(sqInt opcode, sqInt operandOne);
static NoDbgRegParms void (*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void);
static NoDbgRegParms void genEnilopmartReturn(sqInt forCall);
static NoDbgRegParms NeverInline void generateCaptureCStackPointers(sqInt captureFramePointer);
static void generateClosedPICPrototype(void);
static NoDbgRegParms CogMethod * generateCogMethod(sqInt selector);
static NoDbgRegParms sqInt generateMapAtstart(usqInt addressOrNull, usqInt startAddress);
static void generateOpenPICPrototype(void);
static void generateRunTimeTrampolines(void);
static void generateStackPointerCapture(void);
static void generateTrampolines(void);
static NoDbgRegParms BytecodeDescriptor * generatorForPC(sqInt pc);
static void genGetLeafCallStackPointers(void);
static NoDbgRegParms usqInt genInnerPICAbortTrampoline(char *name);
static void (*genInvokeInterpretTrampoline(void))(void);
static NoDbgRegParms void genLoadInlineCacheWithSelector(sqInt selectorIndex);
static usqInt genReturnToInterpreterTrampoline(void);
static NoDbgRegParms sqInt genSmalltalkToCStackSwitch(sqInt pushLinkReg);
static NoDbgRegParms usqInt genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean);
static NoDbgRegParms void genTrampolineReturn(sqInt lnkRegWasPushed);
static NoDbgRegParms AbstractInstruction * gen(sqInt opcode);
static NoDbgRegParms AbstractInstruction * genoperand(sqInt opcode, sqInt operand);
static NoDbgRegParms AbstractInstruction * genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo);
static NoDbgRegParms AbstractInstruction * genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree);
static NoDbgRegParms sqInt getLiteral(sqInt litIndex);
static sqInt getOpcodeIndex(void);
static NoDbgRegParms sqInt incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
extern void initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress);
static sqInt initialMethodUsageCount(void);
static int initialOpenPICUsageCount(void);
static NoDbgRegParms sqInt inverseBranchFor(sqInt opcode);
static NoDbgRegParms int isPCWithinMethodZone(void *address);
extern sqInt isSendReturnPC(sqInt retpc);
static NoDbgRegParms AbstractInstruction * gJumpFPEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPGreaterOrEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPGreater(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gJumpFPNotEqual(void *jumpTarget);
static NoDbgRegParms AbstractInstruction * gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * lastOpcode(void);
extern void linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver);
static BytecodeDescriptor * loadBytesAndGetDescriptor(void);
static NoDbgRegParms void loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc);
static NoDbgRegParms AbstractInstruction * gMoveMwrR(sqInt offset, sqInt baseReg, sqInt destReg);
static NoDbgRegParms sqInt mapEndFor(CogMethod *cogMethod);
static NoDbgRegParms sqInt mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg);
static NoDbgRegParms sqInt mapObjectReferencesInClosedPIC(CogMethod *cPIC);
static void mapObjectReferencesInGeneratedRuntime(void);
static void mapObjectReferencesInMachineCodeForBecome(void);
static void mapObjectReferencesInMachineCodeForFullGC(void);
static void mapObjectReferencesInMachineCodeForYoungGC(void);
extern void mapObjectReferencesInMachineCode(sqInt gcMode);
static void markAndTraceMachineCodeForNewSpaceGC(void);
static void markAndTraceObjectReferencesInGeneratedRuntime(void);
extern void markAndTraceObjectsOrFreeMachineCode(sqInt inFullGC);
static NoDbgRegParms sqInt markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit);
static void markAndTraceOrFreeMachineCodeForFullGC(void);
static NoDbgRegParms sqInt markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC);
static NoDbgRegParms sqInt markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern void markMethodAndReferents(CogBlockMethod *aCogMethod);
static NoDbgRegParms sqInt markYoungObjectspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern usqInt maxCogMethodAddress(void);
static NoDbgRegParms sqInt maximumDistanceFromCodeZone(sqInt anAddress);
static NoDbgRegParms sqInt maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod);
static int mclassIsSmallInteger(void);
extern usqInt mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static NoDbgRegParms sqInt methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral);
extern sqInt mnuOffset(void);
static NoDbgRegParms AbstractInstruction * gNativePopR(sqInt reg);
static NoDbgRegParms AbstractInstruction * gNativePushR(sqInt reg);
static NoDbgRegParms AbstractInstruction * gNativeRetN(sqInt offset);
static NoDbgRegParms sqInt needsFrameIfImmutability(sqInt stackDelta);
static NoDbgRegParms sqInt needsFrameIfInBlock(sqInt stackDelta);
static NoDbgRegParms sqInt needsFrameNever(sqInt stackDelta);
static NoDbgRegParms sqInt noAssertMethodClassAssociationOf(sqInt methodPointer);
static sqInt noCogMethodsMaximallyMarked(void);
static NoDbgRegParms int noTargetsFreeInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms sqInt outputInstructionsAt(sqInt startAddress);
static NoDbgRegParms sqInt outputInstructionsForGeneratedRuntimeAt(sqInt startAddress);
extern sqInt patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver);
static sqInt picAbortDiscriminatorValue(void);
static sqInt picInterpretAbortOffset(void);
extern void printCogMethodFor(void *address);
extern void printTrampolineTable(void);
static sqInt processorHasDivQuoRemAndMClassIsSmallInteger(void);
static sqInt processorHasMultiplyAndMClassIsSmallInteger(void);
static NoDbgRegParms void recordGeneratedRunTimeaddress(char *aString, sqInt address);
extern int recordPrimTraceFunc(void);
static void recordRunTimeObjectReferences(void);
static NoDbgRegParms sqInt registerMaskFor(sqInt reg);
static NoDbgRegParms void relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod);
static NoDbgRegParms void relocateCallsInClosedPIC(CogMethod *cPIC);
static NoDbgRegParms sqInt relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg);
static NoDbgRegParms sqInt remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr);
static NoDbgRegParms sqInt remapMaybeObjRefInClosedPICAt(sqInt mcpc);
static NoDbgRegParms void rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget);
static NoDbgRegParms AbstractInstruction * gSubCqR(sqInt quickConstant, sqInt reg);
static sqInt scanForCleanBlocks(void);
extern void setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop);
static NoDbgRegParms sqInt spanForCleanBlockStartingAt(sqInt startPC);
static NoDbgRegParms usqInt stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc);
static sqInt subsequentPrototypeMethodOop(void);
extern sqInt traceLinkedSendOffset(void);
static NoDbgRegParms char * trampolineNamenumArgs(char *routinePrefix, sqInt numArgs);
static NoDbgRegParms char * trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs);
extern void unflagBecomeFlaggedMethods(void);
static sqInt unknownBytecode(void);
extern void unlinkAllSends(void);
static NoDbgRegParms sqInt unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector);
static NoDbgRegParms sqInt unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static NoDbgRegParms sqInt unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod);
extern void unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector);
static void unlinkSendsToFree(void);
extern void unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue);
extern void unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue);
extern void voidCogCompiledCode(void);
static void zeroOpcodeIndex(void);
static NoDbgRegParms void addToOpenPICList(CogMethod *anOpenPIC);
static NoDbgRegParms void addToYoungReferrers(CogMethod *writableCogMethod);
static NoDbgRegParms usqInt allocate(sqInt numBytes);
extern CogMethod * cogMethodContaining(usqInt mcpc);
static void compactCompiledCode(void);
static NoDbgRegParms void ensureInYoungReferrers(CogMethod *cogMethod);
static NoDbgRegParms void freeMethod(CogMethod *cogMethod);
static void freeOlderMethodsForCompaction(void);
extern sqInt kosherYoungReferrers(void);
static NoDbgRegParms sqInt mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod);
extern CogMethod * methodFor(void *address);
extern sqInt methodsCompiledToMachineCodeInto(sqInt arrayObj);
extern sqInt numMethods(void);
extern sqInt numMethodsOfType(sqInt cogMethodType);
static NoDbgRegParms sqInt occurrencesInYoungReferrers(CogMethod *cogMethod);
static NoDbgRegParms CogMethod * openPICWithSelector(sqInt aSelector);
static void planCompaction(void);
extern void printCogMethods(void);
extern void printCogMethodsOfType(sqInt cmType);
extern void printCogMethodsWithMethod(sqInt methodOop);
extern void printCogMethodsWithPrimitive(sqInt primIdx);
extern void printCogMethodsWithSelector(sqInt selectorOop);
extern void printCogYoungReferrers(void);
extern sqInt printOpenPICList(void);
static sqInt pruneYoungReferrers(void);
static sqInt relocateAndPruneYoungReferrers(void);
static sqInt relocateMethodsPreCompaction(void);
static NoDbgRegParms sqInt removeFromOpenPICList(CogMethod *anOpenPIC);
static NoDbgRegParms sqInt roundUpLength(sqInt numBytes);
static void voidOpenPICList(void);
static void voidUnpairedMethodList(void);
static void voidYoungReferrersPostTenureAll(void);
extern char * whereIsMaybeCodeThing(sqInt anOop);
static NoDbgRegParms sqInt checkValidObjectReference(sqInt anOop);
static NoDbgRegParms AbstractInstruction * genCmpClassFloatCompactIndexR(sqInt reg);
static NoDbgRegParms AbstractInstruction * genCmpClassMethodContextCompactIndexR(sqInt reg);
static NoDbgRegParms sqInt genDoubleArithmeticpreOpCheck(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg));
static NoDbgRegParms sqInt genDoubleComparisoninvert(AbstractInstruction * NoDbgRegParms (*jumpOpcodeGenerator)(void *), sqInt invertComparison);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallIntegersInandscratch(sqInt aRegister, sqInt bRegister, sqInt scratchRegister);
static NoDbgRegParms sqInt genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg);
static sqInt genPrimitiveAdd(void);
static sqInt genPrimitiveAsCharacter(void);
static sqInt genPrimitiveAsFloat(void);
static sqInt genPrimitiveAtPut(void);
static sqInt genPrimitiveBitAnd(void);
static sqInt genPrimitiveBitOr(void);
static sqInt genPrimitiveBitShift(void);
static sqInt genPrimitiveBitXor(void);
static sqInt genPrimitiveClass(void);
static sqInt genPrimitiveDiv(void);
static sqInt genPrimitiveDivide(void);
static sqInt genPrimitiveEqual(void);
static sqInt genPrimitiveFloatAdd(void);
static sqInt genPrimitiveFloatDivide(void);
static sqInt genPrimitiveFloatEqual(void);
static sqInt genPrimitiveFloatGreaterOrEqual(void);
static sqInt genPrimitiveFloatGreaterThan(void);
static sqInt genPrimitiveFloatLessOrEqual(void);
static sqInt genPrimitiveFloatLessThan(void);
static sqInt genPrimitiveFloatMultiply(void);
static sqInt genPrimitiveFloatNotEqual(void);
static sqInt genPrimitiveFloatSquareRoot(void);
static sqInt genPrimitiveFloatSubtract(void);
static sqInt genPrimitiveGreaterOrEqual(void);
static sqInt genPrimitiveGreaterThan(void);
static sqInt genPrimitiveHighBit(void);
static sqInt genPrimitiveIdentical(void);
static sqInt genPrimitiveImmediateAsInteger(void);
static sqInt genPrimitiveIntegerAt(void);
static sqInt genPrimitiveIntegerAtPut(void);
static sqInt genPrimitiveLessOrEqual(void);
static sqInt genPrimitiveLessThan(void);
static sqInt genPrimitiveMakePoint(void);
static sqInt genPrimitiveMod(void);
static sqInt genPrimitiveMultiply(void);
static sqInt genPrimitiveNew(void);
static sqInt genPrimitiveNewMethod(void);
static sqInt genPrimitiveNewWithArg(void);
static sqInt genPrimitiveNotEqual(void);
static sqInt genPrimitiveNotIdentical(void);
static sqInt genPrimitiveObjectAt(void);
static sqInt genPrimitiveQuo(void);
static sqInt genPrimitiveShallowCopy(void);
static sqInt genPrimitiveSlotAt(void);
static sqInt genPrimitiveSlotAtPut(void);
static sqInt genPrimitiveStringAtPut(void);
static sqInt genPrimitiveStringCompareWith(void);
static sqInt genPrimitiveStringReplace(void);
static sqInt genPrimitiveSubtract(void);
static sqInt genPrimitiveUninitializedNewWithArg(void);
static NoDbgRegParms sqInt genPureDoubleArithmeticpreOpCheck(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg));
static NoDbgRegParms sqInt genPureDoubleComparisoninvert(AbstractInstruction * NoDbgRegParms (*jumpOpcodeGenerator)(void *), sqInt invertComparison);
static NoDbgRegParms sqInt genSmallIntegerComparison(sqInt jumpOpcode);
static NoDbgRegParms sqInt genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison);
static NoDbgRegParms sqInt isUnannotatableConstant(CogSimStackEntry *simStackEntry);
static NoDbgRegParms sqInt maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(sqInt oopReg, sqInt scratch, sqInt dpReg, AbstractInstruction *targetInst);
static NoDbgRegParms sqInt maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg);
static NoDbgRegParms sqInt cacheTagIsMarked(sqInt cacheTag);
static NoDbgRegParms sqInt checkValidOopReference(sqInt anOop);
static NoDbgRegParms sqInt classForInlineCacheTag(sqInt inlineCacheTag);
static sqInt compactClassFieldMask(void);
static NoDbgRegParms sqInt couldBeObject(sqInt oop);
static usqInt genActiveContextTrampoline(void);
static NoDbgRegParms sqInt genAddSmallIntegerTagsTo(sqInt aRegister);
static NoDbgRegParms AbstractInstruction * genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static NoDbgRegParms sqInt genClearAndSetSmallIntegerTagsIn(sqInt scratchReg);
static NoDbgRegParms sqInt genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg);
static NoDbgRegParms sqInt genConvertIntegerToSmallIntegerInReg(sqInt reg);
static NoDbgRegParms sqInt genConvertSmallIntegerToIntegerInReg(sqInt reg);
static NoDbgRegParms sqInt genCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock);
static void generateObjectRepresentationTrampolines(void);
static NoDbgRegParms void genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock);
static NoDbgRegParms sqInt genGetClassFormatOfNonIntintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static NoDbgRegParms sqInt genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeAForwarder);
static NoDbgRegParms AbstractInstruction * genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static NoDbgRegParms sqInt genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg);
static NoDbgRegParms sqInt genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg);
static NoDbgRegParms sqInt genGetFixedFieldsOfPointerNonIntintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static NoDbgRegParms sqInt genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg);
static NoDbgRegParms AbstractInstruction * genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry);
static NoDbgRegParms AbstractInstruction * genJumpImmediate(sqInt aRegister);
static NoDbgRegParms AbstractInstruction * genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallIntegerInScratchReg(sqInt aRegister);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static NoDbgRegParms AbstractInstruction * genJumpNotSmallInteger(sqInt aRegister);
static NoDbgRegParms AbstractInstruction * genJumpSmallInteger(sqInt aRegister);
static NoDbgRegParms void genNewArrayOfSizeinitialized(sqInt size, sqInt initialize);
static sqInt genPrimitiveAt(void);
static NoDbgRegParms sqInt genPrimitiveIdenticalOrNotIf(sqInt orNot);
static sqInt genPrimitiveIdentityHash(void);
static sqInt genPrimitiveSize(void);
static sqInt genPrimitiveStringAt(void);
static NoDbgRegParms sqInt genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg);
static NoDbgRegParms sqInt genSetSmallIntegerTagsIn(sqInt scratchReg);
static NoDbgRegParms sqInt genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg);
static NoDbgRegParms sqInt genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck);
static NoDbgRegParms sqInt genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg);
static NoDbgRegParms sqInt inlineCacheTagForInstance(sqInt oop);
static NoDbgRegParms sqInt inlineCacheTagIsYoung(sqInt cacheTag);
static NoDbgRegParms void markAndTraceLiteralIfYoung(sqInt literal);
static NoDbgRegParms void markAndTraceLiteral(sqInt literal);
static sqInt numSmallIntegerBits(void);
static NoDbgRegParms sqInt remapObject(sqInt objOop);
static NoDbgRegParms sqInt remapOop(sqInt oop);
static NoDbgRegParms sqInt shouldAnnotateObjectReference(sqInt anOop);
static NoDbgRegParms sqInt slotOffsetOfInstVarIndex(sqInt index);
static NoDbgRegParms sqInt validInlineCacheTag(sqInt cacheTag);
static NoDbgRegParms sqInt callFullInstructionByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt cmpC32RTempByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt concretizeLiteral(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt inlineCacheTagAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt isPCDependent(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt literalBeforeFollowingAddress(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt followingAddress);
static NoDbgRegParms sqInt literalLoadInstructionBytes(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt loadLiteralByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt machineCodeBytes(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms sqInt nsSendCacheAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt callSiteReturnAddress);
static NoDbgRegParms sqInt outOfLineLiteralOpcodeLimit(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms usqInt pcRelativeAddressAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt instrAddress);
static NoDbgRegParms AbstractInstruction * relocateMethodReferenceBeforeAddressby(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt pc, sqInt delta);
static NoDbgRegParms sqInt rewriteFullTransferAttargetexpectedInstruction(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress, sqInt expectedInstruction);
static NoDbgRegParms sqInt rewriteInlineCacheAttagtarget(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress);
static NoDbgRegParms AbstractInstruction * rewriteInlineCacheTagat(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt cacheTag, sqInt callSiteReturnAddress);
static NoDbgRegParms unsigned char sizePCDependentInstructionAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt eventualAbsoluteAddress);
static NoDbgRegParms AbstractInstruction * storeLiteralbeforeFollowingAddress(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt literal, sqInt followingAddress);
static NoDbgRegParms AbstractInstruction * updateLabel(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, AbstractInstruction *labelInstruction);
static NoDbgRegParms sqInt usesOutOfLineLiteral(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler);
static NoDbgRegParms SimStackEntry * ensureSpilledAtfrom(SimStackEntry *self_in_CogSimStackEntry, sqInt baseOffset, sqInt baseRegister);
static NoDbgRegParms sqInt isSameEntryAs(SimStackEntry *self_in_CogSimStackEntry, CogSimStackEntry *ssEntry);
static NoDbgRegParms sqInt mayBeAForwarder(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms SimStackEntry * popToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg);
static NoDbgRegParms sqInt registerMask(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms sqInt registerMaskOrNone(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms sqInt registerOrNone(SimStackEntry *self_in_CogSimStackEntry);
static NoDbgRegParms SimStackEntry * storeToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg);
static NoDbgRegParms char isBackwardBranchFixup(BytecodeFixup *self_in_CogSSBytecodeFixup);
static NoDbgRegParms int isMergeFixup(BytecodeFixup *self_in_CogSSBytecodeFixup);
static NoDbgRegParms AbstractInstruction * allocateLiteral(sqInt aLiteral);
static NoDbgRegParms AbstractInstruction * checkLiteralforInstruction(sqInt literal, AbstractInstruction *anInstruction);
static NoDbgRegParms sqInt dumpLiterals(sqInt generateBranchAround);
static NoDbgRegParms sqInt literalInstructionInRange(AbstractInstruction *litInst);
static NoDbgRegParms AbstractInstruction * locateLiteral(sqInt aLiteral);
static NoDbgRegParms sqInt ceClosureCopyDescriptor(sqInt descriptor);
extern sqInt cogMethodHasExternalPrim(CogMethod *aCogMethod);
extern sqInt cogMethodHasMachineCodePrim(CogMethod *aCogMethod);
static sqInt compileBlockDispatch(void);
static void compileGetErrorCode(void);
static NoDbgRegParms sqInt compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags);
static NoDbgRegParms AbstractInstruction * compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone);
static NoDbgRegParms void compileOpenPICnumArgs(sqInt selector, sqInt numArgs);
static NoDbgRegParms AbstractInstruction * compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone);
static sqInt compilePrimitive(void);
static sqInt extendedPushBytecode(void);
static sqInt extendedStoreAndPopBytecode(void);
static sqInt extendedStoreBytecode(void);
static NoDbgRegParms int frameOffsetOfTemporary(sqInt index);
static NoDbgRegParms AbstractInstruction * genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg);
static sqInt genExtendedSendBytecode(void);
static sqInt genExtendedSuperBytecode(void);
static sqInt genFastPrimFail(void);
static NoDbgRegParms void genFastPrimTraceUsingand(sqInt r1, sqInt r2);
static void genLoadNewMethod(void);
static sqInt genLongJumpIfFalse(void);
static sqInt genLongJumpIfTrue(void);
static sqInt genLongStoreAndPopTemporaryVariableBytecode(void);
static sqInt genLongUnconditionalBackwardJump(void);
static sqInt genLongUnconditionalForwardJump(void);
static NoDbgRegParms sqInt genLookupForPerformNumArgs(sqInt numArgs);
static NoDbgRegParms AbstractInstruction * genMoveConstantR(sqInt constant, sqInt reg);
static NoDbgRegParms usqInt genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName);
static sqInt genPrimitiveHashMultiply(void);
static NoDbgRegParms void genPrimReturnEnterCogCodeEnilopmart(sqInt profiling);
static sqInt genPushConstantFalseBytecode(void);
static sqInt genPushConstantNilBytecode(void);
static sqInt genPushConstantTrueBytecode(void);
static sqInt genPushLiteralConstantBytecode(void);
static sqInt genPushLiteralVariableBytecode(void);
static sqInt genPushQuickIntegerConstantBytecode(void);
static sqInt genPushReceiverVariableBytecode(void);
static sqInt genPushTemporaryVariableBytecode(void);
extern sqInt genQuickReturnConst(void);
extern sqInt genQuickReturnInstVar(void);
extern sqInt genQuickReturnSelf(void);
static sqInt genReturnFalse(void);
static sqInt genReturnNil(void);
static sqInt genReturnTrue(void);
static sqInt genSecondExtendedSendBytecode(void);
static sqInt genSendLiteralSelector0ArgsBytecode(void);
static sqInt genSendLiteralSelector1ArgBytecode(void);
static sqInt genSendLiteralSelector2ArgsBytecode(void);
static sqInt genShortJumpIfFalse(void);
static sqInt genShortUnconditionalJump(void);
static sqInt genSpecialSelectorEqualsEquals(void);
static sqInt genSpecialSelectorNotEqualsEquals(void);
static sqInt genSpecialSelectorSend(void);
static sqInt genStoreAndPopReceiverVariableBytecode(void);
static sqInt genStoreAndPopRemoteTempLongBytecode(void);
static sqInt genStoreAndPopTemporaryVariableBytecode(void);
static sqInt genStoreRemoteTempLongBytecode(void);
extern sqInt mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj);
static sqInt numSpecialSelectors(void);
static NoDbgRegParms usqInt pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod);
static NoDbgRegParms sqInt pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static PrimitiveDescriptor * primitiveGeneratorOrNil(void);
static NoDbgRegParms int registerisInMask(sqInt reg, sqInt mask);
static NoDbgRegParms sqInt v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms sqInt v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static NoDbgRegParms BlockStart * addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span);
static NoDbgRegParms void adjustArgumentsForPerform(sqInt numArgs);
static NoDbgRegParms sqInt allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask);
static NoDbgRegParms sqInt allocateRegNotConflictingWith(sqInt regMask);
static NoDbgRegParms sqInt anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n);
extern void callCogCodePopReceiverArg0Regs(void);
extern void callCogCodePopReceiverArg1Arg0Regs(void);
static NoDbgRegParms sqInt compileAbstractInstructionsFromthrough(sqInt start, sqInt end);
static sqInt compileBlockBodies(void);
static NoDbgRegParms void compileBlockFrameBuild(BlockStart *blockStart);
static NoDbgRegParms void compileBlockFramelessEntry(BlockStart *blockStart);
static NoDbgRegParms CogMethod * compileCogMethod(sqInt selector);
static sqInt compileEntireMethod(void);
static void compileFrameBuild(void);
#if IMMUTABILITY
static void compileTwoPathFrameBuild(void);
#endif /* IMMUTABILITY */
static void compileTwoPathFramelessInit(void);
static NoDbgRegParms sqInt cPICMissTrampolineFor(sqInt numArgs);
static sqInt doubleExtendedDoAnythingBytecode(void);
static sqInt duplicateTopBytecode(void);
static NoDbgRegParms BytecodeFixup * ensureFixupAt(sqInt targetPC);
static NoDbgRegParms BytecodeFixup * ensureNonMergeFixupAt(sqInt targetPC);
static void ensureReceiverResultRegContainsSelf(void);
static NoDbgRegParms void evaluateat(BytecodeDescriptor *descriptor, sqInt pc);
static NoDbgRegParms sqInt eventualTargetOf(sqInt targetBytecodePC);
static NoDbgRegParms sqInt freeAnyRegNotConflictingWith(sqInt regMask);
static sqInt genBlockReturn(void);
static NoDbgRegParms void (*genCallPICEnilopmartNumArgs(sqInt numArgs))(void);
static sqInt genExternalizePointersForPrimitiveCall(void);
static void generateEnilopmarts(void);
static NoDbgRegParms sqInt generateInstructionsAt(sqInt eventualAbsoluteAddress);
static void generateMissAbortTrampolines(void);
static void generateSendTrampolines(void);
static void generateTracingTrampolines(void);
static NoDbgRegParms sqInt genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot);
static NoDbgRegParms sqInt genInlinedIdenticalOrNotIf(sqInt orNot);
static NoDbgRegParms sqInt genJumpBackTo(sqInt targetBytecodePC);
static NoDbgRegParms sqInt genJumpIfto(sqInt boolean, sqInt targetBytecodePC);
static NoDbgRegParms sqInt genJumpTo(sqInt targetBytecodePC);
static NoDbgRegParms sqInt genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable);
static NoDbgRegParms usqInt genMethodAbortTrampolineFor(sqInt numArgs);
static NoDbgRegParms usqInt genPICAbortTrampolineFor(sqInt numArgs);
static NoDbgRegParms usqInt genPICMissTrampolineFor(sqInt numArgs);
static sqInt genPopStackBytecode(void);
static sqInt genPrimitiveClosureValue(void);
static sqInt genPrimitivePerform(void);
static sqInt genPushActiveContextBytecode(void);
static sqInt genPushClosureCopyCopiedValuesBytecode(void);
static NoDbgRegParms sqInt genPushLiteralIndex(sqInt literalIndex);
static NoDbgRegParms sqInt genPushLiteralVariable(sqInt literalIndex);
static NoDbgRegParms sqInt genPushMaybeContextReceiverVariable(sqInt slotIndex);
static sqInt genPushNewArrayBytecode(void);
static sqInt genPushReceiverBytecode(void);
static NoDbgRegParms sqInt genPushReceiverVariable(sqInt index);
static void genPushRegisterArgs(void);
static sqInt genPushRemoteTempLongBytecode(void);
static NoDbgRegParms sqInt genPushTemporaryVariable(sqInt index);
static sqInt genReturnReceiver(void);
static sqInt genReturnTopFromBlock(void);
static sqInt genReturnTopFromMethod(void);
static NoDbgRegParms sqInt genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs);
static NoDbgRegParms usqInt genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3);
static NoDbgRegParms sqInt genSendnumArgs(sqInt selectorIndex, sqInt numArgs);
static sqInt genSpecialSelectorArithmetic(void);
static sqInt genSpecialSelectorClass(void);
static sqInt genSpecialSelectorComparison(void);
static sqInt genStaticallyResolvedSpecialSelectorComparison(void);
static NoDbgRegParms sqInt genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static NoDbgRegParms sqInt genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck);
static NoDbgRegParms sqInt genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex);
static sqInt genUpArrowReturn(void);
static NoDbgRegParms sqInt genVanillaInlinedIdenticalOrNotIf(sqInt orNot);
static NoDbgRegParms void initSimStackForFramefulMethod(sqInt startpc);
static NoDbgRegParms void initSimStackForFramelessBlock(sqInt startpc);
static NoDbgRegParms void initSimStackForFramelessMethod(sqInt startpc);
static NoDbgRegParms sqInt isNonForwarderReceiver(sqInt reg);
static sqInt liveRegisters(void);
static NoDbgRegParms sqInt mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor);
static NoDbgRegParms void marshallSendArguments(sqInt numArgs);
static sqInt maybeCompilingFirstPassOfBlockWithInitialPushNil(void);
static NoDbgRegParms sqInt mergeWithFixupIfRequired(BytecodeFixup *fixup);
static NoDbgRegParms sqInt methodAbortTrampolineFor(sqInt numArgs);
static NoDbgRegParms sqInt needsFrameIfMod16GENumArgs(sqInt stackDelta);
static NoDbgRegParms sqInt needsFrameIfStackGreaterThanOne(sqInt stackDelta);
static NoDbgRegParms sqInt numberOfSpillsInTopNItems(sqInt n);
static NoDbgRegParms sqInt picAbortTrampolineFor(sqInt numArgs);
static sqInt prevInstIsPCAnnotated(void);
static int receiverIsInReceiverResultReg(void);
static NoDbgRegParms void reinitializeFixupsFromthrough(sqInt start, sqInt end);
static NoDbgRegParms sqInt scanBlock(BlockStart *blockStart);
static sqInt scanMethod(void);
static NoDbgRegParms void ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr);
static NoDbgRegParms void ssFlushUpThroughReceiverVariable(sqInt slotIndex);
static NoDbgRegParms void ssFlushUpThroughTemporaryVariable(sqInt tempIndex);
static NoDbgRegParms void ssPop(sqInt n);
static NoDbgRegParms sqInt ssPushAnnotatedConstant(sqInt literal);
static NoDbgRegParms sqInt ssPushBaseoffset(sqInt reg, sqInt offset);
static NoDbgRegParms sqInt ssPushConstant(sqInt literal);
static NoDbgRegParms sqInt ssPushDesc(SimStackEntry simStackEntry);
static NoDbgRegParms sqInt ssPushRegister(sqInt reg);
static NoDbgRegParms void ssPush(sqInt n);
static SimStackEntry ssSelfDescriptor(void);
static NoDbgRegParms void ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg);
static NoDbgRegParms sqInt ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg);
static NoDbgRegParms void ssStorePoptoReg(sqInt popBoolean, sqInt reg);
static CogSimStackEntry * ssTop(void);
static NoDbgRegParms CogSimStackEntry * ssValue(sqInt n);
static NoDbgRegParms sqInt stackEntryIsBoolean(CogSimStackEntry *simStackEntry);
static sqInt tempsValidAndVolatileEntriesSpilled(void);
static NoDbgRegParms sqInt tryCollapseTempVectorInitializationOfSize(sqInt slots);
static NoDbgRegParms sqInt v3PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils);
static NoDbgRegParms sqInt v3NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt violatesEnsureSpilledSpillAssert(void);
static void voidReceiverResultRegContainsSelf(void);


/*** Variables ***/
static AbstractInstruction * abstractOpcodes;
static usqInt baseAddress;
static sqInt blockCount;
static AbstractInstruction * blockEntryLabel;
static AbstractInstruction * blockEntryNoContextSwitch;
static BlockStart * blockStarts;
static sqInt breakBlock;
static sqInt breakMethod;
static sqInt byte0;
static sqInt byte1;
static sqInt byte2;
static sqInt byte3;
static sqInt bytecodePC;
static sqInt bytecodeSetOffset;
static sqInt ceActiveContextTrampoline;
static sqInt ceByteSizeOfTrampoline;
static sqInt ceClosureCopyTrampoline;
static sqInt ceCPICMissTrampoline;
static sqInt ceCreateNewArrayTrampoline;
static sqInt ceFetchContextInstVarTrampoline;
static sqInt ceFFICalloutTrampoline;
static sqInt ceFloatObjectOfTrampoline;
static sqInt ceFloatValueOfTrampoline;
static sqInt ceFlushICache;
static sqInt ceFreeTrampoline;
static sqInt ceInstantiateClassIndexableSizeTrampoline;
static sqInt ceInstantiateClassTrampoline;
static sqInt ceMallocTrampoline;
static sqInt ceMethodAbortTrampoline;
static sqInt ceNonLocalReturnTrampoline;
static sqInt cePICAbortTrampoline;
static sqInt cePositive32BitIntegerTrampoline;
static sqInt cePositive32BitValueOfTrampoline;
static sqInt cePositive64BitIntegerTrampoline;
static sqInt cePositive64BitValueOfTrampoline;
static sqInt cePrimReturnEnterCogCode;
static sqInt cePrimReturnEnterCogCodeProfiling;
static sqInt ceReapAndResetErrorCodeTrampoline;
static sqInt ceSendMustBeBooleanAddFalseTrampoline;
static sqInt ceSendMustBeBooleanAddTrueTrampoline;
static sqInt ceSigned32BitIntegerTrampoline;
static sqInt ceSigned32BitValueOfTrampoline;
static sqInt ceSigned64BitIntegerTrampoline;
static sqInt ceSigned64BitValueOfTrampoline;
static sqInt ceStoreCheckTrampoline;
static sqInt ceStoreContextInstVarTrampoline;
static sqInt ceTraceBlockActivationTrampoline;
static sqInt ceTraceLinkedSendTrampoline;
static sqInt ceTraceStoreTrampoline;
static sqInt checkedEntryAlignment;
static sqInt closedPICSize;
static sqInt codeBase;
#if DUAL_MAPPED_CODE_ZONE
static sqInt codeToDataDelta;
#else
# define codeToDataDelta 0
#endif
static sqInt cogConstituentIndex;
static sqInt compactionInProgress;
static sqInt compilationPass;
static sqInt compilationTrace;
static sqInt cPICCaseSize;
static sqInt cPICEndOfCodeOffset;
static sqInt cPICEndSize;
static CogMethod * cPICPrototype;
static sqInt currentCallCleanUpSize;
static sqInt debugBytecodePointers;
static sqInt debugFixupBreaks;
static sqInt debugOpcodeIndices;
static sqInt debugStackPointers;
static sqInt directedSuperBindingSendTrampolines;
static sqInt directedSuperSendTrampolines;
static sqInt disassemblingMethod;
static AbstractInstruction * endCPICCase0;
static sqInt endPC;
static AbstractInstruction * entry;
static sqInt entryPointMask;
static CogMethod * enumeratingCogMethod;
static sqInt expectedFPAlignment;
static sqInt expectedSPAlignment;
static sqInt extA;
static sqInt extB;
static sqInt firstCPICCaseOffset;
static sqInt firstOpcodeIndex;
static sqInt firstSend;
static BytecodeFixup * fixups;
static AbstractInstruction * fullBlockEntry;
static AbstractInstruction * fullBlockNoContextSwitchEntry;
static BytecodeDescriptor generatorTable[256] = {
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushReceiverBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantTrueBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantFalseBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantNilBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genReturnReceiver, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTrue, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnFalse, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNil, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromMethod, 0, needsFrameIfInBlock, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extendedPushBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ extendedStoreBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ extendedStoreAndPopBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ doubleExtendedDoAnythingBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtendedSuperBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0 },
	{ genSecondExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genPopStackBytecode, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ duplicateTopBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushActiveContextBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushNewArrayBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genPushRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushClosureCopyCopiedValuesBytecode, v3BlockCodeSize, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AddRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, SubRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLess, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreater, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLessOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreaterOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpNonZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AndRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, OrRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorClass, 0, needsFrameIfStackGreaterThanOne, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorNotEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 }
};
static sqInt guardPageSize;
static sqInt initialPC;
static sqInt introspectionData;
static sqInt introspectionDataIndex;
static int labelCounter;
static sqInt lastDumpedLiteralIndex;
static sqInt lastSend;
static usqInt limitAddress;
static AbstractInstruction * literals;
static sqInt literalsSize;
static sqInt maxLitIndex;
static sqInt methodAbortTrampolines[4];
static sqInt methodBytesFreedSinceLastCompaction;
static sqInt methodCount;
static sqInt methodHeader;
static sqInt methodObj;
static sqInt methodOrBlockNumArgs;
static sqInt methodOrBlockNumTemps;
static usqIntptr_t minValidCallAddress;
static usqInt mzFreeStart;
static sqInt nextLiteralIndex;
static AbstractInstruction * noCheckEntry;
static sqInt numAbstractOpcodes;
static sqInt numExtB;
static usqInt objectReferencesInRuntime[NumObjRefsInRuntime+1];
static sqInt opcodeIndex;
static CogMethod *openPICList = 0;
static sqInt openPICSize;
static sqInt ordinarySendTrampolines[NumSendTrampolines];
static sqInt picAbortTrampolines[4];
static AbstractInstruction * picInterpretAbort;
static sqInt picMissTrampolines[4];
static AbstractInstruction * picMNUAbort;
static BytecodeDescriptor * prevBCDescriptor;
static PrimitiveDescriptor primitiveGeneratorTable[MaxCompiledPrimitiveIndex+1] = {
	{ 0, -1 },
	{ genPrimitiveAdd, 1 },
	{ genPrimitiveSubtract, 1 },
	{ genPrimitiveLessThan, 1 },
	{ genPrimitiveGreaterThan, 1 },
	{ genPrimitiveLessOrEqual, 1 },
	{ genPrimitiveGreaterOrEqual, 1 },
	{ genPrimitiveEqual, 1 },
	{ genPrimitiveNotEqual, 1 },
	{ genPrimitiveMultiply, 1 },
	{ genPrimitiveDivide, 1 },
	{ genPrimitiveMod, 1 },
	{ genPrimitiveDiv, 1 },
	{ genPrimitiveQuo, 1 },
	{ genPrimitiveBitAnd, 1 },
	{ genPrimitiveBitOr, 1 },
	{ genPrimitiveBitXor, 1 },
	{ genPrimitiveBitShift, 1 },
	{ genPrimitiveMakePoint, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAsFloat, 0 },
	{ genPrimitiveFloatAdd, 1 },
	{ genPrimitiveFloatSubtract, 1 },
	{ genPrimitiveFloatLessThan, 1 },
	{ genPrimitiveFloatGreaterThan, 1 },
	{ genPrimitiveFloatLessOrEqual, 1 },
	{ genPrimitiveFloatGreaterOrEqual, 1 },
	{ genPrimitiveFloatEqual, 1 },
	{ genPrimitiveFloatNotEqual, 1 },
	{ genPrimitiveFloatMultiply, 1 },
	{ genPrimitiveFloatDivide, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveFloatSquareRoot, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAt, 1 },
	{ genPrimitiveAtPut, 2 },
	{ genPrimitiveSize, 0 },
	{ genPrimitiveStringAt, 1 },
	{ genPrimitiveStringAtPut, 2 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genPrimitiveObjectAt, 1 },
	{ 0, -1 },
	{ genPrimitiveNew, 0 },
	{ genPrimitiveNewWithArg, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNewMethod, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitivePerform, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringReplace, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentical, 1 },
	{ genPrimitiveClass, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveShallowCopy, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringCompareWith, 1 },
	{ genPrimitiveHashMultiply, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIntegerAt, 1 },
	{ genPrimitiveIntegerAtPut, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNotIdentical, 1 },
	{ genPrimitiveAsCharacter, -1 },
	{ genPrimitiveImmediateAsInteger, 0 },
	{ 0, -1 },
	{ genPrimitiveSlotAt, 1 },
	{ genPrimitiveSlotAtPut, 2 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ genPrimitiveClosureValue, 2 },
	{ genPrimitiveClosureValue, 3 },
	{ genPrimitiveClosureValue, 4 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveHighBit, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveUninitializedNewWithArg, 1 }
};
static sqInt primitiveIndex;
static sqInt processorLock;
static sqInt receiverTags;
static sqInt runtimeObjectRefIndex;
static AbstractInstruction * sendMiss;
static sqInt simNativeStackPtr;
static sqInt simSpillBase;
static SimStackEntry simStack[70];
static sqInt simStackPtr;
static AbstractInstruction * stackCheckLabel;
static AbstractInstruction * stackOverflowCall;
static sqInt superSendTrampolines[NumSendTrampolines];
static sqInt tempOop;
static char *trampolineAddresses[NumTrampolines*2];
static sqInt trampolineTableIndex;
static sqInt uncheckedEntryAlignment;
static usqInt unpairedMethodList;
static sqInt varBaseAddress;
static usqInt youngReferrers;
static unsigned char codeModified;
static unsigned char deadCode;
static unsigned char directedSendUsesBinding;
static unsigned char hasMovableLiteral;
static unsigned char hasYoungReferent;
static unsigned char inBlock;
static unsigned char needsFrame;
static unsigned char regArgsHaveBeenPushed;
static unsigned char useTwoPaths;
static AbstractInstruction aMethodLabel;
static AbstractInstruction * const backEnd = &aMethodLabel;
#if DUAL_MAPPED_CODE_ZONE
static void (*ceFlushDCache)(usqIntptr_t from, usqIntptr_t to);
#endif
static AbstractInstruction * const methodLabel = &aMethodLabel;
sqInt blockNoContextSwitchOffset;
sqInt breakPC;
sqInt cbEntryOffset;
sqInt cbNoSwitchEntryOffset;
sqInt ceBaseFrameReturnTrampoline;
sqInt ceCannotResumeTrampoline;
sqInt ceCheckForInterruptTrampoline;
sqInt ceReturnToInterpreterTrampoline;
#if !defined(cFramePointerInUse)
sqInt cFramePointerInUse;
#endif
sqInt cmEntryOffset;
sqInt cmNoCheckEntryOffset;
usqInt methodZoneBase;
sqInt missOffset;
int traceFlags = 8 /* prim trace log on by default */;
const char * traceFlagsMeanings[] = {
		"1: print trace", "2: trace sends", "4: trace block activations", "8: trace interpreter primitives",
		"16: trace events (context switches, GCs, etc)", "32: trace stack overflow (poll for events hook)",
		"64: trace linked sends", "128: trace fast C call interpreter primitives", null
	};
sqInt traceStores;
void (*ceCall0ArgsPIC)(void);
void (*ceCall1ArgsPIC)(void);
void (*ceCall2ArgsPIC)(void);
void (*ceCallCogCodePopReceiverAndClassRegs)(void);
void (*ceCallCogCodePopReceiverArg0Regs)(void);
void (*ceCallCogCodePopReceiverArg1Arg0Regs)(void);
void (*ceCallCogCodePopReceiverReg)(void);
void (*ceCaptureCStackPointers)(void);
void (*ceEnterCogCodePopReceiverReg)(void);
usqIntptr_t (*ceGetFP)(void);
usqIntptr_t (*ceGetSP)(void);
void (*ceInvokeInterpret)(void);
void (*realCECallCogCodePopReceiverAndClassRegs)(void);
void (*realCECallCogCodePopReceiverArg0Regs)(void);
void (*realCECallCogCodePopReceiverArg1Arg0Regs)(void);
void (*realCECallCogCodePopReceiverReg)(void);
void (*realCEEnterCogCodePopReceiverReg)(void);


/*** Macros ***/
#define inlineCacheValueForSelectorin(backEnd,selector,aCogMethod) (selector)
#define flushDCacheFromto(me,startAddress,endAddress) 0
#define flushICacheFromto(me,startAddress,endAddress) __clear_cache((char*) startAddress, (char*) (endAddress ))
#define roundUpToMethodAlignment(ignored,numBytes) (((numBytes) + 15) & -16)
#define cPICNumCases stackCheckOffset
#define cPICNumCasesHack hack hack hack i.e. the getter macro does all the work
#define abstractInstructionAt(index) (&abstractOpcodes[index])
#define addressIsInInstructions(address) (!((usqInt)(address) & (BytesPerWord-1)) \
							&& (address) >= &abstractOpcodes[0] \
							&& (address) < &abstractOpcodes[opcodeIndex])
#define allocateBlockStarts(numBlocks) do { \
		blockStarts = (numBlocks) ? alloca(sizeof(BlockStart) * (numBlocks)) : 0; \
} while (0)
#define assertValidDualZone() true
#define assertValidDualZoneReadAddress(address) 0
#define assertValidDualZoneWriteAddress(address) 0
#define backEnd() backEnd
#define blockAlignment() 8
#define blockStartAt(index) (&blockStarts[index])
#define ceBaseFrameReturnPC() ceBaseFrameReturnTrampoline
#define ceCannotResumePC() ((usqInt)ceCannotResumeTrampoline)
#define ceCheckForInterruptTrampoline() ceCheckForInterruptTrampoline
#define ceReturnToInterpreterPC() ((usqInt)ceReturnToInterpreterTrampoline)
#define codeByteAtput(address,value) byteAtput((address) + codeToDataDelta, value)
#define codeLong32Atput(address,value) long32Atput((address) + codeToDataDelta, value)
#define codeLong64Atput(address,value) long64Atput((address) + codeToDataDelta, value)
#define codeLongAtput(address,value) longAtput((address) + codeToDataDelta, value)
#define codeMemcpy(dest,src,bytes) memcpy(dest,src,bytes)
#define codeMemmove(dest,src,bytes) memmove((char *)(dest)+codeToDataDelta,src,bytes)
#define cr() putchar('\n')
#define entryOffset() cmEntryOffset
#define generatorAt(index) (&generatorTable[index])
#define getCodeToDataDelta() codeToDataDelta
#define getIsObjectReference() 2
#define halt() warning("halt")
#define haltmsg(msg) warning("halt: " msg)
#define interpretOffset() missOffset
#define mapPerMethodProfile() 0
#define maxCogCodeSize() (16*1024*1024)
#define maybeBreakGeneratingFromto(address,end) 0
#define maybeBreakGeneratingInstructionWithIndex(i) 0
#define maybeHaltIfDebugPC() 0
#define methodLabel() methodLabel
#define methodZoneBase() methodZoneBase
#define minCogMethodAddress() methodZoneBase
#define moveProfileToMethods() 0
#define noCheckEntryOffset() cmNoCheckEntryOffset
#define noContextSwitchBlockEntryOffset() blockNoContextSwitchOffset
#define notYetImplemented() warning("not yet implemented")
#define null 0
#define printNum(n) printf("%" PRIdSQINT, (sqInt) (n))
#define printOnTrace() (traceFlags & 1)
#define recordBlockTrace() (traceFlags & 4)
#define recordEventTrace() (traceFlags & 16)
#define recordFastCCallPrimTrace() (traceFlags & 128)
#define recordOverflowTrace() (traceFlags & 32)
#define recordPrimTrace() (traceFlags & 8)
#define recordSendTrace() (traceFlags & 2)
#define reportError(n) warning("compilation error")
#define setHasMovableLiteral(b) (hasMovableLiteral = (b))
#define setHasYoungReferent(b) (hasYoungReferent = (b))
#define varBaseAddress() varBaseAddress
#define nextOpenPIC methodObject
#define nextOpenPICHack hack hack hack i.e. the getter macro does all the work
#define freeStart() mzFreeStart
#define limitZony() ((CogMethod *)mzFreeStart)
#define methodBytesFreedSinceLastCompaction() methodBytesFreedSinceLastCompaction
#define youngReferrers() youngReferrers
#define numRegArgs() 1
#define maybeConstant(sse) ((sse)->constant)
#define literalInstructionAt(index) (&literals[index])
#define fullBlockEntryOffset() cbEntryOffset
#define fullBlockNoContextSwitchEntryOffset() cbNoSwitchEntryOffset
#define needsFrame() needsFrame
#define fixupAtIndex(index) (&fixups[index])
#define simNativeStackAt(index) (simNativeStack + (index))
#define simSelf() simStack
#define simStackAt(index) (simStack + (index))
#define traceDescriptor(ign) 0
#define traceFixupmerge(igu,ana) 0
#define traceMerge(ign) 0
#define traceSimStack() 0
#define traceSpill(ign) 0
#define allocatype(numElements, elementType) alloca((numElements)*sizeof(elementType))
#define numElementsIn(anArray) (sizeof(anArray)/sizeof(anArray[0]))
#define oopisGreaterThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) >= (usqInt)(otherOop))
#define oopisGreaterThanOrEqualToandLessThanOrEqualTo(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) <= (usqInt)(limitOop))
#define oopisGreaterThanOrEqualToandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisGreaterThan(anOop,otherOop) ((usqInt)(anOop) > (usqInt)(otherOop))
#define oopisGreaterThanandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) > (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisLessThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) <= (usqInt)(otherOop))
#define oopisLessThan(anOop,otherOop) ((usqInt)(anOop) < (usqInt)(otherOop))


/*** Methods ***/

	/* CogAbstractInstruction>>#addDependent: */
static NoDbgRegParms AbstractInstruction *
addDependent(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anInstruction)
{
	if ((self_in_CogAbstractInstruction->dependent)) {
		(anInstruction->dependent = (self_in_CogAbstractInstruction->dependent));
	}
	return ((self_in_CogAbstractInstruction->dependent) = anInstruction);
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them. */

	/* CogAbstractInstruction>>#availableFloatRegisterOrNoneFor: */
static NoDbgRegParms sqInt
availableFloatRegisterOrNoneFor(AbstractInstruction *self_in_CogAbstractInstruction, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << DPFPReg0))) != 0))) {
		return DPFPReg0;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg1))) != 0))) {
		return DPFPReg1;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg2))) != 0))) {
		return DPFPReg2;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg3))) != 0))) {
		return DPFPReg3;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg4))) != 0))) {
		return DPFPReg4;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg5))) != 0))) {
		return DPFPReg5;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg6))) != 0))) {
		return DPFPReg6;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg7))) != 0))) {
		return DPFPReg7;
	}
	return NoReg;
}


/*	For out-of-line literal support, clone a literal from a literal. */

	/* CogAbstractInstruction>>#cloneLiteralFrom: */
static NoDbgRegParms AbstractInstruction *
cloneLiteralFrom(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *existingLiteral)
{
	assert((((existingLiteral->opcode)) == Literal)
	 && ((!((self_in_CogAbstractInstruction->dependent)))
	 && (!((self_in_CogAbstractInstruction->address)))));
	(self_in_CogAbstractInstruction->opcode) = Literal;
	(self_in_CogAbstractInstruction->annotation) = (existingLiteral->annotation);
	((self_in_CogAbstractInstruction->operands))[0] = (((existingLiteral->operands))[0]);
	((self_in_CogAbstractInstruction->operands))[1] = (((existingLiteral->operands))[1]);
	((self_in_CogAbstractInstruction->operands))[2] = (((existingLiteral->operands))[2]);
	return self_in_CogAbstractInstruction;
}


/*	Load the stack pointer register with that of the C stack, effecting
	a switch to the C stack. Used when machine code calls into the
	CoInterpreter run-time (e.g. to invoke interpreter primitives). */

	/* CogAbstractInstruction>>#genLoadCStackPointer */
static NoDbgRegParms sqInt
genLoadCStackPointer(AbstractInstruction *self_in_CogAbstractInstruction)
{
	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cStackPointerAddress(), genoperandoperand(MoveAwR, cStackPointerAddress(), NativeSPReg));
	return 0;
}


/*	Load the frame and stack pointer registers with those of the C stack,
	effecting a switch to the C stack. Used when machine code calls into
	the CoInterpreter run-time (e.g. to invoke interpreter primitives).
	N.B. CoInterpreter stack layout dictates that the stack pointer should be
	loaded first.
	The stack zone is allocated on the C stack before the interpreter runs and
	hence before CStackPointer and CFramePointer are captured. So when running
	in machine
	code the native stack pointer and frame pointer appear to be on a colder
	part of the
	stack to CStackPointer and CFramePointer. When CStackPointerhas been set
	and the frame pointer is still in machine code the current frame looks
	like it has lots of
	stack. If the frame pointer was set to CFramePointer before hand then it
	would be beyond the stack pointer for that one instruction. */

	/* CogAbstractInstruction>>#genLoadCStackPointers */
static NoDbgRegParms sqInt
genLoadCStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{
	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cStackPointerAddress(), genoperandoperand(MoveAwR, cStackPointerAddress(), NativeSPReg));

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cFramePointerAddress(), genoperandoperand(MoveAwR, cFramePointerAddress(), FPReg));
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointerForPrimCall: */
static NoDbgRegParms sqInt
genLoadStackPointerForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg)
{
	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(stackPointerAddress(), genoperandoperand(MoveAwR, stackPointerAddress(), SPReg));
	return 0;
}


/*	Switch back to the Smalltalk stack. Assign SPReg first
	because typically it is used immediately afterwards. */

	/* CogAbstractInstruction>>#genLoadStackPointers */
static NoDbgRegParms sqInt
genLoadStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{
	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(stackPointerAddress(), genoperandoperand(MoveAwR, stackPointerAddress(), SPReg));

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(framePointerAddress(), genoperandoperand(MoveAwR, framePointerAddress(), FPReg));
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointersForPrimCall: */
static NoDbgRegParms sqInt
genLoadStackPointersForPrimCall(AbstractInstruction *self_in_CogAbstractInstruction, sqInt spareReg)
{
	genLoadStackPointers(self_in_CogAbstractInstruction);
	return 0;
}


/*	Save the frame and stack pointer registers to the framePointer
	and stackPointer variables. Used to save the machine code frame
	for use by the run-time when calling into the CoInterpreter run-time. */

	/* CogAbstractInstruction>>#genSaveStackPointers */
static NoDbgRegParms sqInt
genSaveStackPointers(AbstractInstruction *self_in_CogAbstractInstruction)
{
	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(framePointerAddress(), genoperandoperand(MoveRAw, FPReg, framePointerAddress()));

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(stackPointerAddress(), genoperandoperand(MoveRAw, SPReg, stackPointerAddress()));
	return 0;
}


/*	Generic register swap code. Subclasses for processors that have a true
	exchange operation will override to use it. */

	/* CogAbstractInstruction>>#genSwapR:R:Scratch: */
static NoDbgRegParms AbstractInstruction *
genSwapRRScratch(AbstractInstruction *self_in_CogAbstractInstruction, sqInt regA, sqInt regB, sqInt regTmp)
{
    AbstractInstruction *first;

	first = genoperandoperand(MoveRR, regA, regTmp);

	/* MoveR:R: */
	genoperandoperand(MoveRR, regB, regA);

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, regB);
	return first;
}

	/* CogAbstractInstruction>>#genWriteCResultIntoReg: */
static NoDbgRegParms AbstractInstruction *
genWriteCResultIntoReg(AbstractInstruction *self_in_CogAbstractInstruction, sqInt abstractRegister)
{
	if ((abstractRegister != NoReg)
	 && (abstractRegister != ABIResultReg)) {
		genoperandoperand(MoveRR, ABIResultReg, abstractRegister);
	}
	return self_in_CogAbstractInstruction;
}


/*	For out-of-line literal support, initialize a sharable literal. */

	/* CogAbstractInstruction>>#initializeSharableLiteral: */
static NoDbgRegParms AbstractInstruction *
initializeSharableLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal)
{
	(self_in_CogAbstractInstruction->opcode) = Literal;

	/* separate := nil for Slang */
	(self_in_CogAbstractInstruction->annotation) = null;
	(self_in_CogAbstractInstruction->address) = null;
	(self_in_CogAbstractInstruction->dependent) = null;
	((self_in_CogAbstractInstruction->operands))[0] = literal;
	((self_in_CogAbstractInstruction->operands))[1] = (1 + ((((usqInt)(BytesPerOop) << 1))));
	((self_in_CogAbstractInstruction->operands))[2] = -1;
	return self_in_CogAbstractInstruction;
}


/*	For out-of-line literal support, initialize an unsharable literal. */

	/* CogAbstractInstruction>>#initializeUniqueLiteral: */
static NoDbgRegParms AbstractInstruction *
initializeUniqueLiteral(AbstractInstruction *self_in_CogAbstractInstruction, sqInt literal)
{
	(self_in_CogAbstractInstruction->opcode) = Literal;

	/* separate := nil for Slang */
	(self_in_CogAbstractInstruction->annotation) = null;
	(self_in_CogAbstractInstruction->address) = null;
	(self_in_CogAbstractInstruction->dependent) = null;
	((self_in_CogAbstractInstruction->operands))[0] = literal;
	((self_in_CogAbstractInstruction->operands))[1] = (0 + ((((usqInt)(BytesPerOop) << 1))));
	((self_in_CogAbstractInstruction->operands))[2] = -1;
	return self_in_CogAbstractInstruction;
}

	/* CogAbstractInstruction>>#isAnInstruction: */
static NoDbgRegParms sqInt
isAnInstruction(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *addressOrInstruction)
{
	return (addressIsInInstructions(addressOrInstruction))
	 || (addressOrInstruction == (methodLabel));
}

	/* CogAbstractInstruction>>#isJump */
static NoDbgRegParms int
isJump(AbstractInstruction *self_in_CogAbstractInstruction)
{
	return ((((self_in_CogAbstractInstruction->opcode)) >= FirstJump) && (((self_in_CogAbstractInstruction->opcode)) <= LastJump));
}


/*	Set the target of a jump instruction. These all have the target in the
	first operand. */

	/* CogAbstractInstruction>>#jmpTarget: */
static NoDbgRegParms AbstractInstruction *
jmpTarget(AbstractInstruction *self_in_CogAbstractInstruction, AbstractInstruction *anAbstractInstruction)
{
	((self_in_CogAbstractInstruction->operands))[0] = (((usqInt)anAbstractInstruction));
	return anAbstractInstruction;
}


/*	Answer the constant loaded by the instruction sequence just before this
	address: 
 */

	/* CogAbstractInstruction>>#literal32BeforeFollowingAddress: */
static NoDbgRegParms sqInt
literal32BeforeFollowingAddress(AbstractInstruction *self_in_CogAbstractInstruction, sqInt followingAddress)
{
	return literalBeforeFollowingAddress(self_in_CogAbstractInstruction, followingAddress);
}


/*	We assume here that calls and jumps look the same as regards their
	displacement. This works on at least x86, ARM and x86_64. Processors on
	which that isn't the
	case can override as necessary. */

	/* CogAbstractInstruction>>#relocateJumpLongBeforeFollowingAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateJumpLongBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta)
{
	relocateCallBeforeReturnPCby(self_in_CogAbstractInstruction, pc, delta);
	return self_in_CogAbstractInstruction;
}


/*	Relocate a long conditional jump before pc. Default to relocating a
	non-conditional jump.
	Processors that have different formats for conditional and unconditional
	jumps override. */

	/* CogAbstractInstruction>>#relocateJumpLongConditionalBeforeFollowingAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction *self_in_CogAbstractInstruction, sqInt pc, sqInt delta)
{
	relocateJumpLongBeforeFollowingAddressby(self_in_CogAbstractInstruction, pc, delta);
	return self_in_CogAbstractInstruction;
}

	/* CogAbstractInstruction>>#resolveJumpTarget */
static NoDbgRegParms AbstractInstruction *
resolveJumpTarget(AbstractInstruction *self_in_CogAbstractInstruction)
{
    BytecodeFixup *fixup;

	assert(isJump(self_in_CogAbstractInstruction));
	fixup = ((BytecodeFixup *) (((self_in_CogAbstractInstruction->operands))[0]));
	if (addressIsInFixups(fixup)) {
		assert(addressIsInInstructions((fixup->targetInstruction)));
		jmpTarget(self_in_CogAbstractInstruction, (fixup->targetInstruction));
	}
	return self_in_CogAbstractInstruction;
}


/*	Rewrite a conditional jump long to jump to target. This version defaults
	to using
	rewriteJumpLongAt:, which works for many ISAs. Subclasses override if
	necessary.  */

	/* CogAbstractInstruction>>#rewriteConditionalJumpLongAt:target: */
static NoDbgRegParms sqInt
rewriteConditionalJumpLongAttarget(AbstractInstruction *self_in_CogAbstractInstruction, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteJumpLongAttarget(self_in_CogAbstractInstruction, callSiteReturnAddress, callTargetAddress);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	ADDS destReg, srcReg, #immediate ROR #rot - ARM_ARM v7 DDI10406 p. A8-23 */

	/* CogARMCompiler>>#adds:rn:imm:ror: */
static NoDbgRegParms usqInt
addsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc.
	ADD destReg, srcReg, #immediate ROR #rot - ARM_ARM v7 DDI10406 p. A8-23 */

	/* CogARMCompiler>>#add:rn:imm:ror: */
static NoDbgRegParms usqInt
addrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AddOpcode) << 21))) | (0)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	return an ADD destReg, srcReg, addReg instruction
	ADD destReg, srcReg, addReg - ARM_ARM v7 DDI10406 p. A8-24 */

	/* CogARMCompiler>>#add:rn:rm: */
static NoDbgRegParms usqInt
addrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt addReg)
{
	return ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | (addReg & 0xFFF);
}


/*	Answer the address of the __aeabi_idivmod() call provided by the ARM low
	level libs to do an integer divide that returns the quo in R0 and rem in
	R1. A word on the somewhat strange usage of idivmod herein; we need a
	declaration for the _aeabi_idivmod helper function, despite the fact that
	in a simple C program test, you don't.
	To get that declaration we need a variable to hang it off; thus the
	non-existent var idivmod, and in simulation we need to simulate it, which
	is what aeabiDiv:Mod: does.
 */

	/* CogARMCompiler>>#aeabiDivModFunctionAddr */
static NoDbgRegParms usqInt
aeabiDivModFunctionAddr(AbstractInstruction *self_in_CogARMCompiler)
{
    extern void __aeabi_idivmod(int dividend, int divisor);

	return (usqInt)__aeabi_idivmod;
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	ANDS destReg, srcReg, #immediate ROR #rot - ARM_ARM v7 DDI10406 p. A8-34 */

	/* CogARMCompiler>>#ands:rn:imm:ror: */
static NoDbgRegParms usqInt
andsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	AND destReg, srcReg, #immediate ROR #rot - ARM_ARM v7 DDI10406 p. A8-34 */

	/* CogARMCompiler>>#and:rn:imm:ror: */
static NoDbgRegParms usqInt
andrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AndOpcode) << 21))) | (0)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them.
	N.B. Do /not/ allocate TempReg. */
/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them.
	N.B. Do /not/ allocate TempReg. */

	/* CogARMCompiler>>#availableRegisterOrNoneFor: */
static NoDbgRegParms sqInt
availableRegisterOrNoneFor(AbstractInstruction *self_in_CogARMCompiler, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << Extra0Reg))) != 0))) {
		return Extra0Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra1Reg))) != 0))) {
		return Extra1Reg;
	}
	if (!(((liveRegsMask & ((1U << Extra2Reg))) != 0))) {
		return Extra2Reg;
	}
	if (!(((liveRegsMask & ((1U << Arg1Reg))) != 0))) {
		return Arg1Reg;
	}
	if (!(((liveRegsMask & ((1U << Arg0Reg))) != 0))) {
		return Arg0Reg;
	}
	if (!(((liveRegsMask & ((1U << SendNumArgsReg))) != 0))) {
		return SendNumArgsReg;
	}
	if (!(((liveRegsMask & ((1U << ClassReg))) != 0))) {
		return ClassReg;
	}
	if (!(((liveRegsMask & ((1U << ReceiverResultReg))) != 0))) {
		return ReceiverResultReg;
	}
	return NoReg;
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	BICS destReg, srcReg, #immediate ROR #rot - ARM_ARM v7 DDI10406 pp.
	A8-50-1 
 */

	/* CogARMCompiler>>#bics:rn:imm:ror: */
static NoDbgRegParms usqInt
bicsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(BicOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	return a BL offset instruction; offset is signed 24bits of WORD offset, so
	+_32Mbyte range. Return address is in LR
	BL offset - ARM_ARM v7 DDI10406 pp. A8-58-9
 */

	/* CogARMCompiler>>#bl: */
static NoDbgRegParms usqInt
bl(AbstractInstruction *self_in_CogARMCompiler, sqInt offset)
{
	return ((((usqInt)(AL) << 28))) | (((((usqInt)((10 | 1)) << 24))) | ((((usqInt)(offset)) >> 2) & 0xFFFFFF));
}


/*	return a B offset instruction; offset is signed 24bits of WORD offset, so
	+_32Mbyte range
	B offset - ARM_ARM v7 DDI10406 pp. A8-44-5
 */

	/* CogARMCompiler>>#b: */
static NoDbgRegParms usqInt
b(AbstractInstruction *self_in_CogARMCompiler, sqInt offset)
{
	return ((((usqInt)(AL) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offset)) >> 2) & 0xFFFFFF));
}


/*	ARMv8 calls and jumps span +/- 128 mb, more than enough for intra-zone
	calls and jumps.
 */

	/* CogARMCompiler>>#callInstructionByteSize */
static NoDbgRegParms sqInt
callInstructionByteSize(AbstractInstruction *self_in_CogARMCompiler)
{
	return 4;
}


/*	Answer the address that the call immediately preceding
	callSiteReturnAddress will jump to.
 */
/*	this is also used by #jumpLongTargetBeforeFollowingAddress:. */

	/* CogARMCompiler>>#callTargetFromReturnAddress: */
static NoDbgRegParms sqInt
callTargetFromReturnAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress)
{
    sqInt call;
    usqInt callDistance;

	call = longAt(callSiteReturnAddress - 4);
	assert((instructionIsB(self_in_CogARMCompiler, call))
	 || (instructionIsBL(self_in_CogARMCompiler, call)));

	/* begin extractOffsetFromBL: */
	callDistance = call & 0xFFFFFF;
	callDistance = (((callDistance & (0x800000)) != 0)
				? ((int) (((callDistance | 0x3F000000) << 2)))
				: (callDistance << 2));
	return (callSiteReturnAddress + 4) + (((int) callDistance));
}


/*	Because we don't use Thumb, each ARM instruction has 4 bytes. Many
	abstract opcodes need more than one instruction. Instructions that refer
	to constants and/or literals depend on literals being stored in-line or
	out-of-line. 
	N.B. The ^N forms are to get around the bytecode compiler's long branch
	limits which are exceeded when each case jumps around the otherwise. */

	/* CogARMCompiler>>#computeMaximumSize */
static NoDbgRegParms sqInt
computeMaximumSize(AbstractInstruction *self_in_CogARMCompiler)
{
    sqInt constant;
    sqInt i;
    sqInt iSqInt;
    sqInt n;
    sqInt r;
    sqInt value;
    unsigned int value1;

	switch ((self_in_CogARMCompiler->opcode)) {
	case Label:
		return 0;

	case Literal:
	case Fill32:
	case Nop:
	case Call:
	case JumpR:
	case Jump:
	case JumpLong:
	case JumpZero:
	case JumpNonZero:
	case JumpNegative:
	case JumpNonNegative:
	case JumpOverflow:
	case JumpNoOverflow:
	case JumpCarry:
	case JumpNoCarry:
	case JumpLess:
	case JumpGreaterOrEqual:
	case JumpGreater:
	case JumpLessOrEqual:
	case JumpBelow:
	case JumpAboveOrEqual:
	case JumpAbove:
	case JumpBelowOrEqual:
	case JumpLongZero:
	case JumpLongNonZero:
	case Stop:
	case AddRR:
	case AndRR:
	case CmpRR:
	case OrRR:
	case XorRR:
	case SubRR:
	case AddRRR:
	case SubRRR:
	case NegateR:
	case LogicalShiftLeftCqR:
	case LogicalShiftRightCqR:
	case ArithmeticShiftRightCqRR:
	case LogicalShiftRightCqRR:
	case LogicalShiftLeftCqRR:
	case ArithmeticShiftRightCqR:
	case LogicalShiftLeftRR:
	case LogicalShiftRightRR:
	case ArithmeticShiftRightRR:
	case AddRdRd:
	case CmpRdRd:
	case SubRdRd:
	case MulRdRd:
	case DivRdRd:
	case SqrtRd:
	case ClzRR:
	case SMULL:
	case MSR:
	case CMPSMULL:
	case PopLDM:
	case PushSTM:
	case MoveRR:
	case MoveRdRd:
	case MoveRdM64r:
	case MoveM64rRd:
	case MoveXbrRR:
	case MoveRXbrR:
	case MoveXwrRR:
	case MoveRXwrR:
	case PopR:
	case PushR:
		return 4;

	case AlignmentNops:
		return (((self_in_CogARMCompiler->operands))[0]) - 4;

	case CallFull:
	case JumpFull:
	case AddCwR:
	case AndCwR:
	case CmpCwR:
	case OrCwR:
	case SubCwR:
	case XorCwR:
		return 8 /* (literalLoadInstructionBytes + 4) */;

	case JumpFPEqual:
	case JumpFPNotEqual:
	case JumpFPLess:
	case JumpFPGreaterOrEqual:
	case JumpFPGreater:
	case JumpFPLessOrEqual:
	case JumpFPOrdered:
	case JumpFPUnordered:
	case ConvertRRd:
		return 8;

	case RetN:
		return (((self_in_CogARMCompiler->operands))[0]
				? 8
				: 4);

	case AddCqR:
	case AddCqRR:
	case CmpCqR:
	case SubCqR:
		constant = ((self_in_CogARMCompiler->operands))[0];

		/* begin rotateable8bitSignedImmediate:ifTrue:ifFalse: */
		value = constant;
		while (1) {
			if ((value & 0xFF) == value) {
				n = constant != value;
				return 4;
			}
			for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
				if ((value & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == value) {
					r = 32 - iSqInt;
					i = (((usqInt)(value)) >> iSqInt) | (((((usqInt)(value) << (32 - iSqInt)))) & 0xFFFFFFFFU);
					n = constant != value;
					return 4;
				}
			}
			if (!((value == constant)
			 && (constant != 0))) break;
			value = -constant;
		}
		return 8 /* (literalLoadInstructionBytes + 4) */;

	case AndCqR:
	case AndCqRR:
	case XorCqR:
		constant = ((self_in_CogARMCompiler->operands))[0];

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value1 = constant;
		while (1) {
			if ((value1 & 0xFF) == value1) {
				n = constant != value1;
				return 4;
			}
			for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
				if ((value1 & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == value1) {
					r = 32 - iSqInt;
					i = ((value1) >> iSqInt) | (((value1 << (32 - iSqInt))) & 0xFFFFFFFFU);
					n = constant != value1;
					return 4;
				}
			}
			if (!(value1 == constant)) break;
			value1 = (constant < 0
						? -1 - constant
						: (unsigned int)~constant);
		}
		return 8;

	case OrCqR:
	case TstCqR:
	case LoadEffectiveAddressMwrR:
	case MoveM16rR:
		constant = ((self_in_CogARMCompiler->operands))[0];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((constant & 0xFF) == constant) {
			return 4;
		}
		for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
			if ((constant & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == constant) {
				r = 32 - iSqInt;
				i = (((usqInt)(constant)) >> iSqInt) | (((((usqInt)(constant) << (32 - iSqInt)))) & 0xFFFFFFFFU);
				return 4;
			}
		}
		return 8 /* (literalLoadInstructionBytes + 4) */;

	case MoveCqR:
		return 4 /* literalLoadInstructionBytes */;

	case MoveCwR:
		return 4 /* literalLoadInstructionBytes */;

	case MoveAwR:
	case MoveAbR:
	case PrefetchAw:
		return (/* isAddressRelativeToVarBase: */
			((((self_in_CogARMCompiler->operands))[0]) >= (varBaseAddress))
		 && (((((self_in_CogARMCompiler->operands))[0]) - (varBaseAddress)) < (0x1000))
				? 4
				: 8 /* (literalLoadInstructionBytes + 4) */);

	case MoveRAw:
	case MoveRAb:
		return (/* isAddressRelativeToVarBase: */
			((((self_in_CogARMCompiler->operands))[1]) >= (varBaseAddress))
		 && (((((self_in_CogARMCompiler->operands))[1]) - (varBaseAddress)) < (0x1000))
				? 4
				: 8 /* (literalLoadInstructionBytes + 4) */);

	case MoveRMwr:
	case MoveRMbr:
	case MoveRM16r:
		constant = ((self_in_CogARMCompiler->operands))[1];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(constant)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (constant >= 0) {
				return 4;
			}
			else {
				i = SQABS(constant);
				return 4;
			}
		}
		else {
			return 8 /* (literalLoadInstructionBytes + 4) */;
		}

	case MoveMbrR:
	case MoveMwrR:
		constant = ((self_in_CogARMCompiler->operands))[0];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(constant)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (constant >= 0) {
				return 4;
			}
			else {
				i = SQABS(constant);
				return 4;
			}
		}
		else {
			return 8 /* (literalLoadInstructionBytes + 4) */;
		}

	case PushCw:
		return 8 /* (literalLoadInstructionBytes + 4) */;

	case PushCq:
		return 8 /* (literalLoadInstructionBytes + 4) */;

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	Generate concrete machine code for the instruction at actualAddress,
	setting machineCodeSize, and answer the following address. */
/*	Generate concrete machine code for the instruction at actualAddress,
	setting machineCodeSize, and answer the following address. */

	/* CogARMCompiler>>#concretizeAt: */
static NoDbgRegParms sqInt
concretizeAt(AbstractInstruction *self_in_CogARMCompiler, sqInt actualAddress)
{
	assert((actualAddress % 4) == 0);
	(self_in_CogARMCompiler->address) = actualAddress;
	(self_in_CogARMCompiler->machineCodeSize) = dispatchConcretize(self_in_CogARMCompiler);
	assert((((self_in_CogARMCompiler->maxSize)) == null)
	 || (((self_in_CogARMCompiler->maxSize)) >= ((self_in_CogARMCompiler->machineCodeSize))));
	return actualAddress + ((self_in_CogARMCompiler->machineCodeSize));
}


/*	Generate a CMP a, b, ASR #31 instruction, specifically for comparing the
	resutls of SMULLs in genMulR:R:
 */

	/* CogARMCompiler>>#concretizeCMPSMULL */
static NoDbgRegParms sqInt
concretizeCMPSMULL(AbstractInstruction *self_in_CogARMCompiler)
{
    sqInt aWord;
    usqInt hiReg;
    usqInt loReg;

	hiReg = ((self_in_CogARMCompiler->operands))[0];
	loReg = ((self_in_CogARMCompiler->operands))[1];
	aWord = ((((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(CmpOpcode) << 21))) | (0x100000)))) | (((hiReg << 16)) | (0))) + (0xF80)) + (64)) + loReg;

	/* begin machineCodeAt:put: */
	((self_in_CogARMCompiler->machineCode))[0] = aWord;
	return 4;
}


/*	Concretize the current instruction, but with a condition. */

	/* CogARMCompiler>>#concretizeConditionalInstruction */
static NoDbgRegParms unsigned char
concretizeConditionalInstruction(AbstractInstruction *self_in_CogARMCompiler)
{
    sqInt aWord;
    sqInt i;
    sqInt instr;
    unsigned char savedCond;

	assert(((self_in_CogARMCompiler->conditionOrNil)));
	savedCond = (self_in_CogARMCompiler->conditionOrNil);
	(self_in_CogARMCompiler->conditionOrNil) = null;
	(self_in_CogARMCompiler->machineCodeSize) = dispatchConcretize(self_in_CogARMCompiler);
	(self_in_CogARMCompiler->conditionOrNil) = savedCond;
	for (i = 0; i < ((self_in_CogARMCompiler->machineCodeSize)); i += 4) {
		instr = (((((self_in_CogARMCompiler->machineCode))[i / 4]) | (0xF0000000U)) - (0xF0000000U));
		aWord = instr | (((((self_in_CogARMCompiler->conditionOrNil)) & 15) << 28));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[i / 4] = aWord;
	}
	return (self_in_CogARMCompiler->machineCodeSize);
}


/*	fill with operand 0 according to the processor's endianness */

	/* CogARMCompiler>>#concretizeFill32 */
static NoDbgRegParms sqInt
concretizeFill32(AbstractInstruction *self_in_CogARMCompiler)
{
    sqInt aWord;

	aWord = ((self_in_CogARMCompiler->operands))[0];

	/* begin machineCodeAt:put: */
	((self_in_CogARMCompiler->machineCode))[0] = aWord;
	return 4;
}


/*	Generate an MSR CPSR_f, #flags instruction.
	Note that we only have business with the NZCV flags so we use
	N -> 8
	Z -> 4
	C -> 2
	V -> 1.
	You don't want to mess with this too much.
 */

	/* CogARMCompiler>>#concretizeMSR */
static NoDbgRegParms sqInt
concretizeMSR(AbstractInstruction *self_in_CogARMCompiler)
{
    usqInt flags;

	flags = ((self_in_CogARMCompiler->operands))[0];

	/* begin machineCodeAt:put: */
	((self_in_CogARMCompiler->machineCode))[0] = ((321450496) + (flags & 15));
	return 4;
}

	/* CogARMCompiler>>#concretizePushOrPopMultipleRegisters: */
static NoDbgRegParms sqInt
concretizePushOrPopMultipleRegisters(AbstractInstruction *self_in_CogARMCompiler, sqInt doPush)
{
	assert((((self_in_CogARMCompiler->operands))[0]) != 0);
	((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) + ((doPush
		? 0x9200000
		: 0x8B00000))) + ((((usqInt)(SP) << 16)))) + (((self_in_CogARMCompiler->operands))[0]));
	return 4;
}


/*	Generate an SMULL loResultReg, hiResultReg, srcA, srcB instruction */

	/* CogARMCompiler>>#concretizeSMULL */
static NoDbgRegParms sqInt
concretizeSMULL(AbstractInstruction *self_in_CogARMCompiler)
{
    sqInt aWord;
    sqInt hiResultReg;
    usqInt loResultReg;
    usqInt srcA;
    usqInt srcB;

	/* NOTE: srcB contains the other mutiplicand at this point. It is OK to use it as the destination for the low part of the result and in fact this saves us moving it later */
	srcA = ((self_in_CogARMCompiler->operands))[0];
	loResultReg = (srcB = ((self_in_CogARMCompiler->operands))[1]);
	hiResultReg = RISCTempReg;
	aWord = ((((((((usqInt)(AL) << 28))) | ((0) | ((0xC00000) | (0)))) | (((((usqInt)(hiResultReg) << 16))) | ((loResultReg << 12)))) + ((srcA << 8))) + (144)) + srcB;

	/* begin machineCodeAt:put: */
	((self_in_CogARMCompiler->machineCode))[0] = aWord;
	return 4;
}


/*	test for the NV condition code; this isn't allowed as an actual condition
	and is used to encdoe many of the newer instructions
 */

	/* CogARMCompiler>>#conditionIsNotNever: */
static NoDbgRegParms int
conditionIsNotNever(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (((usqInt)(instr)) >> 28) < 15;
}


/*	return an {opcode} destReg, srcReg, addReg lsl #shft */
/*	important detail - a 0 shft requires setting the shift-type code to 0 to
	avoid potential instruction confusion
 */

	/* CogARMCompiler>>#dataOpType:rd:rn:rm:lsr: */
static NoDbgRegParms usqInt
dataOpTyperdrnrmlsr(AbstractInstruction *self_in_CogARMCompiler, sqInt armOpcode, sqInt destReg, sqInt srcReg, sqInt addReg, sqInt shft)
{
	if (shft) {
		return ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(armOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | (((((((usqInt)(shft) << 7))) | 32) | addReg) & 0xFFF);
	}
	else {
		return ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(armOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | (addReg & 0xFFF);
	}
}


/*	Attempt to generate concrete machine code for the instruction at address.
	This is the inner dispatch of concretizeAt: actualAddress which exists
	only to get around the branch size limits in the SqueakV3 (blue book
	derived) bytecode set. */

	/* CogARMCompiler>>#dispatchConcretize */
static NoDbgRegParms sqInt
dispatchConcretize(AbstractInstruction *self_in_CogARMCompiler)
{
    usqInt addressOperand;
    sqInt aWord;
    usqInt base;
    usqInt baseReg;
    usqInt constant;
    AbstractInstruction *dependentChain;
    usqInt dest;
    usqInt destAddr;
    usqInt destReg;
    usqInt distance;
    usqInt distReg;
    usqInt dstReg;
    sqInt flagsOrOpcode;
    sqInt flagsOrOpcodeSqInt;
    usqInt fpReg;
    sqInt hb;
    sqInt hbSqInt;
    sqInt i;
    sqInt immediate;
    sqInt immediateSqInt;
    usqInt index;
    sqInt instrOffset;
    sqInt invert;
    unsigned int invVal;
    AbstractInstruction *jumpTarget;
    usqInt maskReg;
    sqInt negate;
    int offset;
    sqInt offsetSqInt;
    usqInt offsetUsqInt;
    sqInt p;
    int rd;
    sqInt rdSqInt;
    usqInt rdUsqInt;
    usqInt reg;
    usqInt regA;
    usqInt regB;
    usqInt regLHS;
    usqInt regRHS;
    usqInt rn;
    usqInt rn1;
    usqInt rnUsqInt;
    sqInt rot;
    sqInt rotSqInt;
    usqInt src;
    usqInt srcAddr;
    usqInt srcReg;
    int u;
    sqInt val;
    sqInt valSqInt;
    usqInt valUsqInt;
    unsigned int value;
    sqInt valueSqInt;
    sqInt word;
    usqInt wordUsqInt;

	if ((self_in_CogARMCompiler->conditionOrNil)) {
		return concretizeConditionalInstruction(self_in_CogARMCompiler);
	}
	switch ((self_in_CogARMCompiler->opcode)) {
	case Label:
		/* begin concretizeLabel */
		dependentChain = (self_in_CogARMCompiler->dependent);
		while (!(!dependentChain)) {
			/* begin updateLabel: */
			if (((dependentChain->opcode)) != Literal) {
				assert((((dependentChain->opcode)) == MoveCwR)
				 || (((dependentChain->opcode)) == PushCw));
				((dependentChain->operands))[0] = (((self_in_CogARMCompiler->address)) + (((self_in_CogARMCompiler->operands))[1]));
			}
			dependentChain = (dependentChain->dependent);
		}
		return 0;

	case Literal:
		return concretizeLiteral(self_in_CogARMCompiler);

	case AlignmentNops:
		/* begin concretizeAlignmentNops */
		assert((((self_in_CogARMCompiler->machineCodeSize)) % 4) == 0);
		for (p = 0; p < ((self_in_CogARMCompiler->machineCodeSize)); p += 4) {
			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[p / 4] = 0xE1A00000U;
		}
		return (self_in_CogARMCompiler->machineCodeSize);

	case Fill32:
		return concretizeFill32(self_in_CogARMCompiler);

	case Nop:
		/* begin concretizeNop */
		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 0xE1A00000U;
		return 4;

	case Call:
		/* begin concretizeCall */
		assert((((self_in_CogARMCompiler->operands))[0]) != 0);
		assert(((((self_in_CogARMCompiler->operands))[0]) % 4) == 0);

		/* normal pc offset */
		offset = (((int) (((self_in_CogARMCompiler->operands))[0]))) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offset));
		aWord = bl(self_in_CogARMCompiler, offset);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case CallFull:
		/* begin concretizeCallFull */
		/* begin longJumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((usqInt)(AL) << 28))) | ((0x12FFF10 | (32)) | ConcreteIPReg));
		assert(instrOffset == (literalLoadInstructionBytes(self_in_CogARMCompiler)));
		return instrOffset + 4;

	case JumpR:
		/* begin concretizeJumpR */
		/* bx reg */
		reg = ((self_in_CogARMCompiler->operands))[0];
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(AL) << 28))) | ((0x12FFF10 | (0)) | reg));
		return 4;

	case JumpFull:
		/* begin concretizeJumpFull */
		/* begin longJumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((usqInt)(AL) << 28))) | ((0x12FFF10 | (0)) | ConcreteIPReg));
		return instrOffset + 4;

	case JumpLong:
	case Jump:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(AL) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpLongZero:
	case JumpZero:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(EQ) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpLongNonZero:
	case JumpNonZero:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(NE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpNegative:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(MI) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpNonNegative:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(PL) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpOverflow:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(VS) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpNoOverflow:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(VC) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpCarry:
	case JumpAboveOrEqual:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(CS) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpNoCarry:
	case JumpBelow:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(CC) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpLess:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(LT) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpGreaterOrEqual:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(GE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpGreater:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(GT) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpLessOrEqual:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(LE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpAbove:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(HI) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpBelowOrEqual:
		/* begin concretizeConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + 8)));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(LS) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 4;

	case JumpFPEqual:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(EQ) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPNotEqual:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(NE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPLess:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(LT) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPGreaterOrEqual:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(GE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPGreater:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(GT) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPLessOrEqual:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(LE) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPOrdered:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(VC) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case JumpFPUnordered:
		/* begin concretizeFPConditionalJump: */
		/* begin computeJumpTargetOffsetPlus: */
		/* begin jumpTargetAddress */
		jumpTarget = ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]));
		assertSaneJumpTarget(jumpTarget);
		if (/* isAnInstruction: */
			(addressIsInInstructions(jumpTarget))
		 || (jumpTarget == (methodLabel))) {
			jumpTarget = ((AbstractInstruction *) ((jumpTarget->address)));
		}
		assert(jumpTarget != 0);
		offsetSqInt = (((int) jumpTarget)) - (((int) (((self_in_CogARMCompiler->address)) + (12))));
		assert(isInImmediateJumpRange(self_in_CogARMCompiler, offsetSqInt));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = 4008835600U /* fmstat */;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = (((((usqInt)(VS) << 28))) | (((((usqInt)((10)) << 24))) | ((((usqInt)(offsetSqInt)) >> 2) & 0xFFFFFF)));
		return 8;

	case RetN:
		/* begin concretizeRetN */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[0];
		if (!offsetSqInt) {
			aWord = movrn(self_in_CogARMCompiler, PC, LR);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}
		assert(offsetSqInt < 0xFF);
		aWord = addrnimmror(self_in_CogARMCompiler, SP, SP, offsetSqInt, 0);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		aWord = movrn(self_in_CogARMCompiler, PC, LR);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[1] = aWord;
		return 8;

	case Stop:
		/* begin concretizeStop */
		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((usqInt)(AL) << 28))) | ((0x1200000) | (112)));
		return 4;

	case AddCqR:
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin concretizeNegateableDataOperationCqR:R: */
		val = ((self_in_CogARMCompiler->operands))[0];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag
		   - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen. */
		rn = ((self_in_CogARMCompiler->operands))[1];
		valueSqInt = val;
		while (1) {
			if ((valueSqInt & 0xFF) == valueSqInt) {
				negate = val != valueSqInt;
				flagsOrOpcode = (negate
							? inverseOpcodeFor(self_in_CogARMCompiler, AddOpcode)
							: AddOpcode);

				/* begin type:op:set:rn:rd:shifterOperand: */
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | (((0) | valueSqInt) & 0xFFF);
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l4;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((valueSqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == valueSqInt) {
					rot = 32 - i;
					immediate = (((usqInt)(valueSqInt)) >> i) | (((((usqInt)(valueSqInt) << (32 - i)))) & 0xFFFFFFFFU);
					negate = val != valueSqInt;
					flagsOrOpcodeSqInt = (negate
								? inverseOpcodeFor(self_in_CogARMCompiler, AddOpcode)
								: AddOpcode);

					/* begin type:op:set:rn:rd:shifterOperand: */
					aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcodeSqInt) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l4;
				}
			}
			if (!((valueSqInt == val)
			 && (val != 0))) break;
			valueSqInt = -val;
		}
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, AddOpcode, rdSqInt, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;
l4:	/* end rotateable8bitSignedImmediate:ifTrue:ifFalse: */;
		return 0;

	case AddCqRR:
		rdSqInt = ((self_in_CogARMCompiler->operands))[2];

		/* begin concretizeNegateableDataOperationCqR:R: */
		val = ((self_in_CogARMCompiler->operands))[0];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag
		   - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen. */
		rn = ((self_in_CogARMCompiler->operands))[1];
		valueSqInt = val;
		while (1) {
			if ((valueSqInt & 0xFF) == valueSqInt) {
				negate = val != valueSqInt;
				flagsOrOpcode = (negate
							? inverseOpcodeFor(self_in_CogARMCompiler, AddOpcode)
							: AddOpcode);

				/* begin type:op:set:rn:rd:shifterOperand: */
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | (((0) | valueSqInt) & 0xFFF);
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l5;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((valueSqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == valueSqInt) {
					rot = 32 - i;
					immediate = (((usqInt)(valueSqInt)) >> i) | (((((usqInt)(valueSqInt) << (32 - i)))) & 0xFFFFFFFFU);
					negate = val != valueSqInt;
					flagsOrOpcodeSqInt = (negate
								? inverseOpcodeFor(self_in_CogARMCompiler, AddOpcode)
								: AddOpcode);

					/* begin type:op:set:rn:rd:shifterOperand: */
					aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcodeSqInt) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l5;
				}
			}
			if (!((valueSqInt == val)
			 && (val != 0))) break;
			valueSqInt = -val;
		}
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, AddOpcode, rdSqInt, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;
l5:	/* end rotateable8bitSignedImmediate:ifTrue:ifFalse: */;
		return 0;

	case AndCqR:
		/* begin concretizeInvertibleDataOperationCqR: */
		val = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		assert(!((((self_in_CogARMCompiler->opcode)) == CmpOpcode)));

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value = val;
		while (1) {
			if ((value & 0xFF) == value) {
				invert = val != value;
				flagsOrOpcode = (invert
							? inverseOpcodeFor(self_in_CogARMCompiler, AndOpcode)
							: AndOpcode);

				/* begin type:op:set:rn:rd:shifterOperand: */
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((rn << 12)))) | (((0) | value) & 0xFFF);
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l11;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
					rot = 32 - i;
					immediate = ((value) >> i) | (((value << (32 - i))) & 0xFFFFFFFFU);
					invert = val != value;
					flagsOrOpcodeSqInt = (invert
								? inverseOpcodeFor(self_in_CogARMCompiler, AndOpcode)
								: AndOpcode);

					/* begin type:op:set:rn:rd:shifterOperand: */
					aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcodeSqInt) << 21))) | (0x100000)))) | (((rn << 16)) | ((rn << 12)))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l11;
				}
			}
			if (!(value == val)) break;
			value = (val < 0
						? -1 - val
						: (unsigned int)~val);
		}
l11:	/* end rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */;
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, AndOpcode, rn, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCqR: */
		valSqInt = ((self_in_CogARMCompiler->operands))[0];
		rn1 = ((self_in_CogARMCompiler->operands))[1];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen */
		rd = (((self_in_CogARMCompiler->opcode)) == CmpOpcode
					? 0
					: rn1);

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((valSqInt & 0xFF) == valSqInt) {
			aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rn1 << 16)) | ((((usqInt)(rd) << 12))))) | (((0) | valSqInt) & 0xFFF);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l12;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((valSqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == valSqInt) {
				rotSqInt = 32 - i;
				immediateSqInt = (((usqInt)(valSqInt)) >> i) | (((((usqInt)(valSqInt) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rn1 << 16)) | ((((usqInt)(rd) << 12))))) | ((((((usqInt)((((usqInt)(rotSqInt)) >> 1)) << 8))) | immediateSqInt) & 0xFFF);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l12;
			}
		}
l12:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;
		if (valSqInt > 0) {
			hbSqInt = highBit(valSqInt);
			if ((1U << hbSqInt) == (valSqInt + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, AndOpcode, rd, rn1, ConcreteIPReg, 32 - hbSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case AndCqRR:
		/* begin concretizeAndCqRR */
		valUsqInt = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		dstReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value = valUsqInt;
		while (1) {
			if ((value & 0xFF) == value) {
				invert = valUsqInt != value;
				aWord = (invert
							? bicsrnimmror(self_in_CogARMCompiler, dstReg, srcReg, value, 0)
							: andsrnimmror(self_in_CogARMCompiler, dstReg, srcReg, value, 0));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l6;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
					rot = 32 - i;
					immediate = ((value) >> i) | (((value << (32 - i))) & 0xFFFFFFFFU);
					invert = valUsqInt != value;
					aWord = (invert
								? bicsrnimmror(self_in_CogARMCompiler, dstReg, srcReg, immediate, rot)
								: andsrnimmror(self_in_CogARMCompiler, dstReg, srcReg, immediate, rot));

					/* begin machineCodeAt:put: */
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l6;
				}
			}
			if (!(value == valUsqInt)) break;
			value = (valUsqInt < 0
						? -1 - valUsqInt
						: (unsigned int)~valUsqInt);
		}

		/* First see if the constant can be made from a simple shift of 0xFFFFFFFF */
		hb = highBit(((self_in_CogARMCompiler->operands))[0]);
		if ((1U << hb) == (valUsqInt + 1)) {

			/* MVN temp reg, 0, making 0xffffffff */
			aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, AndOpcode, dstReg, srcReg, ConcreteIPReg, 32 - hb);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[1] = aWord;
			return 8;
		}
l6:	/* end rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */;

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((dstReg << 12)))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case CmpCqR:
		/* begin concretizeNegateableDataOperationCqR:R: */
		val = ((self_in_CogARMCompiler->operands))[0];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag
		   - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen. */
		rn = ((self_in_CogARMCompiler->operands))[1];
		valueSqInt = val;
		while (1) {
			if ((valueSqInt & 0xFF) == valueSqInt) {
				negate = val != valueSqInt;
				flagsOrOpcode = (negate
							? inverseOpcodeFor(self_in_CogARMCompiler, CmpOpcode)
							: CmpOpcode);

				/* begin type:op:set:rn:rd:shifterOperand: */
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | (0))) | (((0) | valueSqInt) & 0xFFF);
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l7;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((valueSqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == valueSqInt) {
					rot = 32 - i;
					immediate = (((usqInt)(valueSqInt)) >> i) | (((((usqInt)(valueSqInt) << (32 - i)))) & 0xFFFFFFFFU);
					negate = val != valueSqInt;
					flagsOrOpcodeSqInt = (negate
								? inverseOpcodeFor(self_in_CogARMCompiler, CmpOpcode)
								: CmpOpcode);

					/* begin type:op:set:rn:rd:shifterOperand: */
					aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcodeSqInt) << 21))) | (0x100000)))) | (((rn << 16)) | (0))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l7;
				}
			}
			if (!((valueSqInt == val)
			 && (val != 0))) break;
			valueSqInt = -val;
		}
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, CmpOpcode, 0, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(CmpOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | (0))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;
l7:	/* end rotateable8bitSignedImmediate:ifTrue:ifFalse: */;
		return 0;

	case OrCqR:
		/* begin concretizeDataOperationCqR: */
		val = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen */
		rd = (((self_in_CogARMCompiler->opcode)) == CmpOpcode
					? 0
					: rn);

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((val & 0xFF) == val) {
			aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(OrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (((0) | val) & 0xFFF);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l13;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((val & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == val) {
				rot = 32 - i;
				immediate = (((usqInt)(val)) >> i) | (((((usqInt)(val) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(OrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l13;
			}
		}
l13:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, OrOpcode, rd, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(OrOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case SubCqR:
		/* begin concretizeSubCqR */
		word = ((self_in_CogARMCompiler->operands))[0];
		reg = ((self_in_CogARMCompiler->operands))[1];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((word & 0xFF) == word) {
			aWord = subsrnimmror(self_in_CogARMCompiler, reg, reg, word, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l8;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((word & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == word) {
				rot = 32 - i;
				immediate = (((usqInt)(word)) >> i) | (((((usqInt)(word) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = subsrnimmror(self_in_CogARMCompiler, reg, reg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l8;
			}
		}
l8:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if (((-word) & 0xFF) == (-word)) {
			immediate = -word;
			aWord = addsrnimmror(self_in_CogARMCompiler, reg, reg, immediate, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l9;
		}
		for (i = 2; i <= 30; i += 2) {
			if (((-word) & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == (-word)) {
				rot = 32 - i;
				immediate = (((usqInt)((-word))) >> i) | (((((usqInt)((-word)) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = addsrnimmror(self_in_CogARMCompiler, reg, reg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l9;
			}
		}
l9:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(SubOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((reg << 12)))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case TstCqR:
		/* begin concretizeTstCqR */
		reg = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((constant & 0xFF) == constant) {
			aWord = tstrnimmror(self_in_CogARMCompiler, reg, reg, constant, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l10;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((constant & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == constant) {
				rot = 32 - i;
				immediate = ((constant) >> i) | (((constant << (32 - i))) & 0xFFFFFFFFU);
				aWord = tstrnimmror(self_in_CogARMCompiler, reg, reg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l10;
			}
		}
l10:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;

		/* begin concretizeDataOperationCwR:R: */
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(TstOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((reg << 12)))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case XorCqR:
		/* begin concretizeInvertibleDataOperationCqR: */
		val = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		assert(!((((self_in_CogARMCompiler->opcode)) == CmpOpcode)));

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value = val;
		while (1) {
			if ((value & 0xFF) == value) {
				invert = val != value;
				flagsOrOpcode = (invert
							? inverseOpcodeFor(self_in_CogARMCompiler, XorOpcode)
							: XorOpcode);

				/* begin type:op:set:rn:rd:shifterOperand: */
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((rn << 12)))) | (((0) | value) & 0xFFF);
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l14;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
					rot = 32 - i;
					immediate = ((value) >> i) | (((value << (32 - i))) & 0xFFFFFFFFU);
					invert = val != value;
					flagsOrOpcodeSqInt = (invert
								? inverseOpcodeFor(self_in_CogARMCompiler, XorOpcode)
								: XorOpcode);

					/* begin type:op:set:rn:rd:shifterOperand: */
					aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(flagsOrOpcodeSqInt) << 21))) | (0x100000)))) | (((rn << 16)) | ((rn << 12)))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l14;
				}
			}
			if (!(value == val)) break;
			value = (val < 0
						? -1 - val
						: (unsigned int)~val);
		}
l14:	/* end rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */;
		if (val > 0) {
			hb = highBit(val);
			if ((1U << hb) == (val + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, XorOpcode, rn, rn, ConcreteIPReg, 32 - hb);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCqR: */
		valSqInt = ((self_in_CogARMCompiler->operands))[0];
		rn1 = ((self_in_CogARMCompiler->operands))[1];

		/* Extra note - if ever a version of this code wants to NOT set the Set flag - Cmp must always have it set or it will pretend to be a SMALALBT and Very Bad Things might happen */
		rd = (((self_in_CogARMCompiler->opcode)) == CmpOpcode
					? 0
					: rn1);

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((valSqInt & 0xFF) == valSqInt) {
			aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(XorOpcode) << 21))) | (0x100000)))) | (((rn1 << 16)) | ((((usqInt)(rd) << 12))))) | (((0) | valSqInt) & 0xFFF);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l15;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((valSqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == valSqInt) {
				rotSqInt = 32 - i;
				immediateSqInt = (((usqInt)(valSqInt)) >> i) | (((((usqInt)(valSqInt) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(XorOpcode) << 21))) | (0x100000)))) | (((rn1 << 16)) | ((((usqInt)(rd) << 12))))) | ((((((usqInt)((((usqInt)(rotSqInt)) >> 1)) << 8))) | immediateSqInt) & 0xFFF);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l15;
			}
		}
l15:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;
		if (valSqInt > 0) {
			hbSqInt = highBit(valSqInt);
			if ((1U << hbSqInt) == (valSqInt + 1)) {

				/* MVN temp,  #0, making 0xffffffff */
				aWord = mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, 0, 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				aWord = dataOpTyperdrnrmlsr(self_in_CogARMCompiler, XorOpcode, rd, rn1, ConcreteIPReg, 32 - hbSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[1] = aWord;
				return 8;
			}
		}

		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rnUsqInt = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(XorOpcode) << 21))) | (0x100000)))) | (((rnUsqInt << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case AddCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case AndCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case CmpCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = 0;
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(CmpOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case OrCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(OrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case SubCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(SubOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case XorCwR:
		/* begin concretizeDataOperationCwR: */
		rdSqInt = ((self_in_CogARMCompiler->operands))[1];
		constant = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(XorOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rdSqInt) << 12))))) | 12 /* (ConcreteIPReg bitAnd: 4095) */);
		return instrOffset + 4;

	case AddRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = rn;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case AndRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = rn;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AndOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case CmpRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = 0;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(CmpOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case OrRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = rn;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(OrOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case SubRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = rn;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(SubOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case XorRR:
		/* begin concretizeDataOperationRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rd = rn;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(XorOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((((usqInt)(rd) << 12))))) | (srcReg & 0xFFF));
		return 4;

	case AddRRR:
		/* begin concretizeDataOperationRRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rdUsqInt = ((self_in_CogARMCompiler->operands))[2];

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(AddOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((rdUsqInt << 12)))) | (srcReg & 0xFFF));
		return 4;

	case SubRRR:
		/* begin concretizeDataOperationRRR: */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		rn = ((self_in_CogARMCompiler->operands))[1];
		rdUsqInt = ((self_in_CogARMCompiler->operands))[2];

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(SubOpcode) << 21))) | (0x100000)))) | (((rn << 16)) | ((rdUsqInt << 12)))) | (srcReg & 0xFFF));
		return 4;

	case AddRdRd:
		/* begin concretizeAddRdRd */
		regRHS = ((self_in_CogARMCompiler->operands))[0];
		regLHS = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = (((0xEE300B00U | ((regLHS << 16))) | ((regLHS << 12))) | regRHS);
		return 4;

	case CmpRdRd:
		/* begin concretizeCmpRdRd */
		regA = ((self_in_CogARMCompiler->operands))[0];
		regB = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = ((4004776768U | ((regB << 12))) | regA);
		return 4;

	case DivRdRd:
		/* begin concretizeDivRdRd */
		regRHS = ((self_in_CogARMCompiler->operands))[0];
		regLHS = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = (((0xEE800B00U | ((regLHS << 16))) | ((regLHS << 12))) | regRHS);
		return 4;

	case MulRdRd:
		/* begin concretizeMulRdRd */
		regRHS = ((self_in_CogARMCompiler->operands))[0];
		regLHS = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = (((0xEE200B00U | ((regLHS << 16))) | ((regLHS << 12))) | regRHS);
		return 4;

	case SubRdRd:
		/* begin concretizeSubRdRd */
		regRHS = ((self_in_CogARMCompiler->operands))[0];
		regLHS = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = (((0xEE300B40U | ((regLHS << 16))) | ((regLHS << 12))) | regRHS);
		return 4;

	case SqrtRd:
		/* begin concretizeSqrtRd */
		regLHS = ((self_in_CogARMCompiler->operands))[0];
		((self_in_CogARMCompiler->machineCode))[0] = ((4004580288U | ((regLHS << 12))) | regLHS);
		return 4;

	case NegateR:
		/* begin concretizeNegateR */
		/* RSB destReg, srcReg, #0 */
		reg = ((self_in_CogARMCompiler->operands))[0];
		((self_in_CogARMCompiler->machineCode))[0] = ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(RsbOpcode) << 21))) | (0)))) | (((reg << 16)) | ((reg << 12))));
		return 4;

	case LoadEffectiveAddressMwrR:
		/* begin concretizeLoadEffectiveAddressMwrR */
		offsetUsqInt = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((offsetUsqInt & 0xFF) == offsetUsqInt) {
			aWord = addrnimmror(self_in_CogARMCompiler, destReg, srcReg, offsetUsqInt, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((offsetUsqInt & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == offsetUsqInt) {
				rot = 32 - i;
				immediate = ((offsetUsqInt) >> i) | (((offsetUsqInt << (32 - i))) & 0xFFFFFFFFU);
				aWord = addrnimmror(self_in_CogARMCompiler, destReg, srcReg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
		aWord = addrnrm(self_in_CogARMCompiler, destReg, srcReg, ConcreteIPReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case ArithmeticShiftRightCqR:
		/* begin concretizeArithmeticShiftRightCqR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		reg = ((self_in_CogARMCompiler->operands))[1];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((reg << 12)))) | ((((distance << 7)) | (64 | reg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftRightCqR:
		/* begin concretizeLogicalShiftRightCqR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		reg = ((self_in_CogARMCompiler->operands))[1];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((reg << 12)))) | ((((distance << 7)) | (32 | reg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftLeftCqR:
		/* begin concretizeLogicalShiftLeftCqR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		reg = ((self_in_CogARMCompiler->operands))[1];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((reg << 12)))) | ((((distance << 7)) | reg) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case ArithmeticShiftRightCqRR:
		/* begin concretizeArithmeticShiftRightCqRR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distance << 7)) | (64 | srcReg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftRightCqRR:
		/* begin concretizeLogicalShiftRightCqRR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distance << 7)) | (32 | srcReg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftLeftCqRR:
		/* begin concretizeLogicalShiftLeftCqRR */
		distance = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];
		assert(((distance >= 1) && (distance <= 0x1F)));
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distance << 7)) | srcReg) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case ArithmeticShiftRightRR:
		/* begin concretizeArithmeticShiftRightRR */
		distReg = ((self_in_CogARMCompiler->operands))[0];

		/* cond 000 1101 0 0000 destR distR 0101 srcR */
		destReg = ((self_in_CogARMCompiler->operands))[1];
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distReg << 8)) | (80 | destReg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftLeftRR:
		/* begin concretizeLogicalShiftLeftRR */
		distReg = ((self_in_CogARMCompiler->operands))[0];

		/* cond 000 1101 0 0000 dest dist 0001 srcR */
		destReg = ((self_in_CogARMCompiler->operands))[1];
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distReg << 8)) | (16 | destReg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case LogicalShiftRightRR:
		/* begin concretizeLogicalShiftRightRR */
		distReg = ((self_in_CogARMCompiler->operands))[0];

		/* cond 000 1101 0 0000 dest dist 0011 srcR */
		destReg = ((self_in_CogARMCompiler->operands))[1];
		aWord = ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((destReg << 12)))) | ((((distReg << 8)) | (48 | destReg)) & 0xFFF);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case ClzRR:
		/* begin concretizeClzRR */
		maskReg = ((self_in_CogARMCompiler->operands))[0];
		dest = ((self_in_CogARMCompiler->operands))[1];

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = (((((((usqInt)(AL) << 28))) + 0x16F0F10) + ((dest << 12))) + maskReg);
		return 4;

	case SMULL:
		return concretizeSMULL(self_in_CogARMCompiler);

	case CMPSMULL:
		return concretizeCMPSMULL(self_in_CogARMCompiler);

	case MSR:
		return concretizeMSR(self_in_CogARMCompiler);

	case PopLDM:
		return concretizePushOrPopMultipleRegisters(self_in_CogARMCompiler, 0);

	case PushSTM:
		return concretizePushOrPopMultipleRegisters(self_in_CogARMCompiler, 1);

	case MoveCqR:
		/* begin concretizeMoveCqR */
		word = ((self_in_CogARMCompiler->operands))[0];
		reg = ((self_in_CogARMCompiler->operands))[1];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((word & 0xFF) == word) {
			aWord = movimmror(self_in_CogARMCompiler, reg, word, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
			goto l1;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((word & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == word) {
				rot = 32 - i;
				immediate = (((usqInt)(word)) >> i) | (((((usqInt)(word) << (32 - i)))) & 0xFFFFFFFFU);
				aWord = movimmror(self_in_CogARMCompiler, reg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l1;
			}
		}
l1:	/* end rotateable8bitImmediate:ifTrue:ifFalse: */;
		invVal = (word < 0
					? -1 - word
					: (unsigned int)~word);

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((invVal & 0xFF) == invVal) {
			aWord = mvnimmror(self_in_CogARMCompiler, reg, invVal, 0);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}
		for (i = 2; i <= 30; i += 2) {
			if ((invVal & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == invVal) {
				rot = 32 - i;
				immediate = ((invVal) >> i) | (((invVal << (32 - i))) & 0xFFFFFFFFU);
				aWord = mvnimmror(self_in_CogARMCompiler, reg, immediate, rot);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		return loadCwInto(self_in_CogARMCompiler, ((self_in_CogARMCompiler->operands))[1]);

	case MoveCwR:
		return loadCwInto(self_in_CogARMCompiler, ((self_in_CogARMCompiler->operands))[1]);

	case MoveRR:
		/* begin concretizeMoveRR */
		srcReg = ((self_in_CogARMCompiler->operands))[0];

		/* cond 000 1101 0 0000 dest 0000 0000 srcR */
		destReg = ((self_in_CogARMCompiler->operands))[1];
		aWord = movrn(self_in_CogARMCompiler, destReg, srcReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case MoveAwR:
		/* begin concretizeMoveAwR */
		srcAddr = ((self_in_CogARMCompiler->operands))[0];
		destReg = ((self_in_CogARMCompiler->operands))[1];
		if (/* isAddressRelativeToVarBase: */
			(srcAddr >= (varBaseAddress))
		 && ((srcAddr - (varBaseAddress)) < (0x1000))) {
			aWord = ldrrnplusImm(self_in_CogARMCompiler, destReg, ConcreteVarBaseReg, srcAddr - (varBaseAddress));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
		aWord = ldrrnplusImm(self_in_CogARMCompiler, destReg, ConcreteIPReg, 0);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case MoveRAw:
		/* begin concretizeMoveRAw */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		destAddr = ((self_in_CogARMCompiler->operands))[1];
		if (/* isAddressRelativeToVarBase: */
			(destAddr >= (varBaseAddress))
		 && ((destAddr - (varBaseAddress)) < (0x1000))) {
			aWord = strrnplusImm(self_in_CogARMCompiler, srcReg, ConcreteVarBaseReg, destAddr - (varBaseAddress));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
		aWord = strrnplusImm(self_in_CogARMCompiler, srcReg, ConcreteIPReg, 0);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case MoveAbR:
		/* begin concretizeMoveAbR */
		srcAddr = ((self_in_CogARMCompiler->operands))[0];
		destReg = ((self_in_CogARMCompiler->operands))[1];
		if (/* isAddressRelativeToVarBase: */
			(srcAddr >= (varBaseAddress))
		 && ((srcAddr - (varBaseAddress)) < (0x1000))) {
			aWord = ldrbrnplusimm(self_in_CogARMCompiler, destReg, ConcreteVarBaseReg, 1, srcAddr - (varBaseAddress));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
		aWord = ldrbrnplusimm(self_in_CogARMCompiler, destReg, ConcreteIPReg, 1, 0);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case MoveRAb:
		/* begin concretizeMoveRAb */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		destAddr = ((self_in_CogARMCompiler->operands))[1];
		if (/* isAddressRelativeToVarBase: */
			(destAddr >= (varBaseAddress))
		 && ((destAddr - (varBaseAddress)) < (0x1000))) {
			aWord = strbrnplusimm(self_in_CogARMCompiler, srcReg, ConcreteVarBaseReg, 1, destAddr - (varBaseAddress));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
		aWord = strbrnplusimm(self_in_CogARMCompiler, srcReg, ConcreteIPReg, 1, 0);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case MoveMbrR:
		/* begin concretizeMoveMbrR */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (offsetSqInt >= 0) {
				aWord = ldrbrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = ldrbrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			if (/* isAddressRelativeToVarBase: */
				(offsetSqInt >= (varBaseAddress))
			 && ((offsetSqInt - (varBaseAddress)) < (0x1000))) {
				aWord = addsrnimmror(self_in_CogARMCompiler, ConcreteIPReg, ConcreteVarBaseReg, offsetSqInt - (varBaseAddress), 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			else {
				/* begin moveCw:intoR: */
				assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
				assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
				aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
							? 1
							: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			aWord = ldrbrnrm(self_in_CogARMCompiler, destReg, srcReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}

	case MoveRMbr:
		/* begin concretizeMoveRMbr */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		offsetSqInt = ((self_in_CogARMCompiler->operands))[1];
		baseReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (offsetSqInt >= 0) {
				aWord = strbrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = strbrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			if (/* isAddressRelativeToVarBase: */
				(offsetSqInt >= (varBaseAddress))
			 && ((offsetSqInt - (varBaseAddress)) < (0x1000))) {
				aWord = addsrnimmror(self_in_CogARMCompiler, ConcreteIPReg, ConcreteVarBaseReg, offsetSqInt - (varBaseAddress), 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			else {
				/* begin moveCw:intoR: */
				assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
				assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
				aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
							? 1
							: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			aWord = strbrnrm(self_in_CogARMCompiler, srcReg, baseReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}

	case MoveRM16r:
		/* begin concretizeMoveRM16r */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		offsetSqInt = ((self_in_CogARMCompiler->operands))[1];
		baseReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (offsetSqInt >= 0) {
				aWord = strhrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = strhrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			if (/* isAddressRelativeToVarBase: */
				(offsetSqInt >= (varBaseAddress))
			 && ((offsetSqInt - (varBaseAddress)) < (0x1000))) {
				aWord = addsrnimmror(self_in_CogARMCompiler, ConcreteIPReg, ConcreteVarBaseReg, offsetSqInt - (varBaseAddress), 0);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			else {
				/* begin moveCw:intoR: */
				assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
				assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
				aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
							? 1
							: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
			}
			aWord = strhrnrm(self_in_CogARMCompiler, srcReg, baseReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}

	case MoveM16rR:
		/* begin concretizeMoveM16rR */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is8BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFF) {

			/* (2 raisedTo: 8)-1 */
			if (offsetSqInt >= 0) {
				aWord = ldrhrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = ldrhrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			/* begin moveCw:intoR: */
			assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
			assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
			aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
						? 1
						: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			instrOffset = 4;
			aWord = ldrhrnrm(self_in_CogARMCompiler, destReg, srcReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}
		return 0;

	case MoveM64rRd:
		/* begin concretizeMoveM64rRd */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[0];
		u = (offsetSqInt > 0
					? 1
					: 0);
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];
		((self_in_CogARMCompiler->machineCode))[0] = ((((0xED100B00U | ((srcReg << 16))) | ((destReg << 12))) | ((((usqInt)(u) << 23)))) | (((usqInt)(offsetSqInt)) >> 2));
		return 4;

	case MoveMwrR:
		/* begin concretizeMoveMwrR */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[0];
		srcReg = ((self_in_CogARMCompiler->operands))[1];
		destReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (offsetSqInt >= 0) {
				aWord = ldrrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = ldrrnplusimm(self_in_CogARMCompiler, destReg, srcReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			/* begin moveCw:intoR: */
			assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
			assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
			aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
						? 1
						: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			instrOffset = 4;
			aWord = ldrrnrm(self_in_CogARMCompiler, destReg, srcReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}

	case MoveXbrRR:
		/* begin concretizeMoveXbrRR */
		/* index is number of *bytes* */
		index = ((self_in_CogARMCompiler->operands))[0];
		base = ((self_in_CogARMCompiler->operands))[1];

		/* LDRB	dest, [base, +index, LSL #0] */
		/* cond 011 1100 1 base dest 00000 00 0 inde */
		dest = ((self_in_CogARMCompiler->operands))[2];
		aWord = ldrbrnrm(self_in_CogARMCompiler, dest, base, index);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case MoveRXbrR:
		/* begin concretizeMoveRXbrR */
		src = ((self_in_CogARMCompiler->operands))[0];
		index = ((self_in_CogARMCompiler->operands))[1];

		/* str	b	src, [base, +index, LSL #0] */
		/* cond 011 1100 0 base srcR 00000 00 0 index */
		base = ((self_in_CogARMCompiler->operands))[2];
		aWord = strbrnrm(self_in_CogARMCompiler, src, base, index);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case MoveXwrRR:
		/* begin concretizeMoveXwrRR */
		index = ((self_in_CogARMCompiler->operands))[0];
		base = ((self_in_CogARMCompiler->operands))[1];

		/* LDR	dest, [base, +index, LSL #2] */
		/* cond 011 1100 1 base dest 00010 00 0 inde bulit by lowest level generator so we can do the lsl #2 on the index register */
		dest = ((self_in_CogARMCompiler->operands))[2];

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = ((0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0x100000) | ((((base & 15) << 16)) | ((((dest & 15) << 12)) | (0x100 | (index & 15)))))))))));
		return 4;

	case MoveRXwrR:
		/* begin concretizeMoveRXwrR */
		src = ((self_in_CogARMCompiler->operands))[0];

		/* index is number of *words* = 4* bytes */
		index = ((self_in_CogARMCompiler->operands))[1];

		/* str		src, [base, +index, LSL #2] */
		/* cond 011 1100 0 base srcR 00010 00 0 inde */
		base = ((self_in_CogARMCompiler->operands))[2];
		((self_in_CogARMCompiler->machineCode))[0] = ((0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0) | ((((base & 15) << 16)) | ((((src & 15) << 12)) | (0x100 | (index & 15)))))))))));
		return 4;

	case MoveRMwr:
		/* begin concretizeMoveRMwr */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		offsetSqInt = ((self_in_CogARMCompiler->operands))[1];
		baseReg = ((self_in_CogARMCompiler->operands))[2];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(offsetSqInt)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (offsetSqInt >= 0) {
				aWord = strrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 1, offsetSqInt);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
			else {
				immediate = SQABS(offsetSqInt);
				aWord = strrnplusimm(self_in_CogARMCompiler, srcReg, baseReg, 0, immediate);

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
			}
		}
		else {
			/* begin moveCw:intoR: */
			assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
			assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
			aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
						? 1
						: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			instrOffset = 4;
			aWord = strrnrm(self_in_CogARMCompiler, srcReg, baseReg, ConcreteIPReg);

			/* begin machineCodeAt:put: */
			((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
			return instrOffset + 4;
		}

	case MoveRdM64r:
		/* begin concretizeMoveRdM64r */
		offsetSqInt = ((self_in_CogARMCompiler->operands))[1];
		u = (offsetSqInt > 0
					? 1
					: 0);
		dstReg = ((self_in_CogARMCompiler->operands))[2];
		fpReg = ((self_in_CogARMCompiler->operands))[0];
		((self_in_CogARMCompiler->machineCode))[0] = ((((0xED000B00U | ((dstReg << 16))) | ((fpReg << 12))) | ((((usqInt)(u) << 23)))) | (((usqInt)(offsetSqInt)) >> 2));
		return 4;

	case PopR:
		/* begin concretizePopR */
		/* LDR destReg, [SP], #4 */
		destReg = ((self_in_CogARMCompiler->operands))[0];
		aWord = popR(self_in_CogARMCompiler, destReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case PushR:
		/* begin concretizePushR */
		/* cond | 010 | 1001 | 0 | -Rn- | -Rd- | 0000 0000 0100 */
		/* STR srcReg, [sp, #-4] */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		aWord = pushR(self_in_CogARMCompiler, srcReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		return 4;

	case PushCq:
		/* begin concretizePushCq */
		wordUsqInt = ((self_in_CogARMCompiler->operands))[0];
		value = wordUsqInt;
		while (1) {
			if ((value & 0xFF) == value) {
				invert = wordUsqInt != value;
				aWord = (invert
							? mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, value, 0)
							: movimmror(self_in_CogARMCompiler, ConcreteIPReg, value, 0));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				instrOffset = 4;
				goto l2;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
					rot = 32 - i;
					immediate = ((value) >> i) | (((value << (32 - i))) & 0xFFFFFFFFU);
					invert = wordUsqInt != value;
					aWord = (invert
								? mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, immediate, rot)
								: movimmror(self_in_CogARMCompiler, ConcreteIPReg, immediate, rot));

					/* begin machineCodeAt:put: */
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					instrOffset = 4;
					goto l2;
				}
			}
			if (!(value == wordUsqInt)) break;
			value = (wordUsqInt < 0
						? -1 - wordUsqInt
						: (unsigned int)~wordUsqInt);
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;
l2:	/* end rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */;
		aWord = pushR(self_in_CogARMCompiler, ConcreteIPReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case PushCw:
		/* begin concretizePushCw */
		wordUsqInt = ((self_in_CogARMCompiler->operands))[0];
		if (/* inCurrentCompilation: */
			(/* isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) wordUsqInt)))
		 || ((((AbstractInstruction *) wordUsqInt)) == (methodLabel)))
		 || (/* addressIsInCurrentCompilation: */
			((((usqInt)wordUsqInt)) >= ((methodLabel->address)))
		 && ((((usqInt)wordUsqInt)) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize)))))) {
			instrOffset = loadCwInto(self_in_CogARMCompiler, ConcreteIPReg);
		}
		else {
			/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
			value = wordUsqInt;
			while (1) {
				if ((value & 0xFF) == value) {
					invert = wordUsqInt != value;
					aWord = (invert
								? mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, value, 0)
								: movimmror(self_in_CogARMCompiler, ConcreteIPReg, value, 0));

					/* begin machineCodeAt:put: */
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					instrOffset = 4;
					goto l3;
				}
				for (i = 2; i <= 30; i += 2) {
					if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
						rot = 32 - i;
						immediate = ((value) >> i) | (((value << (32 - i))) & 0xFFFFFFFFU);
						invert = wordUsqInt != value;
						aWord = (invert
									? mvnimmror(self_in_CogARMCompiler, ConcreteIPReg, immediate, rot)
									: movimmror(self_in_CogARMCompiler, ConcreteIPReg, immediate, rot));

						/* begin machineCodeAt:put: */
						((self_in_CogARMCompiler->machineCode))[0] = aWord;
						instrOffset = 4;
						goto l3;
					}
				}
				if (!(value == wordUsqInt)) break;
				value = (wordUsqInt < 0
							? -1 - wordUsqInt
							: (unsigned int)~wordUsqInt);
			}
			instrOffset = loadCwInto(self_in_CogARMCompiler, ConcreteIPReg);
l3:	/* end rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */;
		}
		aWord = pushR(self_in_CogARMCompiler, ConcreteIPReg);

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = aWord;
		return instrOffset + 4;

	case PrefetchAw:
		/* begin concretizePrefetchAw */
		addressOperand = ((self_in_CogARMCompiler->operands))[0];
		if (/* isAddressRelativeToVarBase: */
			(addressOperand >= (varBaseAddress))
		 && ((addressOperand - (varBaseAddress)) < (0x1000))) {
			immediate = addressOperand - (varBaseAddress);

			/* begin pld:plus:offset: */
			aWord = 0xF550F000U | (((((usqInt)(ConcreteVarBaseReg) << 16))) | ((0x800000) | immediate));
			((self_in_CogARMCompiler->machineCode))[0] = aWord;
			return 4;
		}

		/* begin moveCw:intoR: */
		assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
		assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
		aWord = ldrrnplusimm(self_in_CogARMCompiler, ConcreteIPReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
					? 1
					: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[0] = aWord;
		instrOffset = 4;

		/* begin machineCodeAt:put: */
		((self_in_CogARMCompiler->machineCode))[instrOffset / 4] = (0xF550F000U | (((((usqInt)(ConcreteIPReg) << 16))) | ((0x800000))));
		return instrOffset + 4;

	case ConvertRRd:
		/* begin concretizeConvertRRd */
		srcReg = ((self_in_CogARMCompiler->operands))[0];
		destReg = ((self_in_CogARMCompiler->operands))[1];
		((self_in_CogARMCompiler->machineCode))[0] = (fmsrFromto(self_in_CogARMCompiler, srcReg, 9));
		((self_in_CogARMCompiler->machineCode))[1] = (fsitodFromto(self_in_CogARMCompiler, 9, destReg));
		return 8;

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	FMSR or VMOV instruction to move a value from an ARM reg to an fpu double
	register ready for conversion
	FMSR regB, regA - ARM_ARM v5 DDI 01001.pdf pp. C4-68
	VMOV regB, regA - ARM_ARM v7 DDi10406 pp. A8-462-3
 */
/*	the dest reg bits are spread out a little */

	/* CogARMCompiler>>#fmsrFrom:to: */
static NoDbgRegParms unsigned int
fmsrFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt regA, sqInt regB)
{
    usqInt destReg;

	destReg = ((((usqInt)((((usqInt)(regB)) >> 1)) << 16))) | (((regB & 1) << 7));
	return (0xEE000A10U | ((((usqInt)(regA) << 12)))) | destReg;
}


/*	FSITOD or VCVT instruction to move convert an integer value to an fpu
	double FSITOD regB, regA - ARM_ARM v5 DDI 01001.pdf pp. C4-95
	VCVTW. regB, regA - ARM_ARM v7 DDI10406.pdf pp. A8-576-8
 */
/*	the src reg bits are spread out a little */

	/* CogARMCompiler>>#fsitodFrom:to: */
static NoDbgRegParms unsigned int
fsitodFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt regA, sqInt regB)
{
    usqInt srcReg;

	srcReg = (((usqInt)(regA)) >> 1) | (((regA & 1) << 5));
	return (4005039040U | srcReg) | ((((usqInt)(regB) << 12)));
}


/*	Answer if CallFull and/or JumpFull are relative and hence need relocating
	on method
	compation. If so, they are annotated with IsRelativeCall in methods and
	relocated in
	relocateIfCallOrMethodReference:mcpc:delta: */

	/* CogARMCompiler>>#fullCallsAreRelative */
static NoDbgRegParms sqInt
fullCallsAreRelative(AbstractInstruction *self_in_CogARMCompiler)
{
	return 0;
}


/*	Currently no instruction level support for divide on ARM. See also
	#canDivQuoRem 
 */

	/* CogARMCompiler>>#genDivR:R:Quo:Rem: */
static NoDbgRegParms AbstractInstruction *
genDivRRQuoRem(AbstractInstruction *self_in_CogARMCompiler, sqInt abstractRegDivisor, sqInt abstractRegDividend, sqInt abstractRegQuotient, sqInt abstractRegRemainder)
{
    usqInt divRemFunctionAddr;
    AbstractInstruction *inst;
    sqInt rDividend;
    sqInt rDivisor;
    sqInt rQuotient;
    sqInt rRemainder;

	assert(abstractRegDividend != abstractRegDivisor);
	assert(abstractRegQuotient != abstractRegRemainder);
	rDividend = abstractRegDividend;
	rDivisor = abstractRegDivisor;
	if (rDividend) {

		/* we need to move the value in rDividend to CArg0Reg. Best to double check if rDivisor is already using it first */
		if (!rDivisor) {

			/* oh dear; we also need to move rDivisor's value out of the way first.. I'll move it to CArg1Reg and if some nitwit has managed to put rDividend there they deserve the crash */
			if (rDividend == CArg1Reg) {
				error("register choices in genDivR:R:Quo:Rem: made life impossible");
			}

			/* MoveR:R: */
			genoperandoperand(MoveRR, rDivisor, CArg1Reg);
			rDivisor = CArg1Reg;
		}

		/* MoveR:R: */
		genoperandoperand(MoveRR, rDividend, CArg0Reg);
	}
	if (!(rDivisor == CArg1Reg)) {
		/* MoveR:R: */
		genoperandoperand(MoveRR, rDivisor, CArg1Reg);
	}
	divRemFunctionAddr = aeabiDivModFunctionAddr(self_in_CogARMCompiler);

	/* begin saveAndRestoreLinkRegAround: */
	inst = genoperand(PushR, LinkReg);
	CallFullRTregistersToBeSavedMask(((usqInt)divRemFunctionAddr), (1U << CArg2Reg) | (1U << CArg3Reg));

	/* PopR: */
	genoperand(PopR, LinkReg);
	rQuotient = abstractRegQuotient;
	rRemainder = abstractRegRemainder;
	if (rQuotient) {

		/* oh good grief, not again */
		/* MoveR:R: */
		genoperandoperand(MoveRR, CArg0Reg, rQuotient);
		if (rQuotient == CArg1Reg) {
			error("register choices in genDivR:R:Quo:Rem: made life impossible");
		}
	}
	if (!(rRemainder == CArg1Reg)) {
		/* MoveR:R: */
		genoperandoperand(MoveRR, CArg1Reg, rRemainder);
	}
	return self_in_CogARMCompiler;
}


/*	Generate the code to pass up to four arguments in a C run-time call. Hack:
	each argument is
	either a negative number, which encodes a constant, or a non-negative
	number, that of a register.
	The encoding for constants is defined by trampolineArgConstant: &
	trampolineArgValue:. Pass a constant as the result of
	trampolineArgConstant:. 
	Run-time calls have no more than four arguments, so chosen so that on ARM,
	where in its C ABI the
	first four integer arguments are passed in registers, all arguments can be
	passed in registers. We
	defer to the back end to generate this code not so much that the back end
	knows whether it uses
	the stack or registers to pass arguments (it does, but...). In fact we
	defer for an extremely evil reason.
	Doing so allows the x64 (where up to 6 args are passed) to assign the
	register arguments in an order
	that allows some of the argument registers to be used for specific
	abstract registers, specifically
	ReceiverResultReg and ClassReg. This is evil, evil, evil, but also it's
	really nice to keep using the old
	register assignments the original author has grown accustomed to. */

	/* CogARMCompiler>>#genMarshallNArgs:arg:arg:arg:arg: */
static NoDbgRegParms AbstractInstruction *
genMarshallNArgsargargargarg(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3)
{
    AbstractInstruction *anInstruction;

	if (!numArgs) {
		return self_in_CogARMCompiler;
	}
	if (numArgs > 1) {
		if ((!(regOrConst1 < NoReg))
		 && (regOrConst1 == CArg0Reg)) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, regOrConst1, Extra0Reg);
			return genMarshallNArgsargargargarg(self_in_CogARMCompiler, numArgs, regOrConst0, Extra0Reg, regOrConst2, regOrConst3);
		}
		if (numArgs > 2) {
			if ((!(regOrConst2 < NoReg))
			 && ((regOrConst2 == CArg0Reg)
			 || (regOrConst2 == CArg1Reg))) {
				/* MoveR:R: */
				genoperandoperand(MoveRR, regOrConst2, Extra1Reg);
				return genMarshallNArgsargargargarg(self_in_CogARMCompiler, numArgs, regOrConst0, regOrConst1, Extra1Reg, regOrConst3);
			}
			if (numArgs > 3) {
				if ((!(regOrConst3 < NoReg))
				 && ((regOrConst3 == CArg0Reg)
				 || ((regOrConst3 == CArg1Reg)
				 || (regOrConst3 == CArg2Reg)))) {
					/* MoveR:R: */
					genoperandoperand(MoveRR, regOrConst3, Extra2Reg);
					return genMarshallNArgsargargargarg(self_in_CogARMCompiler, numArgs, regOrConst0, regOrConst1, regOrConst2, Extra2Reg);
				}
			}
		}
	}
	if (regOrConst0 < NoReg) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(-2 - regOrConst0));
		}
	}
	else {
		if (regOrConst0) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
		}
	}
	if (numArgs == 1) {
		return self_in_CogARMCompiler;
	}
	if (regOrConst1 < NoReg) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, -2 - regOrConst1, CArg1Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(-2 - regOrConst1));
		}
	}
	else {
		if (regOrConst1 != CArg1Reg) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, regOrConst1, CArg1Reg);
		}
	}
	if (numArgs == 2) {
		return self_in_CogARMCompiler;
	}
	if (regOrConst2 < NoReg) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, -2 - regOrConst2, CArg2Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(-2 - regOrConst2));
		}
	}
	else {
		if (regOrConst2 != CArg2Reg) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, regOrConst2, CArg2Reg);
		}
	}
	if (numArgs == 3) {
		return self_in_CogARMCompiler;
	}
	if (regOrConst3 < NoReg) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, -2 - regOrConst3, CArg3Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(-2 - regOrConst3));
		}
	}
	else {
		if (regOrConst3 != CArg3Reg) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, regOrConst3, CArg3Reg);
		}
	}
	return self_in_CogARMCompiler;
}


/*	Use SMULL to produce a 64-bit result, explicitly in RISCTempReg,regDest. -
	ARM_ARM v7 DDI10406 pp. A8-354-5
	By comparing RISCTempReg with regDest ASR 31(which effectively makes it 0
	or -1) we know that the result being EQ means the hi reg and the top bit
	of the lo reg are the same - ie no overflow. The condition code can then
	be forced to oVerflow by use of MSR APSR_nzcvq, #1, lsl 28
 */

	/* CogARMCompiler>>#genMulR:R: */
static NoDbgRegParms AbstractInstruction *
genMulRR(AbstractInstruction *self_in_CogARMCompiler, sqInt regSource, sqInt regDest)
{
    AbstractInstruction *first;

	/* result in RISCTempReg,regDest */
	first = genoperandoperand(SMULL, regSource, regDest);
	genoperandoperand(CMPSMULL, RISCTempReg, regDest);
	genoperand(MSR, 1);
	return first;
}


/*	Ensure that the register args are pushed before the outer and
	inner retpcs at an entry miss for arity <= self numRegArgs. The
	outer retpc is that of a call at a send site. The inner is the call
	from a method or PIC abort/miss to the trampoline. */
/*	Putting the receiver and args above the return address means the
	CoInterpreter has a single machine-code frame format which saves
	us a lot of work. */
/*	Iff there are register args convert
	sp		->	outerRetpc			(send site retpc)
	linkReg = innerRetpc			(PIC abort/miss retpc)
	to
	base	->	receiver
	(arg0)
	(arg1)
	sp		->	outerRetpc			(send site retpc)
	sp		->	linkReg/innerRetpc	(PIC abort/miss retpc) */

	/* CogARMCompiler>>#genPushRegisterArgsForAbortMissNumArgs: */
static NoDbgRegParms AbstractInstruction *
genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs)
{
    AbstractInstruction *anInstruction;

	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, 0, SPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}

		/* begin MoveR:Mw:r: */
		/* begin gen:operand:quickConstant:operand: */
		anInstruction = genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 0, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
		if (numArgs > 0) {
			/* PushR: */
			genoperand(PushR, Arg0Reg);
			if (numArgs > 1) {
				/* PushR: */
				genoperand(PushR, Arg1Reg);
			}
		}

		/* PushR: */
		genoperand(PushR, TempReg);
	}

	/* PushR: */
	genoperand(PushR, LinkReg);
	return self_in_CogARMCompiler;
}


/*	Ensure that the register args are pushed before the retpc for arity <=
	self numRegArgs.
 */
/*	This is easy on a RISC like ARM because the return address is in the link
	register. Putting
	the receiver and args above the return address means the CoInterpreter has
	a single
	machine-code frame format which saves us a lot of work
	NOTA BENE: we do NOT push the return address here, which means it must be
	dealt with later. */

	/* CogARMCompiler>>#genPushRegisterArgsForNumArgs:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction *self_in_CogARMCompiler, sqInt numArgs, sqInt ignored)
{
	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);

		/* PushR: */
		genoperand(PushR, ReceiverResultReg);
		if (numArgs > 0) {
			/* PushR: */
			genoperand(PushR, Arg0Reg);
			if (numArgs > 1) {
				/* PushR: */
				genoperand(PushR, Arg1Reg);
			}
		}
	}
	return self_in_CogARMCompiler;
}


/*	This is a no-op on ARM since the ABI passes up to 4 args in registers and
	trampolines currently observe that limit.
 */

	/* CogARMCompiler>>#genRemoveNArgsFromStack: */
static NoDbgRegParms sqInt
genRemoveNArgsFromStack(AbstractInstruction *self_in_CogARMCompiler, sqInt n)
{
	assert(n <= 4);
	return 0;
}


/*	Restore the registers in regMask as saved by genSaveRegs:. */

	/* CogARMCompiler>>#genRestoreRegs: */
static NoDbgRegParms AbstractInstruction *
genRestoreRegs(AbstractInstruction *self_in_CogARMCompiler, sqInt regMask)
{
	return /* genPopRegisterMask: */
		(regMask
			? genoperand(PopLDM, regMask)
			: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}


/*	Save the registers in regMask for a call into the C run-time from a
	trampoline 
 */

	/* CogARMCompiler>>#genSaveRegs: */
static NoDbgRegParms AbstractInstruction *
genSaveRegs(AbstractInstruction *self_in_CogARMCompiler, sqInt regMask)
{
	return /* genPushRegisterMask: */
		(regMask
			? genoperand(PushSTM, regMask)
			: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* CogARMCompiler>>#genSubstituteReturnAddress: */
static NoDbgRegParms AbstractInstruction *
genSubstituteReturnAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt retpc)
{
	return checkLiteralforInstruction(retpc, genoperandoperand(MoveCwR, retpc, LR));
}


/*	Answer if the processor has a dedicated callee-saved register to point to
	the base of commonly-accessed variables. On ARM we use R10 for this. */

	/* CogARMCompiler>>#hasVarBaseRegister */
static NoDbgRegParms sqInt
hasVarBaseRegister(AbstractInstruction *self_in_CogARMCompiler)
{
	return 1;
}


/*	Answer the instruction immediately preceding followingAddress. */

	/* CogARMCompiler>>#instructionBeforeAddress: */
static NoDbgRegParms sqInt
instructionBeforeAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt followingAddress)
{
	return longAt(followingAddress - 4);
}


/*	is this a BLX <targetReg> instruction? */

	/* CogARMCompiler>>#instructionIsBLX: */
static NoDbgRegParms sqInt
instructionIsBLX(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && ((instr & 0xFFFFFF0) == 19922736);
}


/*	is this a BL <offset> instruction? */

	/* CogARMCompiler>>#instructionIsBL: */
static NoDbgRegParms sqInt
instructionIsBL(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && ((instr & (0xF000000)) == (0xB000000));
}


/*	is this a BX <targetReg> instruction? */

	/* CogARMCompiler>>#instructionIsBX: */
static NoDbgRegParms sqInt
instructionIsBX(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && ((instr & 0xFFFFFF0) == 0x12FFF10);
}


/*	is this a B <offset> instruction? */

	/* CogARMCompiler>>#instructionIsB: */
static NoDbgRegParms sqInt
instructionIsB(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && ((instr & (0xF000000)) == (0xA000000));
}


/*	is this a CMP instruction? */

	/* CogARMCompiler>>#instructionIsCMP: */
static NoDbgRegParms sqInt
instructionIsCMP(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && (((((usqInt)(instr)) >> 21) & 0x7F) == CmpOpcode);
}


/*	is this any kind of LDR instruction? c.f. memMxr:reg:base:u:b:l:imm: */

	/* CogARMCompiler>>#instructionIsLDR: */
static NoDbgRegParms sqInt
instructionIsLDR(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && (((((usqInt)(instr)) >> 20) & 197) == 65);
}


/*	is this an ORR instruction? */

	/* CogARMCompiler>>#instructionIsOR: */
static NoDbgRegParms sqInt
instructionIsOR(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && (((((usqInt)(instr)) >> 21) & 0x7F) == (16 | OrOpcode));
}


/*	is this a push -str r??, [sp, #-4] - instruction? */

	/* CogARMCompiler>>#instructionIsPush: */
static NoDbgRegParms sqInt
instructionIsPush(AbstractInstruction *self_in_CogARMCompiler, sqInt instr)
{
	return (conditionIsNotNever(self_in_CogARMCompiler, instr))
	 && ((instr & 0xFFF0FFF) == 0x52D0004);
}


/*	Answer the instruction size at pc.Simple on ARM ;-) */

	/* CogARMCompiler>>#instructionSizeAt: */
static NoDbgRegParms sqInt
instructionSizeAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc)
{
	return 4;
}


/*	Several of the opcodes are inverses. Answer the inverse for an opcode if
	it has one.
	See Table A3-2 in sec A3.4 Data-processing instructions of the AARM. */

	/* CogARMCompiler>>#inverseOpcodeFor: */
static NoDbgRegParms sqInt
inverseOpcodeFor(AbstractInstruction *self_in_CogARMCompiler, sqInt armOpcode)
{
	switch (armOpcode) {
	case AddOpcode:
		return SubOpcode;

	case AndOpcode:
		return BicOpcode;

	case BicOpcode:
		return AndOpcode;

	case CmpOpcode:
		return CmpNotOpcode;

	case MoveOpcode:
		return MoveNotOpcode;

	case MoveNotOpcode:
		return MoveOpcode;

	case SubOpcode:
		return AddOpcode;

	default:
		error("opcode has no inverse");
		return -1;
	}
	return 0;
}


/*	Assuming mcpc is a send return pc answer if the instruction before it is a
	call (not a CallFull).
 */
/*	There are two types of calls: BL and/BLX encoding */

	/* CogARMCompiler>>#isCallPrecedingReturnPC: */
static NoDbgRegParms sqInt
isCallPrecedingReturnPC(AbstractInstruction *self_in_CogARMCompiler, sqInt mcpc)
{
    sqInt call;

	call = longAt(mcpc - 4);
	return (instructionIsBL(self_in_CogARMCompiler, call))
	 || (instructionIsBLX(self_in_CogARMCompiler, call));
}


/*	ARM calls and jumps span +/- 32 mb, more than enough for intra-zone calls
	and jumps.
 */

	/* CogARMCompiler>>#isInImmediateJumpRange: */
static NoDbgRegParms int
isInImmediateJumpRange(AbstractInstruction *self_in_CogARMCompiler, usqIntptr_t operand)
{
	return (((((int) operand)) >= -33554432) && ((((int) operand)) <= 0x1FFFFFC));
}

	/* CogARMCompiler>>#isJumpAt: */
static NoDbgRegParms sqInt
isJumpAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc)
{
    int instr;

	instr = long32At(pc);
	return (instructionIsB(self_in_CogARMCompiler, instr))
	 || (instructionIsBX(self_in_CogARMCompiler, instr));
}


/*	add xx, pc, blah or sub xx, pc, blah */

	/* CogARMCompiler>>#isPCRelativeValueLoad: */
static NoDbgRegParms sqInt
isPCRelativeValueLoad(AbstractInstruction *self_in_CogARMCompiler, unsigned int instr)
{
	return (((instr) >> 16) == 57999)
	 || (((instr) >> 16) == 57935);
}


/*	Answer if an address can be reached by a normal Call insruction.
	We assume this is true for 32-bit processors and expect 64-bit processors
	to answer false
	for values in the object memory, and perhaps true in the interpreter. */

	/* CogARMCompiler>>#isWithinCallRange: */
static NoDbgRegParms int
isWithinCallRange(AbstractInstruction *self_in_CogARMCompiler, sqInt anAddress)
{
	return isInImmediateJumpRange(self_in_CogARMCompiler, maximumDistanceFromCodeZone(anAddress));
}


/*	Branch/Call ranges. Jump[Cond] can be generated as short as possible.
	Call/Jump[Cond]Long must be generated
	in the same number of bytes irrespective of displacement since their
	targets may be updated, but they need only
	span 16Mb, the maximum size of the code zone. This allows e.g. ARM to use
	single-word call and jump instructions
	for most calls and jumps. CallFull/JumpFull must also be generated in the
	same number of bytes irrespective of
	displacement for the same reason, but they must be able to span the full
	(32-bit or 64-bit) address space because
	they are used to call code in the C runtime, which may be distant from the
	code zone
 */

	/* CogARMCompiler>>#jumpLongByteSize */
static NoDbgRegParms sqInt
jumpLongByteSize(AbstractInstruction *self_in_CogARMCompiler)
{
	return 4;
}

	/* CogARMCompiler>>#jumpLongConditionalByteSize */
static NoDbgRegParms sqInt
jumpLongConditionalByteSize(AbstractInstruction *self_in_CogARMCompiler)
{
	return jumpLongByteSize(self_in_CogARMCompiler);
}


/*	Answer the target address for the long jump immediately preceding mcpc */

	/* CogARMCompiler>>#jumpLongTargetBeforeFollowingAddress: */
static NoDbgRegParms sqInt
jumpLongTargetBeforeFollowingAddress(AbstractInstruction *self_in_CogARMCompiler, sqInt mcpc)
{
	return callTargetFromReturnAddress(self_in_CogARMCompiler, mcpc);
}

	/* CogARMCompiler>>#jumpTargetPCAt: */
static NoDbgRegParms usqInt
jumpTargetPCAt(AbstractInstruction *self_in_CogARMCompiler, sqInt pc)
{
    usqInt operand;
    int word;

	word = long32At(pc);
	operand = word & 0xFFFFFF;
	if (((operand & 0x800000) != 0)) {
		operand -= 0x1000000;
	}
	return ((operand * 4) + pc) + 8;
}


/*	LDRB destReg, [baseReg, 'u' immediate12bitValue] u=0 -> - ARM_ARM v7
	DDI10406 pp. A8-128-9
	Note that this is a very low level interface that does not check the sign
	of the immediate, nor validity. See for example #concretizeMoveMbrR */

	/* CogARMCompiler>>#ldrb:rn:plus:imm: */
static NoDbgRegParms usqInt
ldrbrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((((u & 1) << 23)) | ((0x400000) | ((0x100000) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	LDRB destReg, [baseReg, + offsetReg] - ARM_ARM v7 DDI10406 pp. A8-132-3
	The contents of offsetReg are assumed to be correctly signed */

	/* CogARMCompiler>>#ldrb:rn:rm: */
static NoDbgRegParms usqInt
ldrbrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg)
{
	return (0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0x400000) | ((0) | ((0x100000) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (offsetReg & 15)))))))));
}


/*	LDRH destReg, [baseReg, 'u' immediate8bitValue] u=0 -> subtract imm; =1 ->
	add imm - ARM_ARM v7 DDI10406 pp. A8-152-3 */

	/* CogARMCompiler>>#ldrh:rn:plus:imm: */
static NoDbgRegParms usqInt
ldrhrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate8bitValue)
{
	return ((((usqInt)(AL) << 28))) | ((0) | ((0x1000000) | (((((usqInt)(u) << 23))) | ((0x400000) | ((0) | ((0x100000) | (((((usqInt)(baseReg) << 16))) | (((((usqInt)(destReg) << 12))) | ((((immediate8bitValue & 240) << 4)) | ((176) | (immediate8bitValue & 15)))))))))));
}


/*	LDRH destReg, [baseReg, +offsetReg] - ARM_ARM v7 DDI10406 pp. A8-156-7
	The contents of offsetReg are assumed to be correctly signed */

	/* CogARMCompiler>>#ldrh:rn:rm: */
static NoDbgRegParms sqInt
ldrhrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg)
{
	return ((((usqInt)(AL) << 28))) | ((0) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0x100000) | (((((usqInt)(baseReg) << 16))) | (((((usqInt)(destReg) << 12))) | (176 | offsetReg)))))))));
}


/*	LDR destReg, [baseReg, +immediate12bitValue] - ARM_ARM v7 DDI10406 pp.
	A8-120-1 
 */

	/* CogARMCompiler>>#ldr:rn:plusImm: */
static NoDbgRegParms usqInt
ldrrnplusImm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((0x800000) | ((0) | ((0x100000) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	LDR destReg, [baseReg, immediate12bitValue] u=0 -> subtract imm; =1 -> add
	imm - ARM_ARM v7 DDI10406 pp. A8-120-1 */

	/* CogARMCompiler>>#ldr:rn:plus:imm: */
static NoDbgRegParms usqInt
ldrrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((((u & 1) << 23)) | ((0) | ((0x100000) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	LDR destReg, [baseReg, + offsetReg] - ARM_ARM v7 DDI10406 pp. A8-124-5
	The contents of offsetReg are assumed to be correctly signed */

	/* CogARMCompiler>>#ldr:rn:rm: */
static NoDbgRegParms usqInt
ldrrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt offsetReg)
{
	return (0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0x100000) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (offsetReg & 15)))))))));
}


/*	Load the operand into the destination register, answering
	the size of the instructions generated to do so. */

	/* CogARMCompiler>>#loadCwInto: */
static NoDbgRegParms sqInt
loadCwInto(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg)
{
    sqInt aWord;
    sqInt distance;
    sqInt i;
    sqInt immediate;
    sqInt negate;
    usqInt operand;
    sqInt rot;
    sqInt value;

	operand = ((self_in_CogARMCompiler->operands))[0];
	if (/* isAnInstruction: */
		(addressIsInInstructions(((AbstractInstruction *) operand)))
	 || ((((AbstractInstruction *) operand)) == (methodLabel))) {
		operand = ((((AbstractInstruction *) operand))->address);
	}
	if (/* addressIsInCurrentCompilation: */
		((((usqInt)operand)) >= ((methodLabel->address)))
	 && ((((usqInt)operand)) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize))))) {
		distance = operand - (((self_in_CogARMCompiler->address)) + 8);

		/* begin rotateable8bitSignedImmediate:ifTrue:ifFalse: */
		value = distance;
		while (1) {
			if ((value & 0xFF) == value) {
				negate = distance != value;
				aWord = (negate
							? subrnimmror(self_in_CogARMCompiler, destReg, PC, value, 0)
							: addrnimmror(self_in_CogARMCompiler, destReg, PC, value, 0));

				/* begin machineCodeAt:put: */
				((self_in_CogARMCompiler->machineCode))[0] = aWord;
				return 4;
				goto l1;
			}
			for (i = 2; i <= 30; i += 2) {
				if ((value & (((0xFFU << i) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - i)))) == value) {
					rot = 32 - i;
					immediate = (((usqInt)(value)) >> i) | (((((usqInt)(value) << (32 - i)))) & 0xFFFFFFFFU);
					negate = distance != value;
					aWord = (negate
								? subrnimmror(self_in_CogARMCompiler, destReg, PC, immediate, rot)
								: addrnimmror(self_in_CogARMCompiler, destReg, PC, immediate, rot));

					/* begin machineCodeAt:put: */
					((self_in_CogARMCompiler->machineCode))[0] = aWord;
					return 4;
					goto l1;
				}
			}
			if (!((value == distance)
			 && (distance != 0))) break;
			value = -distance;
		}
		assert(!((isAnInstruction(self_in_CogARMCompiler, ((AbstractInstruction *) (((self_in_CogARMCompiler->operands))[0]))))));
l1:	/* end rotateable8bitSignedImmediate:ifTrue:ifFalse: */;
	}

	/* begin moveCw:intoR: */
	assert(addressIsInCurrentCompilation((((self_in_CogARMCompiler->dependent))->address)));
	assert((SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8)))) < (0x1000));
	aWord = ldrrnplusimm(self_in_CogARMCompiler, destReg, PC, (((((self_in_CogARMCompiler->dependent))->address)) >= (((self_in_CogARMCompiler->address)) + 8)
				? 1
				: 0), SQABS((((((self_in_CogARMCompiler->dependent))->address)) - (((self_in_CogARMCompiler->address)) + 8))));

	/* begin machineCodeAt:put: */
	((self_in_CogARMCompiler->machineCode))[0] = aWord;
	return 4;
}


/*	Answer the byte size of a MoveCwR opcode's corresponding machine code
	when the argument is a PIC. This is for the self-reference at the end of a
	closed PIC. On ARM this is a single instruction pc-relative register load. */

	/* CogARMCompiler>>#loadPICLiteralByteSize */
static NoDbgRegParms sqInt
loadPICLiteralByteSize(AbstractInstruction *self_in_CogARMCompiler)
{
	return 4;
}

	/* CogARMCompiler>>#machineCodeWords */
static NoDbgRegParms sqInt
machineCodeWords(AbstractInstruction *self_in_CogARMCompiler)
{
	return (machineCodeBytes(self_in_CogARMCompiler)) / 4;
}


/*	MOVS destReg, srcReg - ARM_ARM v7 DDI10406 pp. A8-196-7 */

	/* CogARMCompiler>>#movs:rn: */
static NoDbgRegParms usqInt
movsrn(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg)
{
	return ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0x100000)))) | ((0) | ((((usqInt)(destReg) << 12))))) | (srcReg & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc.
	MOV destReg, #immediate8BitValue ROR rot - ARM_ARM v7 DDI10406 pp.
	A8-194-5 
 */

	/* CogARMCompiler>>#mov:imm:ror: */
static NoDbgRegParms usqInt
movimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(MoveOpcode) << 21))) | (0)))) | ((0) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate8bitValue) & 0xFFF);
}


/*	MOV destReg, srcReg - ARM_ARM v7 DDI10406 pp. A8-196-7 */

	/* CogARMCompiler>>#mov:rn: */
static NoDbgRegParms usqInt
movrn(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg)
{
	return ((((((usqInt)(AL) << 28))) | ((0) | (((((usqInt)(MoveOpcode) << 21))) | (0)))) | ((0) | ((((usqInt)(destReg) << 12))))) | (srcReg & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc.
	MVN destReg, #immediate8BitValue ROR rot - ARM_ARM v7 DDI10406 pp.
	A8-214-5 
 */

	/* CogARMCompiler>>#mvn:imm:ror: */
static NoDbgRegParms usqInt
mvnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(MoveNotOpcode) << 21))) | (0)))) | ((0) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate8bitValue) & 0xFFF);
}

	/* CogARMCompiler>>#numIntRegArgs */
static NoDbgRegParms sqInt
numIntRegArgs(AbstractInstruction *self_in_CogARMCompiler)
{
	return 4;
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc.
	ORR destReg, #immediate8BitValue ROR rot - ARM_ARM v7 DDI10406 pp.
	A8-228-9 
 */

	/* CogARMCompiler>>#orr:imm:ror: */
static NoDbgRegParms usqInt
orrimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt immediate8bitValue, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(OrOpcode) << 21))) | (0)))) | (((((usqInt)(destReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate8bitValue) & 0xFFF);
}

	/* CogARMCompiler>>#padIfPossibleWithStopsFrom:to: */
static NoDbgRegParms AbstractInstruction *
padIfPossibleWithStopsFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt startAddr, sqInt endAddr)
{
    sqInt nullBytes;
    sqInt p;

	nullBytes = ((endAddr - startAddr) + 1) % 4;
	stopsFromto(self_in_CogARMCompiler, startAddr, endAddr - nullBytes);
	for (p = ((endAddr - nullBytes) + 1); p <= endAddr; p += 1) {
		codeByteAtput(p, 0xFF);
	}
	return self_in_CogARMCompiler;
}


/*	pop word off TOS
	LDR srcReg, [sp] #4 - ARM_ARM v7 DDI10406 pp. A8-120-1 */

	/* CogARMCompiler>>#popR: */
static NoDbgRegParms sqInt
popR(AbstractInstruction *self_in_CogARMCompiler, sqInt dstReg)
{
	return ((((usqInt)(AL) << 28))) | ((0x4000000) | ((0) | ((0x800000) | ((0) | ((0) | ((0x100000) | (((((usqInt)(SP) << 16))) | (((((usqInt)(dstReg) << 12))) | 4))))))));
}

	/* CogARMCompiler>>#pushLinkRegisterByteSize */
static NoDbgRegParms sqInt
pushLinkRegisterByteSize(AbstractInstruction *self_in_CogARMCompiler)
{
	return 4;
}


/*	push word to TOS 
	STR srcReg, [sp, #-4]! - ARM_ARM v7 DDI10406 pp. A8-382-3 */

	/* CogARMCompiler>>#pushR: */
static NoDbgRegParms sqInt
pushR(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg)
{
	return ((((usqInt)(AL) << 28))) | ((0x4000000) | ((0x1000000) | ((0) | ((0) | ((0x200000) | ((0) | (((((usqInt)(SP) << 16))) | (((((usqInt)(srcReg) << 12))) | 4))))))));
}

	/* CogARMCompiler>>#relocateCallBeforeReturnPC:by: */
static NoDbgRegParms AbstractInstruction *
relocateCallBeforeReturnPCby(AbstractInstruction *self_in_CogARMCompiler, sqInt retpc, sqInt delta)
{
    usqInt distanceDiv4;
    sqInt instr;

	assert((delta % 4) == 0);
	if (delta) {
		instr = longAt(retpc - 4);
		assert((instructionIsB(self_in_CogARMCompiler, instr))
		 || (instructionIsBL(self_in_CogARMCompiler, instr)));
		distanceDiv4 = instr & 0xFFFFFF;
		distanceDiv4 += delta / 4;
		longAtput(retpc - 4, (instr & 0xFF000000U) | (distanceDiv4 & 0xFFFFFF));
	}
	return self_in_CogARMCompiler;
}


/*	Rewrite a call instruction to call a different target. This variant is
	used to link PICs
	in ceSendMiss et al,. 
	Answer the extent of the code change which is used to compute the range of
	the icache to flush. */

	/* CogARMCompiler>>#rewriteCallAt:target: */
static NoDbgRegParms sqInt
rewriteCallAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress)
{
	return rewriteTransferAttarget(self_in_CogARMCompiler, callSiteReturnAddress, callTargetAddress);
}


/*	Rewrite a callFull instruction to jump to a different target. This variant
	is used to rewrite cached primitive calls where we load the target address
	into ip
	and use the 'blx ip' instruction for the actual call.
	Answer the extent of the
	code change which is used to compute the range of the icache to flush. */

	/* CogARMCompiler>>#rewriteCallFullAt:target: */
static NoDbgRegParms sqInt
rewriteCallFullAttarget(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteFullTransferAttargetexpectedInstruction(self_in_CogARMCompiler, callSiteReturnAddress, callTargetAddress, 0xE12FFF3CU);
}


/*	Rewrite a full jump instruction to jump to a different target. This
	variant is used to rewrite cached primitive calls where we load the target
	address into ip
	and use the 'bx ip' instruction for the actual jump.
	Answer the extent of the
	code change which is used to compute the range of the icache to flush. */

	/* CogARMCompiler>>#rewriteJumpFullAt:target: */
static NoDbgRegParms sqInt
rewriteJumpFullAttarget(AbstractInstruction *self_in_CogARMCompiler, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteFullTransferAttargetexpectedInstruction(self_in_CogARMCompiler, callSiteReturnAddress, callTargetAddress, 3778019100U);
}


/*	Rewrite a jump instruction to call a different target. This variant is
	used to reset the 
	jumps in the prototype CPIC to suit each use,. 
	Answer the extent of the code change which is used to compute the range of
	the icache to flush. */

	/* CogARMCompiler>>#rewriteJumpLongAt:target: */
static NoDbgRegParms sqInt
rewriteJumpLongAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress)
{
	return rewriteTransferAttarget(self_in_CogARMCompiler, callSiteReturnAddress, callTargetAddress);
}


/*	Rewrite a call/jump instruction to call a different target. This variant
	is used to link PICs
	in ceSendMiss et al, and to rewrite call/jumps in CPICs.
	Answer the extent of
	the code change which is used to compute the range of the icache to flush. */
/*	for debug - [cogit disassembleFrom: callSiteReturnAddress - 10 to:
	callSiteReturnAddress - 1].
 */

	/* CogARMCompiler>>#rewriteTransferAt:target: */
static NoDbgRegParms sqInt
rewriteTransferAttarget(AbstractInstruction *self_in_CogARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress)
{
    usqInt callDistance;
    sqInt instr;

	/* pc offset */
	/* return offset */
	callDistance = ((usqInt) (callTargetAddress - ((callSiteReturnAddress + 8) - 4)));
	assert(isInImmediateJumpRange(self_in_CogARMCompiler, callDistance));
	instr = longAt(callSiteReturnAddress - 4);
	assert((instructionIsB(self_in_CogARMCompiler, instr))
	 || (instructionIsBL(self_in_CogARMCompiler, instr)));
	longAtput(callSiteReturnAddress - 4, (instr & 0xFF000000U) | ((callDistance / 4) & 0xFFFFFF));
	assert((callTargetFromReturnAddress(self_in_CogARMCompiler, callSiteReturnAddress)) == callTargetAddress);
	return 4;
}


/*	to save Slang from having to be a real compiler (it can't inline switches
	that return)
 */
/*	Answer if the receiver's opcode sets the condition codes correctly for the
	given conditional jump opcode.
	ARM has to check carefully since the V flag is not affected by
	non-comparison instructions
 */

	/* CogARMCompiler>>#setsConditionCodesFor: */
static NoDbgRegParms sqInt
setsConditionCodesFor(AbstractInstruction *self_in_CogARMCompiler, sqInt aConditionalJumpOpcode)
{
	switch ((self_in_CogARMCompiler->opcode)) {
	case ArithmeticShiftRightCqR:
	case ArithmeticShiftRightRR:
	case LogicalShiftLeftCqR:
	case LogicalShiftLeftRR:
		return shiftSetsConditionCodesFor(self_in_CogARMCompiler, aConditionalJumpOpcode);

	case XorRR:
		return 1;

	case ClzRR:
		return 0;

	default:
		haltmsg("unhandled opcode in setsConditionCodesFor:");
		return 0;
	}
	return 0;
}


/*	check what flags the opcode needs setting - ARM doesn't set V when simply
	MOVing 
 */

	/* CogARMCompiler>>#shiftSetsConditionCodesFor: */
static NoDbgRegParms sqInt
shiftSetsConditionCodesFor(AbstractInstruction *self_in_CogARMCompiler, sqInt aConditionalJumpOpcode)
{
	switch (aConditionalJumpOpcode) {
	case JumpNegative:
	case JumpZero:
	case JumpLess:
		return 1;

	default:
		haltmsg("unhandled opcode in setsConditionCodesFor:");
		return 0;
	}
	return 0;
}


/*	Return a minimum amount of headroom for each stack page (in bytes). In a
	JIT the stack has to have room for interrupt handlers which will run on
	the stack.
	According to ARM architecture v5 reference manual chapter A2.6, the basic
	interrupt procedure does not push anything onto the stack. It uses
	SPSR_err and R14_err to preserve state. Afterwards, it calls an interrupt
	procedure. So leave some room.
 */

	/* CogARMCompiler>>#stackPageInterruptHeadroomBytes */
static NoDbgRegParms sqInt
stackPageInterruptHeadroomBytes(AbstractInstruction *self_in_CogARMCompiler)
{
	return 128;
}

	/* CogARMCompiler>>#stopsFrom:to: */
static NoDbgRegParms AbstractInstruction *
stopsFromto(AbstractInstruction *self_in_CogARMCompiler, sqInt startAddr, sqInt endAddr)
{
    sqInt addr;

	assert((((endAddr - startAddr) + 1) % 4) == 0);
	for (addr = startAddr; addr <= endAddr; addr += 4) {
		codeLongAtput(addr, ((((usqInt)(AL) << 28))) | ((0x1200000) | (112)));
	}
	return self_in_CogARMCompiler;
}


/*	STRB destReg, [baseReg, 'u' immediate12bitValue] u=0 -> subtract imm; =1
	-> add imm - ARM_ARM v7 DDI10406 pp. A8-388-9 */

	/* CogARMCompiler>>#strb:rn:plus:imm: */
static NoDbgRegParms usqInt
strbrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((((u & 1) << 23)) | ((0x400000) | ((0) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	STRB srcReg, [baseReg, + offsetReg] - ARM_ARM v7 DDI10406 pp. A8-390-1
	The contents of offsetReg are assumed to be correctly signed */

	/* CogARMCompiler>>#strb:rn:rm: */
static NoDbgRegParms usqInt
strbrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg)
{
	return (0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0x400000) | ((0) | ((0) | ((((baseReg & 15) << 16)) | ((((srcReg & 15) << 12)) | (offsetReg & 15)))))))));
}


/*	STRH destReg, [baseReg, 'u' immediate8bitValue] u=0 -> subtract imm; =1 ->
	add imm - ARM_ARM v7 DDI10406 pp. A8-408-9 */

	/* CogARMCompiler>>#strh:rn:plus:imm: */
static NoDbgRegParms usqInt
strhrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate8bitValue)
{
	return ((((usqInt)(AL) << 28))) | ((0) | ((0x1000000) | (((((usqInt)(u) << 23))) | ((0x400000) | ((0) | ((0) | (((((usqInt)(baseReg) << 16))) | (((((usqInt)(destReg) << 12))) | ((((immediate8bitValue & 240) << 4)) | ((176) | (immediate8bitValue & 15)))))))))));
}


/*	STRH srcReg, [baseReg, +offsetReg] - ARM_ARM v7 DDI10406 pp. A8-410-1 */

	/* CogARMCompiler>>#strh:rn:rm: */
static NoDbgRegParms sqInt
strhrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg)
{
	return ((((usqInt)(AL) << 28))) | ((0) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0) | (((((usqInt)(baseReg) << 16))) | (((((usqInt)(srcReg) << 12))) | (176 | offsetReg)))))))));
}


/*	STR srcReg, [baseReg, +immediate12bitValue] - ARM_ARM v7 DDI10406 pp.
	A8-382-3 
 */

	/* CogARMCompiler>>#str:rn:plusImm: */
static NoDbgRegParms usqInt
strrnplusImm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((0x800000) | ((0) | ((0) | ((((baseReg & 15) << 16)) | ((((srcReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	STR destReg, [baseReg, 'u' immediate12bitValue] u=0 -> subtract imm; =1 ->
	add imm - ARM_ARM v7 DDI10406 pp. A8-382-3 */

	/* CogARMCompiler>>#str:rn:plus:imm: */
static NoDbgRegParms usqInt
strrnplusimm(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt baseReg, sqInt u, sqInt immediate12bitValue)
{
	return (0xE0000000U) | ((0x5000000) | ((((u & 1) << 23)) | ((0) | ((0) | ((((baseReg & 15) << 16)) | ((((destReg & 15) << 12)) | (immediate12bitValue & 0xFFF)))))));
}


/*	STR srcReg, [baseReg, + offsetReg] - ARM_ARM v7 DDI10406 pp. A8-384-5
	The contents of offsetReg are assumed to be correctly signed */

	/* CogARMCompiler>>#str:rn:rm: */
static NoDbgRegParms usqInt
strrnrm(AbstractInstruction *self_in_CogARMCompiler, sqInt srcReg, sqInt baseReg, sqInt offsetReg)
{
	return (0xE0000000U) | ((0x6000000) | ((0x1000000) | ((0x800000) | ((0) | ((0) | ((0) | ((((baseReg & 15) << 16)) | ((((srcReg & 15) << 12)) | (offsetReg & 15)))))))));
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	SUBS destReg, srcReg, #immediate ROR rot - ARM_ARM v7 DDI10406 pp.
	A8-418-9 
 */

	/* CogARMCompiler>>#subs:rn:imm:ror: */
static NoDbgRegParms usqInt
subsrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(SubOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc
	SUB destReg, srcReg, #immediate ROR rot - ARM_ARM v7 DDI10406 pp. A8-418-9 */

	/* CogARMCompiler>>#sub:rn:imm:ror: */
static NoDbgRegParms usqInt
subrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt destReg, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(SubOpcode) << 21))) | (0)))) | (((((usqInt)(srcReg) << 16))) | ((((usqInt)(destReg) << 12))))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Remember the ROR is doubled by the cpu so use 30>>1 etc */
/*	also note that TST has no destReg
	TST srcReg, #immediate ROR rot - ARM_ARM v7 DDI10406 pp. A8-452-3 */

	/* CogARMCompiler>>#tst:rn:imm:ror: */
static NoDbgRegParms usqInt
tstrnimmror(AbstractInstruction *self_in_CogARMCompiler, sqInt ignored, sqInt srcReg, sqInt immediate, sqInt rot)
{
	return ((((((usqInt)(AL) << 28))) | ((0x2000000) | (((((usqInt)(TstOpcode) << 21))) | (0x100000)))) | (((((usqInt)(srcReg) << 16))) | (0))) | ((((((usqInt)((((usqInt)(rot)) >> 1)) << 8))) | immediate) & 0xFFF);
}


/*	Answer if Call and JumpLong are relative and hence need to take the
	caller's relocation delta into account during code compaction, rather than
	just the
	callee's delta. */

	/* CogARMCompiler>>#zoneCallsAreRelative */
static NoDbgRegParms sqInt
zoneCallsAreRelative(AbstractInstruction *self_in_CogARMCompiler)
{
	return 1;
}

	/* CogBlockMethod>>#cmHomeMethod */
static NoDbgRegParms CogMethod *
cmHomeMethod(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((CogMethod *) ((((usqInt)self_in_CogBlockMethod)) - ((self_in_CogBlockMethod->homeOffset))));
}

	/* CogBlockMethod>>#isCMBlock */
static NoDbgRegParms int
isCMBlock(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMBlock;
}

	/* CogBlockMethod>>#isCMClosedPIC */
static NoDbgRegParms int
isCMClosedPIC(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMClosedPIC;
}

	/* CogBlockMethod>>#isCMFree */
static NoDbgRegParms int
isCMFree(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMFree;
}

	/* CogBlockMethod>>#isCMMethodEtAl */
static NoDbgRegParms int
isCMMethodEtAl(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) >= CMMethod;
}

	/* CogBlockMethod>>#isCMOpenPIC */
static NoDbgRegParms int
isCMOpenPIC(CogBlockMethod *self_in_CogBlockMethod)
{
	return ((self_in_CogBlockMethod->cmType)) == CMOpenPIC;
}

	/* CogBytecodeDescriptor>>#isBranch */
static NoDbgRegParms sqInt
isBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor)
{
	return (((self_in_CogBytecodeDescriptor->spanFunction)))
	 && (!((self_in_CogBytecodeDescriptor->isBlockCreation)));
}

	/* CogBytecodeDescriptor>>#isConditionalBranch */
static NoDbgRegParms sqInt
isConditionalBranch(BytecodeDescriptor *self_in_CogBytecodeDescriptor)
{
	return ((self_in_CogBytecodeDescriptor->isBranchTrue))
	 || ((self_in_CogBytecodeDescriptor->isBranchFalse));
}

	/* Cogit>>#AndCq:R: */
static NoDbgRegParms AbstractInstruction *
gAndCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	return anInstruction;
}


/*	destReg := (signed)srcReg >> quickConstant */

	/* Cogit>>#ArithmeticShiftRightCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(ArithmeticShiftRightCqRR, quickConstant, srcReg, destReg);
	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#abortOffset */
sqInt
abortOffset(void)
{
	return missOffset;
}

	/* Cogit>>#addCleanBlockStarts */
static void
addCleanBlockStarts(void)
{
    sqInt i;
    sqInt lit;
    sqInt startPCOrNil;
    sqInt toDoLimit;

	toDoLimit = literalCountOf(methodObj);
	for (i = 1; i <= toDoLimit; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (startPCOrNil) {
			maxLitIndex = ((maxLitIndex < i) ? i : maxLitIndex);
			addBlockStartAtnumArgsnumCopiedspan(startPCOrNil - 1, argumentCountOfClosure(lit), copiedValueCountOfClosure(lit), spanForCleanBlockStartingAt(startPCOrNil - 1));
		}
	}
}


/*	Perform an integrity/leak check using the heapMap.
	Set a bit at each cog method's header. */

	/* Cogit>>#addCogMethodsToHeapMap */
void
addCogMethodsToHeapMap(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			heapMapAtWordPut(cogMethod, 1);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#addressIsInCurrentCompilation: */
static NoDbgRegParms sqInt
addressIsInCurrentCompilation(sqInt address)
{
	return ((((usqInt)address)) >= ((methodLabel->address)))
	 && ((((usqInt)address)) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize))));
}

	/* Cogit>>#addressIsInFixups: */
static NoDbgRegParms sqInt
addressIsInFixups(BytecodeFixup *address)
{
	return (BytecodeFixup *)address >= fixups && (BytecodeFixup *)address < (fixups + numAbstractOpcodes);
}


/*	calculate the end of the n'th case statement - which is complicated
	because we have case 1 right at the top of our CPIC and then build up from
	the last one. Yes I know this sounds strange, but trust me - I'm an
	Engineer, we do things backwards all the emit
 */

	/* Cogit>>#addressOfEndOfCase:inCPIC: */
static NoDbgRegParms sqInt
addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC)
{
	assert((n >= 1)
	 && (n <= MaxCPICCases));
	return (n == 1
			? (((sqInt)cPIC)) + firstCPICCaseOffset
			: ((((sqInt)cPIC)) + firstCPICCaseOffset) + (((MaxCPICCases + 1) - n) * cPICCaseSize));
}


/*	Align methodZoneBase to that for the start of a method. */

	/* Cogit>>#alignMethodZoneBase */
static void
alignMethodZoneBase(void)
{
    usqInt oldBase;

	oldBase = methodZoneBase;
	methodZoneBase = roundUpToMethodAlignment(backEnd, methodZoneBase);
	stopsFromto(backEnd, oldBase, methodZoneBase - 1);
}

	/* Cogit>>#alignUptoRoutineBoundary: */
static NoDbgRegParms sqInt
alignUptoRoutineBoundary(sqInt anAddress)
{
	return (((anAddress + 7) | 7) - 7);
}


/*	Check that all methods have valid selectors, and that all linked sends are
	to valid targets and have valid cache tags
 */

	/* Cogit>>#allMachineCodeObjectReferencesValid */
static sqInt
allMachineCodeObjectReferencesValid(void)
{
    CogMethod *cogMethod;
    sqInt ok;

	ok = 1;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!(asserta(checkValidOopReference((cogMethod->selector))))) {
				ok = 0;
			}
			if (!(asserta((cogMethodDoesntLookKosher(cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if ((((cogMethod->cmType)) >= CMMethod)
		 || (((cogMethod->cmType)) == CMOpenPIC)) {
			if (!(asserta((mapForperformUntilarg(cogMethod, checkIfValidOopRefAndTargetpccogMethod, cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if (((cogMethod->cmType)) == CMClosedPIC) {
			if (!(asserta(noTargetsFreeInClosedPIC(cogMethod)))) {
				ok = 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#allMethodsHaveCorrectHeader */
static sqInt
allMethodsHaveCorrectHeader(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (!(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}

	/* Cogit>>#annotateAbsolutePCRef: */
static NoDbgRegParms AbstractInstruction *
annotateAbsolutePCRef(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = IsAbsPCReference);
	return abstractInstruction;
}

	/* Cogit>>#annotateBytecode: */
static NoDbgRegParms AbstractInstruction *
annotateBytecode(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = HasBytecodePC);
	return abstractInstruction;
}

	/* Cogit>>#annotate:objRef: */
static NoDbgRegParms AbstractInstruction *
annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop)
{
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(anOop))
	 && (oopisGreaterThan(anOop, trueObject()))) {
		setHasMovableLiteral(1);
		if (isYoungObject(anOop)) {
			setHasYoungReferent(1);
		}
		(abstractInstruction->annotation = IsObjectReference);
	}
	return abstractInstruction;
}

	/* Cogit>>#assertSaneJumpTarget: */
static NoDbgRegParms void
assertSaneJumpTarget(AbstractInstruction *jumpTarget)
{
	assert((!closedPICSize)
	 || ((!openPICSize)
	 || ((addressIsInInstructions(jumpTarget))
	 || ((((((usqInt)jumpTarget)) >= codeBase) && ((((usqInt)jumpTarget)) <= ((((sqInt)(limitZony()))) + (((closedPICSize < openPICSize) ? openPICSize : closedPICSize)))))))));
}


/*	Answer an unused abstract register in the registerMask, or NoReg if none. */

	/* Cogit>>#availableRegisterOrNoneIn: */
static NoDbgRegParms sqInt
availableRegisterOrNoneIn(sqInt liveRegsMask)
{
    sqInt reg;

	if (liveRegsMask) {
		for (reg = 0; reg <= 0x1F; reg += 1) {
			if (((liveRegsMask & (1U << reg)) != 0)) {
				return reg;
			}
		}
	}
	return NoReg;
}


/*	Evaluate binaryFunction with the block start mcpc and supplied arg for
	each entry in the block dispatch. If the function answers non-zero answer
	the value
	it answered. Used to update back-references to the home method in
	compaction.  */

	/* Cogit>>#blockDispatchTargetsFor:perform:arg: */
static NoDbgRegParms sqInt
blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg)
{
    sqInt blockEntry;
    sqInt end;
    sqInt pc;
    sqInt result;
    usqInt targetpc;

	if (!((cogMethod->blockEntryOffset))) {
		return null;
	}
	blockEntry = ((cogMethod->blockEntryOffset)) + (((sqInt)cogMethod));
	pc = blockEntry;
	end = (mapEndFor(cogMethod)) - 1;
	while (pc < end) {
		if (isJumpAt(backEnd, pc)) {
			targetpc = jumpTargetPCAt(backEnd, pc);
			if (targetpc < blockEntry) {
				result = binaryFunction(targetpc, arg);
				if (result) {
					return result;
				}
			}
		}
		pc += instructionSizeAt(backEnd, pc);
	}
	return 0;
}


/*	Answer the zero-relative bytecode pc matching the machine code pc argument
	in cogMethod, given the start of the bytecodes for cogMethod's block or
	method object. */

	/* Cogit>>#bytecodePCFor:startBcpc:in: */
sqInt
bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    sqInt map;
    sqInt mapByte;
    usqInt mcpcUsqInt;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt targetPC;

	/* begin mapFor:bcpc:performUntil:arg: */
	descriptor = ((BytecodeDescriptor *) 0);
	latestContinuation = 0;
	mapByte = 0;
	nextBcpc = 0;
	assert(((cogMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpcUsqInt = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = findIsBackwardBranchMcpcBcpcMatchingMcpc(null, 0 + ((((usqInt)(HasBytecodePC) << 1))), ((char *) mcpcUsqInt), startbcpc, ((void *)mcpc));
	if (result) {
		return result;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		isInBlock = 0 /* cmIsFullBlock */;
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;

		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - BlockCreationBytecodeSize;
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
		: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpcUsqInt += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
		: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
								? nExts + 1
								: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
					 && ((/* begin isBackwardBranch:at:exts:in: */
						assert(((descriptor->spanFunction))),
					(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = findIsBackwardBranchMcpcBcpcMatchingMcpc(descriptor, (isBackwardBranch
							? ((((usqInt)(annotation) << 1))) + 1
							: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpcUsqInt), (isBackwardBranch
							? bcpc - (2 * nExts)
							: bcpc), ((void *)mcpc));
				if (result) {
					return result;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
							? nExts + 1
							: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpcUsqInt += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}

	/* Cogit>>#CallFullRT:registersToBeSavedMask: */
static NoDbgRegParms AbstractInstruction *
CallFullRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved)
{
    int callerSavedRegsToBeSaved;
    AbstractInstruction *lastInst;

	callerSavedRegsToBeSaved = CallerSavedRegisterMask & registersToBeSaved;

	/* genPushRegisterMask: */
	if (callerSavedRegsToBeSaved) {
		genoperand(PushSTM, callerSavedRegsToBeSaved);
	}
	else {
		/* Label */
		genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	lastInst = checkLiteralforInstruction(callTarget, genoperand(CallFull, callTarget));
	return /* genPopRegisterMask: */
		(callerSavedRegsToBeSaved
			? genoperand(PopLDM, callerSavedRegsToBeSaved)
			: genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}


/*	Big assumption here that calls and jumps look the same as regards their
	displacement. This works on at least x86, ARM and x86_64. */

	/* Cogit>>#CallRT: */
static NoDbgRegParms AbstractInstruction *
CallRT(sqInt callTarget)
{
    AbstractInstruction *abstractInstruction;

	abstractInstruction = genoperand(Call, callTarget);

	/* begin annotateCall: */
	(abstractInstruction->annotation = IsRelativeCall);
	return abstractInstruction;
}


/*	This is a static version of ceCallCogCodePopReceiverReg for break-pointing
	when debugging in C. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#callCogCodePopReceiver */
static void
callCogCodePopReceiver(void)
{
	realCECallCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	This is a static version of ceCallCogCodePopReceiverAndClassRegs for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#callCogCodePopReceiverAndClassRegs */
static void
callCogCodePopReceiverAndClassRegs(void)
{
	realCECallCogCodePopReceiverAndClassRegs();
}


/*	Code entry closed PIC miss. A send has fallen
	through a closed (finite) polymorphic inline cache.
	Either extend it or patch the send site to an open PIC.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */
/*	Marked <api> so the code generator won't delete it. */

	/* Cogit>>#ceCPICMiss:receiver: */
static NoDbgRegParms sqInt
ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver)
{
    sqInt cacheTag;
    sqInt errorSelectorOrNil;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    sqInt outerReturn;
    sqInt result;
    sqInt selector;

	outerReturn = stackTop();
	assert(!(((inlineCacheTagAt(backEnd, outerReturn)) == (picAbortDiscriminatorValue()))));
	if (((cPIC->cPICNumCases)) < MaxCPICCases) {
		selector = (cPIC->selector);

		/* begin lookup:for:methodAndErrorSelectorInto: */
		methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorCannotInterpret;
				goto l1;
			}
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {

				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, selector);
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = null;
			goto l1;
		}
		if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
			methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
			if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
				assert(isOopCompiledMethod(methodOrSelectorIndex));
				if ((!(methodHasCogMethod(methodOrSelectorIndex)))
				 && (methodShouldBeCogged(methodOrSelectorIndex))) {

					/* We assume cog:selector: will *not* reclaim the method zone */
					cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
				}
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorDoesNotUnderstand;
				goto l1;
			}
			newTargetMethodOrNil = null;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = methodOrSelectorIndex;
l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	}
	else {
		newTargetMethodOrNil = (errorSelectorOrNil = null);
	}
	assert(outerReturn == (stackTop()));

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cacheTag = inlineCacheTagForInstance(receiver);
	if ((((cPIC->cPICNumCases)) >= MaxCPICCases)
	 || (/* closedPICInappropriateForCacheTag:targetMethod:orErrorSelector: */
		((errorSelectorOrNil)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((inlineCacheTagIsYoung(cacheTag))
	 || ((!newTargetMethodOrNil)
	 || (isYoung(newTargetMethodOrNil)))))) {
		result = patchToOpenPICFornumArgsreceiver((cPIC->selector), (cPIC->cmNumArgs), receiver);
		assert(!result);

		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		return ceSendFromInLineCacheMiss(cPIC);
	}
	cogExtendPICCaseNMethodtagisMNUCase(cPIC, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	executeCogPICfromLinkedSendWithReceiverandCacheTag(cPIC, receiver, longAt(pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8)))));
	return null;
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceFree: */
static NoDbgRegParms void
ceFree(void *pointer)
{
	free(pointer);
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceMalloc: */
static NoDbgRegParms void*
ceMalloc(size_t size)
{
	return malloc(size);
}


/*	An in-line cache check in a method has failed. The failing entry check has
	jumped to the ceMethodAbort abort call at the start of the method which
	has called this routine.
	If possible allocate a closed PIC for the current and existing classes.
	The stack looks like:
	receiver
	args
	sender return address
	sp=>	ceMethodAbort call return address
	So we can find the method that did the failing entry check at
	ceMethodAbort call return address - missOffset
	and we can find the send site from the outer return address.
	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it.  */

	/* Cogit>>#ceSICMiss: */
static NoDbgRegParms sqInt
ceSICMiss(sqInt receiver)
{
    sqInt cacheTag;
    sqInt entryPoint;
    sqInt errorSelectorOrNil;
    sqInt extent;
    usqInt innerReturn;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    usqInt outerReturn;
    CogMethod *pic;
    sqInt result;
    sqInt selector;
    CogMethod *targetMethod;


	/* Whether we can relink to a PIC or not we need to pop off the inner return and identify the target method. */
	innerReturn = ((usqInt)(popStack()));
	targetMethod = ((CogMethod *) (innerReturn - missOffset));
	outerReturn = ((usqInt)(stackTop()));
	assert(((outerReturn >= methodZoneBase) && (outerReturn <= (freeStart()))));
	entryPoint = callTargetFromReturnAddress(backEnd, outerReturn);
	assert(((targetMethod->selector)) != (nilObject()));
	assert(((((sqInt)targetMethod)) + cmEntryOffset) == entryPoint);
	selector = (targetMethod->selector);

	/* begin lookup:for:methodAndErrorSelectorInto: */
	methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
	if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
		if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorCannotInterpret;
			goto l1;
		}
		if ((!(methodHasCogMethod(methodOrSelectorIndex)))
		 && (methodShouldBeCogged(methodOrSelectorIndex))) {

			/* We assume cog:selector: will *not* reclaim the method zone */
			cogselector(methodOrSelectorIndex, selector);
		}
		newTargetMethodOrNil = methodOrSelectorIndex;
		errorSelectorOrNil = null;
		goto l1;
	}
	if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
		methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			assert(isOopCompiledMethod(methodOrSelectorIndex));
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {

				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = SelectorDoesNotUnderstand;
		goto l1;
	}
	newTargetMethodOrNil = null;
	errorSelectorOrNil = methodOrSelectorIndex;
l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	assert(outerReturn == (stackTop()));
	cacheTag = inlineCacheTagForInstance(receiver);
	if ((/* closedPICInappropriateForCacheTag:targetMethod:orErrorSelector: */
		((errorSelectorOrNil)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((inlineCacheTagIsYoung(cacheTag))
	 || ((!newTargetMethodOrNil)
	 || (isYoung(newTargetMethodOrNil)))))
	 || ((longAt(pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8))))) == 0 /* picAbortDiscriminatorValue */)) {
		result = patchToOpenPICFornumArgsreceiver((targetMethod->selector), (targetMethod->cmNumArgs), receiver);
		assert(!result);
		return ceSendFromInLineCacheMiss(targetMethod);
	}

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	pic = openPICWithSelector((targetMethod->selector));
	if (!pic) {

		/* otherwise attempt to create a closed PIC for the two cases. */
		pic = cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase((targetMethod->selector), (targetMethod->cmNumArgs), targetMethod, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);
		if ((((((sqInt)pic)) >= MaxNegativeErrorCode) && ((((sqInt)pic)) <= -1))) {

			/* For some reason the PIC couldn't be generated, most likely a lack of code memory.
			   Continue as if this is an unlinked send. */
			if ((((sqInt)pic)) == InsufficientCodeSpace) {
				callForCogCompiledCodeCompaction();
			}

			/* begin ensureExecutableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
#      endif

			return ceSendFromInLineCacheMiss(targetMethod);
		}
	}
	extent = (((pic->cmType)) == CMOpenPIC
				? rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), mframeHomeMethodExport()), (((sqInt)pic)) + cmEntryOffset)
				: rewriteCallAttarget(backEnd, outerReturn, (((sqInt)pic)) + cmEntryOffset));
	(((usqInt)pic)) + closedPICSize;

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, (((usqInt)pic)) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	flushICacheFromto(backEnd, ((usqInt)pic), (((usqInt)pic)) + closedPICSize);
	flushICacheFromto(backEnd, (((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
	executeCogPICfromLinkedSendWithReceiverandCacheTag(pic, receiver, longAt(pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8)))));
	return null;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRefAndTarget:pc:cogMethod: */
static NoDbgRegParms sqInt
checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt cacheTagSqInt;
    sqInt entryPoint;
    sqInt entryPointSqInt;
    sqInt literal;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    sqInt tagCouldBeObj;
    CogMethod *targetMethod1;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (!(asserta(checkValidOopReference(literal)))) {
			return 1;
		}
		if ((couldBeObject(literal))
		 && (isReallyYoungObject(literal))) {
			if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
				return 2;
			}
		}
	}
	if (annotation >= IsSendCall) {
		if (!(asserta(isCMMethodEtAl(((CogBlockMethod *) (((CogMethod *) cogMethod))))))) {
			return 3;
		}

		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTagSqInt = longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPointSqInt = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj = !(BytesPerWord == 8);
		entryPoint = entryPointSqInt;
		if (tagCouldBeObj) {
			if (couldBeObject(cacheTagSqInt)) {
				if (!(asserta(checkValidOopReference(cacheTagSqInt)))) {
					return 4;
				}
			}
			else {
				if (!(asserta(validInlineCacheTag(cacheTagSqInt)))) {
					return 5;
				}
			}
			if ((couldBeObject(cacheTagSqInt))
			 && (isReallyYoungObject(cacheTagSqInt))) {
				if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
					return 6;
				}
			}
		}
		else {
			if (!(asserta(validInlineCacheTag(cacheTagSqInt)))) {
				return 9;
			};
		}
		if (entryPoint > methodZoneBase) {

			/* It's a linked send; find which kind. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (!(asserta((isCMMethodEtAl(((CogBlockMethod *) targetMethod1)))
				 || ((isCMClosedPIC(((CogBlockMethod *) targetMethod1)))
				 || (isCMOpenPIC(((CogBlockMethod *) targetMethod1))))))) {
				return 10;
			}
		}
	}
	return 0;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRef:pc:cogMethod: */
static NoDbgRegParms sqInt
checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt entryPoint;
    sqInt literal;
    sqInt offset;
    sqInt offsetSqInt;
    sqInt selectorOrCacheTag;
    sqInt *sendTable;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (!(checkValidOopReference(literal))) {
			print("object ref leak in CM ");
			printHex(((sqInt)cogMethod));
			print(" @ ");
			printHex(((sqInt)mcpc));
			eekcr();
			return 1;
		}
	}
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {
			offset = entryPoint;
		}
		else {
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable = superSendTrampolines;
			}
			offset = offsetSqInt;
		}
		selectorOrCacheTag = longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))));
		if ((entryPoint > methodZoneBase)
		 && ((offset != cmNoCheckEntryOffset)
		 && (!((((((CogMethod *) (entryPoint - offset)))->cmType)) == CMOpenPIC)))) {

			/* linked non-super send, cacheTag is a cacheTag */
			if (!(validInlineCacheTag(selectorOrCacheTag))) {
				print("cache tag leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" @ ");
				printHex(((sqInt)mcpc));
				eekcr();
				return 1;
			}
		}
		else {

			/* unlinked send or super send; cacheTag is a selector unless 64-bit, in which case it is an index. */
			if (!(checkValidOopReference(selectorOrCacheTag))) {
				print("selector leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" @ ");
				printHex(((sqInt)mcpc));
				eekcr();
				return 1;
			}
		}
	}
	return 0;
}


/*	Answer if all references to objects in machine-code are valid. */

	/* Cogit>>#checkIntegrityOfObjectReferencesInCode: */
sqInt
checkIntegrityOfObjectReferencesInCode(sqInt gcModes)
{
    CogMethod *cogMethod;
    sqInt count;
    sqInt ok;

	cogMethod = ((CogMethod *) methodZoneBase);
	ok = 1;
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((cogMethod->cmRefersToYoung)) {
				if (((count = occurrencesInYoungReferrers(cogMethod))) != 1) {
					print("young referrer CM ");
					printHex(((sqInt)cogMethod));
					if (count) {
						print(" is in youngReferrers ");
						printNum(count);
						print(" times!");
						eekcr();
					}
					else {
						print(" is not in youngReferrers");
						eekcr();
					}
					ok = 0;
				}
			}
			if (!(checkValidOopReference((cogMethod->selector)))) {
				print("object leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" selector");
				eekcr();
				ok = 0;
			}
			if (((cogMethod->cmType)) >= CMMethod) {
				assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
				if (!(checkValidObjectReference((cogMethod->methodObject)))) {
					print("object leak in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if (!(isOopCompiledMethod((cogMethod->methodObject)))) {
					print("non-method in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if (mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) {
					ok = 0;
				}
				if (((gcModes & GCModeNewSpace) != 0)) {
					if (((isYoungObject((cogMethod->methodObject)))
					 || (isYoung((cogMethod->selector))))
					 && (!((cogMethod->cmRefersToYoung)))) {
						print("CM ");
						printHex(((sqInt)cogMethod));
						print(" refers to young but not marked as such");
						eekcr();
						ok = 0;
					}
				}
			}
			else {
				if (((cogMethod->cmType)) == CMClosedPIC) {
					if (!(checkValidObjectReferencesInClosedPIC(cogMethod))) {
						ok = 0;
					}
				}
				else {
					if (((cogMethod->cmType)) == CMOpenPIC) {
						if (mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) {
							ok = 0;
						}
					}
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#checkMaybeObjRefInClosedPIC: */
static NoDbgRegParms sqInt
checkMaybeObjRefInClosedPIC(sqInt maybeObject)
{
	if (!maybeObject) {
		return 1;
	}
	if (!(couldBeObject(maybeObject))) {
		return 1;
	}
	return checkValidObjectReference(maybeObject);
}

	/* Cogit>>#checkValidObjectReferencesInClosedPIC: */
static NoDbgRegParms sqInt
checkValidObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt ok;
    sqInt pc;

	ok = 1;

	/* first we check the obj ref at the beginning of the CPIC */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		print("object leak in CPIC ");
		printHex(((sqInt)cPIC));
		print(" @ ");
		printHex(pc - (jumpLongByteSize(backEnd)));
		cr();
		ok = 0;
	}

	/* For each case we check any object reference at the end address - sizeof(conditional instruction) and then increment the end address by case size */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (!(checkMaybeObjRefInClosedPIC(literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd)))))) {
			print("object leak in CPIC ");
			printHex(((sqInt)cPIC));
			print(" @ ");
			printHex((pc - (jumpLongConditionalByteSize(backEnd))) - (loadLiteralByteSize(backEnd)));
			cr();
			ok = 0;
		}
		if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			print("object leak in CPIC ");
			printHex(((sqInt)cPIC));
			print(" @ ");
			printHex(pc - (jumpLongConditionalByteSize(backEnd)));
			cr();
			ok = 0;
		}
		pc += cPICCaseSize;
	}
	return ok;
}


/*	i.e. this should never be called, so keep it out of the main path. */

	/* Cogit>>#cleanUpFailingCogCodeConstituents: */
static NoDbgRegParms NeverInline sqInt
cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg)
{
    CogMethod *cogMethod;

	cogMethod = cogMethodArg;
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMClosedPIC) {
			(cogMethod->methodObject = 0);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	popRemappableOop();
	return null;
}


/*	Answer if the ClosedPIC refers to any unmarked objects or freed/freeable
	target methods,
	applying markAndTraceOrFreeCogMethod:firstVisit: to those targets to
	determine if freed/freeable.
 */

	/* Cogit>>#closedPICRefersToUnmarkedObject: */
static NoDbgRegParms sqInt
closedPICRefersToUnmarkedObject(CogMethod *cPIC)
{
    sqInt i;
    sqInt object;
    sqInt pc;

	if (!((isImmediate((cPIC->selector)))
		 || (isMarked((cPIC->selector))))) {
		return 1;
	}
	pc = addressOfEndOfCaseinCPIC(1, cPIC);
	if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		if (!(isMarked(object))) {
			return 1;
		}
	}
	if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
		return 1;
	}
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (couldBeObject((object = literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd)))))) {
			if (!(isMarked(object))) {
				return 1;
			}
		}
		if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			if (!(isMarked(object))) {
				return 1;
			}
		}
		if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
			return 1;
		}
	}
	return 0;
}

	/* Cogit>>#codeEntryFor: */
char *
codeEntryFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i + 1];
		}
	}
	return null;
}

	/* Cogit>>#codeEntryNameFor: */
char *
codeEntryNameFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i];
		}
	}
	return null;
}


/*	used e.g. in the platform's backtrace generators. Declared api to place it
	in cogit.h
 */

	/* Cogit>>#cogCodeBase */
sqInt
cogCodeBase(void)
{
	return codeBase;
}


/*	Answer the contents of the code zone as an array of pair-wise element,
	address in ascending address order.
	Answer a string for a runtime routine or abstract label (beginning, end,
	etc), a CompiledMethod for a CMMethod,
	or a selector (presumably a Symbol) for a PIC.
	If withDetails is true
	- answer machine-code to bytecode pc mapping information for methods
	- answer class, target pair information for closed PIC
	N.B. Since the class tag for the first case of a closed PIC is stored at
	the send site, it must be collected
	by scanning methods (see
	collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method:). Since closed PICs
	are never shared they always come after the method that references them,
	so we don't need an extra pass
	to collect the first case class tags, which are (temporarily) assigned to
	each closed PIC's methodObject field.
	But we do need to reset the methodObject fields to zero. This is done in
	createPICData:, unless memory
	runs out, in which case it is done by cleanUpFailingCogCodeConstituents:. */

	/* Cogit>>#cogCodeConstituents: */
sqInt
cogCodeConstituents(sqInt withDetails)
{
    CogMethod *cogMethod;
    sqInt constituents;
    sqInt count;
    sqInt i;
    sqInt label;
    sqInt profileData;
    sqInt value;

	/* + 3 for start, freeStart and end */
	count = (trampolineTableIndex / 2) + 3;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			count += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	constituents = instantiateClassindexableSize(classArray(), count * 2);
	if (!constituents) {
		return constituents;
	}
	pushRemappableOop(constituents);
	if ((!((label = stringForCString("CogCode"))))
	 || (!((value = positive32BitIntegerFor(codeBase))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, topRemappableOop(), label);
	storePointerUncheckedofObjectwithValue(1, topRemappableOop(), value);
	for (i = 0; i < trampolineTableIndex; i += 2) {
		if ((!((label = stringForCString(trampolineAddresses[i]))))
		 || (!((value = positive32BitIntegerFor(((usqInt)(trampolineAddresses[i + 1]))))))) {
			popRemappableOop();
			return null;
		}
		storePointerUncheckedofObjectwithValue(2 + i, topRemappableOop(), label);
		storePointerUncheckedofObjectwithValue(3 + i, topRemappableOop(), value);
	}
	count = trampolineTableIndex + 2;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			profileData = /* profileDataFor:withDetails: */
					(((cogMethod->cmType)) >= CMMethod
						? (cogMethod->methodObject)
						: (withDetails
						 && (((cogMethod->cmType)) == CMClosedPIC)
								? createCPICData(cogMethod)
								: (cogMethod->selector)));
			if (!profileData) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count, topRemappableOop(), profileData);
			value = (withDetails
						? collectCogMethodConstituent(cogMethod)
						: positive32BitIntegerFor(((usqInt)cogMethod)));
			if (!value) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count + 1, topRemappableOop(), value);
			count += 2;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if ((!((label = stringForCString("CCFree"))))
	 || (!((value = positive32BitIntegerFor(mzFreeStart))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count, topRemappableOop(), label);
	storePointerUncheckedofObjectwithValue(count + 1, topRemappableOop(), value);
	if ((!((label = stringForCString("CCEnd"))))
	 || (!((value = positive32BitIntegerFor(limitAddress))))) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count + 2, topRemappableOop(), label);
	storePointerUncheckedofObjectwithValue(count + 3, topRemappableOop(), value);
	constituents = popRemappableOop();
	beRootIfOld(constituents);
	return constituents;
}


/*	Extend the cPIC with the supplied case. If caseNMethod is cogged dispatch
	direct to
	its unchecked entry-point. If caseNMethod is not cogged, jump to the fast
	interpreter dispatch, and if isMNUCase then dispatch to fast MNU
	invocation and mark the cPIC as
	having the MNU case for cache flushing. */

	/* Cogit>>#cogExtendPIC:CaseNMethod:tag:isMNUCase: */
static NoDbgRegParms void
cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase)
{
    sqInt address;
    usqInt addressFollowingJump;
    usqInt jumpTargetAddr;
    sqInt operand;
    sqInt target;

	compilationBreakpointclassTagisMNUCase((cPIC->selector), caseNTag, isMNUCase);
	assert(!(inlineCacheTagIsYoung(caseNTag)));
	assert((caseNMethod)
	 && (!(isYoung(caseNMethod))));
	if ((!isMNUCase)
	 && (methodHasCogMethod(caseNMethod))) {

		/* this isn't an MNU and we have an already cogged method to jump to */
		operand = 0;
		target = (((sqInt)(cogMethodOf(caseNMethod)))) + cmNoCheckEntryOffset;
	}
	else {
		operand = caseNMethod;
		if (isMNUCase) {

			/* this is an MNU so tag the CPIC header and setup a jump to the MNUAbort */
			/* cpicHasMNUCase: */
			((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cpicHasMNUCaseOrCMIsFullBlock) = 1;
			target = (((sqInt)cPIC)) + (sizeof(CogMethod));
		}
		else {

			/* setup a jump to the interpretAborth so we can cog the target method */
			target = (((sqInt)cPIC)) + (picInterpretAbortOffset());
		}
	}
	address = addressOfEndOfCaseinCPIC(((cPIC->cPICNumCases)) + 1, cPIC);
	rewriteCPICCaseAttagobjReftarget(address, caseNTag, operand, target);

	/* begin rewriteCPIC:caseJumpTo: */
	addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	jumpTargetAddr = address - cPICCaseSize;

	/* begin rewriteCPICJumpAt:target: */
	rewriteTransferAttarget(((AbstractInstruction *) backEnd), addressFollowingJump, jumpTargetAddr);
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = ((cPIC->cPICNumCases)) + 1);
	flushICacheFromto(backEnd, ((usqInt)cPIC), (((usqInt)cPIC)) + closedPICSize);
	(((usqInt)cPIC)) + closedPICSize;

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, (((usqInt)cPIC)) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif
}

	/* Cogit>>#cogitPostGCAction: */
void
cogitPostGCAction(sqInt gcMode)
{
	if (gcMode == GCModeFull) {
		voidYoungReferrersPostTenureAll();
	}
	assert(allMethodsHaveCorrectHeader());
	assert(((!(gcMode & (GCModeFull + GCModeNewSpace))))
	 || (kosherYoungReferrers()));

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Check that the header fields onf a non-free method are consistent with
	the type. Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#cogMethodDoesntLookKosher: */
static NoDbgRegParms sqInt
cogMethodDoesntLookKosher(CogMethod *cogMethod)
{
	if (((((cogMethod->blockSize)) & (BytesPerWord - 1)) != 0)
	 || ((((cogMethod->blockSize)) < (sizeof(CogMethod)))
	 || (((cogMethod->blockSize)) >= 0x8000))) {
		return 1;
	}
	if (((cogMethod->cmType)) == CMFree) {
		return 2;
	}
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!((((cogMethod->methodHeader)) & 1))) {
			return 11;
		}
		if (!(couldBeObject((cogMethod->methodObject)))) {
			return 12;
		}
		if ((((cogMethod->stackCheckOffset)) > 0)
		 && (((cogMethod->stackCheckOffset)) < cmNoCheckEntryOffset)) {
			return 13;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (((cogMethod->blockSize)) != openPICSize) {
			return 21;
		}
		if ((cogMethod->methodHeader)) {
			return 22;
		}
		if (((cogMethod->objectHeader)) >= 0) {
			if (!((((cogMethod->methodObject)) == 0)
				 || (compactionInProgress
				 || (((cogMethod->methodObject)) == (((usqInt)(methodFor(((void *)((cogMethod->methodObject))))))))))) {
				return 23;
			}
		}
		if ((cogMethod->stackCheckOffset)) {
			return 24;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (((cogMethod->blockSize)) != closedPICSize) {
			return 0x1F;
		}
		if (!(((((cogMethod->cPICNumCases)) >= 1) && (((cogMethod->cPICNumCases)) <= MaxCPICCases)))) {
			return 32;
		}
		if ((cogMethod->methodHeader)) {
			return 33;
		}
		if ((cogMethod->methodObject)) {
			return 34;
		}
		return 0;
	}
	return 9;
}


/*	Attempt to create a one-case PIC for an MNU.
	The tag for the case is at the send site and so doesn't need to be
	generated. 
 */

	/* Cogit>>#cogMNUPICSelector:receiver:methodOperand:numArgs: */
CogMethod *
cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs)
{
    CogMethod *actualPIC;
    usqInt startAddress;
    CogMethod *writablePIC;

	if ((isYoung(selector))
	 || ((inlineCacheTagForInstance(rcvr)) == 0 /* picAbortDiscriminatorValue */)) {
		return 0;
	}
	compilationBreakpointclassTagisMNUCase(selector, fetchClassTagOf(rcvr), 1);
	assert(endCPICCase0);
	startAddress = allocate(closedPICSize);
	if (!startAddress) {
		callForCogCompiledCodeCompaction();
		return 0;
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif


	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);

	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = CMMaxUsageCount / 2);

	/* cpicHasMNUCase: */
	(writablePIC->cpicHasMNUCaseOrCMIsFullBlock) = 1;
	(writablePIC->cPICNumCases = 1);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 1);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureMNUCPICmethodOperandnumArgsdelta((actualPIC = ((CogMethod *) startAddress)), methodOperand, numArgs, startAddress - (((usqInt)cPICPrototype)));
	flushICacheFromto(backEnd, startAddress, startAddress + closedPICSize);
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, startAddress + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return actualPIC;
}


/*	Create an Open PIC. Temporarily create a direct call of
	ceSendFromOpenPIC:. Should become a probe of the first-level method lookup
	cache followed by a
	call of ceSendFromOpenPIC: if the probe fails. */

	/* Cogit>>#cogOpenPICSelector:numArgs: */
static NoDbgRegParms CogMethod *
cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs)
{
    sqInt codeSize;
    sqInt end;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;
    CogMethod *pic;
    usqInt startAddress;

	compilationBreakpointisMNUCase(selector, 0);
	startAddress = allocate(openPICSize);
	if (!startAddress) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	(methodLabel->address = startAddress);
	(methodLabel->dependent = null);

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	compileOpenPICnumArgs(selector, numArgs);
	computeMaximumSizes();
	concretizeAt(methodLabel, startAddress);
	codeSize = generateInstructionsAt(startAddress + (sizeof(CogMethod)));

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	mapSize = generateMapAtstart((startAddress + openPICSize) - 1, startAddress + cmNoCheckEntryOffset);
	assert((((entry->address)) - startAddress) == cmEntryOffset);
	assert(((roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpLength(mapSize))) <= openPICSize);
	end = outputInstructionsAt(startAddress + (sizeof(CogMethod)));
	pic = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));

	/* begin fillInOPICHeader:numArgs:selector: */
	(pic->cmType = CMOpenPIC);
	(pic->objectHeader = 0);
	(pic->blockSize = openPICSize);
	addToOpenPICList(pic);
	(pic->methodHeader = 0);
	(pic->selector = selector);
	(pic->cmNumArgs = numArgs);
	(pic->cmHasMovableLiteral = isNonImmediate(selector));
	if ((pic->cmRefersToYoung = isYoung(selector))) {
		addToYoungReferrers(pic);
	}
	(pic->cmUsageCount = initialOpenPICUsageCount());

	/* cpicHasMNUCase: */
	(pic->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(pic->cPICNumCases = 0);
	(pic->blockEntryOffset = 0);
	flushICacheFromto(backEnd, (((usqInt)pic)) - codeToDataDelta, ((((usqInt)pic)) - codeToDataDelta) + openPICSize);
	assert(isCMOpenPIC(((CogBlockMethod *) pic)));
	assert(((pic->selector)) == selector);
	assert(((pic->cmNumArgs)) == numArgs);
	assert((callTargetFromReturnAddress(backEnd, ((((sqInt)pic)) - codeToDataDelta) + missOffset)) == (picAbortTrampolineFor(numArgs)));
	assert(openPICSize == (roundUpLength(openPICSize)));
	((((usqInt)pic)) - codeToDataDelta) + openPICSize;

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, ((((usqInt)pic)) - codeToDataDelta) + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return ((CogMethod *) startAddress);
}


/*	Attempt to create a two-case PIC for case0CogMethod and
	case1Method,case1Tag. The tag for case0CogMethod is at the send site and
	so doesn't need to be generated.
	case1Method may be any of
	- a Cog method; link to its unchecked entry-point
	- a CompiledMethod; link to ceInterpretMethodFromPIC:
	- a CompiledMethod; link to ceMNUFromPICMNUMethod:receiver: */

	/* Cogit>>#cogPICSelector:numArgs:Case0Method:Case1Method:tag:isMNUCase: */
static NoDbgRegParms CogMethod *
cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase)
{
    CogMethod *actualPIC;
    usqInt startAddress;
    CogMethod *writablePIC;

	if (isYoung(selector)) {
		return ((CogMethod *) YoungSelectorInPIC);
	}
	compilationBreakpointclassTagisMNUCase(selector, case1Tag, isMNUCase);
	startAddress = allocate(closedPICSize);
	if (!startAddress) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);

	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);

	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = CMMaxUsageCount / 2);

	/* cpicHasMNUCase: */
	(writablePIC->cpicHasMNUCaseOrCMIsFullBlock) = isMNUCase;
	(writablePIC->cPICNumCases = 2);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 2);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta((actualPIC = ((CogMethod *) startAddress)), case0CogMethod, case1MethodOrNil, case1Tag, isMNUCase, numArgs, startAddress - (((usqInt)cPICPrototype)));
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));
	return actualPIC;
}


/*	Attempt to produce a machine code method for the bytecode method
	object aMethodObj. N.B. If there is no code memory available do *NOT*
	attempt to reclaim the method zone. Certain clients (e.g. ceSICMiss:)
	depend on the zone remaining constant across method generation. */

	/* Cogit>>#cog:selector: */
CogMethod *
cogselector(sqInt aMethodObj, sqInt aSelectorOop)
{
    CogMethod *cogMethod;
    sqInt selector;

	assert(!((methodHasCogMethod(aMethodObj))));

	/* coInterpreter stringOf: selector */
	selector = (aSelectorOop == (nilObject())
				? maybeSelectorOfMethod(aMethodObj)
				: aSelectorOop);
	if (selector) {
		compilationBreakpointisMNUCase(selector, 0);
	}
	if (aMethodObj == breakMethod) {
		haltmsg("Compilation of breakMethod");
	}
	if (methodUsesAlternateBytecodeSet(aMethodObj)) {
		if ((numElementsIn(generatorTable)) <= 0x100) {
			return null;
		}
		bytecodeSetOffset = 0x100;
	}
	else {
		bytecodeSetOffset = 0;
	}
	assert(!((isFullBlockMethod(aMethodObj))));
	methodObj = aMethodObj;
	methodHeader = methodHeaderOf(aMethodObj);

	/* lazy initialization */
	receiverTags = -1;
	cogMethod = compileCogMethod(aSelectorOop);
	if ((((((sqInt)cogMethod)) >= MaxNegativeErrorCode) && ((((sqInt)cogMethod)) <= -1))) {
		if ((((sqInt)cogMethod)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		return null;
	}
	return cogMethod;
}

	/* Cogit>>#collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
static NoDbgRegParms sqInt
collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt address;
    sqInt annotation;
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;

	if (!descriptor) {
		return 0;
	}
	if (!((descriptor->isMapped))) {
		return 0;
	}
	address = positive32BitIntegerFor(((usqInt)mcpc));
	if (!address) {
		return PrimErrNoMemory;
	}
	storePointerUncheckedofObjectwithValue(cogConstituentIndex, topRemappableOop(), address);
	storePointerUncheckedofObjectwithValue(cogConstituentIndex + 1, topRemappableOop(), (((usqInt)bcpc << 1) | 1));

	/* Collect any first case classTags for closed PICs. */
	cogConstituentIndex += 2;
	if (((!(isBackwardBranchAndAnnotation & 1)))
	 && (/* isSendAnnotation: */
		((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= IsSendCall))) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* send is linked */
			annotation = ((usqInt)(isBackwardBranchAndAnnotation)) >> 1;

			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (((targetMethod1->cmType)) == CMClosedPIC) {
				(targetMethod1->methodObject = classForInlineCacheTag(longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))))));
			}
		}
	}
	return 0;
}


/*	Answer a description of the mapping between machine code pointers and
	bytecode pointers for the Cog Method.
	First value is the address of the cog method.
	Following values are pairs of machine code pc and bytecode pc */

	/* Cogit>>#collectCogMethodConstituent: */
static NoDbgRegParms sqInt
collectCogMethodConstituent(CogMethod *cogMethod)
{
    sqInt address;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    CogBlockMethod *cogBlockMethod;
    sqInt data;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    sqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt nSlots;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;

	if (!(((cogMethod->cmType)) >= CMMethod)) {
		return positive32BitIntegerFor(((usqInt)cogMethod));
	}
	cogBlockMethod = ((CogBlockMethod *) cogMethod);
	if (!((cogBlockMethod->stackCheckOffset))) {

		/* isFrameless ? */
		return positive32BitIntegerFor(((usqInt)cogMethod));
	}

	/* +1 for first address */
	nSlots = ((((byteSizeOf((cogMethod->methodObject))) - (startPCOfMethodHeader((cogMethod->methodHeader)))) * 2) + (minSlotsForShortening())) + 1;
	data = instantiateClassindexableSize(splObj(ClassArray), nSlots);
	if (!data) {
		return null;
	}
	pushRemappableOop(data);
	address = positive32BitIntegerFor(((usqInt)cogMethod));
	if (!address) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, topRemappableOop(), address);
	cogConstituentIndex = 1;
	startbcpc = startPCOfMethod((cogMethod->methodObject));

	/* begin mapFor:bcpc:performUntil:arg: */
	descriptor = ((BytecodeDescriptor *) 0);
	latestContinuation = 0;
	mapByte = 0;
	nextBcpc = 0;
	assert(((cogBlockMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset));
	result = collectCogConstituentForAnnotationMcpcBcpcMethod(null, 0 + ((((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)cogMethod));
	if (result) {
		errCode = result;
		goto l1;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogBlockMethod->cmType)) >= CMMethod) {
		isInBlock = 0 /* cmIsFullBlock */;
		homeMethod = ((CogMethod *) cogBlockMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;

		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogBlockMethod->startpc)));
		homeMethod = cmHomeMethod(cogBlockMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogBlockMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - BlockCreationBytecodeSize;
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
		: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l1;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l1;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
		: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
								? nExts + 1
								: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
					 && ((/* begin isBackwardBranch:at:exts:in: */
						assert(((descriptor->spanFunction))),
					(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = collectCogConstituentForAnnotationMcpcBcpcMethod(descriptor, (isBackwardBranch
							? ((((usqInt)(annotation) << 1))) + 1
							: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
							? bcpc - (2 * nExts)
							: bcpc), ((void *)cogMethod));
				if (result) {
					errCode = result;
					goto l1;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
							? nExts + 1
							: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	errCode = 0;
l1:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode) {
		popRemappableOop();
		return null;
	}
	if (cogConstituentIndex < nSlots) {
		shortentoIndexableSize(topRemappableOop(), cogConstituentIndex);
	}
	return popRemappableOop();
}

	/* Cogit>>#compactCogCompiledCode */
void
compactCogCompiledCode(void)
{
	assertValidDualZone();
	assert(noCogMethodsMaximallyMarked());
	moveProfileToMethods();

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	markActiveMethodsAndReferents();
	freeOlderMethodsForCompaction();
	compactPICsWithFreedTargets();
	planCompaction();
	updateStackZoneReferencesToCompiledCodePreCompaction();
	relocateMethodsPreCompaction();
	assertValidDualZone();
	compactCompiledCode();
	stopsFromto(backEnd, freeStart(), (youngReferrers) - 1);
	flushICacheFromto(backEnd, ((usqInt)methodZoneBase), ((usqInt)(youngReferrers)));
	assert(allMethodsHaveCorrectHeader());
	assert(kosherYoungReferrers());
	assertValidDualZone();
}

	/* Cogit>>#compactPICsWithFreedTargets */
static void
compactPICsWithFreedTargets(void)
{
    CogMethod *cogMethod;
    sqInt count;

	cogMethod = ((CogMethod *) methodZoneBase);
	count = 0;
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) == CMClosedPIC)
		 && (cPICCompactAndIsNowEmpty(cogMethod))) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMFree);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		count += 1;
	}
	assert(count == (numMethods()));
}


/*	The start of a CogMethod has a call to a run-time abort routine that
	either handles an in-line cache failure or a stack overflow. The routine
	selects the
	path depending on ReceiverResultReg; if zero it takes the stack overflow
	path; if nonzero the in-line cache miss path. Neither of these paths
	returns. The abort routine must be called; In the callee the method is
	located by
	adding the relevant offset to the return address of the call.
	
	N.B. This code must match that in compilePICAbort: so that the offset of
	the return address of the call is the same in methods and closed PICs. */

	/* Cogit>>#compileAbort */
static AbstractInstruction *
compileAbort(void)
{
    AbstractInstruction *anInstruction;
    sqInt callTarget;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	stackOverflowCall = anInstruction;
	sendMiss = genoperand(PushR, LinkReg);
	callTarget = methodAbortTrampolineFor(methodOrBlockNumArgs);

	/* begin Call: */
	return genoperand(Call, callTarget);
}

	/* Cogit>>#compileBlockDispatchFrom:to: */
static NoDbgRegParms sqInt
compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex)
{
    AbstractInstruction *anInstruction;
    BlockStart *blockStart;
    sqInt halfWay;
    AbstractInstruction *jmp;
    sqInt quickConstant;

	if (lowBlockStartIndex == highBlockStartIndex) {
		blockStart = blockStartAt(lowBlockStartIndex);

		/* Jump: */
		genoperand(Jump, ((sqInt)((blockStart->entryLabel))));
		return null;
	}
	halfWay = (highBlockStartIndex + lowBlockStartIndex) / 2;
	assert(((halfWay >= lowBlockStartIndex) && (halfWay <= highBlockStartIndex)));

	/* N.B. FLAGS := TempReg - startpc */
	blockStart = blockStartAt(halfWay);
	quickConstant = (((usqInt)(((blockStart->startpc)) + 1) << 1) | 1);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	if (lowBlockStartIndex == halfWay) {
		/* JumpLessOrEqual: */
		genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)((blockStart->entryLabel))));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
		return null;
	}
	if ((halfWay + 1) == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);

		/* JumpGreater: */
		genConditionalBranchoperand(JumpGreater, ((sqInt)((blockStart->entryLabel))));
		return compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	}
	jmp = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	if (halfWay == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);
		jmpTarget(jmp, (blockStart->entryLabel));
	}
	else {
		jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
	}
	return 0;
}


/*	Compile a block's entry. This looks like a dummy CogBlockMethod header
	(for frame parsing)
	followed by either a frame build, if a frame is required, or nothing. The
	CogMethodHeader's objectHeader field is a back pointer to the method, but
	this can't be filled in until code generation. */

	/* Cogit>>#compileBlockEntry: */
static NoDbgRegParms void
compileBlockEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    sqInt alignment;

	alignment = blockAlignment();

	/* begin AlignmentNops: */
	genoperand(AlignmentNops, alignment);
	(blockStart->fakeHeader = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	switch (sizeof(CogBlockMethod)) {
	case 8:
		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);
		break;
	case 12:
		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);
		break;
	case 16:
		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);

		/* Fill32: */
		genoperand(Fill32, 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	(blockStart->entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (needsFrame) {
		compileBlockFrameBuild(blockStart);
		if (recordBlockTrace()) {
			/* begin CallRT: */
			abstractInstruction = genoperand(Call, ceTraceBlockActivationTrampoline);
			(abstractInstruction->annotation = IsRelativeCall);
		}
	}
	else {
		compileBlockFramelessEntry(blockStart);
	}
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. The encoding for constants is defined by
	trampolineArgConstant: & trampolineArgValue:. Pass a constant as the
	result of trampolineArgConstant:. */

	/* Cogit>>#compileCallFor:numArgs:arg:arg:arg:arg:resultReg:regsToSave: */
static NoDbgRegParms void
compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask)
{
    AbstractInstruction *anInstruction;
    const int cStackAlignment = STACK_ALIGN_BYTES & STACK_ALIGN_MASK;
    sqInt delta;
    sqInt numRegsPushed;
    usqInt regMaskCopy;
    sqInt regsToSave;
    sqInt wordsPushedModAlignment;

	regsToSave = (resultRegOrNone == NoReg
				? regMask
				: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1U << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1U << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
		regMaskCopy = ((usqInt)regsToSave);
		numRegsPushed = 0;
		while (regMaskCopy != 0) {
			numRegsPushed += regMaskCopy & 1;
			regMaskCopy = ((regMaskCopy) >> 1);
		}
		if ((numRegsPushed == 0)
		 && ((numIntRegArgs(((AbstractInstruction *) backEnd))) >= numArgs)) {
			goto l1;
		}
		wordsPushedModAlignment = (numRegsPushed + ((((numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd)))) < 0) ? 0 : (numArgs - (numIntRegArgs(((AbstractInstruction *) backEnd))))))) % (cStackAlignment / BytesPerWord);
		if (wordsPushedModAlignment) {
			delta = (cStackAlignment / BytesPerWord) - wordsPushedModAlignment;

			/* begin SubCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(SubCqR, delta * BytesPerWord, SPReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(delta * BytesPerWord));
			}
		}
l1:	/* end genAlignCStackSavingRegisters:numArgs:wordAlignment: */;
	}

	/* #genSaveRegs: #genPushRegisterMask: */
	if (regsToSave) {
		genoperand(PushSTM, regsToSave);
	}
	else {
		/* Label */
		genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	genMarshallNArgsargargargarg(backEnd, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3);

	/* #CallFullRT: #CallFull: #gen:literal: */
	checkLiteralforInstruction(((usqInt)aRoutine), genoperand(CallFull, ((usqInt)aRoutine)));
	genWriteCResultIntoReg(backEnd, resultRegOrNone);

	/* begin genRemoveNArgsFromStack: */
	assert(numArgs <= 4);

	/* #genRestoreRegs: #genPopRegisterMask: */
	if (regsToSave) {
		genoperand(PopLDM, regsToSave);
	}
	else {
		/* Label */
		genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
}


/*	Compile the cache tag computation and the first comparison. Answer the
	address of that comparison. */

	/* Cogit>>#compileCPICEntry */
static AbstractInstruction *
compileCPICEntry(void)
{
	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);

	/* begin CmpR:R: */
	assert(!(0 /* (ClassReg = SPReg) */));
	genoperandoperand(CmpRR, ClassReg, TempReg);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}


/*	The entry code to a method checks that the class of the current receiver
	matches that in the inline cache. Other non-obvious elements are that its
	alignment must be
	different from the alignment of the noCheckEntry so that the method map
	machinery can distinguish normal and super sends (super sends bind to the
	noCheckEntry).  */

	/* Cogit>>#compileEntry */
static void
compileEntry(void)
{
    AbstractInstruction *inst;

	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);

	/* begin CmpR:R: */
	assert(!(0 /* (ClassReg = SPReg) */));
	genoperandoperand(CmpRR, ClassReg, TempReg);

	/* JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)sendMiss));
	noCheckEntry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (((traceFlags & 64) == 64)) {
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);

		/* #CallFullRT: #CallFull: #gen:literal: */
		checkLiteralforInstruction(ceTraceLinkedSendTrampoline, genoperand(CallFull, ceTraceLinkedSendTrampoline));

		/* PopR: */
		genoperand(PopR, LinkReg);
	}
}


/*	Compile the top-level method body. */

	/* Cogit>>#compileMethodBody */
static sqInt
compileMethodBody(void)
{
	if (endPC < initialPC) {
		return 0;
	}
	return compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
}


/*	The start of a PIC has a call to a run-time abort routine that either
	handles a dispatch to an
	interpreted method or a dispatch of an MNU case. The routine selects the
	path by testing
	ClassReg, which holds the inline cache tag; if equal to the
	picAbortDiscriminatorValue (zero)
	it takes the MNU path; if nonzero the dispatch to interpreter path.
	Neither of these paths
	returns. The abort routine must be called; In the callee the PIC is
	located by adding the
	relevant offset to the return address of the call.
	
	N.B. This code must match that in compileAbort so that the offset of the
	return address of
	the call is the same in methods and closed PICs. */

	/* Cogit>>#compilePICAbort: */
static NoDbgRegParms sqInt
compilePICAbort(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    sqInt callTarget;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, 0 /* picAbortDiscriminatorValue */, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0 /* picAbortDiscriminatorValue */));
	}
	picMNUAbort = anInstruction;
	picInterpretAbort = genoperand(PushR, LinkReg);
	callTarget = picAbortTrampolineFor(numArgs);

	/* begin Call: */
	genoperand(Call, callTarget);
	return 0;
}


/*	Compile the compare of stackLimit against the stack pointer, jumping to
	the stackOverflowCall if
	the stack pointer is below the limit. Answer a bytecode annotated label
	that follows the sequence.
	
	The stack check functions both as a genuine stack limit check to prevent
	calls overflowing stack pages,
	and as an event/context-switch break out. To cause an event check
	(including a check for a required
	context switch), stackLimit is set to the highest possible value, and
	hence all stack limit checks will
	fail. A path in the stack overflow abort then arranges to call event
	checking if it has been requested.
	
	Certain block activations (e.g. valueNoContextSwitch:) must not context
	switch, and in that
	case, SendNumArgs is set to zero to communicate to the stack overflow
	abort that it should
	not perform event/context-switch (yet). */

	/* Cogit>>#compileStackOverflowCheck: */
static NoDbgRegParms AbstractInstruction *
compileStackOverflowCheck(sqInt canContextSwitch)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSkip;
    AbstractInstruction *label;

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(stackLimitAddress(), genoperandoperand(MoveAwR, stackLimitAddress(), TempReg));

	/* begin CmpR:R: */
	assert(!(0 /* (TempReg = SPReg) */));
	genoperandoperand(CmpRR, TempReg, SPReg);
	if (canContextSwitch) {
		/* JumpBelow: */
		genConditionalBranchoperand(JumpBelow, ((sqInt)stackOverflowCall));
		label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	else {
		jumpSkip = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}

		/* Jump: */
		genoperand(Jump, ((sqInt)stackOverflowCall));
		jmpTarget(jumpSkip, (label = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	}

	/* begin annotateBytecode: */
	(label->annotation = HasBytecodePC);
	return label;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg: */
static NoDbgRegParms void
compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsargargargargresultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeEntryOffsets */
static void
computeEntryOffsets(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    AbstractInstruction *sendMissCall;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 24;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	methodOrBlockNumArgs = 0;
	sendMissCall = compileAbort();
	compileEntry();
	computeMaximumSizes();
	generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	cmEntryOffset = ((entry->address)) - methodZoneBase;
	cmNoCheckEntryOffset = ((noCheckEntry->address)) - methodZoneBase;
	missOffset = (((sendMissCall->address)) + ((sendMissCall->machineCodeSize))) - methodZoneBase;
	entryPointMask = BytesPerWord - 1;
	while ((cmEntryOffset & entryPointMask) == (cmNoCheckEntryOffset & entryPointMask)) {
		entryPointMask = (entryPointMask + entryPointMask) + 1;
	}
	if (entryPointMask >= (roundUpToMethodAlignment(backEnd, 1))) {
		error("cannot differentiate checked and unchecked entry-points with current cog method alignment");
	}
	checkedEntryAlignment = cmEntryOffset & entryPointMask;
	uncheckedEntryAlignment = cmNoCheckEntryOffset & entryPointMask;
	assert(checkedEntryAlignment != uncheckedEntryAlignment);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeFullBlockEntryOffsets */
static void
computeFullBlockEntryOffsets(void)
{
}


/*	While we order variables in the CoInterpreter in order of dynamic
	frequency, and hence
	expect that stackPointer will be output first, C optimizers and linkers
	may get their own
	ideas and ``improve upon'' this ordering. So we cannot depend on
	stackPointer being
	at the lowest address of the variables we want to access through
	VarBaseReg. Here we
	choose the minimum amongst a set to try to choose a varBaseAddress that is
	just less
	than but within range of all variables we want to access through it. */

	/* Cogit>>#computeGoodVarBaseAddress */
static usqInt
computeGoodVarBaseAddress(void)
{
    usqInt minAddress;

	/* stackLimit was e.g. lowest using the clang toolchain on MacOS X (prior to the use of variable_order) */
	minAddress = stackLimitAddress();
	if ((stackPointerAddress()) < minAddress) {
		minAddress = stackPointerAddress();
	}
	if ((framePointerAddress()) < minAddress) {
		minAddress = framePointerAddress();
	}
	if ((instructionPointerAddress()) < minAddress) {
		minAddress = instructionPointerAddress();
	}
	if ((argumentCountAddress()) < minAddress) {
		minAddress = argumentCountAddress();
	}
	if ((primFailCodeAddress()) < minAddress) {
		minAddress = primFailCodeAddress();
	}
	return minAddress;
}


/*	This pass assigns maximum sizes to all abstract instructions and
	eliminates jump fixups.
	It hence assigns the maximum address an instruction will occur at which
	allows the next
	pass to conservatively size jumps. */

	/* Cogit>>#computeMaximumSizes */
static void
computeMaximumSizes(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt relativeAddress;

	dumpLiterals(0);
	relativeAddress = 0;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		(abstractInstruction->address = relativeAddress);
		(abstractInstruction->maxSize = computeMaximumSize(abstractInstruction));
		relativeAddress += (abstractInstruction->maxSize);
	}
}


/*	Configure a copy of the prototype CPIC for a two-case PIC for 
	case0CogMethod and
	case1Method
	case1Tag.
	The tag for case0CogMethod is at the send site and so doesn't need to be
	generated. case1Method may be any of
	- a Cog method; jump to its unchecked entry-point
	- a CompiledMethod; jump to the ceInterpretFromPIC trampoline
	- nil; call ceMNUFromPIC
	addDelta is the address change from the prototype to the new CPIC
	location, needed
	because the loading of the CPIC label at the end may use a literal instead
	of a pc relative load. */
/*	self disassembleFrom: cPIC asInteger + (self sizeof: CogMethod) to: cPIC
	asInteger + closedPICSize
 */

	/* Cogit>>#configureCPIC:Case0:Case1Method:tag:isMNUCase:numArgs:delta: */
static NoDbgRegParms sqInt
configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta)
{
    sqInt caseEndAddress;
    int operand;
    sqInt targetEntry;

	assert(case1Method);
	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));
	assert(!(inlineCacheTagIsYoung(case1Tag)));
	if ((!isMNUCase)
	 && (methodHasCogMethod(case1Method))) {
		operand = 0;
		targetEntry = (((sqInt)(cogMethodOf(case1Method)))) + cmNoCheckEntryOffset;
	}
	else {

		/* We do not scavenge PICs, hence we cannot cache the MNU method if it is in new space. */
		operand = ((!case1Method)
			 || (isYoungObject(case1Method))
					? 0
					: case1Method);
		targetEntry = (case1Method
					? (((sqInt)cPIC)) + (picInterpretAbortOffset())
					: (((sqInt)cPIC)) + (sizeof(CogMethod)));
	}
	rewriteJumpLongAttarget(backEnd, (((sqInt)cPIC)) + firstCPICCaseOffset, (((sqInt)case0CogMethod)) + cmNoCheckEntryOffset);

	/* update the cpic case */
	caseEndAddress = addressOfEndOfCaseinCPIC(2, cPIC);
	rewriteCPICCaseAttagobjReftarget(caseEndAddress, case1Tag, operand, ((sqInt)((isMNUCase
		? (((sqInt)cPIC)) + (sizeof(CogMethod))
		: targetEntry))));
	relocateMethodReferenceBeforeAddressby(backEnd, ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd)), addrDelta);
	rewriteJumpLongAttarget(backEnd, (((sqInt)cPIC)) + cPICEndOfCodeOffset, cPICMissTrampolineFor(numArgs));
	return 0;
}


/*	Configure a copy of the prototype CPIC for a one-case MNU CPIC that calls
	ceMNUFromPIC for
	case0Tag The tag for case0 is at the send site and so doesn't need to be
	generated. addDelta is the address change from the prototype to the new
	CPIC location, needed
	because the loading of the CPIC label at the end may be a literal instead
	of a pc-relative load. */
/*	adjust the jump at missOffset, the ceAbortXArgs */

	/* Cogit>>#configureMNUCPIC:methodOperand:numArgs:delta: */
static NoDbgRegParms sqInt
configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta)
{
    usqInt addressFollowingJump;
    usqInt jumpTargetAddr;
    int operand;
    sqInt target;

	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));

	/* set the jump to the case0 method */
	operand = ((!methodOperand)
		 || (isYoungObject(methodOperand))
				? 0
				: methodOperand);
	rewriteJumpLongAttarget(backEnd, (((sqInt)cPIC)) + firstCPICCaseOffset, (((sqInt)cPIC)) + (sizeof(CogMethod)));
	storeLiteralbeforeFollowingAddress(backEnd, operand, ((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd)));
	rewriteJumpLongAttarget(backEnd, (((sqInt)cPIC)) + cPICEndOfCodeOffset, cPICMissTrampolineFor(numArgs));
	relocateMethodReferenceBeforeAddressby(backEnd, ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd)), addrDelta);
	target = addressOfEndOfCaseinCPIC(2, cPIC);

	/* begin rewriteCPIC:caseJumpTo: */
	addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	jumpTargetAddr = target;

	/* begin rewriteCPICJumpAt:target: */
	rewriteTransferAttarget(((AbstractInstruction *) backEnd), addressFollowingJump, jumpTargetAddr);
	return 0;
}


/*	Scan the CPIC for target methods that have been freed and eliminate them.
	Since the first entry cannot be eliminated, answer that the PIC should be
	freed if the first entry is to a free target. Answer if the PIC is now
	empty or should be freed. */

	/* Cogit>>#cPICCompactAndIsNowEmpty: */
static NoDbgRegParms sqInt
cPICCompactAndIsNowEmpty(CogMethod *cPIC)
{
    usqInt addressFollowingJump;
    sqInt entryPoint;
    sqInt i;
    usqInt jumpTargetAddr;
    sqInt methods[MaxCPICCases];
    sqInt pc;
    int tags[MaxCPICCases];
    CogMethod *targetMethod;
    sqInt targets[MaxCPICCases];
    sqInt used;
    sqInt valid;

	used = 0;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));

		/* Collect all target triples except for triples whose entry-point is a freed method */
		valid = 1;
		if (!(/* containsAddress: */
				((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				if (i == 1) {
					return 1;
				}
				valid = 0;
			}
		}
		if (valid) {
			tags[used] = ((i > 1
		? literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd)))
		: 0));
			targets[used] = entryPoint;
			methods[used] = (literalBeforeFollowingAddress(backEnd, pc - ((i == 1
		? jumpLongByteSize(backEnd)
		: (jumpLongConditionalByteSize(backEnd)) + (cmpC32RTempByteSize(backEnd))))));
			used += 1;
		}
	}
	if (used == ((cPIC->cPICNumCases))) {
		return 0;
	}
	if (!used) {
		return 1;
	}
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = used);
	if (used == 1) {
		pc = addressOfEndOfCaseinCPIC(2, cPIC);

		/* begin rewriteCPIC:caseJumpTo: */
		addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
		jumpTargetAddr = pc;

		/* begin rewriteCPICJumpAt:target: */
		rewriteTransferAttarget(((AbstractInstruction *) backEnd), addressFollowingJump, jumpTargetAddr);
		return 0;
	}
	for (i = 1; i < used; i += 1) {
		pc = addressOfEndOfCaseinCPIC(i + 1, cPIC);
		rewriteCPICCaseAttagobjReftarget(pc, tags[i], methods[i], targets[i]);
	}

	/* begin rewriteCPIC:caseJumpTo: */
	addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	jumpTargetAddr = pc - cPICCaseSize;

	/* begin rewriteCPICJumpAt:target: */
	rewriteTransferAttarget(((AbstractInstruction *) backEnd), addressFollowingJump, jumpTargetAddr);
	return 0;
}


/*	scan the CPIC for target methods that have been freed. */

	/* Cogit>>#cPICHasFreedTargets: */
static NoDbgRegParms sqInt
cPICHasFreedTargets(CogMethod *cPIC)
{
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    CogMethod *targetMethod;

	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);

		/* Find target from jump.  Ignore jumps to the interpret and MNU calls within this PIC */
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));
		if (!(/* containsAddress: */
				((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				return 1;
			}
		}
	}
	return 0;
}


/*	Whimsey; we want 16rCA5E10 + cPICPrototypeCaseOffset to be somewhere in
	the middle of the zone.
 */

	/* Cogit>>#cPICPrototypeCaseOffset */
static sqInt
cPICPrototypeCaseOffset(void)
{
	return ((methodZoneBase + (youngReferrers)) / 2) - 13262352;
}


/*	Are any of the jumps from this CPIC to targetMethod? */

	/* Cogit>>#cPIC:HasTarget: */
static NoDbgRegParms sqInt
cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod)
{
    sqInt i;
    sqInt pc;
    sqInt target;

	target = (((usqInt)targetMethod)) + cmNoCheckEntryOffset;

	/* Since this is a fast test doing simple compares we don't need to care that some
	   cases have nonsense addresses in there. Just zip on through. */
	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
		return 1;
	}
	for (i = 2; i <= MaxCPICCases; i += 1) {
		pc += cPICCaseSize;
		if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
			return 1;
		}
	}
	return 0;
}


/*	Answer an Array of the PIC's selector, followed by class and
	targetMethod/doesNotUnderstand: for each entry in the PIC.
 */

	/* Cogit>>#createCPICData: */
static NoDbgRegParms sqInt
createCPICData(CogMethod *cPIC)
{
    sqInt class;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt picData;
    sqInt target;
    CogMethod *targetMethod;

	assert((((cPIC->methodObject)) == 0)
	 || (addressCouldBeOop((cPIC->methodObject))));
	picData = instantiateClassindexableSize(classArray(), (((cPIC->cPICNumCases)) * 2) + 1);
	if (!picData) {
		return picData;
	}
	storePointerUncheckedofObjectwithValue(0, picData, (cPIC->selector));
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {
			/* first case may have been collected and stored here by collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
			class = (cPIC->methodObject);
			if (!class) {
				class = nilObject();
			}
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			class = classForInlineCacheTag(literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd))));
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		if (/* containsAddress: */
			((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
			target = splObj(SelectorDoesNotUnderstand);
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			target = (targetMethod->methodObject);
		}
		storePointerUncheckedofObjectwithValue((i * 2) - 1, picData, class);
		storePointerUncheckedofObjectwithValue(i * 2, picData, target);
	}
	beRootIfOld(picData);
	(cPIC->methodObject = 0);
	return picData;
}


/*	Division is a little weird on some processors. Defer to the backEnd
	to allow it to generate any special code it may need to. */

	/* Cogit>>#DivR:R:Quo:Rem: */
static NoDbgRegParms AbstractInstruction *
gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder)
{
	genDivRRQuoRem(backEnd, rDivisor, rDividend, rQuotient, rRemainder);
	return abstractInstructionAt(opcodeIndex - 1);
}


/*	Return the default number of bytes to allocate for native code at startup.
	The actual value can be set via vmParameterAt: and/or a preference in the
	ini file. */

	/* Cogit>>#defaultCogCodeSize */
int
defaultCogCodeSize(void)
{
	return 0x140000;
}


/*	Answer the number of bytecodes to skip to get to the first bytecode
	past the primitive call and any store of the error code. */

	/* Cogit>>#deltaToSkipPrimAndErrorStoreIn:header: */
static NoDbgRegParms sqInt
deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader)
{
	return (/* methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(aMethodObj, aMethodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(aMethodHeader)) == (fetchByteofObject((startPCOfMethodHeader(aMethodHeader)) + (sizeOfCallPrimitiveBytecode(aMethodHeader)), aMethodObj)))
			? (sizeOfCallPrimitiveBytecode(aMethodHeader)) + (sizeOfLongStoreTempBytecode(aMethodHeader))
			: 0);
}

	/* Cogit>>#endPCOf: */
static NoDbgRegParms sqInt
endPCOf(sqInt aMethod)
{
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt end;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt pc;
    sqInt prim;
    sqInt targetPC;

	pc = (latestContinuation = startPCOfMethod(aMethod));
	if (((prim = primitiveIndexOf(aMethod))) > 0) {
		if (isQuickPrimitiveIndex(prim)) {
			return pc - 1;
		}
	}

	/* begin bytecodeSetOffsetFor: */
	assert(!((methodUsesAlternateBytecodeSet(aMethod))));
	bsOffset = 0;
	nExts = 0;
	end = numBytesOf(aMethod);
	while (pc <= end) {
		byte = fetchByteofObject(pc, aMethod);
		descriptor = generatorAt(byte + bsOffset);
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			end = pc;
		}
		if ((isBranch(descriptor))
		 || ((descriptor->isBlockCreation))) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, aMethod);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
			if ((descriptor->isBlockCreation)) {
				pc += distance;
			}
		}
		else {
		}
		nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
		pc += (descriptor->numBytes);
	}
	return end;
}


/*	This is a static version of ceEnterCogCodePopReceiverReg for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#enterCogCodePopReceiver */
static void
enterCogCodePopReceiver(void)
{
	realCEEnterCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	Answer if the entryPoint's tag is expected to be a selector reference, as
	opposed to a class tag.
 */

	/* Cogit>>#entryPointTagIsSelector: */
static NoDbgRegParms sqInt
entryPointTagIsSelector(sqInt entryPoint)
{
	return (entryPoint < methodZoneBase)
	 || (((entryPoint & entryPointMask) == uncheckedEntryAlignment)
	 || (((entryPoint & entryPointMask) == checkedEntryAlignment)
	 && ((((((CogMethod *) (entryPoint - cmEntryOffset)))->cmType)) == CMOpenPIC)));
}


/*	Use asserts to check if the ClosedPICPrototype is as expected from
	compileClosedPICPrototype, and can be updated as required via
	rewriteCPICCaseAt:tag:objRef:target:. If all asserts pass, answer
	0, otherwise answer a bit mask identifying all the errors. */
/*	self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to:
	methodZoneBase + closedPICSize
 */

	/* Cogit>>#expectedClosedPICPrototype: */
static NoDbgRegParms sqInt
expectedClosedPICPrototype(CogMethod *cPIC)
{
    sqInt classTag;
    sqInt classTagPC;
    sqInt entryPoint;
    sqInt errors;
    sqInt i;
    sqInt methodObjPC;
    sqInt object;
    sqInt pc;

	errors = 0;

	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((usqInt)cPIC)) + firstCPICCaseOffset;
	object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)));
	if (!(asserta(object == (firstPrototypeMethodOop())))) {
		errors = 1;
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
	if (!(asserta(entryPoint == ((cPICPrototypeCaseOffset()) + 13262352)))) {
		errors += 2;
	}
	for (i = 1; i < MaxCPICCases; i += 1) {
		/* verify information in case is as expected. */
		pc += cPICCaseSize;
		methodObjPC = (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == ((subsequentPrototypeMethodOop()) + i)))) {
			errors = errors | 4;
		}
		classTagPC = pc - (jumpLongConditionalByteSize(backEnd));
		classTag = literalBeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == (0xBABE1F15U + i)))) {
			errors = errors | 8;
		}
		entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		if (!(asserta(entryPoint == (((cPICPrototypeCaseOffset()) + 13262352) + (i * 16))))) {
			errors = errors | 16;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == (((subsequentPrototypeMethodOop()) + i) ^ 0xA5A5A5A5U)))) {
			errors = errors | 32;
		}
		classTag = literalBeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == ((0xBABE1F15U + i) ^ 0x5A5A5A5A)))) {
			errors = errors | 64;
		}
		entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		if (!(asserta(entryPoint == ((((cPICPrototypeCaseOffset()) + 13262352) + (i * 16)) ^ 0x55AA50)))) {
			errors = errors | 128;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, (((usqInt)cPIC)) + cPICEndOfCodeOffset);
	if (!(asserta(entryPoint == (cPICMissTrampolineFor(0))))) {
		errors += 0x100;
	}
	return errors;
}


/*	Fill in the block headers now we know the exact layout of the code. */

	/* Cogit>>#fillInBlockHeadersAt: */
static NoDbgRegParms sqInt
fillInBlockHeadersAt(sqInt startAddress)
{
    sqInt aCogMethodOrInteger;
    CogBlockMethod *blockHeader;
    BlockStart *blockStart;
    sqInt i;

	if (!(needsFrame
		 && (blockCount > 0))) {
		return null;
	}
	if (blockNoContextSwitchOffset) {
		assert(blockNoContextSwitchOffset == (((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address))));
	}
	else {
		blockNoContextSwitchOffset = ((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address));
	}
	for (i = 0; i < blockCount; i += 1) {
		blockStart = blockStartAt(i);
		aCogMethodOrInteger = (((blockStart->fakeHeader))->address);

		/* begin writableBlockMethodFor: */
		blockHeader = ((CogBlockMethod *) ((((usqInt)aCogMethodOrInteger)) + codeToDataDelta));
		(blockHeader->homeOffset = ((((blockStart->fakeHeader))->address)) - startAddress);
		(blockHeader->startpc = (blockStart->startpc));
		(blockHeader->cmType = CMBlock);
		(blockHeader->cmNumArgs = (blockStart->numArgs));
		(blockHeader->cbUsesInstVars = (blockStart->hasInstVarRef));
		(blockHeader->stackCheckOffset = ((blockStart->stackCheckLabel)
				? ((((blockStart->stackCheckLabel))->address)) - ((((blockStart->fakeHeader))->address))
				: 0));
	}
	return 0;
}


/*	Fill in the header for theCogMethod method. This may be located at the
	writable mapping. */

	/* Cogit>>#fillInMethodHeader:size:selector: */
static NoDbgRegParms void
fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector)
{
    sqInt actualMethodLocation;
    CogMethod *originalMethod;
    sqInt rawHeader;

	actualMethodLocation = (((usqInt)method)) - codeToDataDelta;
	(method->cmType = CMMethod);
	(method->objectHeader = nullHeaderForMachineCodeMethod());
	(method->blockSize = size);
	(method->methodObject = methodObj);

	/* If the method has already been cogged (e.g. Newspeak accessors) then
	   leave the original method attached to its cog method, but get the right header. */
	rawHeader = rawHeaderOf(methodObj);
	if (isCogMethodReference(rawHeader)) {
		originalMethod = ((CogMethod *) rawHeader);
		assert(((originalMethod->blockSize)) == size);
		assert(methodHeader == ((originalMethod->methodHeader)));
	}
	else {
		rawHeaderOfput(methodObj, actualMethodLocation);
	}
	(method->methodHeader = methodHeader);
	(method->selector = selector);
	(method->cmNumArgs = argumentCountOfMethodHeader(methodHeader));
	(method->cmHasMovableLiteral = hasMovableLiteral);
	if ((method->cmRefersToYoung = hasYoungReferent)) {
		addToYoungReferrers(method);
	}
	(method->cmUsageCount = initialMethodUsageCount());

	/* cpicHasMNUCase: */
	(method->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(method->cmUsesPenultimateLit = maxLitIndex >= ((literalCountOfMethodHeader(methodHeader)) - 2));
	(method->blockEntryOffset = (blockEntryLabel
			? ((blockEntryLabel->address)) - actualMethodLocation
			: 0));
	if (needsFrame) {
		if (!((((stackCheckLabel->address)) - actualMethodLocation) <= MaxStackCheckOffset)) {
			error("too much code for stack check offset");
		}
	}
	(method->stackCheckOffset = (needsFrame
			? ((stackCheckLabel->address)) - actualMethodLocation
			: 0));
	assert((callTargetFromReturnAddress(backEnd, actualMethodLocation + missOffset)) == (methodAbortTrampolineFor((method->cmNumArgs))));
	assert(size == (roundUpLength(size)));

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, actualMethodLocation + cmNoCheckEntryOffset, codeToDataDelta);
#  endif
}

	/* Cogit>>#findBackwardBranch:IsBackwardBranch:Mcpc:Bcpc:MatchingBcpc: */
static NoDbgRegParms sqInt
findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc)
{
	return ((((isBackwardBranchAndAnnotation & 1) != 0))
	 && ((((sqInt)targetBcpc)) == bcpc)
			? ((sqInt)mcpc)
			: 0);
}

	/* Cogit>>#findBlockMethodWithEntry:startBcpc: */
static NoDbgRegParms usqInt
findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((cogBlockMethod->startpc)) == startBcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}

	/* Cogit>>#findMapLocationForMcpc:inMethod: */
static NoDbgRegParms sqInt
findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;

	mapByte = 0;
	mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	if (mcpc == targetMcpc) {
		return map;
	}
	while (((mapByte = byteAt(map))) != MapEnd) {
		annotation = ((usqInt)(mapByte)) >> AnnotationShift;
		if (annotation != IsAnnotationExtension) {
			mcpc += 4 /* codeGranularity */ * ((annotation
		? mapByte & DisplacementMask
		: ((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))));
		}
		if (mcpc >= targetMcpc) {
			assert(mcpc == targetMcpc);
			if (!annotation) {
				map -= 1;
				mapByte = byteAt(map);
				annotation = ((usqInt)(mapByte)) >> AnnotationShift;
				assert(annotation > IsAnnotationExtension);
			}
			return map;
		}
		map -= 1;
	}
	return 0;
}


/*	Find the CMMethod or CMBlock that has zero-relative startbcpc as its first
	bytecode pc.
	As this is for cannot resume processing and/or conversion to machine-code
	on backward
	branch, it doesn't have to be fast. Enumerate block returns and map to
	bytecode pcs. */

	/* Cogit>>#findMethodForStartBcpc:inHomeMethod: */
CogBlockMethod *
findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	if (startbcpc == (startPCOfMethodHeader((cogMethod->methodHeader)))) {
		return ((CogBlockMethod *) cogMethod);
	}
	assert(((cogMethod->blockEntryOffset)) != 0);
	return ((CogBlockMethod *) (blockDispatchTargetsForperformarg(cogMethod, findBlockMethodWithEntrystartBcpc, startbcpc)));
}


/*	Machine code addresses map to the following bytecode for all bytecodes
	except backward branches, where they map to the backward branch itself.
	This is so that loops continue, rather than terminate prematurely. */

	/* Cogit>>#find:IsBackwardBranch:Mcpc:Bcpc:MatchingMcpc: */
static NoDbgRegParms sqInt
findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc)
{
	return (targetMcpc == mcpc
			? ((!descriptor)
			 || (((isBackwardBranchAndAnnotation & 1) != 0))
					? bcpc
					: bcpc + ((descriptor->numBytes)))
			: 0);
}

	/* Cogit>>#firstMappedPCFor: */
static NoDbgRegParms sqInt
firstMappedPCFor(CogMethod *cogMethod)
{
	return (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
}


/*	Answer a fake value for the first method oop in the PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address. */

	/* Cogit>>#firstPrototypeMethodOop */
static sqInt
firstPrototypeMethodOop(void)
{
	return (/* addressIsInCurrentCompilation: */
		((((usqInt)0x5EAF00D)) >= ((methodLabel->address)))
	 && ((((usqInt)0x5EAF00D)) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize))))
			? 0xCA7F00D
			: 0x5EAF00D);
}

	/* Cogit>>#fixupAt: */
static NoDbgRegParms BytecodeFixup *
fixupAt(sqInt fixupPC)
{
	return fixupAtIndex(fixupPC - initialPC);
}

	/* Cogit>>#flagCogMethodForBecome: */
void
flagCogMethodForBecome(CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethodFlaggedForBecome);
}


/*	N.B. because becomeEffectFlags indicates whether jitted methods were
	becommed or not, if this method is called flagged methods exist, will be
	freed, and so on. So there is no need to check. Just do it. */

	/* Cogit>>#freeBecomeFlaggedMethods */
void
freeBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	unlinkSendsToFree();

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#freeCogMethod: */
void
freeCogMethod(CogMethod *cogMethod)
{
	moveProfileToMethods();
	freeMethod(cogMethod);

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Call ceSendMustBeBooleanTo: via the relevant trampoline. */

	/* Cogit>>#genCallMustBeBooleanFor: */
static NoDbgRegParms AbstractInstruction *
genCallMustBeBooleanFor(sqInt boolean)
{
    AbstractInstruction *abstractInstruction;
    sqInt callTarget;

	callTarget = (boolean == (falseObject())
				? ceSendMustBeBooleanAddFalseTrampoline
				: ceSendMustBeBooleanAddTrueTrampoline);

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, callTarget);
	(abstractInstruction->annotation = IsRelativeCall);
	return abstractInstruction;
}

	/* Cogit>>#genConditionalBranch:operand: */
static NoDbgRegParms AbstractInstruction *
genConditionalBranchoperand(sqInt opcode, sqInt operandOne)
{
	return genoperand(opcode, operandOne);
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. The desired arguments and entry-point are pushed on a stackPage's
	stack. The enilopmart pops off the values to be loaded into registers and
	then executes a return instruction to pop off the entry-point and jump to
	it. 
	BEFORE				AFTER			(stacks grow down)
	whatever			stackPointer ->	whatever
	target address =>	reg1 = reg1val, etc
	reg1val				pc = target address
	reg2val
	stackPointer ->	reg3val */

	/* Cogit>>#genEnilopmartFor:and:and:forCall:called: */
static NoDbgRegParms void
(*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void)
{
    AbstractInstruction *anInstruction;
    sqInt endAddress;
    usqInt enilopmart;
    sqInt quickConstant;
    sqInt size;

	zeroOpcodeIndex();
	quickConstant = varBaseAddress;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	genLoadStackPointers(backEnd);
	if (regArg3OrNone != NoReg) {
		/* PopR: */
		genoperand(PopR, regArg3OrNone);
	}
	if (regArg2OrNone != NoReg) {
		/* PopR: */
		genoperand(PopR, regArg2OrNone);
	}

	/* PopR: */
	genoperand(PopR, regArg1);
	genEnilopmartReturn(forCall);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineName, enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. At the point the enilopmart enters machine code via a return
	instruction, any argument registers have been loaded with their values and
	the stack, if
	for call, looks like
	ret pc
	stackPointer ->	target address
	
	and if not for call, looks like
	whatever
	stackPointer ->	target address
	
	If forCall and running on a CISC, ret pc must be left on the stack. If
	forCall and
	running on a RISC, ret pc must be popped into LinkReg. In either case,
	target address must be removed from the stack and jumped/returned to. */

	/* Cogit>>#genEnilopmartReturn: */
static NoDbgRegParms void
genEnilopmartReturn(sqInt forCall)
{
	if (forCall) {
		/* PopR: */
		genoperand(PopR, RISCTempReg);

		/* PopR: */
		genoperand(PopR, LinkReg);

		/* JumpR: */
		genoperand(JumpR, RISCTempReg);
	}
	else {
		/* PopR: */
		genoperand(PopR, PCReg);
	}
}


/*	Generate the routine that writes the current values of the C frame and
	stack pointers into
	variables. These are used to establish the C stack in trampolines back
	into the C run-time.
	This routine assumes the system's frame pointer is the same as that used
	in generated code. */

	/* Cogit>>#generateCaptureCStackPointers: */
static NoDbgRegParms NeverInline void
generateCaptureCStackPointers(sqInt captureFramePointer)
{
    AbstractInstruction *anInstruction;
    sqInt callerSavedReg;
    sqInt fixupSize;
    sqInt offset;
    sqInt opcodeSize;
    sqInt pushedVarBaseReg;
    sqInt quickConstant;
    usqInt startAddress;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 32;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;

	/* Must happen first; value may be used in accessing any of the following addresses */
	startAddress = methodZoneBase;
	callerSavedReg = 0;
	pushedVarBaseReg = 0;
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {

		/* VarBaseReg is not caller-saved; must save and restore it, either by using an available caller-saved reg or push/pop. */
		/* TempReg used below */
		callerSavedReg = availableRegisterOrNoneIn(((ABICallerSavedRegisterMask | (1U << TempReg)) - (1U << TempReg)));
		if (callerSavedReg == NoReg) {
			gNativePushR(VarBaseReg);
			pushedVarBaseReg = 1;
		}
		else {
			/* MoveR:R: */
			genoperandoperand(MoveRR, VarBaseReg, callerSavedReg);
		}
	}
	quickConstant = varBaseAddress;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	if (captureFramePointer) {
		/* #MoveR:Aw: #gen:operand:literal: */
		checkLiteralforInstruction(cFramePointerAddress(), genoperandoperand(MoveRAw, FPReg, cFramePointerAddress()));
	}
	if (pushedVarBaseReg) {
		offset = (pushedVarBaseReg
					? 0 /* leafCallStackPointerDelta */ + BytesPerWord
					: 0 /* leafCallStackPointerDelta */);

		/* begin LoadEffectiveAddressMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, NativeSPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}

		/* #MoveR:Aw: #gen:operand:literal: */
		checkLiteralforInstruction(cStackPointerAddress(), genoperandoperand(MoveRAw, TempReg, cStackPointerAddress()));
	}
	else {
		/* #MoveR:Aw: #gen:operand:literal: */
		checkLiteralforInstruction(cStackPointerAddress(), genoperandoperand(MoveRAw, NativeSPReg, cStackPointerAddress()));
	}
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {
		if (pushedVarBaseReg) {
			gNativePopR(VarBaseReg);
		}
		else {
			/* MoveR:R: */
			genoperandoperand(MoveRR, callerSavedReg, VarBaseReg);
		}
	}
	gNativeRetN(0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	flushICacheFromto(backEnd, ((usqInt)startAddress), ((usqInt)methodZoneBase));
	recordGeneratedRunTimeaddress("ceCaptureCStackPointers", startAddress);
	ceCaptureCStackPointers = ((void (*)(void)) startAddress);
}


/*	Generate the prototype ClosedPIC to determine how much space a full closed
	PIC takes.
	When we first allocate a closed PIC it only has one or two cases and we
	want to grow it.
	So we have to determine how big a full one is before hand. */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateClosedPICPrototype */
static void
generateClosedPICPrototype(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    CogMethod *cPIC;
    AbstractInstruction * cPICEndOfCodeLabel;
    sqInt endAddress;
    AbstractInstruction * endCPICCase1;
    sqInt fixupSize;
    sqInt h;
    AbstractInstruction *jumpNext;
    sqInt jumpTarget;
    sqInt numArgs;
    sqInt opcodeSize;
    sqInt wordConstant;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = MaxCPICCases * 9;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);

	/* begin compileClosedPICPrototype */
	compilePICAbort((numArgs = 0));

	/* At the end of the entry code we need to jump to the first case code, which is actually the last chunk.
	   On each entension we must update this jump to move back one case. */
	jumpNext = compileCPICEntry();
	wordConstant = firstPrototypeMethodOop();

	/* begin MoveUniqueCw:R: */
	/* begin gen:uniqueLiteral:operand: */
	anInstruction = genoperandoperand(MoveCwR, wordConstant, SendNumArgsReg);
	assert(usesOutOfLineLiteral(anInstruction));
	(anInstruction->dependent = allocateLiteral(wordConstant));
	jumpTarget = (((methodZoneBase + (youngReferrers)) / 2) - 13262352) + 13262352;

	/* begin JumpLong: */
	genoperand(JumpLong, jumpTarget);
	endCPICCase0 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (h = 1; h < MaxCPICCases; h += 1) {
		if (h == (MaxCPICCases - 1)) {
			jmpTarget(jumpNext, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		wordConstant = (subsequentPrototypeMethodOop()) + h;

		/* begin MoveUniqueCw:R: */
		/* begin gen:uniqueLiteral:operand: */
		anInstruction1 = genoperandoperand(MoveCwR, wordConstant, SendNumArgsReg);
		assert(usesOutOfLineLiteral(anInstruction1));
		(anInstruction1->dependent = allocateLiteral(wordConstant));

		/* begin CmpC32:R: */
		/* begin gen:literal32:operand: */
		anInstruction = ((sqInt) (genoperandoperand(CmpCwR, 0xBABE1F15U + h, TempReg)));
		checkLiteralforInstruction(0xBABE1F15U + h, anInstruction);
		jumpTarget = ((((methodZoneBase + (youngReferrers)) / 2) - 13262352) + 13262352) + (h * 16);

		/* begin JumpLongZero: */
		genConditionalBranchoperand(JumpLongZero, ((sqInt)jumpTarget));
		if (h == 1) {
			endCPICCase1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
	}

	/* #MoveCw:R: #gen:literal:operand: */
	checkLiteralforInstruction((methodLabel->address), genoperandoperand(MoveCwR, (methodLabel->address), ClassReg));
	jumpTarget = cPICMissTrampolineFor(numArgs);

	/* begin JumpLong: */
	genoperand(JumpLong, jumpTarget);
	cPICEndOfCodeLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	dumpLiterals(0);
	computeMaximumSizes();
	cPIC = ((CogMethod *) methodZoneBase);
	closedPICSize = (sizeof(CogMethod)) + (generateInstructionsAt(methodZoneBase + (sizeof(CogMethod))));
	endAddress = outputInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	assert((methodZoneBase + closedPICSize) == endAddress);
	firstCPICCaseOffset = ((endCPICCase0->address)) - methodZoneBase;
	cPICEndOfCodeOffset = ((cPICEndOfCodeLabel->address)) - methodZoneBase;
	cPICCaseSize = ((endCPICCase1->address)) - ((endCPICCase0->address));
	cPICEndSize = closedPICSize - (((MaxCPICCases - 1) * cPICCaseSize) + firstCPICCaseOffset);
	closedPICSize = roundUpToMethodAlignment(backEnd, closedPICSize);
	assert(((picInterpretAbort->address)) == (((methodLabel->address)) + (picInterpretAbortOffset())));
	assert((expectedClosedPICPrototype(cPIC)) == 0);
	storeLiteralbeforeFollowingAddress(backEnd, 0, ((endCPICCase0->address)) - (jumpLongByteSize(backEnd)));
	methodZoneBase = alignUptoRoutineBoundary(endAddress);

	/* self cCode: ''
	   inSmalltalk:
	   [self disassembleFrom: cPIC + (self sizeof: CogMethod) to: cPIC + closedPICSize - 1.
	   self halt] */
	cPICPrototype = cPIC;
}


/*	We handle jump sizing simply. First we make a pass that asks each
	instruction to compute its maximum size. Then we make a pass that
	sizes jumps based on the maxmimum sizes. Then we make a pass
	that fixes up jumps. When fixing up a jump the jump is not allowed to
	choose a smaller offset but must stick to the size set in the second pass. */

	/* Cogit>>#generateCogMethod: */
static NoDbgRegParms CogMethod *
generateCogMethod(sqInt selector)
{
    sqInt codeSize;
    usqIntptr_t headerSize;
    sqInt mapSize;
    sqInt result;
    usqInt startAddress;
    sqInt totalSize;

	headerSize = sizeof(CogMethod);
	(methodLabel->address = freeStart());
	computeMaximumSizes();
	concretizeAt(methodLabel, freeStart());
	codeSize = generateInstructionsAt(((methodLabel->address)) + headerSize);
	mapSize = generateMapAtstart(null, ((methodLabel->address)) + cmNoCheckEntryOffset);
	totalSize = roundUpToMethodAlignment(backEnd, (headerSize + codeSize) + mapSize);
	if (totalSize > MaxMethodSize) {
		return ((CogMethod *) MethodTooBig);
	}
	startAddress = allocate(totalSize);
	if (!startAddress) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	assert((startAddress + cmEntryOffset) == ((entry->address)));
	assert((startAddress + cmNoCheckEntryOffset) == ((noCheckEntry->address)));
	result = outputInstructionsAt(startAddress + headerSize);
	assert(((startAddress + headerSize) + codeSize) == result);
	padIfPossibleWithStopsFromto(backEnd, result, ((startAddress + totalSize) - mapSize) - 1);
	generateMapAtstart((startAddress + totalSize) - 1, startAddress + cmNoCheckEntryOffset);
	fillInBlockHeadersAt(startAddress);
	fillInMethodHeadersizeselector(((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta)), totalSize, selector);
	flushICacheFromto(backEnd, startAddress, startAddress + totalSize);
	return ((CogMethod *) startAddress);
}


/*	Generate the method map at addressrNull (or compute it if addressOrNull is
	null). Answer the length of the map in byes. Each entry in the map is in
	two parts. In the
	least signficant bits are a displacement of how far from the start or
	previous entry,
	unless it is an IsAnnotationExtension byte, in which case those bits are
	the extension.
	In the most signficant bits are the type of annotation at the point
	reached. A null
	byte ends the map. */

	/* Cogit>>#generateMapAt:start: */
static NoDbgRegParms sqInt
generateMapAtstart(usqInt addressOrNull, usqInt startAddress)
{
    unsigned char annotation;
    sqInt delta;
    sqInt i;
    AbstractInstruction *instruction;
    sqInt length;
    usqInt location;
    sqInt mapEntry;
    sqInt maxDelta;
    usqInt mcpc;

	delta = 0;
	length = 0;
	location = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		annotation = (instruction->annotation);
		if (annotation) {
			/* begin assertValidAnnotation:for: */
			assert((annotation != (getIsObjectReference()))
			 || (((instruction->opcode)) == Literal));
			mcpc = /* mapEntryAddress */
					(((instruction->opcode)) == Literal
						? (instruction->address)
						: ((instruction->address)) + ((instruction->machineCodeSize)));
			while (((delta = (mcpc - location) / 4 /* codeGranularity */)) > DisplacementMask) {
				maxDelta = (((((delta < MaxX2NDisplacement) ? delta : MaxX2NDisplacement)) | DisplacementMask) - DisplacementMask);
				assert((((usqInt)(maxDelta)) >> AnnotationShift) <= DisplacementMask);
				if (addressOrNull) {
					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, (((usqInt)(maxDelta)) >> AnnotationShift) + DisplacementX2N);
				}
				location += maxDelta * 4 /* codeGranularity */;
				length += 1;
			}
			if (addressOrNull) {
				mapEntry = delta + ((((usqInt)((((annotation < IsSendCall) ? annotation : IsSendCall))) << AnnotationShift)));

				/* begin addToMap:instruction:byte:at:for: */
				codeByteAtput(addressOrNull - length, mapEntry);
			}
			location += delta * 4 /* codeGranularity */;
			length += 1;
			if (annotation > IsSendCall) {

				/* Add the necessary IsAnnotationExtension */
				if (addressOrNull) {
					mapEntry = ((((usqInt)(IsAnnotationExtension) << AnnotationShift))) + (annotation - IsSendCall);

					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, mapEntry);
				}
				length += 1;
			}
		}
	}
	if (addressOrNull) {
		/* begin addToMap:instruction:byte:at:for: */
		codeByteAtput(addressOrNull - length, MapEnd);
	}
	return length + 1;
}


/*	Generate the prototype OpenPIC to determine how much space an open PIC
	takes. 
 */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateOpenPICPrototype */
static void
generateOpenPICPrototype(void)
{
    sqInt codeSize;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);
	compileOpenPICnumArgs(specialSelector(0), numRegArgs());
	computeMaximumSizes();
	concretizeAt(methodLabel, methodZoneBase);
	codeSize = generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	mapSize = generateMapAtstart(null, methodZoneBase + cmNoCheckEntryOffset);

	/* self cCode: ''
	   inSmalltalk:
	   [| end |
	   end := self outputInstructionsAt: methodZoneBase + headerSize.
	   self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to: end - 1.
	   self halt] */
	openPICSize = (roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpToMethodAlignment(backEnd, mapSize));
}


/*	Generate the run-time entries at the base of the native code zone and
	update the base.
 */

	/* Cogit>>#generateRunTimeTrampolines */
static void
generateRunTimeTrampolines(void)
{
	ceSendMustBeBooleanAddFalseTrampoline = genMustBeBooleanTrampolineForcalled(falseObject(), "ceSendMustBeBooleanAddFalseTrampoline");
	ceSendMustBeBooleanAddTrueTrampoline = genMustBeBooleanTrampolineForcalled(trueObject(), "ceSendMustBeBooleanAddTrueTrampoline");

	/* begin genNonLocalReturnTrampoline */
	zeroOpcodeIndex();

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveRAw, LinkReg, instructionPointerAddress()));
	ceNonLocalReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNonLocalReturn, "ceNonLocalReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);

	/* begin genCheckForInterruptsTrampoline */
	zeroOpcodeIndex();

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveRAw, LinkReg, instructionPointerAddress()));
	ceCheckForInterruptTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCheckForInterrupt, "ceCheckForInterruptTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
	ceFetchContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVar, "ceFetchContextInstVarTrampoline", 2, ReceiverResultReg, SendNumArgsReg, null, null, 0 /* emptyRegisterMask */, 1, SendNumArgsReg, 0);

	/* to keep ReceiverResultReg live. */
	ceStoreContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVarvalue, "ceStoreContextInstVarTrampoline", 3, ReceiverResultReg, SendNumArgsReg, ClassReg, null, 0 /* emptyRegisterMask */, 1, ReceiverResultReg, 0);

	/* ceInvokeInterpreter is an optimization and a work-around. Historically we used setjmp/longjmp to reenter the
	   interpreter at the current C stack base.  The C stack base is set at start-up and on each callback enter and
	   callback return. The interpreter must be invoked whenever a non-machine-code method must be run.  That might
	   be when invoking an interpreter method from one of the send linking routines (ceSend:...), or on continuing from
	   an evaluation primitive such as primitiveExecuteMethod.  The problem here is that such primitives could have
	   been invoked by the interpreter or by machine code.  So some form of non-local jump is required. But at least as
	   early as MSVC Community 2017, the Microshaft longjmp performs stack unwinding which gets hoplessly confused
	   (bless its little heart) by any stack switch between machine code and C stack, and raises a spurious
	   Stack cookie instrumentation code detected a stack-based buffer overrun
	   error from the bowels of gs_report.c _GSHandlerCheck.
	   Since the CoInterpreter maintains the base of the C stack in CFramePointer & CStackPointer, it is straight-forward
	   for us to simply call interpret after doing the switch to the C stack, avoiding the stack unwind issue altogether. */
	ceCannotResumeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCannotResume, "ceCannotResumeTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 0);

	/* These two are unusual; they are reached by return instructions. */
	ceInvokeInterpret = genInvokeInterpretTrampoline();
	ceReturnToInterpreterTrampoline = genReturnToInterpreterTrampoline();
	ceBaseFrameReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceBaseFrameReturn, "ceBaseFrameReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 0);
}


/*	Generate a routine ceCaptureCStackPointers that will capture the C stack
	pointer, and, if it is in use, the C frame pointer. These are used in
	trampolines to call
	run-time routines in the interpreter from machine-code. */

	/* Cogit>>#generateStackPointerCapture */
static void
generateStackPointerCapture(void)
{
    usqInt oldMethodZoneBase;
    sqInt oldTrampolineTableIndex;


#  if defined(cFramePointerInUse)
	assertCStackWellAligned();
	generateCaptureCStackPointers(cFramePointerInUse);
#  else
	/* For the benefit of the following assert, assume the minimum at first. */
	cFramePointerInUse = 0;
	assertCStackWellAligned();
	oldMethodZoneBase = methodZoneBase;
	oldTrampolineTableIndex = trampolineTableIndex;
	generateCaptureCStackPointers(1);
	ceCaptureCStackPointers();
	if (!((cFramePointerInUse = checkIfCFramePointerInUse()))) {
		methodZoneBase = oldMethodZoneBase;
		trampolineTableIndex = oldTrampolineTableIndex;
		generateCaptureCStackPointers(0);
	}
#  endif // defined(cFramePointerInUse)


	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	assertCStackWellAligned();
}


/*	Generate the run-time entries and exits at the base of the native code
	zone and update the base.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* Cogit>>#generateTrampolines */
static void
generateTrampolines(void)
{
    sqInt fixupSize;
    usqInt methodZoneStart;
    sqInt opcodeSize;

	methodZoneStart = methodZoneBase;
	(methodLabel->address = methodZoneStart);

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 80;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	setHasYoungReferent(0);
	generateSendTrampolines();
	generateMissAbortTrampolines();
	generateObjectRepresentationTrampolines();
	generateRunTimeTrampolines();
	generateEnilopmarts();
	generateTracingTrampolines();
	recordGeneratedRunTimeaddress("methodZoneBase", methodZoneBase);
}

	/* Cogit>>#generatorForPC: */
static NoDbgRegParms BytecodeDescriptor *
generatorForPC(sqInt pc)
{
	return generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
}


/*	Generate a pair of routines that answer the frame pointer, and the stack
	pointer immediately
	after a leaf call, used for checking stack pointer alignment, frame
	pointer usage, etc. N.B.
	these are exported to the CoInterpreter et al via Cogit
	class>>mustBeGlobal:. 
 */

	/* Cogit>>#genGetLeafCallStackPointers */
static void
genGetLeafCallStackPointers(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    usqInt startAddress;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 4;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
	startAddress = methodZoneBase;

	/* MoveR:R: */
	genoperandoperand(MoveRR, FPReg, ABIResultReg);

	/* RetN: */
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetFP", startAddress);
	ceGetFP = ((usqIntptr_t (*)(void)) startAddress);
	startAddress = methodZoneBase;
	zeroOpcodeIndex();

	/* MoveR:R: */
	genoperandoperand(MoveRR, NativeSPReg, ABIResultReg);

	/* RetN: */
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetSP", startAddress);
	ceGetSP = ((usqIntptr_t (*)(void)) startAddress);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged target
	or a call of ceMNUFromPICMNUMethod:receiver: to handle an MNU dispatch
	in a closed PIC. It distinguishes the two by testing ClassReg. If the
	register is zero then this is an MNU.
	
	This poses a problem in 32-bit Spur, where zero is the cache tag for
	immediate characters (tag pattern 2r10) because SmallIntegers have tag
	patterns 2r11
	and 2r01, so anding with 1 reduces these to 0 & 1. We solve the ambiguity
	by patching send sites with a 0 cache tag to open PICs instead of closed
	PICs.  */

	/* Cogit>>#genInnerPICAbortTrampoline: */
static NoDbgRegParms usqInt
genInnerPICAbortTrampoline(char *name)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpMNUCase;

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0 /* picAbortDiscriminatorValue */, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0 /* picAbortDiscriminatorValue */));
	}
	jumpMNUCase = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceInterpretMethodFromPICreceiver, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpMNUCase, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceMNUFromPICMNUMethodreceiver, name, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
}


/*	Switch to the C stack (do *not* save the Smalltalk stack pointers;
	this is the caller's responsibility), and invoke interpret PDQ. */

	/* Cogit>>#genInvokeInterpretTrampoline */
static void
(*genInvokeInterpretTrampoline(void))(void)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	quickConstant = varBaseAddress;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cReturnAddressAddress(), genoperandoperand(MoveAwR, cReturnAddressAddress(), LinkReg));

	/* #JumpFullRT: #JumpFull: #gen:literal: */
	checkLiteralforInstruction(((sqInt)(((usqInt)interpret))), genoperand(JumpFull, ((sqInt)(((usqInt)interpret)))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceInvokeInterpret", startAddress);
	return ((void (*)(void)) startAddress);
}


/*	The in-line cache for a send is implemented as a constant load into
	ClassReg. We always use a 32-bit load, even in 64-bits.
	
	In the initial (unlinked) state the in-line cache is notionally loaded
	with the selector.
	But since in 64-bits an arbitrary selector oop won't fit in a 32-bit
	constant load, we
	instead load the cache with the selector's index, either into the literal
	frame of the
	current method, or into the special selector array. Negative values are
	1-relative indices into the special selector array.
	
	When a send is linked, the load of the selector, or selector index, is
	overwritten with a
	load of the receiver's class, or class tag. Hence, the 64-bit VM is
	currently constrained
	to use class indices as cache tags. If out-of-line literals are used,
	distinct caches /must
	not/ share acche locations, for if they do, send cacheing will be confused
	by the sharing.
	Hence we use the MoveUniqueC32:R: instruction that will not share literal
	locations.  */

	/* Cogit>>#genLoadInlineCacheWithSelector: */
static NoDbgRegParms void
genLoadInlineCacheWithSelector(sqInt selectorIndex)
{
    AbstractInstruction *anInstruction;
    sqInt cacheValue;
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt initialNumLiterals;
    AbstractInstruction *literalInstruction;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;
    sqInt selector;

	assert((selectorIndex < 0
			? (((-selectorIndex) >= 1) && ((-selectorIndex) <= (numSpecialSelectors())))
			: ((selectorIndex >= 0) && (selectorIndex <= ((literalCountOf(methodObj)) - 1)))));
	selector = (selectorIndex < 0
				? specialSelector(-1 - selectorIndex)
				: getLiteral(selectorIndex));
	assert(addressCouldBeOop(selector));
	if (isNonImmediate(selector)) {
		setHasMovableLiteral(1);
	}
	if (isYoung(selector)) {
		setHasYoungReferent(1);
	}
	cacheValue = selector;

	/* begin MoveUniqueC32:R: */
	/* begin gen:uniqueLiteral32:operand: */
	anInstruction = genoperandoperand(MoveCwR, cacheValue, ClassReg);
	assert(usesOutOfLineLiteral(anInstruction));
	if (nextLiteralIndex >= literalsSize) {
		initialNumLiterals = literalsSize + 8;

		/* begin allocateLiterals: */
		if (initialNumLiterals > literalsSize) {
			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (literals) {
				for (i = 0; i < nextLiteralIndex; i += 1) {
					existingInst = literalInstructionAt(i);
					newInst = (&(newLiterals[i]));
					cloneLiteralFrom(newInst, existingInst);
					assert(!((existingInst->dependent)));
					(existingInst->dependent = newInst);
				}
				for (i = 0; i < opcodeIndex; i += 1) {
					existingInst = abstractInstructionAt(i);
					if ((((existingInst->dependent)))
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	literalInstruction = literalInstructionAt(nextLiteralIndex);
	initializeUniqueLiteral(literalInstruction, cacheValue);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}

	/* setLiteralSize: */
	(anInstruction->dependent = literalInstruction);
}

	/* Cogit>>#genReturnToInterpreterTrampoline */
static usqInt
genReturnToInterpreterTrampoline(void)
{
    AbstractInstruction *anInstruction;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();

	/* PushR: */
	genoperand(PushR, ReceiverResultReg);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, FoxIFSavedIP, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(FoxIFSavedIP));
	}

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveRAw, TempReg, instructionPointerAddress()));
	genSmalltalkToCStackSwitch(0);
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cReturnAddressAddress(), genoperandoperand(MoveAwR, cReturnAddressAddress(), LinkReg));

	/* #JumpFullRT: #JumpFull: #gen:literal: */
	checkLiteralforInstruction(((sqInt)(((usqInt)interpret))), genoperand(JumpFull, ((sqInt)(((usqInt)interpret)))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceReturnToInterpreterTrampoline", startAddress);
	return startAddress;
}


/*	If the client requires, then on an ARM-like RISC processor, the return
	address needs to
	be pushed to the stack so that the interpreter sees the same stack layout
	as on CISC.
 */

	/* Cogit>>#genSmalltalkToCStackSwitch: */
static NoDbgRegParms sqInt
genSmalltalkToCStackSwitch(sqInt pushLinkReg)
{
	if (pushLinkReg) {
		/* PushR: */
		genoperand(PushR, LinkReg);
	}
	genSaveStackPointers(backEnd);
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	return 0;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg:appendOpcodes: */
static NoDbgRegParms usqInt
genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();

	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	assertCoherentCodeAtdelta(backEnd, codeBase + cmNoCheckEntryOffset, codeToDataDelta);
#  endif

	return startAddress;
}


/*	To return from a trampoline call we have to take the return address off
	the stack,
	iof it has been saved */

	/* Cogit>>#genTrampolineReturn: */
static NoDbgRegParms void
genTrampolineReturn(sqInt lnkRegWasPushed)
{
	if (lnkRegWasPushed) {
		/* PopR: */
		genoperand(PopR, PCReg);
	}
	else {
		/* RetN: */
		genoperand(RetN, 0);
	}
}


/*	<Integer> */

	/* Cogit>>#gen: */
static NoDbgRegParms AbstractInstruction *
gen(sqInt opcode)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand: */
static NoDbgRegParms AbstractInstruction *
genoperand(sqInt opcode, sqInt operand)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operand;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand: */
static NoDbgRegParms AbstractInstruction *
genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand:operand: */
static NoDbgRegParms AbstractInstruction *
genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	((abstractInstruction->operands))[2] = operandThree;
	return abstractInstruction;
}

	/* Cogit>>#getLiteral: */
static NoDbgRegParms sqInt
getLiteral(sqInt litIndex)
{
	if (maxLitIndex < litIndex) {
		maxLitIndex = litIndex;
	}
	return literalofMethod(litIndex, methodObj);
}


/*	Access for the literal manager. */

	/* Cogit>>#getOpcodeIndex */
static sqInt
getOpcodeIndex(void)
{
	return opcodeIndex;
}

	/* Cogit>>#incrementUsageOfTargetIfLinkedSend:mcpc:ignored: */
static NoDbgRegParms sqInt
incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;

	if (annotation >= IsSendCall) {
		assert(annotation != IsNSSendCall);
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (((targetMethod1->cmUsageCount)) < (CMMaxUsageCount / 2)) {
				((((CogMethod *) ((((usqInt)targetMethod1)) + codeToDataDelta)))->cmUsageCount = ((targetMethod1->cmUsageCount)) + 1);
			}
		}
	}
	return 0;
}

	/* Cogit>>#initializeCodeZoneFrom:upTo: */
void
initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress)
{
    usqInt endAddressUsqInt;
    AbstractInstruction *existingInst;
    sqInt i;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;
    usqInt startAddressUsqInt;

	/* begin initializeBackend */
	(methodLabel->machineCodeSize = 0);
	(methodLabel->opcode = Label);
	((methodLabel->operands))[0] = 0;
	((methodLabel->operands))[1] = 0;
	assert((!((registerMaskFor(VarBaseReg)) & CallerSavedRegisterMask)));
	varBaseAddress = computeGoodVarBaseAddress();
	assert((stackLimitAddress()) >= varBaseAddress);
	assert((cStackPointerAddress()) >= varBaseAddress);
	assert((cFramePointerAddress()) >= varBaseAddress);
	assert((cReturnAddressAddress()) >= varBaseAddress);
	assert((nextProfileTickAddress()) >= varBaseAddress);

	/* begin allocateLiterals: */
	if (4 > literalsSize) {
		/* Must copy across state (not using realloc, cuz...) and
		   must also update existing instructions to refer to the new ones...
		   It's either this or modify all generation routines to be able to retry
		   with more literals after running out of literals. */
		newLiterals = calloc(4, sizeof(CogAbstractInstruction));
		if (literals) {
			for (i = 0; i < nextLiteralIndex; i += 1) {
				existingInst = literalInstructionAt(i);
				newInst = (&(newLiterals[i]));
				cloneLiteralFrom(newInst, existingInst);
				assert(!((existingInst->dependent)));
				(existingInst->dependent = newInst);
			}
			for (i = 0; i < opcodeIndex; i += 1) {
				existingInst = abstractInstructionAt(i);
				if ((((existingInst->dependent)))
				 && (((((existingInst->dependent))->opcode)) == Literal)) {
					(existingInst->dependent = (((existingInst->dependent))->dependent));
				}
			}
		}
		free(literals);
		literals = newLiterals;
		literalsSize = 4;
	}

	/* begin resetLiterals */
	/* an impossibly high value */
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	sqMakeMemoryExecutableFromToCodeToDataDelta(startAddress, endAddress, 
#  if DUAL_MAPPED_CODE_ZONE
		(&codeToDataDelta)
#  else
		null
#  endif
		);
	codeBase = (methodZoneBase = startAddress);
	stopsFromto(backEnd, startAddress, endAddress - 1);

	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	assertValidDualZone();

	/* detectFeatures */

	/* begin maybeGenerateCacheFlush */
	genGetLeafCallStackPointers();
	generateStackPointerCapture();
	generateTrampolines();
	computeEntryOffsets();
	generateClosedPICPrototype();
	alignMethodZoneBase();
	flushICacheFromto(backEnd, startAddress, ((usqInt)methodZoneBase));
	startAddressUsqInt = startAddress;
	endAddressUsqInt = ((usqInt)methodZoneBase);

	/* begin maybeFlushWritableZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	if (codeToDataDelta > 0) {
		flushDCacheFromto(backEnd, startAddressUsqInt, endAddressUsqInt);
	}
#  endif


	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	generateOpenPICPrototype();
}


/*	Answer a usage count that reflects likely long-term usage.
	Answer 1 for non-primitives or quick primitives (inst var accessors),
	2 for methods with interpreter primitives, and 3 for compiled primitives. */

	/* Cogit>>#initialMethodUsageCount */
static sqInt
initialMethodUsageCount(void)
{
	if ((primitiveIndex == 1)
	 || (isQuickPrimitiveIndex(primitiveIndex))) {
		return 1;
	}
	if (!(primitiveGeneratorOrNil())) {
		return 2;
	}
	return 3;
}


/*	Answer a usage count that reflects likely long-term usage. */

	/* Cogit>>#initialOpenPICUsageCount */
static int
initialOpenPICUsageCount(void)
{
	return CMMaxUsageCount - 1;
}

	/* Cogit>>#inverseBranchFor: */
static NoDbgRegParms sqInt
inverseBranchFor(sqInt opcode)
{
	switch (opcode) {
	case JumpLongZero:
		return JumpLongNonZero;

	case JumpLongNonZero:
		return JumpLongZero;

	case JumpZero:
		return JumpNonZero;

	case JumpNonZero:
		return JumpZero;

	case JumpNegative:
		return JumpNonNegative;

	case JumpNonNegative:
		return JumpNegative;

	case JumpOverflow:
		return JumpNoOverflow;

	case JumpNoOverflow:
		return JumpOverflow;

	case JumpCarry:
		return JumpNoCarry;

	case JumpNoCarry:
		return JumpCarry;

	case JumpLess:
		return JumpGreaterOrEqual;

	case JumpGreaterOrEqual:
		return JumpLess;

	case JumpGreater:
		return JumpLessOrEqual;

	case JumpLessOrEqual:
		return JumpGreater;

	case JumpBelow:
		return JumpAboveOrEqual;

	case JumpAboveOrEqual:
		return JumpBelow;

	case JumpAbove:
		return JumpBelowOrEqual;

	case JumpBelowOrEqual:
		return JumpAbove;

	default:
		error("Case not found and no otherwise clause");
	}
	error("invalid opcode for inverse");
	return 0;
}


/*	Useful for debugging. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#isPCWithinMethodZone: */
static NoDbgRegParms int
isPCWithinMethodZone(void *address)
{
	return (((((usqInt)address)) >= methodZoneBase) && ((((usqInt)address)) <= (freeStart())));
}


/*	Answer if the instruction preceding retpc is a call instruction. */

	/* Cogit>>#isSendReturnPC: */
sqInt
isSendReturnPC(sqInt retpc)
{
    sqInt target;

	if (!(isCallPrecedingReturnPC(backEnd, retpc))) {
		return 0;
	}
	target = callTargetFromReturnAddress(backEnd, retpc);
	return (((target >= firstSend) && (target <= lastSend)))
	 || (((target >= methodZoneBase) && (target <= (freeStart()))));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPEqual(void *jumpTarget)
{
	return genoperand(JumpFPEqual, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreaterOrEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPGreaterOrEqual(void *jumpTarget)
{
	return genoperand(JumpFPGreaterOrEqual, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreater: */
static NoDbgRegParms AbstractInstruction *
gJumpFPGreater(void *jumpTarget)
{
	return genoperand(JumpFPGreater, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPNotEqual: */
static NoDbgRegParms AbstractInstruction *
gJumpFPNotEqual(void *jumpTarget)
{
	return genoperand(JumpFPNotEqual, ((sqInt)jumpTarget));
}


/*	destReg := srcReg << quickConstant */

	/* Cogit>>#LogicalShiftLeftCq:R:R: */
static NoDbgRegParms AbstractInstruction *
gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(LogicalShiftLeftCqRR, quickConstant, srcReg, destReg);
	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#lastOpcode */
static AbstractInstruction *
lastOpcode(void)
{
	assert(opcodeIndex > 0);
	return abstractInstructionAt(opcodeIndex - 1);
}

	/* Cogit>>#linkSendAt:in:to:offset:receiver: */
void
linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver)
{
    sqInt extent;
    sqInt inlineCacheTag;

	assert((theEntryOffset == cmEntryOffset)
	 || (theEntryOffset == cmNoCheckEntryOffset));
	assert(((callSiteReturnAddress >= methodZoneBase) && (callSiteReturnAddress <= (freeStart()))));

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	if (theEntryOffset == cmNoCheckEntryOffset) {

		/* no need to change selector cache tag */
		extent = rewriteCallAttarget(backEnd, callSiteReturnAddress, (((sqInt)targetMethod)) + cmNoCheckEntryOffset);
	}
	else {
		inlineCacheTag = inlineCacheTagForInstance(receiver);
		if (inlineCacheTagIsYoung(inlineCacheTag)) {
			ensureInYoungReferrers(sendingMethod);
		}
		extent = rewriteInlineCacheAttagtarget(backEnd, callSiteReturnAddress, inlineCacheTag, (((sqInt)targetMethod)) + theEntryOffset);
	}
	flushICacheFromto(backEnd, (((usqInt)callSiteReturnAddress)) - extent, ((usqInt)callSiteReturnAddress));
}

	/* Cogit>>#loadBytesAndGetDescriptor */
static BytecodeDescriptor *
loadBytesAndGetDescriptor(void)
{
    BytecodeDescriptor *descriptor;

	byte0 = (fetchByteofObject(bytecodePC, methodObj)) + bytecodeSetOffset;
	descriptor = generatorAt(byte0);
	loadSubsequentBytesForDescriptorat(descriptor, bytecodePC);
	return descriptor;
}

	/* Cogit>>#loadSubsequentBytesForDescriptor:at: */
static NoDbgRegParms void
loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc)
{
	if (((descriptor->numBytes)) > 1) {
		byte1 = fetchByteofObject(pc + 1, methodObj);
		if (((descriptor->numBytes)) > 2) {
			byte2 = fetchByteofObject(pc + 2, methodObj);
			if (((descriptor->numBytes)) > 3) {
				byte3 = fetchByteofObject(pc + 3, methodObj);
				if (((descriptor->numBytes)) > 4) {
					notYetImplemented();
				}
			}
		}
	}
}

	/* Cogit>>#MoveMw:r:R: */
static NoDbgRegParms AbstractInstruction *
gMoveMwrR(sqInt offset, sqInt baseReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, baseReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	return anInstruction;
}


/*	Answer the address of the null byte at the end of the method map. */

	/* Cogit>>#mapEndFor: */
static NoDbgRegParms sqInt
mapEndFor(CogMethod *cogMethod)
{
    sqInt end;

	end = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while ((byteAt(end)) != MapEnd) {
		end -= 1;
		assert(end > (firstMappedPCFor(cogMethod)));
	}
	return end;
}


/*	Unlinking/GC/Disassembly support */
/*	most of the time arg is a CogMethod... */

	/* Cogit>>#mapFor:performUntil:arg: */
static NoDbgRegParms sqInt
mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg)
{
    sqInt annotation;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	mapByte = 0;
	mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = functionSymbol(annotation, (((char *) mcpc)), arg);
			if (result) {
				return result;
			}
		}
		else {
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}


/*	Remap all object references in the closed PIC. Answer if any references
	are young.
	Set codeModified if any modifications are made. */

	/* Cogit>>#mapObjectReferencesInClosedPIC: */
static NoDbgRegParms sqInt
mapObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt pc;
    sqInt refersToYoung;

	/* first we check the potential method oop load at the beginning of the CPIC */
	pc = addressOfEndOfCaseinCPIC(1, cPIC);

	/* We find the end address of the cPICNumCases'th case and can then just step forward by the case size thereafter */
	refersToYoung = remapMaybeObjRefInClosedPICAt(pc - (jumpLongByteSize(backEnd)));

	/* Next we check the potential class ref in the compare instruction, and the potential method oop load for each case. */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (remapMaybeObjRefInClosedPICAt(pc - (jumpLongConditionalByteSize(backEnd)))) {
			refersToYoung = 1;
		}
		if (remapMaybeObjRefInClosedPICAt((pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))) {
			refersToYoung = 1;
		}
		pc += cPICCaseSize;
	}
	return refersToYoung;
}


/*	Update all references to objects in the generated runtime. */

	/* Cogit>>#mapObjectReferencesInGeneratedRuntime */
static void
mapObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    sqInt mappedLiteral;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = longAt(mcpc);
		mappedLiteral = remapObject(literal);
		if (mappedLiteral != literal) {
			/* begin setCodeModified */
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			codeModified = 1;
#      endif


			/* begin storeLiteral:atAnnotatedAddress:using: */
			codeLongAtput(mcpc, mappedLiteral);
		}
	}
}


/*	Update all references to objects in machine code for a become.
	Unlike incrementalGC or fullGC a method that does not refer to young may
	refer to young as a result of the become operation. Unlike incrementalGC
	or fullGC the reference from a Cog method to its methodObject *must not*
	change since the two are two halves of the same object. */

	/* Cogit>>#mapObjectReferencesInMachineCodeForBecome */
static void
mapObjectReferencesInMachineCodeForBecome(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedPIC;
    sqInt hasYoungObj;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt remappedMethod;
    sqInt result;
    CogMethod *writableCogMethod;

	hasYoungObj = 0;
	codeModified = (freedPIC = 0);
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		assert(!hasYoungObj);
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				if ((isYoung((cogMethod->selector)))
				 || (mapObjectReferencesInClosedPIC(cogMethod))) {
					freedPIC = 1;
					freeMethod(cogMethod);
				}
			}
			else {
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					remappedMethod = remapOop((cogMethod->methodObject));
					if (remappedMethod != ((cogMethod->methodObject))) {
						if (methodHasCogMethod(remappedMethod)) {
							error("attempt to become two cogged methods");
						}
						if (!(withoutForwardingOnandwithsendToCogit((cogMethod->methodObject), remappedMethod, (cogMethod->cmUsesPenultimateLit), methodhasSameCodeAscheckPenultimate))) {
							error("attempt to become cogged method into different method");
						}
						if ((rawHeaderOf((cogMethod->methodObject))) == (((sqInt)cogMethod))) {
							rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
							rawHeaderOfput(remappedMethod, ((sqInt)cogMethod));
						}
						else {
							assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
						}
					}
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}

				/* begin mapFor:performUntil:arg: */
				mapByte = 0;
				mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), ((void *)((&hasYoungObj))));
						if (result) {
							goto l1;
						}
					}
					else {
						if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
l1:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					ensureInYoungReferrers(cogMethod);
					hasYoungObj = 0;
				}
				else {
					(cogMethod->cmRefersToYoung = 0);
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (freedPIC) {
		unlinkSendsToFree();
	}
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)codeBase), freeStart());
	}
}


/*	Update all references to objects in machine code for a full gc. Since
	the current (New)ObjectMemory GC makes everything old in a full GC
	a method not referring to young will not refer to young afterwards */

	/* Cogit>>#mapObjectReferencesInMachineCodeForFullGC */
static void
mapObjectReferencesInMachineCodeForFullGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableCogMethod;

	codeModified = 0;
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(!((cogMethod->cmRefersToYoung)));
				mapObjectReferencesInClosedPIC(cogMethod);
			}
			else {
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
				}

				/* begin mapFor:performUntil:arg: */
				mapByte = 0;
				mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), 0);
						if (result) {
							goto l1;
						}
					}
					else {
						if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
l1:	/* end mapFor:performUntil:arg: */;
				if ((cogMethod->cmRefersToYoung)) {
					(writableCogMethod->cmRefersToYoung = 0);
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)codeBase), freeStart());
	}
}


/*	Update all references to objects in machine code for either a Spur
	scavenging gc
	or a Squeak V3 incremental GC. Avoid scanning all code by using the
	youngReferrers list. In a young gc a method referring to young may no
	longer refer to young, but a
	method not referring to young cannot and will not refer to young
	afterwards.  */

	/* Cogit>>#mapObjectReferencesInMachineCodeForYoungGC */
static void
mapObjectReferencesInMachineCodeForYoungGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt hasYoungObj;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    usqInt pointer;
    sqInt result;
    CogMethod *writableCogMethod;
    sqInt zoneIsWritable;

	codeModified = (zoneIsWritable = (hasYoungObj = 0));
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		assert(!hasYoungObj);
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (((cogMethod->cmType)) == CMFree) {
			assert(!((cogMethod->cmRefersToYoung)));
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			if ((cogMethod->cmRefersToYoung)) {
				assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
				 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
				if (!zoneIsWritable) {
					/* begin ensureWritableCodeZone */
#          if !DUAL_MAPPED_CODE_ZONE
#          endif

					zoneIsWritable = 1;
				}
				writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
				(writableCogMethod->selector = remapOop((cogMethod->selector)));
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}

				/* begin mapFor:performUntil:arg: */
				mapByte = 0;
				mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, ((char *) mcpc), ((void *)((&hasYoungObj))));
						if (result) {
							goto l1;
						}
					}
					else {
						if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
l1:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					hasYoungObj = 0;
				}
				else {
					(writableCogMethod->cmRefersToYoung = 0);
				}
			}
		}
		pointer += BytesPerWord;
	}
	pruneYoungReferrers();
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Update all references to objects in machine code. */

	/* Cogit>>#mapObjectReferencesInMachineCode: */
void
mapObjectReferencesInMachineCode(sqInt gcMode)
{
	switch (gcMode) {
	case GCModeNewSpace:

		/* N.B. do *not* ensureWritableCodeZone for every scavenge. */
		mapObjectReferencesInMachineCodeForYoungGC();
		break;
	case GCModeFull:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		mapObjectReferencesInMachineCodeForFullGC();
		break;
	case GCModeBecome:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		mapObjectReferencesInMachineCodeForBecome();
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	mapPerMethodProfile();
	if (!(asserta((freeStart()) <= (youngReferrers)))) {
		error("youngReferrers list overflowed");
	}
}


/*	Free any methods that refer to unmarked objects, unlinking sends to freed
	methods. 
 */

	/* Cogit>>#markAndTraceMachineCodeForNewSpaceGC */
static void
markAndTraceMachineCodeForNewSpaceGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    usqInt pointer;
    sqInt result;

	if (leakCheckNewSpaceGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	codeModified = 0;
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if ((cogMethod->cmRefersToYoung)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
			 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
			if (isYoung((cogMethod->selector))) {
				markAndTrace((cogMethod->selector));
			}
			if (((cogMethod->cmType)) >= CMMethod) {
				if (isYoung((cogMethod->methodObject))) {
					markAndTrace((cogMethod->methodObject));
				}

				/* begin markYoungObjectsIn: */
				assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
				 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
				if (isYoung((cogMethod->selector))) {
					markAndTrace((cogMethod->selector));
				}
				if ((((cogMethod->cmType)) >= CMMethod)
				 && (isYoung((cogMethod->methodObject)))) {
					markAndTrace((cogMethod->methodObject));
				}

				/* begin mapFor:performUntil:arg: */
				mapByte = 0;
				mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = markYoungObjectspcmethod(annotation, ((char *) mcpc), cogMethod);
						if (result) {
							goto l1;
						}
					}
					else {
						if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
l1:	/* end mapFor:performUntil:arg: */;
			}
		}
		pointer += BytesPerWord;
	}
	if (leakCheckNewSpaceGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Mark and trace any object references in the generated run-time. */

	/* Cogit>>#markAndTraceObjectReferencesInGeneratedRuntime */
static void
markAndTraceObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = longAt(mcpc);

		/* begin markAndTraceLiteral:in:atpc: */
		markAndTraceLiteral(literal);
	}
}

	/* Cogit>>#markAndTraceObjectsOrFreeMachineCode: */
void
markAndTraceObjectsOrFreeMachineCode(sqInt inFullGC)
{
	if (inFullGC) {
		markAndTraceOrFreeMachineCodeForFullGC();
	}
	else {
		markAndTraceMachineCodeForNewSpaceGC();
	}
}


/*	Mark and trace objects in the argument and free if it is appropriate.
	Answer if the method has been freed. firstVisit is a hint used to avoid
	scanning methods we've already seen. False positives are fine.
	For a CMMethod this
	frees if the bytecode method isnt marked,
	marks and traces object literals and selectors,
	unlinks sends to targets that should be freed.
	For a CMClosedPIC this
	frees if it refers to anything that should be freed or isn't marked.
	For a CMOpenPIC this
	frees if the selector isn't marked. */
/*	this recurses at most one level down */

	/* Cogit>>#markAndTraceOrFreeCogMethod:firstVisit: */
static NoDbgRegParms sqInt
markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit)
{
    sqInt annotation;
    sqInt literal;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (((cogMethod->cmType)) == CMFree) {
		return 1;
	}
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!(isMarked((cogMethod->methodObject)))) {
			/* begin ensureWritableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
#      endif

			freeMethod(cogMethod);
			return 1;
		}
		if (firstVisit) {
			/* begin markLiteralsAndUnlinkUnmarkedSendsIn: */
			assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
			assert(isMarked((cogMethod->methodObject)));
			literal = (cogMethod->selector);
			(&((cogMethod->selector)));

			/* begin markAndTraceLiteral:in:at: */
			markAndTraceLiteral(literal);
			mapByte = 0;
			mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = markLiteralsAndUnlinkIfUnmarkedSendpcmethod(annotation, ((char *) mcpc), cogMethod);
					if (result) {
						goto l1;
					}
				}
				else {
					if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
l1:	/* end mapFor:performUntil:arg: */;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (!(closedPICRefersToUnmarkedObject(cogMethod))) {
			return 0;
		}

		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		freeMethod(cogMethod);
		return 1;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (isMarked((cogMethod->selector))) {
			return 0;
		}

		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		freeMethod(cogMethod);
		return 1;
	}
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || ((isCMClosedPIC(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod)))));
	return 0;
}


/*	Free any methods that refer to unmarked objects, unlinking sends to freed
	methods. 
 */

	/* Cogit>>#markAndTraceOrFreeMachineCodeForFullGC */
static void
markAndTraceOrFreeMachineCodeForFullGC(void)
{
    CogMethod *cogMethod;

	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	codeModified = 0;
	markAndTraceObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		markAndTraceOrFreeCogMethodfirstVisit(cogMethod, 1);
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	If entryPoint is that of some method, then mark and trace objects in it
	and free if it is appropriate.
	Answer if the method has been freed. */

	/* Cogit>>#markAndTraceOrFreePICTarget:in: */
static NoDbgRegParms sqInt
markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC)
{
    CogMethod *targetMethod;

	assert((entryPoint > methodZoneBase)
	 && (entryPoint < (freeStart())));
	if (/* containsAddress: */
		((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
	 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
		return 0;
	}
	targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
	assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
	 || (isCMFree(((CogBlockMethod *) targetMethod))));
	return markAndTraceOrFreeCogMethodfirstVisit(targetMethod, (((usqInt)targetMethod)) > (((usqInt)cPIC)));
}


/*	Mark and trace literals. Unlink sends that have unmarked cache tags or
	targets. 
 */

	/* Cogit>>#markLiteralsAndUnlinkIfUnmarkedSend:pc:method: */
static NoDbgRegParms sqInt
markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt cacheTagMarked;
    sqInt cacheTagSqInt;
    sqInt entryPointSqInt;
    sqInt literal;
    sqInt literalSqInt;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    sqInt tagCouldBeObjSqInt;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		((usqInt)mcpc);

		/* begin markAndTraceLiteral:in:atpc: */
		markAndTraceLiteral(literal);
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTagSqInt = longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPointSqInt = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObjSqInt = !(BytesPerWord == 8);
		cacheTagMarked = tagCouldBeObjSqInt
			 && (cacheTagIsMarked(cacheTagSqInt));
		if (entryPointSqInt > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPointSqInt - offsetSqInt));
			if ((!cacheTagMarked)
			 || (markAndTraceOrFreeCogMethodfirstVisit(targetMethod1, (((usqInt)targetMethod1)) > (((usqInt)mcpc))))) {

				/* Either the cacheTag is unmarked (e.g. new class) or the target
				   has been freed (because it is unmarked), so unlink the send. */
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
				literalSqInt = (targetMethod1->selector);
				(&((targetMethod1->selector)));

				/* begin markAndTraceLiteral:in:at: */
				markAndTraceLiteral(literalSqInt);
			}
		}
		else {

			/* cacheTag is selector */
			((usqInt)mcpc);

			/* begin markAndTraceCacheTagLiteral:in:atpc: */
			markAndTraceLiteral(cacheTagSqInt);
		}
	}
	return 0;
}

	/* Cogit>>#markMethodAndReferents: */
void
markMethodAndReferents(CogBlockMethod *aCogMethod)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableMethod;

	assert((isCMMethodEtAl(aCogMethod))
	 || (isCMBlock(aCogMethod)));
	cogMethod = (((aCogMethod->cmType)) >= CMMethod
				? ((CogMethod *) aCogMethod)
				: cmHomeMethod(aCogMethod));
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmUsageCount = CMMaxUsageCount);

	/* begin mapFor:performUntil:arg: */
	mapByte = 0;
	mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = incrementUsageOfTargetIfLinkedSendmcpcignored(annotation, ((char *) mcpc), 0);
			if (result) {
				goto l1;
			}
		}
		else {
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
l1:	/* end mapFor:performUntil:arg: */;
}


/*	Mark and trace young literals. */

	/* Cogit>>#markYoungObjects:pc:method: */
static NoDbgRegParms sqInt
markYoungObjectspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt cacheTagSqInt;
    sqInt entryPointSqInt;
    sqInt literal;
    sqInt tagCouldBeObjSqInt;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		markAndTraceLiteralIfYoung(literal);
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTagSqInt = longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPointSqInt = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObjSqInt = !(BytesPerWord == 8);
		if (tagCouldBeObjSqInt) {
			markAndTraceLiteralIfYoung(cacheTagSqInt);
		}
	}
	return 0;
}

	/* Cogit>>#maxCogMethodAddress */
usqInt
maxCogMethodAddress(void)
{
	return ((usqInt)(limitZony()));
}

	/* Cogit>>#maximumDistanceFromCodeZone: */
static NoDbgRegParms sqInt
maximumDistanceFromCodeZone(sqInt anAddress)
{
	return (anAddress > codeBase
			? anAddress - codeBase
			: limitAddress - anAddress);
}


/*	Check that the header fields are consistent with the type.
	Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#maybeFreeCogMethodDoesntLookKosher: */
static NoDbgRegParms sqInt
maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod)
{
    sqInt result;

	result = cogMethodDoesntLookKosher(cogMethod);
	return (result == 2
			? 0
			: result);
}

	/* Cogit>>#mclassIsSmallInteger */
static int
mclassIsSmallInteger(void)
{
	if (receiverTags < 0) {
		receiverTags = receiverTagBitsForMethod(methodObj);
	}
	return (receiverTags & 1);
}


/*	Answer the absolute machine code pc matching the zero-relative
	bytecode pc of a backward branch in cogMethod, given the start
	of the bytecodes for cogMethod's block or method object. */

	/* Cogit>>#mcPCForBackwardBranch:startBcpc:in: */
usqInt
mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpcSqInt;
    sqInt bsOffset;
    sqInt byte;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    sqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt targetPC;

	/* begin mapFor:bcpc:performUntil:arg: */
	descriptor = ((BytecodeDescriptor *) 0);
	latestContinuation = 0;
	mapByte = 0;
	nextBcpc = 0;
	assert(((cogMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(null, 0 + ((((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)bcpc));
	if (result) {
		return result;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpcSqInt = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		isInBlock = 0 /* cmIsFullBlock */;
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;

		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		bcpcSqInt += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpcSqInt == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpcSqInt = startbcpc - BlockCreationBytecodeSize;
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		byte = (fetchByteofObject(bcpcSqInt, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpcSqInt + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpcSqInt, -1, aMethodObj)
		: 0));
		bcpcSqInt = startbcpc;
	}
	nExts = 0;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpcSqInt, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpcSqInt >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpcSqInt >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpcSqInt, nExts, aMethodObj);
							targetPC = (bcpcSqInt + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpcSqInt + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpcSqInt, nExts, aMethodObj)
		: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpcSqInt = nextBcpc;
					nExts = ((descriptor->isExtension)
								? nExts + 1
								: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
					 && ((/* begin isBackwardBranch:at:exts:in: */
						assert(((descriptor->spanFunction))),
					(((descriptor->spanFunction))(descriptor, bcpcSqInt, nExts, aMethodObj)) < 0));
				result = findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(descriptor, (isBackwardBranch
							? ((((usqInt)(annotation) << 1))) + 1
							: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
							? bcpcSqInt - (2 * nExts)
							: bcpcSqInt), ((void *)bcpc));
				if (result) {
					return result;
				}
				bcpcSqInt = nextBcpc;
				nExts = ((descriptor->isExtension)
							? nExts + 1
							: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}


/*	For the purposes of become: see if the two methods are similar, i.e. can
	be safely becommed.
	This is pretty strict. All literals and bytecodes must be identical. Only
	trailer bytes and header
	flags can differ. */

	/* Cogit>>#method:hasSameCodeAs:checkPenultimate: */
static NoDbgRegParms sqInt
methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral)
{
    sqInt bi;
    sqInt endPCA;
    sqInt headerA;
    sqInt headerB;
    sqInt li;
    sqInt numLitsA;

	headerA = methodHeaderOf(methodA);
	headerB = methodHeaderOf(methodB);
	numLitsA = literalCountOfMethodHeader(headerA);
	endPCA = endPCOf(methodA);
	if (((argumentCountOfMethodHeader(headerA)) != (argumentCountOfMethodHeader(headerB)))
	 || (((temporaryCountOfMethodHeader(headerA)) != (temporaryCountOfMethodHeader(headerB)))
	 || (((primitiveIndexOfMethodheader(methodA, headerA)) != (primitiveIndexOfMethodheader(methodB, headerB)))
	 || ((numLitsA != (literalCountOfMethodHeader(headerB)))
	 || (endPCA > (numBytesOf(methodB))))))) {
		return 0;
	}
	for (li = 1; li < numLitsA; li += 1) {
		if ((fetchPointerofObject(li, methodA)) != (fetchPointerofObject(li, methodB))) {
			if ((li < (numLitsA - 1))
			 || (comparePenultimateLiteral)) {
				return 0;
			}
		}
	}
	for (bi = (startPCOfMethodHeader(headerA)); bi <= endPCA; bi += 1) {
		if ((fetchByteofObject(bi, methodA)) != (fetchByteofObject(bi, methodB))) {
			return 0;
		}
	}
	return 1;
}

	/* Cogit>>#mnuOffset */
sqInt
mnuOffset(void)
{
	return missOffset;
}

	/* Cogit>>#NativePopR: */
static NoDbgRegParms AbstractInstruction *
gNativePopR(sqInt reg)
{
	return genoperand(PopR, reg);
}

	/* Cogit>>#NativePushR: */
static NoDbgRegParms AbstractInstruction *
gNativePushR(sqInt reg)
{
	return genoperand(PushR, reg);
}

	/* Cogit>>#NativeRetN: */
static NoDbgRegParms AbstractInstruction *
gNativeRetN(sqInt offset)
{
	return genoperand(RetN, offset);
}

	/* Cogit>>#needsFrameIfImmutability: */
static NoDbgRegParms sqInt
needsFrameIfImmutability(sqInt stackDelta)
{
	return IMMUTABILITY;
}

	/* Cogit>>#needsFrameIfInBlock: */
static NoDbgRegParms sqInt
needsFrameIfInBlock(sqInt stackDelta)
{
	return inBlock > 0;
}

	/* Cogit>>#needsFrameNever: */
static NoDbgRegParms sqInt
needsFrameNever(sqInt stackDelta)
{
	return 0;
}

	/* Cogit>>#noAssertMethodClassAssociationOf: */
static NoDbgRegParms sqInt
noAssertMethodClassAssociationOf(sqInt methodPointer)
{
	return literalofMethod((literalCountOfMethodHeader(noAssertHeaderOf(methodPointer))) - 1, methodPointer);
}


/*	Check that no method is maximally marked. A maximal mark is an indication
	the method has been scanned to increase the usage count of its referent
	methods.  */

	/* Cogit>>#noCogMethodsMaximallyMarked */
static sqInt
noCogMethodsMaximallyMarked(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->cmUsageCount)) == CMMaxUsageCount)) {
			return 0;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}


/*	Answer if all targets in the PIC are in-use methods. */

	/* Cogit>>#noTargetsFreeInClosedPIC: */
static NoDbgRegParms int
noTargetsFreeInClosedPIC(CogMethod *cPIC)
{
	return !(cPICHasFreedTargets(cPIC));
}


/*	Store the generated machine code, answering the last address */

	/* Cogit>>#outputInstructionsAt: */
static NoDbgRegParms sqInt
outputInstructionsAt(sqInt startAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt j;

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	absoluteAddress = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		assert(((abstractInstruction->address)) == absoluteAddress);

		/* begin outputMachineCodeAt: */
		for (j = 0; j < ((abstractInstruction->machineCodeSize)); j += 4) {
			longAtput(absoluteAddress + j, ((abstractInstruction->machineCode))[j / 4]);
		}
		absoluteAddress += (abstractInstruction->machineCodeSize);
	}
	return absoluteAddress;
}


/*	Output instructions generated for one of the generated run-time routines,
	a trampoline, etc
 */

	/* Cogit>>#outputInstructionsForGeneratedRuntimeAt: */
static NoDbgRegParms sqInt
outputInstructionsForGeneratedRuntimeAt(sqInt startAddress)
{
    sqInt endAddress;
    sqInt size;

	computeMaximumSizes();
	(methodLabel->address = startAddress);
	size = generateInstructionsAt(startAddress);
	endAddress = outputInstructionsAt(startAddress);
	assert((startAddress + size) == endAddress);
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	return startAddress;
}


/*	Code entry closed PIC full or miss to an instance of a young class or to a
	young target method.
	Attempt to patch the send site to an open PIC. Answer if the attempt
	succeeded; in fact it will
	only return if the attempt failed.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */

	/* Cogit>>#patchToOpenPICFor:numArgs:receiver: */
sqInt
patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver)
{
    sqInt extent;
    CogMethod *oPIC;
    sqInt outerReturn;

	/* See if an Open PIC is already available. */
	outerReturn = stackTop();
	oPIC = openPICWithSelector(selector);
	if (!oPIC) {

		/* otherwise attempt to create an Open PIC. */
		oPIC = cogOpenPICSelectornumArgs(selector, numArgs);
		if ((((((sqInt)oPIC)) >= MaxNegativeErrorCode) && ((((sqInt)oPIC)) <= -1))) {

			/* For some reason the PIC couldn't be generated, most likely a lack of code memory. */
			if ((((sqInt)oPIC)) == InsufficientCodeSpace) {
				callForCogCompiledCodeCompaction();
			}
			return 0;
		}
	}

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	extent = rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, selector, mframeHomeMethodExport()), (((sqInt)oPIC)) + cmEntryOffset);
	flushICacheFromto(backEnd, (((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
	flushICacheFromto(backEnd, ((usqInt)oPIC), (((usqInt)oPIC)) + openPICSize);
	executeCogMethodfromLinkedSendWithReceiver(oPIC, receiver);
	return 1;
}


/*	This value is used to decide between MNU processing
	or interpretation in the closed PIC aborts. */

	/* Cogit>>#picAbortDiscriminatorValue */
static sqInt
picAbortDiscriminatorValue(void)
{
	return 0;
}


/*	Answer the start of the abort sequence for invoking the interpreter in a
	closed PIC.
 */

	/* Cogit>>#picInterpretAbortOffset */
static sqInt
picInterpretAbortOffset(void)
{
	return (interpretOffset()) - ((pushLinkRegisterByteSize(backEnd)) + (callInstructionByteSize(backEnd)));
}


/*	useful for debugging */

	/* Cogit>>#printCogMethodFor: */
void
printCogMethodFor(void *address)
{
    CogMethod *cogMethod;

	cogMethod = methodFor(address);
	if (cogMethod) {
		printCogMethod(cogMethod);
	}
	else {
		if (codeEntryFor(address)) {
			print("trampoline ");
			print(codeEntryNameFor(address));
			cr();
		}
		else {
			print("not a method");
			cr();
		}
	}
}


/*	useful for debugging */

	/* Cogit>>#printTrampolineTable */
void
printTrampolineTable(void)
{
    sqInt i;

	for (i = 0; i < trampolineTableIndex; i += 2) {
		fprintf(getTranscript(),
				"%p: %s\n",
				((void *)(trampolineAddresses[i + 1])),
				((char *)(trampolineAddresses[i])));
	}
}

	/* Cogit>>#processorHasDivQuoRemAndMClassIsSmallInteger */
static sqInt
processorHasDivQuoRemAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#processorHasMultiplyAndMClassIsSmallInteger */
static sqInt
processorHasMultiplyAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#recordGeneratedRunTime:address: */
static NoDbgRegParms void
recordGeneratedRunTimeaddress(char *aString, sqInt address)
{
	assert((trampolineTableIndex + 2) <= (NumTrampolines * 2));
	trampolineAddresses[trampolineTableIndex] = aString;
	trampolineAddresses[trampolineTableIndex + 1] = (((char *) address));

	/* self printTrampolineTable */
	trampolineTableIndex += 2;
}


/*	This one for C support code. */

	/* Cogit>>#recordPrimTraceFunc */
int
recordPrimTraceFunc(void)
{
	return recordPrimTrace();
}

	/* Cogit>>#recordRunTimeObjectReferences */
static void
recordRunTimeObjectReferences(void)
{
    sqInt i;
    AbstractInstruction *instruction;

	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		if (((instruction->annotation)) == IsObjectReference) {
			assert(runtimeObjectRefIndex < NumObjRefsInRuntime);
			assert(!hasYoungReferent);
			if (hasYoungReferent) {
				error("attempt to generate run-time routine containing young object reference.  Cannot initialize Cogit run-time.");
			}
			objectReferencesInRuntime[runtimeObjectRefIndex] = (((usqInt)(/* mapEntryAddress */
	(((instruction->opcode)) == Literal
		? (instruction->address)
		: ((instruction->address)) + ((instruction->machineCodeSize))))));
			runtimeObjectRefIndex += 1;
		}
	}
}


/*	N.B. (self registerMaskFor: NoReg) = 0 */

	/* Cogit>>#registerMaskFor: */
static NoDbgRegParms sqInt
registerMaskFor(sqInt reg)
{
	return ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1U << reg));
}

	/* Cogit>>#relocateCallsAndSelfReferencesInMethod: */
static NoDbgRegParms void
relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt callDelta;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt refDelta;
    sqInt result;

	refDelta = (cogMethod->objectHeader);
	callDelta = refDelta;
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cogMethod)) + missOffset)) == ((isCMMethodEtAl(((CogBlockMethod *) cogMethod))
			? methodAbortTrampolineFor((cogMethod->cmNumArgs))
			: picAbortTrampolineFor((cogMethod->cmNumArgs)))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cogMethod)) + missOffset, -callDelta);

	/* begin mapFor:performUntil:arg: */
	mapByte = 0;
	mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {
			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = relocateIfCallOrMethodReferencemcpcdelta(annotation, ((char *) mcpc), ((void *)refDelta));
			if (result) {
				goto l1;
			}
		}
		else {
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
l1:	/* end mapFor:performUntil:arg: */;
}

	/* Cogit>>#relocateCallsInClosedPIC: */
static NoDbgRegParms void
relocateCallsInClosedPIC(CogMethod *cPIC)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt refDelta;
    CogMethod *targetMethod;

	refDelta = (cPIC->objectHeader);
	callDelta = refDelta;
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cPIC)) + missOffset)) == (picAbortTrampolineFor((cPIC->cmNumArgs))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cPIC)) + missOffset, -callDelta);
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		entryPoint = (jumpLongTargetBeforeFollowingAddress(backEnd, pc));
		if (/* containsAddress: */
			((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {

			/* Interpret/MNU */
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			if (i == 1) {
				relocateJumpLongBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
			else {
				relocateJumpLongConditionalBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
		}
	}
	assert(((cPIC->cPICNumCases)) > 0);
	relocateMethodReferenceBeforeAddressby(backEnd, (addressOfEndOfCaseinCPIC(2, cPIC)) + (loadPICLiteralByteSize(backEnd)), refDelta);
	relocateJumpLongBeforeFollowingAddressby(backEnd, (((sqInt)cPIC)) + cPICEndOfCodeOffset, -callDelta);
}


/*	To placate the C static type system... */

	/* Cogit>>#relocateIfCallOrMethodReference:mcpc:delta: */
static NoDbgRegParms sqInt
relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt refDelta;
    sqInt *sendTable1;
    CogMethod *targetMethod;
    sqInt unlinkedRoutine;

	refDelta = ((sqInt) refDeltaArg);
	callDelta = refDelta;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {

			/* send is not linked; just relocate */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
			return 0;
		}

		/* begin offsetAndSendTableFor:annotation:into: */
		if (annotation == IsSendCall) {
			offsetSqInt = cmEntryOffset;
			sendTable1 = ordinarySendTrampolines;
		}
		else {
			assert(annotation == IsSuperSend);
			offsetSqInt = cmNoCheckEntryOffset;
			sendTable1 = superSendTrampolines;
		}
		targetMethod = ((CogMethod *) (entryPoint - offsetSqInt));
		if (!(((targetMethod->cmType)) == CMFree)) {

			/* send target not freed; just relocate. */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -(callDelta - ((targetMethod->objectHeader))));
			return 0;
		}
		unlinkedRoutine = sendTable1[((((targetMethod->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod->cmNumArgs)) : (NumSendTrampolines - 1))];
		unlinkedRoutine -= callDelta;
		rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), enumeratingCogMethod), unlinkedRoutine);
		return 0;
	}
	if (annotation == IsRelativeCall) {
		relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
		return 0;
	}
	if (annotation == IsAbsPCReference) {
		relocateMethodReferenceBeforeAddressby(backEnd, ((sqInt)mcpc), refDelta);
	}
	return 0;
}


/*	to placate the C static type system... */

	/* Cogit>>#remapIfObjectRef:pc:hasYoung: */
static NoDbgRegParms sqInt
remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr)
{
    sqInt cacheTagSqInt;
    sqInt entryPointSqInt;
    sqInt literal;
    sqInt mappedCacheTag;
    sqInt mappedLiteral;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    sqInt tagCouldBeObjSqInt;
    CogMethod *targetMethod1;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (couldBeObject(literal)) {
			mappedLiteral = remapObject(literal);
			if (literal != mappedLiteral) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif


				/* begin storeLiteral:atAnnotatedAddress:using: */
				codeLongAtput(((usqInt)mcpc), mappedLiteral);
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedLiteral))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		cacheTagSqInt = longAt(pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPointSqInt = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObjSqInt = !(BytesPerWord == 8);
		if (tagCouldBeObjSqInt
		 && (couldBeObject(cacheTagSqInt))) {
			mappedCacheTag = remapObject(cacheTagSqInt);
			if (cacheTagSqInt != mappedCacheTag) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif


				/* begin rewriteInlineCacheTag:at: */
				longAtput(pcRelativeAddressAt(((AbstractInstruction *) backEnd), (((usqInt)mcpc)) - 8), mappedCacheTag);
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedCacheTag))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
		if (hasYoungPtr) {

			/* Since the unlinking routines may rewrite the cacheTag to the send's selector, and
			   since they don't have the cogMethod to hand and can't add it to youngReferrers,
			   the method must remain in youngReferrers if the targetMethod's selector is young. */
			if (entryPointSqInt > methodZoneBase) {

				/* It's a linked send. */
				/* begin targetMethodAndSendTableFor:annotation:into: */
				/* begin offsetAndSendTableFor:annotation:into: */
				if (annotation == IsSendCall) {
					offsetSqInt = cmEntryOffset;
					sendTable1 = ordinarySendTrampolines;
				}
				else {
					assert(annotation == IsSuperSend);
					offsetSqInt = cmNoCheckEntryOffset;
					sendTable1 = superSendTrampolines;
				}
				targetMethod1 = ((CogMethod *) (entryPointSqInt - offsetSqInt));
				if (isYoung((targetMethod1->selector))) {
					(*((sqInt *) hasYoungPtr) = 1);
				}
			}
		}
	}
	return 0;
}


/*	Remap a potential object reference from a closed PIC.
	This may be an object reference, an inline cache tag or null.
	Answer if the updated literal is young.
	mcpc is the address of the next instruction following either
	the load of the method literal or the compare of the class tag. */

	/* Cogit>>#remapMaybeObjRefInClosedPICAt: */
static NoDbgRegParms sqInt
remapMaybeObjRefInClosedPICAt(sqInt mcpc)
{
    sqInt object;
    sqInt subject;

	object = literalBeforeFollowingAddress(backEnd, mcpc);
	if (!(couldBeObject(object))) {
		return 0;
	}
	subject = remapOop(object);
	if (object != subject) {
		/* begin setCodeModified */
#    if DUAL_MAPPED_CODE_ZONE
		codeModified = 1;
#    else
		codeModified = 1;
#    endif

		storeLiteralbeforeFollowingAddress(backEnd, subject, mcpc);
	}
	return isYoungObject(subject);
}


/*	Rewrite the three values involved in a CPIC case. Used by the initialize &
	extend CPICs.
	c.f. expectedClosedPICPrototype: */
/*	write the obj ref/operand via the second ldr */

	/* Cogit>>#rewriteCPICCaseAt:tag:objRef:target: */
static NoDbgRegParms void
rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget)
{
    sqInt classTagPC;
    sqInt methodObjPC;

	methodObjPC = (followingAddress - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
	storeLiteralbeforeFollowingAddress(backEnd, newObjRef, methodObjPC);

	/* rewite the tag via the first ldr */
	classTagPC = followingAddress - (jumpLongConditionalByteSize(backEnd));

	/* begin storeLiteral32:beforeFollowingAddress: */
	storeLiteralbeforeFollowingAddress(((AbstractInstruction *) backEnd), newTag, classTagPC);
	rewriteConditionalJumpLongAttarget(backEnd, followingAddress, newTarget);
}

	/* Cogit>>#SubCq:R: */
static NoDbgRegParms AbstractInstruction *
gSubCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	return anInstruction;
}


/*	Answer the number of clean blocks found in the literal frame */

	/* Cogit>>#scanForCleanBlocks */
static sqInt
scanForCleanBlocks(void)
{
    sqInt i;
    sqInt lit;
    sqInt numCleanBlocks;
    sqInt startPCOrNil;
    sqInt toDoLimit;

	numCleanBlocks = 0;
	toDoLimit = literalCountOf(methodObj);
	for (i = 1; i <= toDoLimit; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (startPCOrNil) {
			numCleanBlocks += 1;
		}
	}
	return numCleanBlocks;
}


/*	If a method is compiled to machine code via a block entry it won't have a
	selector. A subsequent send can find the method and hence fill in the
	selector. 
 */
/*	self disassembleMethod: cogMethod */

	/* Cogit>>#setSelectorOf:to: */
void
setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop)
{
	compilationBreakpointisMNUCase(aSelectorOop, 0);
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->selector = aSelectorOop);
	if (isYoung(aSelectorOop)) {
		ensureInYoungReferrers(cogMethod);
	}

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#spanForCleanBlockStartingAt: */
static NoDbgRegParms sqInt
spanForCleanBlockStartingAt(sqInt startPC)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt pc;

	pc = startPC;
	end = numBytesOf(methodObj);
	while (pc <= end) {
		/* begin generatorForPC: */
		descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
		pc += (descriptor->numBytes);
		if ((descriptor->isReturn)) {
			return pc - startPC;
		}
	}
	error("couldn't locate end of clean block");
	return 0;
}

	/* Cogit>>#stackCheckOffsetOfBlockAt:isMcpc: */
static NoDbgRegParms usqInt
stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((((sqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset))) == mcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}


/*	Answer a fake value for the method oop in other than the first case in the
	PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address.
 */

	/* Cogit>>#subsequentPrototypeMethodOop */
static sqInt
subsequentPrototypeMethodOop(void)
{
	return (/* addressIsInCurrentCompilation: */
		((((usqInt)0xBADA550)) >= ((methodLabel->address)))
	 && ((((usqInt)0xBADA550)) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize))))
			? 0xDEADEAD
			: 0xBADA550);
}

	/* Cogit>>#traceLinkedSendOffset */
sqInt
traceLinkedSendOffset(void)
{
	return (cmNoCheckEntryOffset + (callFullInstructionByteSize(backEnd))) + (pushLinkRegisterByteSize(backEnd));
}

	/* Cogit>>#trampolineName:numArgs: */
static NoDbgRegParms char *
trampolineNamenumArgs(char *routinePrefix, sqInt numArgs)
{
    char *theString;

	/* begin trampolineName:numArgs:limit: */
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, (numArgs <= (NumSendTrampolines - 2)
			? '0' + numArgs
			: 'N'));
	return theString;
}

	/* Cogit>>#trampolineName:numRegArgs: */
static NoDbgRegParms char *
trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs)
{
    char *theString;

	/* begin trampolineName:numArgs:limit: */
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, (numArgs <= (numRegArgs())
			? '0' + numArgs
			: 'N'));
	return theString;
}

	/* Cogit>>#unflagBecomeFlaggedMethods */
void
unflagBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#unknownBytecode */
static sqInt
unknownBytecode(void)
{
	return EncounteredUnknownBytecode;
}


/*	Unlink all sends in cog methods. */

	/* Cogit>>#unlinkAllSends */
void
unlinkAllSends(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	cogMethod = ((CogMethod *) methodZoneBase);
	voidOpenPICList();
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mapByte = 0;
			mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcignored(annotation, ((char *) mcpc), 0);
					if (result) {
						goto l1;
					}
				}
				else {
					if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (!(((cogMethod->cmType)) == CMFree)) {
				freeMethod(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfFreeOrLinkedSend:pc:of: */
static NoDbgRegParms sqInt
unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector)
{
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if ((((targetMethod1->cmType)) == CMFree)
			 || (((targetMethod1->selector)) == (((sqInt) theSelector)))) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSendToFree:pc:ignored: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (((targetMethod1->cmType)) == CMFree) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfLinkedSend:pc:if: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg)
{
    sqInt (*criterion)(CogMethod *);
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	criterion = ((void *)criterionArg);
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (criterion(targetMethod1)) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:ignored: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));

			/* begin unlinkSendAt:targetMethod:sendTable: */
			unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			codeModified = 1;
#      endif

			rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:to: */
static NoDbgRegParms sqInt
unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod)
{
    sqInt entryPoint;
    sqInt offsetSqInt;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offsetSqInt = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offsetSqInt = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offsetSqInt));
			if (targetMethod1 == theCogMethod) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				codeModified = 1;
#        endif

				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	Unlink all sends in cog methods. Free all Closed PICs with the selector,
	or with an MNU case if isMNUSelector. First check if any method actually
	has the selector; if not there can't be any linked send to it. This
	routine (including descendents) is performance critical. It contributes
	perhaps 30% of entire execution time in Compiler recompileAll. */

	/* Cogit>>#unlinkSendsOf:isMNUSelector: */
void
unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt mustScanAndUnlink;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	cogMethod = ((CogMethod *) methodZoneBase);
	mustScanAndUnlink = 0;
	if (isMNUSelector) {
		while (cogMethod < (limitZony())) {
			if (!(((cogMethod->cmType)) == CMFree)) {
				if ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock)) {
					assert(isCMClosedPIC(((CogBlockMethod *) cogMethod)));
					freeMethod(cogMethod);
					mustScanAndUnlink = 1;
				}
				else {
					if (((cogMethod->selector)) == selector) {
						mustScanAndUnlink = 1;
						if (((cogMethod->cmType)) == CMClosedPIC) {
							freeMethod(cogMethod);
						}
					}
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	else {
		while (cogMethod < (limitZony())) {
			if ((!(((cogMethod->cmType)) == CMFree))
			 && (((cogMethod->selector)) == selector)) {
				mustScanAndUnlink = 1;
				if (((cogMethod->cmType)) == CMClosedPIC) {
					freeMethod(cogMethod);
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	if (!mustScanAndUnlink) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mapByte = 0;
			mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfFreeOrLinkedSendpcof(annotation, ((char *) mcpc), ((CogMethod *) selector));
					if (result) {
						goto l1;
					}
				}
				else {
					if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
l1:	/* end mapFor:performUntil:arg: */;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {

		/* After possibly updating inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Unlink all sends in cog methods to free methods and/or pics. */

	/* Cogit>>#unlinkSendsToFree */
static void
unlinkSendsToFree(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mapByte = 0;
			mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendToFreepcignored(annotation, ((char *) mcpc), 0);
					if (result) {
						goto l1;
					}
				}
				else {
					if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(noTargetsFreeInClosedPIC(cogMethod));
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {

		/* After possibly updating inline caches we need to flush the icache. */
		flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
	}
}


/*	Unlink all sends in cog methods to methods with a machine code
	primitive, and free machine code primitive methods if freeIfTrue.
	To avoid having to scan PICs, free any and all PICs */

	/* Cogit>>#unlinkSendsToMethodsSuchThat:AndFreeIf: */
void
unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedSomething;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = (freedSomething = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (freeIfTrue
			 && (criterion(cogMethod))) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
			else {
				/* begin mapFor:performUntil:arg: */
				mapByte = 0;
				mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {
						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = unlinkIfLinkedSendpcif(annotation, ((char *) mcpc), ((void *)criterion));
						if (result) {
							goto l1;
						}
					}
					else {
						if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
l1:	/* end mapFor:performUntil:arg: */;
			}
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedSomething) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {

			/* After possibly updating inline caches we need to flush the icache. */
			flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
		}
	}

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Unlink all sends in cog methods to a particular target method.
	If targetMethodObject isn't actually a method (perhaps being
	used via invokeAsMethod) then there's nothing to do. */

	/* Cogit>>#unlinkSendsTo:andFreeIf: */
void
unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt freedPIC;
    sqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *targetMethod;

	if (!((isOopCompiledMethod(targetMethodObject))
		 && (methodHasCogMethod(targetMethodObject)))) {
		return;
	}
	targetMethod = cogMethodOf(targetMethodObject);
	if (!methodZoneBase) {
		return;
	}

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	codeModified = (freedPIC = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mapByte = 0;
			mcpc = (((usqInt)cogMethod)) + cmNoCheckEntryOffset;
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {
					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcto(annotation, ((char *) mcpc), targetMethod);
					if (result) {
						goto l1;
					}
				}
				else {
					if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
l1:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if ((((cogMethod->cmType)) == CMClosedPIC)
			 && (cPICHasTarget(cogMethod, targetMethod))) {
				freeMethod(cogMethod);
				freedPIC = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freeIfTrue) {
		freeMethod(targetMethod);
	}
	if (freedPIC) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {

			/* After possibly updating inline caches we need to flush the icache. */
			flushICacheFromto(backEnd, ((usqInt)methodZoneBase), freeStart());
		}
	}

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}

	/* Cogit>>#voidCogCompiledCode */
void
voidCogCompiledCode(void)
{
    CogMethod *cogMethod;
    usqInt theLimitAddress;
    usqInt theStartAddress;

	/* begin clearCogCompiledCode */
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) >= CMMethod) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	theStartAddress = baseAddress;
	theLimitAddress = limitAddress;

	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = theStartAddress);
	youngReferrers = (limitAddress = theLimitAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;

	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif
}


/*	Access for the object representations when they need to prepend code to
	trampolines. 
 */
/*	Eliminate stale dependent info. */

	/* Cogit>>#zeroOpcodeIndex */
static void
zeroOpcodeIndex(void)
{
    sqInt i;

	for (i = 0; i < opcodeIndex; i += 1) {
		((abstractOpcodes[i]).dependent = null);
	}

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
}

	/* CogMethodZone>>#addToOpenPICList: */
static NoDbgRegParms void
addToOpenPICList(CogMethod *anOpenPIC)
{
	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	assert((openPICList == null)
	 || (isCMOpenPIC(((CogBlockMethod *) openPICList))));
	assertValidDualZoneWriteAddress(anOpenPIC);
	(anOpenPIC->nextOpenPIC = ((usqInt)openPICList));
	openPICList = ((CogMethod *) ((((usqInt)anOpenPIC)) - (getCodeToDataDelta())));
}

	/* CogMethodZone>>#addToYoungReferrers: */
static NoDbgRegParms void
addToYoungReferrers(CogMethod *writableCogMethod)
{
	assertValidDualZoneWriteAddress(writableCogMethod);
	assert((occurrencesInYoungReferrers(writableCogMethod)) == 0);
	assert((writableCogMethod->cmRefersToYoung));
	assert((youngReferrers <= limitAddress)
	 && (youngReferrers >= (limitAddress - (methodCount * BytesPerWord))));
	if (!(asserta((limitAddress - (methodCount * BytesPerWord)) >= mzFreeStart))) {
		error("no room on youngReferrers list");
	}
	youngReferrers -= BytesPerWord;
	codeLongAtput(youngReferrers, (((usqInt)writableCogMethod)) - (getCodeToDataDelta()));
}

	/* CogMethodZone>>#allocate: */
static NoDbgRegParms usqInt
allocate(sqInt numBytes)
{
    usqInt allocation;
    sqInt roundedBytes;

	roundedBytes = (numBytes + 7) & -8;
	if ((mzFreeStart + roundedBytes) >= (limitAddress - (methodCount * BytesPerWord))) {
		return 0;
	}
	allocation = mzFreeStart;
	mzFreeStart += roundedBytes;
	methodCount += 1;
	return allocation;
}


/*	Answer the method containing mcpc for the purposes of code zone
	compaction, where mcpc is actually the value of instructionPointer at the
	time of a compaction. */

	/* CogMethodZone>>#cogMethodContaining: */
CogMethod *
cogMethodContaining(usqInt mcpc)
{
    CogMethod *cogMethod;
    CogMethod *prevMethod;

	if (mcpc > limitAddress) {
		return null;
	}
	if (mcpc < baseAddress) {
		/* assertMcpcIsPrimReturn: */
		assert((mcpc == cePrimReturnEnterCogCode)
		 || (mcpc == cePrimReturnEnterCogCodeProfiling));
		return null;
	}
	assert(mcpc < (freeStart()));
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mcpc) {
		prevMethod = cogMethod;
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	assert((prevMethod)
	 && ((mcpc == ((((usqInt)prevMethod)) + ((prevMethod->stackCheckOffset))))
	 || ((mcpcisAtStackCheckOfBlockMethodIn(mcpc, prevMethod))
	 || (((primitiveIndexOfMethodheader((prevMethod->methodObject), (prevMethod->methodHeader))) > 0)
	 || ((isCallPrecedingReturnPC(backEnd, mcpc))
	 && ((callTargetFromReturnAddress(backEnd, mcpc)) == (ceCheckForInterruptTrampoline)))))));
	return prevMethod;
}

	/* CogMethodZone>>#compactCompiledCode */
static void
compactCompiledCode(void)
{
    unsigned short bytes;
    CogMethod *dest;
    sqInt objectHeaderValue;
    CogMethod *source;
    CogMethod *writableVersion;

	compactionInProgress = 1;
	methodCount = 0;
	objectHeaderValue = nullHeaderForMachineCodeMethod();
	source = ((CogMethod *) baseAddress);
	voidOpenPICList();
	while ((source < (limitZony()))
	 && (!(((source->cmType)) == CMFree))) {
		assert((cogMethodDoesntLookKosher(source)) == 0);
		writableVersion = ((CogMethod *) ((((usqInt)source)) + codeToDataDelta));
		(writableVersion->objectHeader = objectHeaderValue);
		if (((source->cmUsageCount)) > 0) {
			(writableVersion->cmUsageCount = ((source->cmUsageCount)) / 2);
		}

		/* begin maybeLinkOnUnpairedMethodList: */
		if (((source->cmType)) == CMOpenPIC) {
			addToOpenPICList(writableVersion);
		}
		methodCount += 1;
		source = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)source)) + ((source->blockSize)))));
	}
	if (source >= (limitZony())) {
		haltmsg("no free methods; cannot compact.");
		return;
	}
	dest = source;
	while (source < (limitZony())) {
		assert((maybeFreeCogMethodDoesntLookKosher(source)) == 0);
		bytes = (source->blockSize);
		if (!(((source->cmType)) == CMFree)) {
			methodCount += 1;
			codeMemmove(dest, source, bytes);

			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), (((usqInt)dest)) + bytes);
			}
#      endif

			((writableVersion = ((CogMethod *) ((((usqInt)dest)) + codeToDataDelta)))->objectHeader = objectHeaderValue);
			if (((dest->cmType)) >= CMMethod) {

				/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
				   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
				/* Only update the original method's header if it is referring to this CogMethod. */
				if ((((sqInt)(rawHeaderOf((dest->methodObject))))) == (((sqInt)source))) {
					rawHeaderOfput((dest->methodObject), ((sqInt)dest));
				}
				else {
					assert((noAssertMethodClassAssociationOf((dest->methodObject))) == (nilObject()));

					/* begin linkOnUnpairedMethodList: */
				}
			}
			else {
				/* begin clearSavedPICUsageCount: */
				if (((dest->cmType)) == CMOpenPIC) {
					addToOpenPICList(writableVersion);
				}
			}
			if (((dest->cmUsageCount)) > 0) {
				(writableVersion->cmUsageCount = ((dest->cmUsageCount)) / 2);
			}

			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), ((usqInt)(dest + 1)));
			}
#      endif

			dest = ((CogMethod *) ((((usqInt)dest)) + bytes));
		}
		source = ((CogMethod *) ((((usqInt)source)) + bytes));
	}
	mzFreeStart = ((usqInt)dest);
	methodBytesFreedSinceLastCompaction = 0;
	compactionInProgress = 0;
}

	/* CogMethodZone>>#ensureInYoungReferrers: */
static NoDbgRegParms void
ensureInYoungReferrers(CogMethod *cogMethod)
{
    CogMethod *writableMethod;

	assertValidDualZoneReadAddress(cogMethod);
	if (!((cogMethod->cmRefersToYoung))) {
		assert((occurrencesInYoungReferrers(cogMethod)) == 0);

		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
#    endif

		((writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmRefersToYoung = 1);
		addToYoungReferrers(writableMethod);
	}
}

	/* CogMethodZone>>#freeMethod: */
static NoDbgRegParms void
freeMethod(CogMethod *cogMethod)
{
    CogMethod *writableMethod;

	assert(!((isCMFree(((CogBlockMethod *) cogMethod)))));
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
#  endif

	if (((cogMethod->cmType)) >= CMMethod) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
		}
		else {

			/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
			   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
			/* Only reset the original method's header if it is referring to this CogMethod. */
			if ((((sqInt)(rawHeaderOf((cogMethod->methodObject))))) == (((sqInt)cogMethod))) {
				rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
			}
			else {
				assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
			}
		}
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		removeFromOpenPICList(cogMethod);
	}
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmRefersToYoung = 0);
	(writableMethod->cmType = CMFree);
	methodBytesFreedSinceLastCompaction += (cogMethod->blockSize);
}


/*	Free methods, preferring older methods for compaction, up to some
	fraction, currently a quarter.
 */

	/* CogMethodZone>>#freeOlderMethodsForCompaction */
static void
freeOlderMethodsForCompaction(void)
{
    sqInt amountToFree;
    CogMethod *cogMethod;
    sqInt freeableUsage;
    sqInt freedSoFar;
    sqInt initialFreeSpace;
    sqInt zoneSize;

	zoneSize = limitAddress - baseAddress;
	initialFreeSpace = (limitAddress - mzFreeStart) + methodBytesFreedSinceLastCompaction;
	freedSoFar = initialFreeSpace;

	/* 4 needs to be e.g. a start-up parameter */
	amountToFree = zoneSize / 4;
	freeableUsage = 0;
	do {
		cogMethod = ((CogMethod *) baseAddress);
		while (((((usqInt)cogMethod)) < mzFreeStart)
		 && (freedSoFar < amountToFree)) {
			if (/* shouldFreeMethod:given: */
				(!(((cogMethod->cmType)) == CMFree))
			 && (((cogMethod->cmUsageCount)) <= freeableUsage)) {
				freeMethod(cogMethod);
				freedSoFar += (cogMethod->blockSize);
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	} while((freedSoFar < amountToFree)
		 && (((freeableUsage += 1)) < CMMaxUsageCount));
}


/*	Answer that all entries in youngReferrers are in-use and have the
	cmRefersToYoung flag set.
	Used to check that the youngreferrers pruning routines work correctly. */

	/* CogMethodZone>>#kosherYoungReferrers */
sqInt
kosherYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;
    CogMethod *prevMethod;

	if ((youngReferrers > limitAddress)
	 || (youngReferrers < mzFreeStart)) {
		return 0;
	}
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!((cogMethod->cmRefersToYoung))) {
				return 0;
			}
			if ((occurrencesInYoungReferrers(cogMethod)) != 1) {
				return 0;
			}
		}
		pointer += BytesPerWord;
	}
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		prevMethod = cogMethod;
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((occurrencesInYoungReferrers(cogMethod)) != (((cogMethod->cmRefersToYoung)
					? 1
					: 0))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (cogMethod == prevMethod) {
			return 0;
		}
	}
	return 1;
}


/*	For assert checking... */

	/* CogMethodZone>>#mcpc:isAtStackCheckOfBlockMethodIn: */
static NoDbgRegParms sqInt
mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod)
{
	if (!((cogMethod->blockEntryOffset))) {
		return 0;
	}
	return (blockDispatchTargetsForperformarg(cogMethod, stackCheckOffsetOfBlockAtisMcpc, mcpc)) != 0;
}

	/* CogMethodZone>>#methodFor: */
CogMethod *
methodFor(void *address)
{
    CogMethod *cogMethod;
    CogMethod *nextMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((cogMethod < (limitZony()))
	 && ((((usqInt)cogMethod)) <= (((usqInt)address)))) {
		nextMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (nextMethod == cogMethod) {
			return null;
		}
		if (((((usqInt)address)) >= (((usqInt)cogMethod)))
		 && ((((usqInt)address)) < (((usqInt)nextMethod)))) {
			return cogMethod;
		}
		cogMethod = nextMethod;
	}
	return null;
}

	/* CogMethodZone>>#methodsCompiledToMachineCodeInto: */
sqInt
methodsCompiledToMachineCodeInto(sqInt arrayObj)
{
    CogMethod *cogMethod;
    sqInt methodIndex;

	methodIndex = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			storePointerUncheckedofObjectwithValue(methodIndex, arrayObj, (cogMethod->methodObject));
			methodIndex += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return methodIndex;
}

	/* CogMethodZone>>#numMethods */
sqInt
numMethods(void)
{
	return methodCount;
}

	/* CogMethodZone>>#numMethodsOfType: */
sqInt
numMethodsOfType(sqInt cogMethodType)
{
    CogMethod *cogMethod;
    sqInt n;

	n = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cogMethodType) {
			n += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return n;
}

	/* CogMethodZone>>#occurrencesInYoungReferrers: */
static NoDbgRegParms sqInt
occurrencesInYoungReferrers(CogMethod *cogMethod)
{
    sqInt count;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	count = 0;
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		if ((((sqInt)cogMethod)) == (longAt(pointer))) {
			count += 1;
		}
		pointer += BytesPerWord;
	}
	return count;
}

	/* CogMethodZone>>#openPICWithSelector: */
static NoDbgRegParms CogMethod *
openPICWithSelector(sqInt aSelector)
{
    CogMethod *openPIC;

	openPIC = openPICList;
	do {
		if ((openPIC == null)
		 || (((openPIC->selector)) == aSelector)) {
			return openPIC;
		}
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	Some methods have been freed. Compute how much each survivor needs to
	move during the ensuing compaction and record it in the objectHeader
	field. 
	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#planCompaction */
static void
planCompaction(void)
{
    CogMethod *cogMethod;
    sqInt delta;

	delta = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) == CMFree) {
			delta -= (cogMethod->blockSize);
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->objectHeader = delta);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethods */
void
printCogMethods(void)
{
    CogMethod *cogMethod;
    sqInt nb;
    sqInt nc;
    sqInt nf;
    sqInt nm;
    sqInt no;
    sqInt nu;

	/* begin printCogMethodsSummarizing: */
	nm = (nb = (nc = (no = (nf = (nu = 0)))));
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		printCogMethod(cogMethod);
		switch ((cogMethod->cmType)) {
		case CMFree:
			nf += 1;
			break;
		case CMMethod:
			nm += 1;
			break;
		case CMMethodFlaggedForBecome:
			nb += 1;
			break;
		case CMClosedPIC:
			nc += 1;
			break;
		case CMOpenPIC:
			no += 1;
			break;
		default:
			nu += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	print("CMMethod ");
	printNum(nm);
	if (nb > 0) {
		print(" (flagged for become: ");
		printNum(nb);
		print(")");
	}
	print(" CMClosedPIC ");
	printNum(nc);
	print(" CMOpenPIC ");
	printNum(no);
	print(" CMFree ");
	printNum(nf);
	if (nu > 0) {
		print(" UNKNOWN ");
		printNum(nu);
	}
	print(" total ");
	printNum(((((nm + nc) + no) + nf) + nu) + nb);
	cr();
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsOfType: */
void
printCogMethodsOfType(sqInt cmType)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cmType) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithMethod: */
void
printCogMethodsWithMethod(sqInt methodOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->methodObject)) == methodOop)) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithPrimitive: */
void
printCogMethodsWithPrimitive(sqInt primIdx)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (primIdx == (primitiveIndexOfMethodheader((cogMethod->methodObject), (cogMethod->methodHeader))))) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithSelector: */
void
printCogMethodsWithSelector(sqInt selectorOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->selector)) == selectorOop)) {
			printCogMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogYoungReferrers */
void
printCogYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!((cogMethod->cmRefersToYoung))) {
			print("*");
		}
		if (((cogMethod->cmType)) == CMFree) {
			print("!");
		}
		if (!(((cogMethod->cmRefersToYoung))
			 && (!(((cogMethod->cmType)) == CMFree)))) {
			print(" ");
		}
		printCogMethod(cogMethod);
		pointer += BytesPerWord;
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printOpenPICList */
sqInt
printOpenPICList(void)
{
    sqInt n;
    CogMethod *openPIC;

	/* begin printOpenPICListSummarizing: */
	n = 0;
	openPIC = openPICList;
	while (!(openPIC == null)) {
		n += 1;
		printCogMethod(openPIC);
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	}
	return n;
}

	/* CogMethodZone>>#pruneYoungReferrers */
static sqInt
pruneYoungReferrers(void)
{
    usqInt dest;
    usqInt next;
    usqInt source;

	next = 0;
	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && (((((CogMethod *) (longAt(next))))->cmRefersToYoung)))) break;
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		if (((((CogMethod *) (longAt(source))))->cmRefersToYoung)) {
			assert(source < (dest - BytesPerWord));
			if (next) {

				/* convenient first-time flag */
				next = null;

				/* begin ensureWritableCodeZone */
#        if !DUAL_MAPPED_CODE_ZONE
#        endif

			}
			codeLongAtput((dest -= BytesPerWord), longAt(source));
		}
		source -= BytesPerWord;
	}
	youngReferrers = dest;
	assert(kosherYoungReferrers());
	return 0;
}

	/* CogMethodZone>>#relocateAndPruneYoungReferrers */
static sqInt
relocateAndPruneYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt dest;
    usqInt next;
    usqInt source;

	cogMethod = ((CogMethod *) 0);
	next = 0;
	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && ((!(isCMFree(((CogBlockMethod *) ((cogMethod = ((CogMethod *) (longAt(next)))))))))
		 && ((cogMethod->cmRefersToYoung))))) break;
		if ((cogMethod->objectHeader)) {
			codeLongAtput(next, (((sqInt)cogMethod)) + ((cogMethod->objectHeader)));
		}
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		cogMethod = ((CogMethod *) (longAt(source)));
		if ((!(((cogMethod->cmType)) == CMFree))
		 && ((cogMethod->cmRefersToYoung))) {
			assert(source < (dest - BytesPerWord));
			if ((cogMethod->objectHeader)) {
				cogMethod = ((CogMethod *) ((((sqInt)cogMethod)) + (((sqInt)((cogMethod->objectHeader))))));
			}
			codeLongAtput((dest -= BytesPerWord), ((sqInt)cogMethod));
		}
		source -= BytesPerWord;
	}

	/* this assert must be deferred until after compaction.  See the end of compactCogCompiledCode */
	/* self assert: self kosherYoungReferrers */
	youngReferrers = dest;
	return 0;
}


/*	All surviving methods have had the amount they are going to relocate by
	stored in their objectHeader fields. Relocate all relative calls so that
	after the compaction of both the method containing each call and the call
	target the calls invoke the same target. */

	/* CogMethodZone>>#relocateMethodsPreCompaction */
static sqInt
relocateMethodsPreCompaction(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				relocateCallsInClosedPIC(cogMethod);
			}
			else {
				relocateCallsAndSelfReferencesInMethod(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd, (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	relocateAndPruneYoungReferrers();
	return 1;
}

	/* CogMethodZone>>#removeFromOpenPICList: */
static NoDbgRegParms sqInt
removeFromOpenPICList(CogMethod *anOpenPIC)
{
    CogMethod *prevPIC;

	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	if (!openPICList) {
		return null;
	}
	assert((isCMOpenPIC(((CogBlockMethod *) openPICList)))
	 && ((!((openPICList->nextOpenPIC)))
	 || (isCMOpenPIC(((CogBlockMethod *) (((CogMethod *) ((openPICList->nextOpenPIC)))))))));
	if (anOpenPIC == openPICList) {

		/* N.B. Use self rather than coInterpreter to avoid attempting to cast nil.
		   Conversion to CogMethod done in the nextOpenPIC accessor. */
		openPICList = ((CogMethod *) ((anOpenPIC->nextOpenPIC)));
		return null;
	}
	prevPIC = openPICList;
	do {
		assert((prevPIC != null)
		 && (isCMOpenPIC(((CogBlockMethod *) prevPIC))));
		if (((prevPIC->nextOpenPIC)) == (((usqInt)anOpenPIC))) {
			((((CogMethod *) ((((usqInt)prevPIC)) + codeToDataDelta)))->nextOpenPIC = (anOpenPIC->nextOpenPIC));
			return null;
		}
		prevPIC = ((CogMethod *) ((prevPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	Determine the default alignment for the start of a CogMethod, which in
	turn determines the size of the mask used to distinguish the checked and
	unchecked entry-points, used to distinguish normal and super sends on
	method unlinking.
	This is passed onto the backEnd to allow processors with coarse
	instructions (ARM) to increase the alignment if required. */

	/* CogMethodZone>>#roundUpLength: */
static NoDbgRegParms sqInt
roundUpLength(sqInt numBytes)
{
	return roundUpToMethodAlignment(backEnd, numBytes);
}

	/* CogMethodZone>>#voidOpenPICList */
static void
voidOpenPICList(void)
{
	openPICList = null;
}

	/* CogMethodZone>>#voidUnpairedMethodList */
static void
voidUnpairedMethodList(void)
{
}

	/* CogMethodZone>>#voidYoungReferrersPostTenureAll */
static void
voidYoungReferrersPostTenureAll(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			(cogMethod->cmRefersToYoung = 0);
		}
		pointer += BytesPerWord;
	}
	youngReferrers = limitAddress;
}

	/* CogMethodZone>>#whereIsMaybeCodeThing: */
char *
whereIsMaybeCodeThing(sqInt anOop)
{
	if (oopisGreaterThanOrEqualToandLessThan(anOop, codeBase, limitAddress)) {
		if (oopisLessThan(anOop, minCogMethodAddress())) {
			return " is in generated runtime";
		}
		if (oopisLessThan(anOop, mzFreeStart)) {
			return " is in generated methods";
		}
		if (oopisLessThan(anOop, youngReferrers)) {
			return " is in code zone";
		}
		return " is in young referrers";
	}
	return null;
}

	/* CogObjectRepresentation>>#checkValidObjectReference: */
static NoDbgRegParms sqInt
checkValidObjectReference(sqInt anOop)
{
	return (!(isImmediate(anOop)))
	 && ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentation>>#genCmpClassFloatCompactIndexR: */
static NoDbgRegParms AbstractInstruction *
genCmpClassFloatCompactIndexR(sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, ClassFloatCompactIndex, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(ClassFloatCompactIndex));
	}
	return anInstruction;
}

	/* CogObjectRepresentation>>#genCmpClassMethodContextCompactIndexR: */
static NoDbgRegParms AbstractInstruction *
genCmpClassMethodContextCompactIndexR(sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(ClassMethodContextCompactIndex));
	}
	return anInstruction;
}

	/* CogObjectRepresentation>>#genDoubleArithmetic:preOpCheck: */
static NoDbgRegParms sqInt
genDoubleArithmeticpreOpCheck(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg))
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpFailClass;
    AbstractInstruction *jumpImmediate;

	jumpFailCheck = ((AbstractInstruction *) 0);
	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpFailClass = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (preOpCheckOrNil) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(Arg0Reg, TempReg, DPFPReg1, doOp);
	genConvertSmallIntegerToIntegerInReg(ClassReg);

	/* ConvertR:Rd: */
	genoperandoperand(ConvertRRd, ClassReg, DPFPReg1);

	/* Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpFailAlloc, jmpTarget(jumpFailClass, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	if (preOpCheckOrNil) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailClass->operands))[0])));
	}
	return 0;
}

	/* CogObjectRepresentation>>#genDoubleComparison:invert: */
static NoDbgRegParms sqInt
genDoubleComparisoninvert(AbstractInstruction * NoDbgRegParms (*jumpOpcodeGenerator)(void *), sqInt invertComparison)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *compare;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpImmediate;

	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	if (invertComparison) {

		/* May need to invert for NaNs */
		compare = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		compare = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpOpcodeGenerator(0);

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(falseObject()));
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, genMoveConstantR(trueObject(), ReceiverResultReg));

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(Arg0Reg, TempReg, DPFPReg1, compare);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);

	/* ConvertR:Rd: */
	genoperandoperand(ConvertRRd, Arg0Reg, DPFPReg1);

	/* Jump: */
	genoperand(Jump, ((sqInt)compare));
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Generate a compare and branch to test if aRegister and bRegister contains
	other than SmallIntegers,
	i.e. don't branch if both aRegister and bRegister contain SmallIntegers.
	Answer the jump. Destroy scratchRegister if required. */

	/* CogObjectRepresentation>>#genJumpNotSmallIntegersIn:and:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallIntegersInandscratch(sqInt aRegister, sqInt bRegister, sqInt scratchRegister)
{
	/* MoveR:R: */
	genoperandoperand(MoveRR, aRegister, scratchRegister);

	/* AndR:R: */
	genoperandoperand(AndRR, bRegister, scratchRegister);
	return genJumpNotSmallIntegerInScratchReg(scratchRegister);
}

	/* CogObjectRepresentation>>#genLoadSlot:sourceReg:destReg: */
static NoDbgRegParms sqInt
genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    sqInt offset;

	offset = (index * BytesPerWord) + BaseHeaderSize;

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveAdd */
static sqInt
genPrimitiveAdd(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);

	/* AddR:R: */
	genoperandoperand(AddRR, ReceiverResultReg, ClassReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveAsCharacter */
static sqInt
genPrimitiveAsCharacter(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveAsFloat */
static sqInt
genPrimitiveAsFloat(void)
{
    AbstractInstruction *jumpFailAlloc;


	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);

	/* ConvertR:Rd: */
	genoperandoperand(ConvertRRd, TempReg, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveAtPut */
static sqInt
genPrimitiveAtPut(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitAnd */
static sqInt
genPrimitiveBitAnd(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* Whether the SmallInteger tags are zero or non-zero, anding them together will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);

	/* AndR:R: */
	genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitOr */
static sqInt
genPrimitiveBitOr(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* Whether the SmallInteger tags are zero or non-zero, oring them together will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);

	/* OrR:R: */
	genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	rTemp := rArg0
	rClass := tTemp
	rTemp := rTemp & 1
	jz nonInt
	rClass >>= 1
	cmp 0,rClass
	jge neg
	cmp 31,rClass // numSmallIntegerBits, jge for sign
	jge tooBig
	rTemp := rReceiver
	rTemp <<= rClass
	rTemp >>= rClass (arithmetic)
	cmp rTemp,rReceiver
	jnz ovfl
	rReceiver := rReceiver - 1
	rReceiver := rReceiver <<= rClass
	rReceiver := rReceiver + 1
	ret
	neg:
	rClass := 0 - rClass
	cmp 31,rClass // numSmallIntegerBits
	jge inRange
	rClass := 31
	inRange
	rReceiver := rReceiver >>= rClass.
	rReceiver := rReceiver | smallIntegerTags.
	ret
	ovfl
	tooBig
	nonInt:
	fail
 */

	/* CogObjectRepresentation>>#genPrimitiveBitShift */
static sqInt
genPrimitiveBitShift(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpInRange;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;
    AbstractInstruction *jumpTooBig;
    sqInt quickConstant;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpNegative))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}
	jumpNegative = genConditionalBranchoperand(JumpNegative, ((sqInt)0));
	quickConstant = numSmallIntegerBits();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpTooBig = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);

	/* LogicalShiftLeftR:R: */
	genoperandoperand(LogicalShiftLeftRR, ClassReg, TempReg);

	/* ArithmeticShiftRightR:R: */
	genoperandoperand(ArithmeticShiftRightRR, ClassReg, TempReg);

	/* begin CmpR:R: */
	assert(!(0 /* (TempReg = SPReg) */));
	genoperandoperand(CmpRR, TempReg, ReceiverResultReg);
	jumpOvfl = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);

	/* LogicalShiftLeftR:R: */
	genoperandoperand(LogicalShiftLeftRR, ClassReg, ReceiverResultReg);
	genAddSmallIntegerTagsTo(ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegative, genoperand(NegateR, ClassReg));
	quickConstant = numSmallIntegerBits();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpInRange = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	quickConstant = numSmallIntegerBits();

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jmpTarget(jumpInRange, genoperandoperand(ArithmeticShiftRightRR, ClassReg, ReceiverResultReg));
	genClearAndSetSmallIntegerTagsIn(ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, jmpTarget(jumpTooBig, jmpTarget(jumpOvfl, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitXor */
static sqInt
genPrimitiveBitXor(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* Clear one or the other tag so that xoring will preserve them. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);

	/* XorR:R: */
	genoperandoperand(XorRR, Arg0Reg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveClass */
static sqInt
genPrimitiveClass(void)
{
    sqInt reg;

	reg = ReceiverResultReg;
	if (methodOrBlockNumArgs > 0) {
		if (methodOrBlockNumArgs > 1) {
			return UnimplementedPrimitive;
		}
		reg = Arg0Reg;
		/* empty genLoadArgAtDepth:into: */;
	}
	if ((genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ReceiverResultReg, TempReg, reg != ReceiverResultReg)) == BadRegisterSet) {
		genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ClassReg, TempReg, reg != ReceiverResultReg);

		/* MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDiv */
static sqInt
genPrimitiveDiv(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);

	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* If arg and remainder signs are different we must round down. */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));

	/* begin SubCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	jmpTarget(jumpSameSign, (convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDivide */
static sqInt
genPrimitiveDivide(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpInexact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOverflow;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);

	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* test for overflow; the only case is SmallInteger minVal / -1 */
	jumpInexact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	jumpOverflow = genJumpNotSmallIntegerValuescratch(TempReg, Arg1Reg);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOverflow, jmpTarget(jumpInexact, jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveEqual */
static sqInt
genPrimitiveEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpZero, gJumpFPEqual, 0)
			: genSmallIntegerComparison(JumpZero));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatAdd */
static sqInt
genPrimitiveFloatAdd(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleArithmeticpreOpCheck(AddRdRd, null)
			: genPureDoubleArithmeticpreOpCheck(AddRdRd, null));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatDivide */
static sqInt
genPrimitiveFloatDivide(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleArithmeticpreOpCheck(DivRdRd, genDoubleFailIfZeroArgRcvrarg)
			: genPureDoubleArithmeticpreOpCheck(DivRdRd, genDoubleFailIfZeroArgRcvrarg));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatEqual */
static sqInt
genPrimitiveFloatEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPEqual, 0)
			: genPureDoubleComparisoninvert(gJumpFPEqual, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatGreaterOrEqual */
static sqInt
genPrimitiveFloatGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPGreaterOrEqual, 0)
			: genPureDoubleComparisoninvert(gJumpFPGreaterOrEqual, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatGreaterThan */
static sqInt
genPrimitiveFloatGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPGreater, 0)
			: genPureDoubleComparisoninvert(gJumpFPGreater, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatLessOrEqual */
static sqInt
genPrimitiveFloatLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPGreaterOrEqual, 1)
			: genPureDoubleComparisoninvert(gJumpFPGreaterOrEqual, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatLessThan */
static sqInt
genPrimitiveFloatLessThan(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPGreater, 1)
			: genPureDoubleComparisoninvert(gJumpFPGreater, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatMultiply */
static sqInt
genPrimitiveFloatMultiply(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleArithmeticpreOpCheck(MulRdRd, null)
			: genPureDoubleArithmeticpreOpCheck(MulRdRd, null));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatNotEqual */
static sqInt
genPrimitiveFloatNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleComparisoninvert(gJumpFPNotEqual, 0)
			: genPureDoubleComparisoninvert(gJumpFPNotEqual, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSquareRoot */
static sqInt
genPrimitiveFloatSquareRoot(void)
{
    AbstractInstruction *jumpFailAlloc;

	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);

	/* SqrtRd: */
	genoperand(SqrtRd, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSubtract */
static sqInt
genPrimitiveFloatSubtract(void)
{
	return (primitiveDoMixedArithmetic()
			? genDoubleArithmeticpreOpCheck(SubRdRd, null)
			: genPureDoubleArithmeticpreOpCheck(SubRdRd, null));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterOrEqual */
static sqInt
genPrimitiveGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreaterOrEqual, gJumpFPGreaterOrEqual, 0)
			: genSmallIntegerComparison(JumpGreaterOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterThan */
static sqInt
genPrimitiveGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreater, gJumpFPGreater, 0)
			: genSmallIntegerComparison(JumpGreater));
}


/*	Implementation notes: there are two reasons to use TempReg
	-1) if primitive fails, ReceiverResultReg must remain unchanged (we
	CompletePrimitive) -2) CLZ/BSR only work on 64bits for registers R0-R7 on
	Intel X64. But Win64 uses R9
	Normally, this should be backEnd dependent, but for now we have a single
	64bits target...
 */

	/* CogObjectRepresentation>>#genPrimitiveHighBit */
static sqInt
genPrimitiveHighBit(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpNegativeReceiver;


	/* remove excess tag bits from the receiver oop */
	backEnd;

	/* begin genHighBitIn:ofSmallIntegerOopWithSingleTagBit: */
	/* begin genHighBitClzIn:ofSmallIntegerOopWithSingleTagBit: */
	/* ClzR:R: */
	genoperandoperand(ClzRR, ReceiverResultReg, TempReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}

	/* Note the nice bit trick below:
	   highBit_1based_of_small_int_value = (BytesPerWord * 8) - leadingZeroCout_of_oop - 1 toAccountForTagBit.
	   This is like 2 complements (- reg - 1) on (BytesPerWord * 8) log2 bits, or exactly a bit invert operation... */
	jumpNegativeReceiver = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* #XorCw:R: #gen:literal:operand: */
	checkLiteralforInstruction((BytesPerWord * 8) - 1, genoperandoperand(XorCwR, (BytesPerWord * 8) - 1, TempReg));
	if (!jumpNegativeReceiver) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegativeReceiver, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveIdentical */
static sqInt
genPrimitiveIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(0);
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveImmediateAsInteger */
static sqInt
genPrimitiveImmediateAsInteger(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveIntegerAt */
static sqInt
genPrimitiveIntegerAt(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveIntegerAtPut */
static sqInt
genPrimitiveIntegerAtPut(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveLessOrEqual */
static sqInt
genPrimitiveLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLessOrEqual, gJumpFPGreaterOrEqual, 1)
			: genSmallIntegerComparison(JumpLessOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveLessThan */
static sqInt
genPrimitiveLessThan(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLess, gJumpFPGreater, 1)
			: genSmallIntegerComparison(JumpLess));
}


/*	subclasses override if they so desire */

	/* CogObjectRepresentation>>#genPrimitiveMakePoint */
static sqInt
genPrimitiveMakePoint(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveMod */
static sqInt
genPrimitiveMod(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, Arg1Reg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genRemoveSmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* If arg and remainder signs are different we must reflect around zero. */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));

	/* XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);

	/* AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ClassReg);
	jmpTarget(jumpSameSign, jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genSetSmallIntegerTagsIn(ClassReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveMultiply */
static sqInt
genPrimitiveMultiply(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(processorHasMultiplyAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, Arg1Reg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	genRemoveSmallIntegerTagsInScratchReg(Arg1Reg);

	/* MulOverflowR:R: */
	genMulRR(backEnd, Arg1Reg, ClassReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genSetSmallIntegerTagsIn(ClassReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveNew */
static sqInt
genPrimitiveNew(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveNewMethod */
static sqInt
genPrimitiveNewMethod(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveNewWithArg */
static sqInt
genPrimitiveNewWithArg(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveNotEqual */
static sqInt
genPrimitiveNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
			? genSmallIntegerComparisonorDoubleComparisoninvert(JumpNonZero, gJumpFPNotEqual, 0)
			: genSmallIntegerComparison(JumpNonZero));
}

	/* CogObjectRepresentation>>#genPrimitiveNotIdentical */
static sqInt
genPrimitiveNotIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(1);
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveObjectAt */
static sqInt
genPrimitiveObjectAt(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveQuo */
static sqInt
genPrimitiveQuo(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);

	/* We must shift away the tags, not just subtract them, so that the
	   overflow case doesn't actually overflow the machine instruction. */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	if (!(setsConditionCodesFor(lastOpcode(), JumpZero))) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
	}
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveShallowCopy */
static sqInt
genPrimitiveShallowCopy(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveSlotAt */
static sqInt
genPrimitiveSlotAt(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveSlotAtPut */
static sqInt
genPrimitiveSlotAtPut(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveStringAtPut */
static sqInt
genPrimitiveStringAtPut(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveStringCompareWith */
static sqInt
genPrimitiveStringCompareWith(void)
{
	return UnimplementedPrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveStringReplace */
static sqInt
genPrimitiveStringReplace(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveSubtract */
static sqInt
genPrimitiveSubtract(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);

	/* SubR:R: */
	genoperandoperand(SubRR, Arg0Reg, TempReg);
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genAddSmallIntegerTagsTo(TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveUninitializedNewWithArg */
static sqInt
genPrimitiveUninitializedNewWithArg(void)
{
	return UnimplementedPrimitive;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentation>>#genPureDoubleArithmetic:preOpCheck: */
static NoDbgRegParms sqInt
genPureDoubleArithmeticpreOpCheck(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg))
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpFailClass;
    AbstractInstruction *jumpImmediate;

	jumpFailCheck = ((AbstractInstruction *) 0);
	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpFailClass = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (preOpCheckOrNil) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(Arg0Reg, TempReg, DPFPReg1, doOp);
	jmpTarget(jumpFailAlloc, jmpTarget(jumpFailClass, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	if (preOpCheckOrNil) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailClass->operands))[0])));
	}
	return 0;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentation>>#genPureDoubleComparison:invert: */
static NoDbgRegParms sqInt
genPureDoubleComparisoninvert(AbstractInstruction * NoDbgRegParms (*jumpOpcodeGenerator)(void *), sqInt invertComparison)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *compare;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpImmediate;

	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	jumpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	if (invertComparison) {

		/* May need to invert for NaNs */
		compare = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		compare = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpOpcodeGenerator(0);

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(falseObject()));
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, genMoveConstantR(trueObject(), ReceiverResultReg));

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(Arg0Reg, TempReg, DPFPReg1, compare);
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genSmallIntegerComparison: */
static NoDbgRegParms sqInt
genSmallIntegerComparison(sqInt jumpOpcode)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpTrue;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	jumpFail = genJumpNotSmallInteger(Arg0Reg);

	/* begin CmpR:R: */
	assert(!(0 /* (Arg0Reg = SPReg) */));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(falseObject()));
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, genMoveConstantR(trueObject(), ReceiverResultReg));

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Stack looks like
	return address */

	/* CogObjectRepresentation>>#genSmallIntegerComparison:orDoubleComparison:invert: */
static NoDbgRegParms sqInt
genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpFail;
    sqInt r;

	r = genSmallIntegerComparison(jumpOpcode);
	if (r < 0) {
		return r;
	}
#  if defined(DPFPReg0)

	/* Fall through on non-SmallInteger argument.  Argument may be a Float : let us check or fail */
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);

	/* It was a Float, so convert the receiver to double and perform the operation */
	jumpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);

	/* ConvertR:Rd: */
	genoperandoperand(ConvertRRd, ReceiverResultReg, DPFPReg0);
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	if (invertComparison) {

		/* May need to invert for NaNs */
		/* CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		/* CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(falseObject()));
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, genMoveConstantR(trueObject(), ReceiverResultReg));

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
#  endif // defined(DPFPReg0)

	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#isUnannotatableConstant: */
static NoDbgRegParms sqInt
isUnannotatableConstant(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((isImmediate((simStackEntry->constant)))
	 || (!(/* shouldAnnotateObjectReference: */
		(isNonIntegerObject((simStackEntry->constant)))
	 && (oopisGreaterThan((simStackEntry->constant), trueObject())))));
}


/*	If the receiver supports immediate floats then generate a test for a
	smallFloat in oopReg,
	converting it to the float value in dpReg and jumping to targetInst.
	Otherwise do nothing. */

	/* CogObjectRepresentation>>#maybeGenConvertIfSmallFloatIn:scratchReg:into:andJumpTo: */
static NoDbgRegParms sqInt
maybeGenConvertIfSmallFloatInscratchRegintoandJumpTo(sqInt oopReg, sqInt scratch, sqInt dpReg, AbstractInstruction *targetInst)
{
	return 0;
}


/*	If required, generate a shift of the register containing the class tag in
	a method cache probe.
	By default this is a no-op. Subclasses redefine as required. */

	/* CogObjectRepresentation>>#maybeShiftClassTagRegisterForMethodCacheProbe: */
static NoDbgRegParms sqInt
maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg)
{
	return 0;
}


/*	Answer if the cacheTag is not unmarked, i.e. answer true for compact
	class indices and immediates; only answer false for unmarked objects */

	/* CogObjectRepresentationForSqueakV3>>#cacheTagIsMarked: */
static NoDbgRegParms sqInt
cacheTagIsMarked(sqInt cacheTag)
{
	if (!(couldBeObject(cacheTag))) {
		return 1;
	}
	assert(addressCouldBeObj(cacheTag));
	return isMarked(cacheTag);
}

	/* CogObjectRepresentationForSqueakV3>>#checkValidOopReference: */
static NoDbgRegParms sqInt
checkValidOopReference(sqInt anOop)
{
	return ((anOop & 1))
	 || ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentationForSqueakV3>>#classForInlineCacheTag: */
static NoDbgRegParms sqInt
classForInlineCacheTag(sqInt inlineCacheTag)
{
	if ((inlineCacheTag & 1)) {
		return classSmallInteger();
	}
	if (couldBeObject(inlineCacheTag)) {
		return inlineCacheTag;
	}
	return compactClassAt(((usqInt)(inlineCacheTag)) >> (compactClassFieldLSB()));
}


/*	This is the mask for the field when shifted into the least significant
	bits 
 */

	/* CogObjectRepresentationForSqueakV3>>#compactClassFieldMask */
static sqInt
compactClassFieldMask(void)
{
	return (1U << (compactClassFieldWidth())) - 1;
}


/*	Note this version filters-out compact class indices via the >= nilObj
	clause 
 */

	/* CogObjectRepresentationForSqueakV3>>#couldBeObject: */
static NoDbgRegParms sqInt
couldBeObject(sqInt oop)
{
	return (isNonIntegerObject(oop))
	 && (oopisGreaterThanOrEqualTo(oop, nilObject()));
}


/*	Short-circuit the interpreter call if a frame is already married. */

	/* CogObjectRepresentationForSqueakV3>>#genActiveContextTrampoline */
static usqInt
genActiveContextTrampoline(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSingle;

	zeroOpcodeIndex();

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, FoxMethod, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(FoxMethod));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, MFMethodFlagHasContextFlag, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(MFMethodFlagHasContextFlag));
	}
	jumpSingle = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, FoxThisContext, FPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(FoxThisContext));
	}

	/* RetN: */
	genoperand(RetN, 0);
	jmpTarget(jumpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceActiveContext, "ceActiveContextTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 1, ReceiverResultReg, 1);
}

	/* CogObjectRepresentationForSqueakV3>>#genAddSmallIntegerTagsTo: */
static NoDbgRegParms sqInt
genAddSmallIntegerTagsTo(sqInt aRegister)
{
    AbstractInstruction *anInstruction;

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, 1, aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genAllocFloatValue:into:scratchReg:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    usqIntptr_t allocSize;
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpFail;
    usqIntptr_t newFloatHeaderSansHash;

	allocSize = BaseHeaderSize + (sizeof(double));
	newFloatHeaderSansHash = ((((((usqInt)(ClassFloatCompactIndex) << (compactClassFieldLSB())))) | (formatOfClass(classFloat()))) | allocSize) | HeaderTypeShort;

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(freeStartAddress(), genoperandoperand(MoveAwR, freeStartAddress(), resultReg));

	/* begin LoadEffectiveAddressMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(allocSize));
	}

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(scavengeThresholdAddress(), genoperandoperand(MoveAwR, scavengeThresholdAddress(), scratch2));

	/* begin CmpR:R: */
	assert(!((scratch2 == SPReg)));
	genoperandoperand(CmpRR, scratch2, scratch1);
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, resultReg, scratch2);
	flag("newObjectHash");

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int)((usqInt)(HashMaskUnshifted) << BytesPerWord)), scratch2);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int)((usqInt)(HashMaskUnshifted) << BytesPerWord))));
	}

	/* LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, HashBitsOffset - BytesPerWord, scratch2);

	/* begin OrCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(OrCqR, newFloatHeaderSansHash, scratch2);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(newFloatHeaderSansHash));
	}

	/* begin MoveR:Mw:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRMwr, scratch2, 0, resultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* begin MoveRd:M64:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRdM64r, dpreg, BaseHeaderSize, resultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BaseHeaderSize));
	}

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(freeStartAddress(), genoperandoperand(MoveRAw, scratch1, freeStartAddress()));
	return jumpFail;
}


/*	Set the SmallInteger tag bits when the tag bits may be filled with
	garbage. 
 */

	/* CogObjectRepresentationForSqueakV3>>#genClearAndSetSmallIntegerTagsIn: */
static NoDbgRegParms sqInt
genClearAndSetSmallIntegerTagsIn(sqInt scratchReg)
{
	return genSetSmallIntegerTagsIn(scratchReg);
}

	/* CogObjectRepresentationForSqueakV3>>#genConvertIntegerInReg:toSmallIntegerInReg: */
static NoDbgRegParms sqInt
genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	gLogicalShiftLeftCqRR(1, srcReg, destReg);

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, 1, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genConvertIntegerToSmallIntegerInReg: */
static NoDbgRegParms sqInt
genConvertIntegerToSmallIntegerInReg(sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, 1, reg);

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, 1, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genConvertSmallIntegerToIntegerInReg: */
static NoDbgRegParms sqInt
genConvertSmallIntegerToIntegerInReg(sqInt reg)
{
	/* ArithmeticShiftRightCq:R: */
	genoperandoperand(ArithmeticShiftRightCqR, 1, reg);
	return 0;
}


/*	Create a closure with the given startpc, numArgs and numCopied
	within a context with ctxtNumArgs, large if isLargeCtxt that is in a
	block if isInBlock. If numCopied > 0 pop those values off the stack. */
/*	see ceClosureCopyDescriptor: */

	/* CogObjectRepresentationForSqueakV3>>#genCreateClosureAt:numArgs:numCopied:contextNumArgs:large:inBlock: */
static NoDbgRegParms sqInt
genCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	quickConstant = (numArgs + ((((usqInt)(numCopied) << 6)))) + ((((usqInt)(bcpc) << 12)));

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceClosureCopyTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	if (numCopied > 0) {
		/* begin AddCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(AddCqR, numCopied * BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(numCopied * BytesPerWord));
		}
	}
	return 0;
}


/*	Do the store check. Answer the argument for the benefit of the code
	generator; ReceiverResultReg may be caller-saved and hence smashed by this
	call. Answering
	it allows the code generator to reload ReceiverResultReg cheaply. */

	/* CogObjectRepresentationForSqueakV3>>#generateObjectRepresentationTrampolines */
static void
generateObjectRepresentationTrampolines(void)
{
	/* begin genTrampolineFor:called:arg:regsToSave:result: */
	ceStoreCheckTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceStoreCheck, "ceStoreCheckTrampoline", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, /* returnRegForStoreCheck */
			(((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
				? ReceiverResultReg
				: ABIResultReg), 0);
	ceCreateNewArrayTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNewArraySlotSize, "ceCreateNewArrayTrampoline", 1, SendNumArgsReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, ReceiverResultReg, 0);
	cePositive32BitIntegerTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(positive32BitIntegerFor, "cePositive32BitIntegerTrampoline", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, TempReg, 0);
	ceActiveContextTrampoline = genActiveContextTrampoline();
	ceClosureCopyTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceClosureCopyDescriptor, "ceClosureCopyTrampoline", 1, SendNumArgsReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, ReceiverResultReg, 0);
}


/*	Get the active context into ReceiverResultReg, creating it if necessary. */

	/* CogObjectRepresentationForSqueakV3>>#genGetActiveContextNumArgs:large:inBlock: */
static NoDbgRegParms void
genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceActiveContextTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
}


/*	Fetch the instance's class format into destReg, assuming the object is
	non-int. 
 */

	/* CogObjectRepresentationForSqueakV3>>#genGetClassFormatOfNonInt:into:scratchReg: */
static NoDbgRegParms sqInt
genGetClassFormatOfNonIntintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpCompact;
    AbstractInstruction *jumpGotClass;
    sqInt offset;
    sqInt quickConstant;


	/* Get header word in destReg */
	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	quickConstant = (compactClassFieldLSB()) - (shiftForWord());

	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, destReg);
	quickConstant = ((sqInt)((usqInt)(((1U << (compactClassFieldWidth())) - 1)) << (shiftForWord())));

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpCompact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = classFieldOffset();

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, instReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) AllButTypeMask), scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) AllButTypeMask)));
	}

	/* Don't have to subtract one from the destReg compactClassArray index because of the header word. */
	jumpGotClass = genoperand(Jump, ((sqInt)0));
	assert(BaseHeaderSize == BytesPerWord);
	jmpTarget(jumpCompact, annotateobjRef(gMoveMwrR(splObj(CompactClasses), destReg, scratchReg), splObj(CompactClasses)));
	jmpTarget(jumpGotClass, gMoveMwrR(((((usqInt)(InstanceSpecificationIndex) << (shiftForWord())))) + BytesPerWord, scratchReg, destReg));
	return 0;
}


/*	Fetch the instance's class into destReg. This is almost identical
	to genGetClassFormatOfNonInt:into:scratchReg: but because we
	put the fetch of SmallInteger between the then and the else for 
	compact class/non-compact class we cannot easily share code.
	instRegIsReceiver is ignored. It is for Spur compatibility where
	objects may be forwarded. */

	/* CogObjectRepresentationForSqueakV3>>#genGetClassObjectOf:into:scratchReg:mayBeAForwarder: */
static NoDbgRegParms sqInt
genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeAForwarder)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpCompact;
    AbstractInstruction *jumpGotClass;
    AbstractInstruction *jumpGotClass2;
    AbstractInstruction *jumpIsInt;
    sqInt offset;
    sqInt quickConstant;

	/* MoveR:R: */
	genoperandoperand(MoveRR, instReg, scratchReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* Get header word in scratchReg */
	jumpIsInt = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	quickConstant = (compactClassFieldLSB()) - (shiftForWord());

	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, quickConstant, scratchReg);
	quickConstant = ((sqInt)((usqInt)(((1U << (compactClassFieldWidth())) - 1)) << (shiftForWord())));

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpCompact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = classFieldOffset();

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) AllButTypeMask), destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) AllButTypeMask)));
	}
	jumpGotClass = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsInt, genMoveConstantR(classSmallInteger(), destReg));

	/* Don't have to subtract one from the destReg compactClassArray index because of the header word. */
	jumpGotClass2 = genoperand(Jump, ((sqInt)0));
	assert(BaseHeaderSize == BytesPerWord);
	jmpTarget(jumpCompact, annotateobjRef(gMoveMwrR(splObj(CompactClasses), scratchReg, destReg), splObj(CompactClasses)));
	jmpTarget(jumpGotClass, jmpTarget(jumpGotClass2, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	Compatibility with SpurObjectRepresentation/SpurMemoryManager. */

	/* CogObjectRepresentationForSqueakV3>>#genGetClassTagOf:into:scratchReg: */
static NoDbgRegParms AbstractInstruction *
genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
    AbstractInstruction *entryLabel;

	/* AlignmentNops: */
	genoperand(AlignmentNops, ((BytesPerWord < 8) ? 8 : BytesPerWord));
	entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (genGetClassObjectOfintoscratchRegmayBeAForwarder(instReg, destReg, scratchReg, null)) {
		error("internal error");
	}
	return entryLabel;
}


/*	Fetch the instance's compact class index into destReg. */
/*	Get header word in scratchReg */

	/* CogObjectRepresentationForSqueakV3>>#genGetCompactClassIndexNonImmOf:into: */
static NoDbgRegParms sqInt
genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, compactClassFieldLSB(), destReg);
	quickConstant = (1U << (compactClassFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genGetDoubleValueOf:into: */
static NoDbgRegParms sqInt
genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg)
{
    AbstractInstruction *anInstruction;

	/* begin MoveM64:r:Rd: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveM64rRd, BaseHeaderSize, srcReg, destFPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BaseHeaderSize));
	}
	return 0;
}


/*	Fetch the instance's class format into destReg, assuming the object is
	pointers and non-int
 */

	/* CogObjectRepresentationForSqueakV3>>#genGetFixedFieldsOfPointerNonInt:into:scratchReg: */
static NoDbgRegParms sqInt
genGetFixedFieldsOfPointerNonIntintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;

	genGetClassFormatOfNonIntintoscratchReg(instReg, destReg, scratchReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, destReg, scratchReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 2, destReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 11, scratchReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 0x3F, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0x3F));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 192, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(192));
	}

	/* begin SubCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, 1, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* AddR:R: */
	genoperandoperand(AddRR, scratchReg, destReg);
	return 0;
}


/*	Fetch the instance's identity hash into destReg, encoded as a
	SmallInteger. 
 */
/*	Get header word in scratchReg */

	/* CogObjectRepresentationForSqueakV3>>#genGetHashFieldNonImmOf:asSmallIntegerInto: */
static NoDbgRegParms sqInt
genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, HashBitsOffset - 1, destReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int)((usqInt)(HashMaskUnshifted) << 1)), destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int)((usqInt)(HashMaskUnshifted) << 1))));
	}

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, 1, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}


/*	Extract the inline cache tag for the object in sourceReg into destReg. The
	inline cache tag
	for a given object is the value loaded in inline caches to distinguish
	objects of different
	classes. In Spur this is either the tags for immediates, or the receiver's
	classIndex. The inline cache tag for a given object is the value loaded in
	inline caches to distinguish
	objects of different classes. In SqueakV3 the tag is the integer tag bit
	for SmallIntegers (1),
	the compact class index shifted by log: 2 word size for objects with
	compact classes
	(1 * 4 to: 31 * 4 by: 4), or the class. These ranges cannot overlap
	because the heap
	(and hence the lowest class object) is beyond the machine code zone.
	If forEntry is true answer the entry label at which control is to enter
	(cmEntryOffset). If forEntry is false, control enters at the start. */

	/* CogObjectRepresentationForSqueakV3>>#genGetInlineCacheClassTagFrom:into:forEntry: */
static NoDbgRegParms AbstractInstruction *
genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *entryLabel;
    AbstractInstruction *jumpCompact;
    AbstractInstruction *jumpIsInt;
    sqInt offset;
    sqInt quickConstant;

	/* AlignmentNops: */
	genoperand(AlignmentNops, ((BytesPerWord < 8) ? 8 : BytesPerWord));
	entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* MoveR:R: */
	genoperandoperand(MoveRR, sourceReg, destReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 1, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* Get header word in destReg */
	jumpIsInt = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	assert(((((usqInt)((compactClassFieldMask())) << (compactClassFieldLSB())))) < (((usqInt)(nilObject()))));
	quickConstant = ((sqInt)((usqInt)(((1U << (compactClassFieldWidth())) - 1)) << (compactClassFieldLSB())));

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpCompact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = classFieldOffset();

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) AllButTypeMask), destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) AllButTypeMask)));
	}
	jmpTarget(jumpCompact, jmpTarget(jumpIsInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return entryLabel;
}

	/* CogObjectRepresentationForSqueakV3>>#genJumpImmediate: */
static NoDbgRegParms AbstractInstruction *
genJumpImmediate(sqInt aRegister)
{
	return genJumpSmallInteger(aRegister);
}


/*	Generate a test for aRegister containing an integer value in the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. ObjectMemory>>isIntegerValue: */

	/* CogObjectRepresentationForSqueakV3>>#genJumpIsSmallIntegerValue:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
	gLogicalShiftLeftCqRR(1, aRegister, scratchReg);

	/* XorR:R: */
	genoperandoperand(XorRR, aRegister, scratchReg);
	return genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
}

	/* CogObjectRepresentationForSqueakV3>>#genJumpNotSmallIntegerInScratchReg: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallIntegerInScratchReg(sqInt aRegister)
{
    AbstractInstruction *anInstruction;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 1, aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}


/*	Generate a test for aRegister containing an integer value outside the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. ObjectMemory>>isIntegerValue: */

	/* CogObjectRepresentationForSqueakV3>>#genJumpNotSmallIntegerValue:scratch: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
	gArithmeticShiftRightCqRR(1, aRegister, scratchReg);

	/* XorR:R: */
	genoperandoperand(XorRR, aRegister, scratchReg);
	return genConditionalBranchoperand(JumpLess, ((sqInt)0));
}

	/* CogObjectRepresentationForSqueakV3>>#genJumpNotSmallInteger: */
static NoDbgRegParms AbstractInstruction *
genJumpNotSmallInteger(sqInt aRegister)
{
    AbstractInstruction *anInstruction;

	/* begin TstCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(TstCqR, 1, aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}

	/* CogObjectRepresentationForSqueakV3>>#genJumpSmallInteger: */
static NoDbgRegParms AbstractInstruction *
genJumpSmallInteger(sqInt aRegister)
{
    AbstractInstruction *anInstruction;

	/* begin TstCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(TstCqR, 1, aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}


/*	Generate a call to code that allocates a new Array of size.
	The Array should be initialized with nils iff initialized is true.
	The size arg is passed in SendNumArgsReg, the result
	must come back in ReceiverResultReg. */

	/* CogObjectRepresentationForSqueakV3>>#genNewArrayOfSize:initialized: */
static NoDbgRegParms void
genNewArrayOfSizeinitialized(sqInt size, sqInt initialize)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, size, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(size));
	}

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceCreateNewArrayTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
}


/*	c.f. StackInterpreter>>stSizeOf: lengthOf:baseHeader:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationForSqueakV3>>#genPrimitiveAt */
static sqInt
genPrimitiveAt(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpFmtGeFirstByte;
    AbstractInstruction *jumpBounds;
    AbstractInstruction *jumpFmtEq2;
    AbstractInstruction *jumpFmtGt11;
    AbstractInstruction *jumpFmtGt4;
    AbstractInstruction *jumpFmtIsArray;
    AbstractInstruction *jumpFmtLeWeakArray;
    AbstractInstruction *jumpFmtLt8;
    AbstractInstruction *jumpGotByteSize;
    AbstractInstruction *jumpGotWordSize;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsContext1;
    AbstractInstruction *jumpLarge;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotIndexable1;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpShortHeader;
    AbstractInstruction *jumpSI;
    AbstractInstruction *jumpSkip;
    sqInt offset;
    sqInt quickConstant;

	jumpSI = genJumpSmallInteger(ReceiverResultReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);

	/* begin genGetSizeOf:into:formatReg:scratchReg:abortJumpsInto: */
	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, instFormatFieldLSB(), SendNumArgsReg);
	quickConstant = (1U << (instFormatFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	quickConstant = arrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpNotIndexable1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, compactClassFieldLSB(), TempReg);
	quickConstant = (1U << (compactClassFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(ClassMethodContextCompactIndex));
	}
	jumpIsContext1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, TypeMask, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(TypeMask));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, HeaderTypeSizeAndClass, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(HeaderTypeSizeAndClass));
	}
	jumpShortHeader = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	assert(Size4Bit == 0);
	offset = 0 - (2 * BytesPerWord);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, ReceiverResultReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) LongSizeMask), ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) LongSizeMask)));
	}
	jumpSkip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpShortHeader, gAndCqR(SizeMask, ClassReg));
	jmpTarget(jumpSkip, gSubCqR(BaseHeaderSize, ClassReg));
	quickConstant = weakArrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpFmtLeWeakArray = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	quickConstant = firstByteFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jmpFmtGeFirstByte = genConditionalBranchoperand(JumpLess, ((sqInt)0));

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 3, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(3));
	}

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jumpGotByteSize = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpFmtGeFirstByte, genoperandoperand(LogicalShiftRightCqR, 2, ClassReg));
	jumpGotWordSize = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpFmtLeWeakArray, genoperandoperand(Label, (labelCounter += 1), bytecodePC));

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), ClassReg);
	quickConstant = arrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpFmtIsArray = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetFixedFieldsOfPointerNonIntintoscratchReg(ReceiverResultReg, SendNumArgsReg, TempReg);

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jmpTarget(jumpFmtIsArray, jmpTarget(jumpGotWordSize, jmpTarget(jumpGotByteSize, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	jumpNotIndexable = jumpNotIndexable1;
	jumpIsContext = jumpIsContext1;
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);

	/* begin SubCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, 1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* begin CmpR:R: */
	assert(!(0 /* (ClassReg = SPReg) */));
	genoperandoperand(CmpRR, ClassReg, Arg1Reg);

	/* This is tedious.  Because of register pressure on x86 (and the baroque
	   complexity of the size computation) we have to recompute the format
	   because it may have been smashed computing the fixed fields.  But at
	   least we have the fixed fields, if any, in formatReg and recomputing
	   these is more expensive than recomputing format.  In any case this
	   should still be faster than the interpreter and we hope this object
	   representation's days are numbered. */
	jumpBounds = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, instFormatFieldLSB(), ClassReg);
	quickConstant = (1U << (instFormatFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 4, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(4));
	}
	jumpFmtGt4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 2, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(2));
	}
	jumpFmtEq2 = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* AddR:R: */
	genoperandoperand(AddRR, SendNumArgsReg, Arg1Reg);
	jmpTarget(jumpFmtEq2, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	quickConstant = BaseHeaderSize / BytesPerWord;

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFmtGt4, genoperandoperand(Label, (labelCounter += 1), bytecodePC));

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 8, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(8));
	}
	jumpFmtLt8 = genConditionalBranchoperand(JumpLess, ((sqInt)0));

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 11, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(11));
	}
	jumpFmtGt11 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BaseHeaderSize));
	}

	/* MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFmtLt8, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	assert(BytesPerWord == 4);
	quickConstant = BaseHeaderSize / BytesPerWord;

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0x3FFFFFFF, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0x3FFFFFFF));
	}
	jumpLarge = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpLarge, CallRT(cePositive32BitIntegerTrampoline));

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpSI, jmpTarget(jumpNotSI, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpBounds, jmpTarget(jumpFmtGt11, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))));
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genPrimitiveIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genPrimitiveIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpCmp;

	/* begin CmpR:R: */
	assert(!(0 /* (Arg0Reg = SPReg) */));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpCmp = (orNot
				? genConditionalBranchoperand(JumpZero, ((sqInt)0))
				: genConditionalBranchoperand(JumpNonZero, ((sqInt)0)));

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(trueObject()));
	}

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCmp, genMoveConstantR(falseObject(), ReceiverResultReg));

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentationForSqueakV3>>#genPrimitiveIdentityHash */
static sqInt
genPrimitiveIdentityHash(void)
{
    AbstractInstruction *jumpSI;

	jumpSI = genJumpSmallInteger(ReceiverResultReg);
	genGetHashFieldNonImmOfasSmallIntegerInto(ReceiverResultReg, TempReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	c.f. StackInterpreter>>stSizeOf: lengthOf:baseHeader:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationForSqueakV3>>#genPrimitiveSize */
static sqInt
genPrimitiveSize(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpFmtGeFirstByte;
    AbstractInstruction *jumpFmtIsArray;
    AbstractInstruction *jumpFmtLeWeakArray;
    AbstractInstruction *jumpGotByteSize;
    AbstractInstruction *jumpGotWordSize;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsContext1;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotIndexable1;
    AbstractInstruction *jumpShortHeader;
    AbstractInstruction *jumpSI;
    AbstractInstruction *jumpSkip;
    sqInt offset;
    sqInt quickConstant;

	jumpSI = genJumpSmallInteger(ReceiverResultReg);

	/* begin genGetSizeOf:into:formatReg:scratchReg:abortJumpsInto: */
	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, instFormatFieldLSB(), SendNumArgsReg);
	quickConstant = (1U << (instFormatFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	quickConstant = arrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpNotIndexable1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, compactClassFieldLSB(), TempReg);
	quickConstant = (1U << (compactClassFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(ClassMethodContextCompactIndex));
	}
	jumpIsContext1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, TypeMask, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(TypeMask));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, HeaderTypeSizeAndClass, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(HeaderTypeSizeAndClass));
	}
	jumpShortHeader = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	assert(Size4Bit == 0);
	offset = 0 - (2 * BytesPerWord);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, ReceiverResultReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) LongSizeMask), ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) LongSizeMask)));
	}
	jumpSkip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpShortHeader, gAndCqR(SizeMask, ClassReg));
	jmpTarget(jumpSkip, gSubCqR(BaseHeaderSize, ClassReg));
	quickConstant = weakArrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpFmtLeWeakArray = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	quickConstant = firstByteFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jmpFmtGeFirstByte = genConditionalBranchoperand(JumpLess, ((sqInt)0));

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 3, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(3));
	}

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jumpGotByteSize = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpFmtGeFirstByte, genoperandoperand(LogicalShiftRightCqR, 2, ClassReg));
	jumpGotWordSize = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpFmtLeWeakArray, genoperandoperand(Label, (labelCounter += 1), bytecodePC));

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), ClassReg);
	quickConstant = arrayFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpFmtIsArray = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetFixedFieldsOfPointerNonIntintoscratchReg(ReceiverResultReg, SendNumArgsReg, TempReg);

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jmpTarget(jumpFmtIsArray, jmpTarget(jumpGotWordSize, jmpTarget(jumpGotByteSize, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	jumpNotIndexable = jumpNotIndexable1;
	jumpIsContext = jumpIsContext1;
	genConvertIntegerInRegtoSmallIntegerInReg(ClassReg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpSI, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return 0;
}


/*	c.f. StackInterpreter>>stSizeOf: lengthOf:baseHeader:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationForSqueakV3>>#genPrimitiveStringAt */
static sqInt
genPrimitiveStringAt(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;
    AbstractInstruction *jumpBounds;
    AbstractInstruction *jumpNotByteIndexable;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpShortHeader;
    AbstractInstruction *jumpSkip;
    sqInt offset;
    sqInt quickConstant;

	/* MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ClassReg);

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, instFormatFieldLSB(), TempReg);
	quickConstant = (1U << (instFormatFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* MoveR:R: */
	genoperandoperand(MoveRR, TempReg, SendNumArgsReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, 3, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(3));
	}

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, TempReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 8, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(8));
	}
	jumpNotByteIndexable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, TypeMask, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(TypeMask));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, HeaderTypeSizeAndClass, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(HeaderTypeSizeAndClass));
	}
	jumpShortHeader = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	assert(Size4Bit == 0);
	offset = 0 - (2 * BytesPerWord);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, ReceiverResultReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, ((int) LongSizeMask), ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int) LongSizeMask)));
	}
	jumpSkip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpShortHeader, gAndCqR(SizeMask, ClassReg));
	jmpTarget(jumpSkip, gSubCqR(BaseHeaderSize, ClassReg));

	/* SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);

	/* begin SubCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, 1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* begin CmpR:R: */
	assert(!(0 /* (ClassReg = SPReg) */));
	genoperandoperand(CmpRR, ClassReg, Arg1Reg);
	jumpBounds = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BaseHeaderSize));
	}
	constant = characterTable();

	/* begin genMoveConstant:R: */
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(constant))
	 && (oopisGreaterThan(constant, trueObject()))) {
		annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(MoveCwR, constant, Arg0Reg)), constant);
	}
	else {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, constant, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(constant));
		}
	}

	/* MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	quickConstant = BaseHeaderSize / BytesPerWord;

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, ReceiverResultReg, Arg0Reg, ReceiverResultReg);

	/* genPrimReturn */
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, jmpTarget(jumpNotByteIndexable, jmpTarget(jumpBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genRemoveSmallIntegerTagsInScratchReg: */
static NoDbgRegParms sqInt
genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg)
{
    AbstractInstruction *anInstruction;

	/* begin SubCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(SubCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genSetSmallIntegerTagsIn: */
static NoDbgRegParms sqInt
genSetSmallIntegerTagsIn(sqInt scratchReg)
{
    AbstractInstruction *anInstruction;

	/* begin OrCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(OrCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genShiftAwaySmallIntegerTagsInScratchReg: */
static NoDbgRegParms sqInt
genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg)
{
	/* ArithmeticShiftRightCq:R: */
	genoperandoperand(ArithmeticShiftRightCqR, 1, scratchReg);
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genStoreSourceReg:slotIndex:destReg:scratchReg:inFrame:needsStoreCheck: */
static NoDbgRegParms sqInt
genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *inst;
    AbstractInstruction *jmpAlreadyRoot;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;
    int mask;
    sqInt offset;
    sqInt rootBitByteOffset;

	/* begin genTraceStores */
	if (traceStores > 0) {
		/* MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);

		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	offset = (index * BytesPerWord) + BaseHeaderSize;

	/* begin MoveR:Mw:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRMwr, sourceReg, offset, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	if (!needsStoreCheck) {
		return 0;
	}

	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(sourceReg);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(youngStartAddress(), genoperandoperand(MoveAwR, youngStartAddress(), scratchReg));

	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, destReg);

	/* Is value stored old?  If so we're done. */
	jmpDestYoung = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, sourceReg);

	/* value is young and target is old.
	   Need to make this a root if the root bit is not already set.
	   Test the root bit.  Only need to fetch the byte containing it,
	   which reduces the size of the mask constant. */
	jmpSourceOld = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	rootBitByteOffset = RootBitDigitLength - 1;
	mask = ((usqInt)(RootBit)) >> ((RootBitDigitLength - 1) * 8);

	/* begin MoveMb:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMbrR, rootBitByteOffset, destReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(rootBitByteOffset));
	}

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, mask, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(mask));
	}

	/* Root bit is not set.  Call store check to insert dest into root table. */
	jmpAlreadyRoot = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	assert(destReg == ReceiverResultReg);

	/* begin evaluateTrampolineCallBlock:protectLinkRegIfNot: */
	if (inFrame) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);
		abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);

		/* PopR: */
		genoperand(PopR, LinkReg);
	}
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, jmpTarget(jmpAlreadyRoot, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return 0;
}

	/* CogObjectRepresentationForSqueakV3>>#genStoreSourceReg:slotIndex:intoNewObjectInDestReg: */
static NoDbgRegParms sqInt
genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    sqInt offset;

	offset = (index * BytesPerWord) + BaseHeaderSize;

	/* begin MoveR:Mw:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRMwr, sourceReg, offset, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	return 0;
}


/*	Answer the relevant inline cache tag for an instance.
	c.f. getInlineCacheClassTagFrom:into: & inlineCacheTagForClass: */

	/* CogObjectRepresentationForSqueakV3>>#inlineCacheTagForInstance: */
static NoDbgRegParms sqInt
inlineCacheTagForInstance(sqInt oop)
{
    sqInt cci;

	if ((oop & 1)) {
		return (((usqInt)0 << 1) | 1);
	}
	if (((cci = noShiftCompactClassIndexOf(oop))) > 0) {
		return cci;
	}
	return (classHeader(oop)) & AllButTypeMask;
}

	/* CogObjectRepresentationForSqueakV3>>#inlineCacheTagIsYoung: */
static NoDbgRegParms sqInt
inlineCacheTagIsYoung(sqInt cacheTag)
{
	return isYoung(cacheTag);
}

	/* CogObjectRepresentationForSqueakV3>>#markAndTraceLiteralIfYoung: */
static NoDbgRegParms void
markAndTraceLiteralIfYoung(sqInt literal)
{
	if ((couldBeObject(literal))
	 && (isYoungObject(literal))) {
		assert(addressCouldBeObj(literal));
		markAndTrace(literal);
	}
}

	/* CogObjectRepresentationForSqueakV3>>#markAndTraceLiteral: */
static NoDbgRegParms void
markAndTraceLiteral(sqInt literal)
{
	if (couldBeObject(literal)) {
		assert(addressCouldBeObj(literal));
		markAndTrace(literal);
	}
}

	/* CogObjectRepresentationForSqueakV3>>#numSmallIntegerBits */
static sqInt
numSmallIntegerBits(void)
{
	return 0x1F;
}

	/* CogObjectRepresentationForSqueakV3>>#remapObject: */
static NoDbgRegParms sqInt
remapObject(sqInt objOop)
{
	assert(addressCouldBeObj(objOop));
	return remap(objOop);
}

	/* CogObjectRepresentationForSqueakV3>>#remapOop: */
static NoDbgRegParms sqInt
remapOop(sqInt oop)
{
	return ((oop & 1)
			? oop
			: remap(oop));
}


/*	In V3 newSpace is at the top of the heap, hence all above false
	(everything except nil, true, & false) need to be annotated.
 */

	/* CogObjectRepresentationForSqueakV3>>#shouldAnnotateObjectReference: */
static NoDbgRegParms sqInt
shouldAnnotateObjectReference(sqInt anOop)
{
	return (isNonIntegerObject(anOop))
	 && (oopisGreaterThan(anOop, trueObject()));
}

	/* CogObjectRepresentationForSqueakV3>>#slotOffsetOfInstVarIndex: */
static NoDbgRegParms sqInt
slotOffsetOfInstVarIndex(sqInt index)
{
	return (index * BytesPerWord) + BaseHeaderSize;
}

	/* CogObjectRepresentationForSqueakV3>>#validInlineCacheTag: */
static NoDbgRegParms sqInt
validInlineCacheTag(sqInt cacheTag)
{
	return (cacheTag == ConstZero)
	 || ((((cacheTag & ((1U << (shiftForWord())) - 1)) == 0)
	 && (((cacheTag >= (1U << (compactClassFieldLSB()))) && (cacheTag <= (((compactClassIndexOfHeader(-1)) << (compactClassFieldLSB())))))))
	 || (checkValidObjectReference(cacheTag)));
}

	/* CogOutOfLineLiteralsARMCompiler>>#callFullInstructionByteSize */
static NoDbgRegParms sqInt
callFullInstructionByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 8;
}

	/* CogOutOfLineLiteralsARMCompiler>>#cmpC32RTempByteSize */
static NoDbgRegParms sqInt
cmpC32RTempByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 8;
}


/*	Generate an out-of-line literal. Copy the value and any annotation from
	the stand-in in the literals manager. */

	/* CogOutOfLineLiteralsARMCompiler>>#concretizeLiteral */
static NoDbgRegParms sqInt
concretizeLiteral(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
    usqInt literal;
    AbstractInstruction *literalAsInstruction;

	literalAsInstruction = ((AbstractInstruction *) (((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]));
	literal = (/* isAnInstruction: */
			(addressIsInInstructions(literalAsInstruction))
		 || (literalAsInstruction == (methodLabel))
				? (literalAsInstruction->address)
				: ((usqInt)literalAsInstruction));
	assert((((self_in_CogOutOfLineLiteralsARMCompiler->dependent)))
	 && (((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->opcode)) == Literal));
	if ((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->annotation)) {
		assert(!((self_in_CogOutOfLineLiteralsARMCompiler->annotation)));
		(self_in_CogOutOfLineLiteralsARMCompiler->annotation) = (((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->annotation);
	}
	if ((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->address)) {
		assert(((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->address)) == ((self_in_CogOutOfLineLiteralsARMCompiler->address)));
	}
	(((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->address = (self_in_CogOutOfLineLiteralsARMCompiler->address));

	/* begin machineCodeAt:put: */
	((self_in_CogOutOfLineLiteralsARMCompiler->machineCode))[0] = literal;
	return 4;
}

	/* CogOutOfLineLiteralsARMCompiler>>#inlineCacheTagAt: */
static NoDbgRegParms sqInt
inlineCacheTagAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt callSiteReturnAddress)
{
	return longAt(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, ((usqInt)(callSiteReturnAddress - 8))));
}


/*	Answer if the receiver is a pc-dependent instruction. With out-of-line
	literals any instruction
	that refers to a literal depends on the address of the literal, so add
	them in addition to the jumps. */

	/* CogOutOfLineLiteralsARMCompiler>>#isPCDependent */
static NoDbgRegParms sqInt
isPCDependent(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return (isJump(self_in_CogOutOfLineLiteralsARMCompiler))
	 || ((((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == AlignmentNops)
	 || ((((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) != Literal)
	 && ((((self_in_CogOutOfLineLiteralsARMCompiler->dependent)))
	 && (((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->opcode)) == Literal))));
}


/*	Return the literal referenced by the instruction immediately preceding
	followingAddress. 
 */

	/* CogOutOfLineLiteralsARMCompiler>>#literalBeforeFollowingAddress: */
static NoDbgRegParms sqInt
literalBeforeFollowingAddress(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt followingAddress)
{
	return longAt(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, (instructionIsLDR(self_in_CogOutOfLineLiteralsARMCompiler, longAt(followingAddress - 4))
			? followingAddress - 4
			: (followingAddress - 4) - 4)));
}


/*	Answer the size of a literal load instruction (which does not include the
	size of the literal).
	With out-of-line literals this is always a single LDR instruction that
	refers to the literal.
 */

	/* CogOutOfLineLiteralsARMCompiler>>#literalLoadInstructionBytes */
static NoDbgRegParms sqInt
literalLoadInstructionBytes(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 4;
}


/*	Answer the byte size of a MoveCwR opcode's corresponding machine code. On
	ARM this is a single instruction pc-relative register load - unless we
	have made a mistake and not turned on the out of line literals manager
 */

	/* CogOutOfLineLiteralsARMCompiler>>#loadLiteralByteSize */
static NoDbgRegParms sqInt
loadLiteralByteSize(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 4;
}


/*	Answer the maximum number of bytes of machine code generated for any
	abstract instruction.
 */

	/* CogOutOfLineLiteralsARMCompiler>>#machineCodeBytes */
static NoDbgRegParms sqInt
machineCodeBytes(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 8;
}


/*	Answer the NSSendCache for the return address of a Newspeak
	self, super, outer, or implicit receiver send. */

	/* CogOutOfLineLiteralsARMCompiler>>#nsSendCacheAt: */
static NoDbgRegParms sqInt
nsSendCacheAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt callSiteReturnAddress)
{
	return longAt(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, ((usqInt)(callSiteReturnAddress - 8))));
}


/*	The maximum offset in a LDR is (1<<12)-1, or (1<<10)-1 instructions.
	Be conservative. The issue is that one abstract instruction can emit
	multiple hardware instructions so we assume a 2 to 1 worst case of
	hardware instructions to abstract opcodes.. */

	/* CogOutOfLineLiteralsARMCompiler>>#outOfLineLiteralOpcodeLimit */
static NoDbgRegParms sqInt
outOfLineLiteralOpcodeLimit(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
	return 0x1FF;
}


/*	Extract the address of the ldr rX, [pc, #NNN] instruction at address */

	/* CogOutOfLineLiteralsARMCompiler>>#pcRelativeAddressAt: */
static NoDbgRegParms usqInt
pcRelativeAddressAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt instrAddress)
{
    sqInt inst;
    usqInt offset;

	inst = longAt(instrAddress);
	assert((inst & 0xFF5F0000U) == (ldrrnplusimm(self_in_CogOutOfLineLiteralsARMCompiler, 0, PC, 0, 0)));
	offset = inst & 0xFFF;
	return (instrAddress + 8) + ((((inst & (0x800000)) != 0)
		? offset
		: -offset));
}


/*	If possible we generate the method address using pc-relative addressing.
	If so we don't need to relocate it in code. So check if pc-relative code
	was generated, and if not, adjust a load literal. There are two cases, a
	push or a register load. If a push, then there is a register load, but in
	the instruction
	before. */

	/* CogOutOfLineLiteralsARMCompiler>>#relocateMethodReferenceBeforeAddress:by: */
static NoDbgRegParms AbstractInstruction *
relocateMethodReferenceBeforeAddressby(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt pc, sqInt delta)
{
    usqInt litAddr;
    sqInt pcPrecedingLoad;
    sqInt reference;

	/* If the load is not done via pc-relative addressing we have to relocate. */
	pcPrecedingLoad = (instructionIsPush(self_in_CogOutOfLineLiteralsARMCompiler, longAt(pc - 4))
				? pc - 4
				: pc);
	if (!(isPCRelativeValueLoad(self_in_CogOutOfLineLiteralsARMCompiler, longAt(pcPrecedingLoad - 4)))) {
		litAddr = pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, pcPrecedingLoad);
		reference = longAt(litAddr);
		longAtput(litAddr, reference + delta);
	}
	return self_in_CogOutOfLineLiteralsARMCompiler;
}


/*	Rewrite a CallFull or JumpFull instruction to transfer to a different
	target. This variant is used to rewrite cached primitive calls where we
	load the target address into ip
	and use the 'bx ip' or 'blx ip' instruction for the actual jump or call.
	Answer the extent
	of the code change which is used to compute the range of the icache to
	flush. 
 */

	/* CogOutOfLineLiteralsARMCompiler>>#rewriteFullTransferAt:target:expectedInstruction: */
static NoDbgRegParms sqInt
rewriteFullTransferAttargetexpectedInstruction(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, usqInt callSiteReturnAddress, usqInt callTargetAddress, sqInt expectedInstruction)
{
	assert((instructionBeforeAddress(self_in_CogOutOfLineLiteralsARMCompiler, callSiteReturnAddress)) == expectedInstruction);
	longAtput(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, callSiteReturnAddress - 8), callTargetAddress);
	return 0;
}


/*	Rewrite an inline cache to call a different target for a new tag. This
	variant is used
	to link unlinked sends in ceSend:to:numArgs: et al. Answer the extent of
	the code
	change which is used to compute the range of the icache to flush. */

	/* CogOutOfLineLiteralsARMCompiler>>#rewriteInlineCacheAt:tag:target: */
static NoDbgRegParms sqInt
rewriteInlineCacheAttagtarget(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress)
{
    usqInt call;
    usqInt callDistance;

	/* pc offset */
	/* return offset */
	callDistance = ((usqInt) (callTargetAddress - ((callSiteReturnAddress + 8) - 4)));
	assert(isInImmediateJumpRange(self_in_CogOutOfLineLiteralsARMCompiler, callDistance));
	call = bl(self_in_CogOutOfLineLiteralsARMCompiler, callDistance);
	longAtput(callSiteReturnAddress - 4, call);
	longAtput(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, callSiteReturnAddress - 8), cacheTag);
	assert((inlineCacheTagAt(self_in_CogOutOfLineLiteralsARMCompiler, callSiteReturnAddress)) == cacheTag);
	return 4;
}


/*	Rewrite an inline cache with a new tag. This variant is used
	by the garbage collector. */

	/* CogOutOfLineLiteralsARMCompiler>>#rewriteInlineCacheTag:at: */
static NoDbgRegParms AbstractInstruction *
rewriteInlineCacheTagat(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt cacheTag, sqInt callSiteReturnAddress)
{
	longAtput(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, callSiteReturnAddress - 8), cacheTag);
	return self_in_CogOutOfLineLiteralsARMCompiler;
}


/*	Size a jump and set its address. The target may be another instruction
	or an absolute address. On entry the address inst var holds our virtual
	address. On exit address is set to eventualAbsoluteAddress, which is
	where this instruction will be output. The span of a jump to a following
	instruction is therefore between that instruction's address and this
	instruction's address ((which are both still their virtual addresses), but
	the span of a jump to a preceding instruction or to an absolute address is
	between that instruction's address (which by now is its eventual absolute
	address) or absolute address and eventualAbsoluteAddress.
	
	ARM is simple; the 26-bit call/jump range means no short jumps. This
	routine only has to determine the targets of jumps, not determine sizes.
	
	This version also deals with out-of-line literals. If this is the real
	literal, update the stand-in in literalsManager with the address (because
	instructions referring to the literal are referring to the stand-in). If
	this is annotated with
	IsObjectReference transfer the annotation to the stand-in, whence it will
	be transferred to the real literal, simplifying update of literals. */

	/* CogOutOfLineLiteralsARMCompiler>>#sizePCDependentInstructionAt: */
static NoDbgRegParms unsigned char
sizePCDependentInstructionAt(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt eventualAbsoluteAddress)
{
    usqInt alignment;

	if (((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == AlignmentNops) {
		(self_in_CogOutOfLineLiteralsARMCompiler->address) = eventualAbsoluteAddress;
		alignment = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];
		return ((self_in_CogOutOfLineLiteralsARMCompiler->machineCodeSize) = ((eventualAbsoluteAddress + (alignment - 1)) & (-alignment)) - eventualAbsoluteAddress);
	}
	assert((isJump(self_in_CogOutOfLineLiteralsARMCompiler))
	 || ((((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == Call)
	 || ((((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == CallFull)
	 || ((((self_in_CogOutOfLineLiteralsARMCompiler->dependent)))
	 && (((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->opcode)) == Literal)))));
	if (isJump(self_in_CogOutOfLineLiteralsARMCompiler)) {
		resolveJumpTarget(self_in_CogOutOfLineLiteralsARMCompiler);
	}
	(self_in_CogOutOfLineLiteralsARMCompiler->address) = eventualAbsoluteAddress;
	if ((((self_in_CogOutOfLineLiteralsARMCompiler->dependent)))
	 && (((((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->opcode)) == Literal)) {
		if (((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == Literal) {
			(((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->address = (self_in_CogOutOfLineLiteralsARMCompiler->address));
		}
		if (((self_in_CogOutOfLineLiteralsARMCompiler->annotation)) == (getIsObjectReference())) {
			(((self_in_CogOutOfLineLiteralsARMCompiler->dependent))->annotation = (self_in_CogOutOfLineLiteralsARMCompiler->annotation));
			(self_in_CogOutOfLineLiteralsARMCompiler->annotation) = null;
		}
	}
	return ((self_in_CogOutOfLineLiteralsARMCompiler->machineCodeSize) = (self_in_CogOutOfLineLiteralsARMCompiler->maxSize));
}


/*	Rewrite the literal in the instruction immediately preceding
	followingAddress. 
 */

	/* CogOutOfLineLiteralsARMCompiler>>#storeLiteral:beforeFollowingAddress: */
static NoDbgRegParms AbstractInstruction *
storeLiteralbeforeFollowingAddress(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, sqInt literal, sqInt followingAddress)
{
	longAtput(pcRelativeAddressAt(self_in_CogOutOfLineLiteralsARMCompiler, (instructionIsLDR(self_in_CogOutOfLineLiteralsARMCompiler, longAt(followingAddress - 4))
			? followingAddress - 4
			: (followingAddress - 4) - 4)), literal);
	return self_in_CogOutOfLineLiteralsARMCompiler;
}


/*	Update an instruction that depends on a label outside
	of generated code (e.g. a method or block header). */

	/* CogOutOfLineLiteralsARMCompiler>>#updateLabel: */
static NoDbgRegParms AbstractInstruction *
updateLabel(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler, AbstractInstruction *labelInstruction)
{
	if (((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) != Literal) {
		assert((((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == MoveCwR)
		 || (((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) == PushCw));
		((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0] = (((labelInstruction->address)) + (((labelInstruction->operands))[1]));
	}
	return self_in_CogOutOfLineLiteralsARMCompiler;
}


/*	Answer if the receiver uses an out-of-line literal. Needs only
	to work for the opcodes created with gen:literal:operand: et al. */

	/* CogOutOfLineLiteralsARMCompiler>>#usesOutOfLineLiteral */
static NoDbgRegParms sqInt
usesOutOfLineLiteral(AbstractInstruction *self_in_CogOutOfLineLiteralsARMCompiler)
{
    sqInt constant;
    sqInt i;
    sqInt iSqInt;
    sqInt n;
    sqInt r;
    sqInt value;
    unsigned int value1;

	switch ((self_in_CogOutOfLineLiteralsARMCompiler->opcode)) {
	case CallFull:
	case JumpFull:
	case AddCwR:
	case AndCwR:
	case CmpCwR:
	case OrCwR:
	case SubCwR:
	case XorCwR:
		return 1;

	case AddCqR:
	case AddCqRR:
	case CmpCqR:
	case SubCqR:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];

		/* begin rotateable8bitSignedImmediate:ifTrue:ifFalse: */
		value = constant;
		while (1) {
			if ((value & 0xFF) == value) {
				n = constant != value;
				return 0;
			}
			for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
				if ((value & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == value) {
					r = 32 - iSqInt;
					i = (((usqInt)(value)) >> iSqInt) | (((((usqInt)(value) << (32 - iSqInt)))) & 0xFFFFFFFFU);
					n = constant != value;
					return 0;
				}
			}
			if (!((value == constant)
			 && (constant != 0))) break;
			value = -constant;
		}
		return 1;

	case AndCqR:
	case AndCqRR:
	case OrCqRR:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value1 = constant;
		while (1) {
			if ((value1 & 0xFF) == value1) {
				n = constant != value1;
				return 0;
			}
			for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
				if ((value1 & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == value1) {
					r = 32 - iSqInt;
					i = ((value1) >> iSqInt) | (((value1 << (32 - iSqInt))) & 0xFFFFFFFFU);
					n = constant != value1;
					return 0;
				}
			}
			if (!(value1 == constant)) break;
			value1 = (constant < 0
						? -1 - constant
						: (unsigned int)~constant);
		}
		return (1U << (highBit(((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]))) != ((((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]) + 1);

	case OrCqR:
	case TstCqR:
	case LoadEffectiveAddressMwrR:
	case MoveCqR:
	case MoveM16rR:
	case PushCq:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];

		/* begin rotateable8bitImmediate:ifTrue:ifFalse: */
		if ((constant & 0xFF) == constant) {
			return 0;
		}
		for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
			if ((constant & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == constant) {
				r = 32 - iSqInt;
				i = (((usqInt)(constant)) >> iSqInt) | (((((usqInt)(constant) << (32 - iSqInt)))) & 0xFFFFFFFFU);
				return 0;
			}
		}
		return 1;

	case XorCqR:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];

		/* begin rotateable8bitBitwiseImmediate:ifTrue:ifFalse: */
		value1 = constant;
		while (1) {
			if ((value1 & 0xFF) == value1) {
				n = constant != value1;
				return 0;
			}
			for (iSqInt = 2; iSqInt <= 30; iSqInt += 2) {
				if ((value1 & (((0xFFU << iSqInt) & 0xFFFFFFFFU) | (((usqInt)(0xFF)) >> (32 - iSqInt)))) == value1) {
					r = 32 - iSqInt;
					i = ((value1) >> iSqInt) | (((value1 << (32 - iSqInt))) & 0xFFFFFFFFU);
					n = constant != value1;
					return 0;
				}
			}
			if (!(value1 == constant)) break;
			value1 = (constant < 0
						? -1 - constant
						: (unsigned int)~constant);
		}
		return 1;

	case MoveCwR:
	case PushCw:
		return !(/* inCurrentCompilation: */
			(/* isAnInstruction: */
			(addressIsInInstructions(((AbstractInstruction *) (((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]))))
		 || ((((AbstractInstruction *) (((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]))) == (methodLabel)))
		 || (/* addressIsInCurrentCompilation: */
			((((usqInt)(((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]))) >= ((methodLabel->address)))
		 && ((((usqInt)(((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]))) < ((((youngReferrers) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers) : (((methodLabel->address)) + MaxMethodSize))))));

	case MoveAwR:
	case MoveAbR:
	case PrefetchAw:
		return (/* isAddressRelativeToVarBase: */
			((((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]) >= (varBaseAddress))
		 && (((((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0]) - (varBaseAddress)) < (0x1000))
				? 0
				: 1);

	case MoveRAw:
	case MoveRAb:
		return (/* isAddressRelativeToVarBase: */
			((((self_in_CogOutOfLineLiteralsARMCompiler->operands))[1]) >= (varBaseAddress))
		 && (((((self_in_CogOutOfLineLiteralsARMCompiler->operands))[1]) - (varBaseAddress)) < (0x1000))
				? 0
				: 1);

	case MoveRMwr:
	case MoveRdM64r:
	case MoveRMbr:
	case MoveRM16r:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[1];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(constant)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (constant >= 0) {
				return 0;
			}
			else {
				i = SQABS(constant);
				return 0;
			}
		}
		else {
			return 1;
		}

	case MoveMbrR:
	case MoveM64rRd:
	case MoveMwrR:
		constant = ((self_in_CogOutOfLineLiteralsARMCompiler->operands))[0];

		/* begin is12BitValue:ifTrue:ifFalse: */
		if ((SQABS(constant)) <= 0xFFF) {

			/* (2 raisedTo: 12)-1 */
			if (constant >= 0) {
				return 0;
			}
			else {
				i = SQABS(constant);
				return 0;
			}
		}
		else {
			return 1;
		}

	default:
		assert(0);
	}
	return 0;
}

	/* CogSimStackEntry>>#ensureSpilledAt:from: */
static NoDbgRegParms SimStackEntry *
ensureSpilledAtfrom(SimStackEntry *self_in_CogSimStackEntry, sqInt baseOffset, sqInt baseRegister)
{
    AbstractInstruction *anInstruction;
    sqInt baseReg;
    sqInt constant;
    AbstractInstruction *inst;
    sqInt offset;

	if ((self_in_CogSimStackEntry->spilled)) {
		if (((self_in_CogSimStackEntry->type)) == SSSpill) {
			assert(((((self_in_CogSimStackEntry->offset)) == baseOffset)
			 && (((self_in_CogSimStackEntry->registerr)) == baseRegister))
			 || (violatesEnsureSpilledSpillAssert()));
			return self_in_CogSimStackEntry;
		}
	}
	assert(((self_in_CogSimStackEntry->type)) != SSSpill);
	traceSpill(self_in_CogSimStackEntry);
	if (((self_in_CogSimStackEntry->type)) == SSConstant) {
		constant = (self_in_CogSimStackEntry->constant);

		/* begin genPushConstant: */
		if (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(constant))
		 && (oopisGreaterThan(constant, trueObject()))) {
			inst = annotateobjRef(checkLiteralforInstruction(constant, genoperand(PushCw, constant)), constant);
		}
		else {
			/* begin PushCq: */
			/* begin gen:quickConstant: */
			anInstruction = genoperand(PushCq, constant);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(constant));
			}
			inst = anInstruction;
		}
	}
	else {
		if (((self_in_CogSimStackEntry->type)) == SSBaseOffset) {
			offset = (self_in_CogSimStackEntry->offset);
			baseReg = (self_in_CogSimStackEntry->registerr);

			/* begin MoveMw:r:R: */
			/* begin gen:quickConstant:operand:operand: */
			anInstruction = genoperandoperandoperand(MoveMwrR, offset, baseReg, TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(offset));
			}
			inst = genoperand(PushR, TempReg);
		}
		else {
			assert(((self_in_CogSimStackEntry->type)) == SSRegister);
			inst = genoperand(PushR, (self_in_CogSimStackEntry->registerr));
		}
		(self_in_CogSimStackEntry->type) = SSSpill;
		(self_in_CogSimStackEntry->offset) = baseOffset;
		(self_in_CogSimStackEntry->registerr) = baseRegister;
	}
	(self_in_CogSimStackEntry->spilled) = 1;
	return self_in_CogSimStackEntry;
}

	/* CogSimStackEntry>>#isSameEntryAs: */
static NoDbgRegParms sqInt
isSameEntryAs(SimStackEntry *self_in_CogSimStackEntry, CogSimStackEntry *ssEntry)
{
	return (((self_in_CogSimStackEntry->type)) == ((ssEntry->type)))
	 && ((((((self_in_CogSimStackEntry->type)) == SSBaseOffset)
	 || (((self_in_CogSimStackEntry->type)) == SSSpill))
	 && ((((self_in_CogSimStackEntry->offset)) == ((ssEntry->offset)))
	 && (((self_in_CogSimStackEntry->registerr)) == ((ssEntry->registerr)))))
	 || (((((self_in_CogSimStackEntry->type)) == SSRegister)
	 && (((self_in_CogSimStackEntry->registerr)) == ((ssEntry->registerr))))
	 || ((((self_in_CogSimStackEntry->type)) == SSConstant)
	 && (((self_in_CogSimStackEntry->constant)) == ((ssEntry->constant))))));
}


/*	Receiver is not a forwarder, except in blocks with no inst var access.
	For now we optimize only the case where receiver is accessed in a method. */

	/* CogSimStackEntry>>#mayBeAForwarder */
static NoDbgRegParms sqInt
mayBeAForwarder(SimStackEntry *self_in_CogSimStackEntry)
{
	if ((((self_in_CogSimStackEntry->type)) == SSRegister)
	 && (isNonForwarderReceiver((self_in_CogSimStackEntry->registerr)))) {
		return 0;
	}
	return ((self_in_CogSimStackEntry->type)) != SSConstant;
}

	/* CogSimStackEntry>>#popToReg: */
static NoDbgRegParms SimStackEntry *
popToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt baseReg;
    sqInt constant;
    sqInt offset;

	if ((self_in_CogSimStackEntry->spilled)) {
		/* PopR: */
		genoperand(PopR, reg);
	}
	else {
		switch ((self_in_CogSimStackEntry->type)) {
		case SSBaseOffset:
			offset = (self_in_CogSimStackEntry->offset);
			baseReg = (self_in_CogSimStackEntry->registerr);

			/* begin MoveMw:r:R: */
			/* begin gen:quickConstant:operand:operand: */
			anInstruction = genoperandoperandoperand(MoveMwrR, offset, baseReg, reg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(offset));
			}
			break;
		case SSConstant:
			constant = (self_in_CogSimStackEntry->constant);

			/* begin genMoveConstant:R: */
			if (/* shouldAnnotateObjectReference: */
				(isNonIntegerObject(constant))
			 && (oopisGreaterThan(constant, trueObject()))) {
				annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(MoveCwR, constant, reg)), constant);
			}
			else {
				/* begin MoveCq:R: */
				/* begin gen:quickConstant:operand: */
				anInstruction = genoperandoperand(MoveCqR, constant, reg);
				if (usesOutOfLineLiteral(anInstruction)) {
					(anInstruction->dependent = locateLiteral(constant));
				}
			}
			break;
		case SSRegister:
			if (reg != ((self_in_CogSimStackEntry->registerr))) {
				/* MoveR:R: */
				genoperandoperand(MoveRR, (self_in_CogSimStackEntry->registerr), reg);
			}
			else {
				/* Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
	return self_in_CogSimStackEntry;
}


/*	Answer a bit mask for the receiver's register, if any. */

	/* CogSimStackEntry>>#registerMask */
static NoDbgRegParms sqInt
registerMask(SimStackEntry *self_in_CogSimStackEntry)
{
	return ((((self_in_CogSimStackEntry->type)) == SSBaseOffset)
	 || (((self_in_CogSimStackEntry->type)) == SSRegister)
			? ((((self_in_CogSimStackEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackEntry->registerr)))) : (1U << ((self_in_CogSimStackEntry->registerr))))
			: 0);
}

	/* CogSimStackEntry>>#registerMaskOrNone */
static NoDbgRegParms sqInt
registerMaskOrNone(SimStackEntry *self_in_CogSimStackEntry)
{
	return (((self_in_CogSimStackEntry->type)) == SSRegister
			? ((((self_in_CogSimStackEntry->registerr)) < 0) ? (((usqInt)(1)) >> (-((self_in_CogSimStackEntry->registerr)))) : (1U << ((self_in_CogSimStackEntry->registerr))))
			: 0);
}

	/* CogSimStackEntry>>#registerOrNone */
static NoDbgRegParms sqInt
registerOrNone(SimStackEntry *self_in_CogSimStackEntry)
{
	return (((self_in_CogSimStackEntry->type)) == SSRegister
			? (self_in_CogSimStackEntry->registerr)
			: NoReg);
}

	/* CogSimStackEntry>>#storeToReg: */
static NoDbgRegParms SimStackEntry *
storeToReg(SimStackEntry *self_in_CogSimStackEntry, sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt baseReg;
    sqInt constant;
    sqInt offset;

	switch ((self_in_CogSimStackEntry->type)) {
	case SSBaseOffset:
	case SSSpill:
		offset = (self_in_CogSimStackEntry->offset);
		baseReg = (self_in_CogSimStackEntry->registerr);

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, baseReg, reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
		break;
	case SSConstant:
		constant = (self_in_CogSimStackEntry->constant);

		/* begin genMoveConstant:R: */
		if (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(constant))
		 && (oopisGreaterThan(constant, trueObject()))) {
			annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(MoveCwR, constant, reg)), constant);
		}
		else {
			/* begin MoveCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(MoveCqR, constant, reg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(constant));
			}
		}
		break;
	case SSRegister:
		if (reg != ((self_in_CogSimStackEntry->registerr))) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, (self_in_CogSimStackEntry->registerr), reg);
		}
		else {
			/* Label */
			genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	return self_in_CogSimStackEntry;
}

	/* CogSSBytecodeFixup>>#isBackwardBranchFixup */
static NoDbgRegParms char
isBackwardBranchFixup(BytecodeFixup *self_in_CogSSBytecodeFixup)
{
	return (self_in_CogSSBytecodeFixup->isTargetOfBackwardBranch);
}

	/* CogSSBytecodeFixup>>#isMergeFixup */
static NoDbgRegParms int
isMergeFixup(BytecodeFixup *self_in_CogSSBytecodeFixup)
{
	return (((usqInt)((self_in_CogSSBytecodeFixup->targetInstruction)))) == NeedsMergeFixupFlag;
}


/*	Allocate an unsharable Literal instruction for the literal and answer it. */

	/* OutOfLineLiteralsManager>>#allocateLiteral: */
static NoDbgRegParms AbstractInstruction *
allocateLiteral(sqInt aLiteral)
{
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt initialNumLiterals;
    AbstractInstruction *litInst;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;

	if (nextLiteralIndex >= literalsSize) {
		initialNumLiterals = literalsSize + 8;

		/* begin allocateLiterals: */
		if (initialNumLiterals > literalsSize) {
			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (literals) {
				for (i = 0; i < nextLiteralIndex; i += 1) {
					existingInst = literalInstructionAt(i);
					newInst = (&(newLiterals[i]));
					cloneLiteralFrom(newInst, existingInst);
					assert(!((existingInst->dependent)));
					(existingInst->dependent = newInst);
				}
				for (i = 0; i < opcodeIndex; i += 1) {
					existingInst = abstractInstructionAt(i);
					if ((((existingInst->dependent)))
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	litInst = literalInstructionAt(nextLiteralIndex);
	initializeUniqueLiteral(litInst, aLiteral);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}
	return litInst;
}

	/* OutOfLineLiteralsManager>>#checkLiteral:forInstruction: */
static NoDbgRegParms AbstractInstruction *
checkLiteralforInstruction(sqInt literal, AbstractInstruction *anInstruction)
{
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(literal));
	}
	return anInstruction;
}


/*	Output all pending literal instructions, making the originals dependents
	on the generated ones
	so that a later pass will copy the address of each generated literal inst
	to its original in literals,
	and hence allow the instruction using the literal to compute the correct
	address.. 
 */

	/* OutOfLineLiteralsManager>>#dumpLiterals: */
static NoDbgRegParms sqInt
dumpLiterals(sqInt generateBranchAround)
{
    sqInt i;
    AbstractInstruction *jump;
    AbstractInstruction *litInst;

	jump = ((AbstractInstruction *) 0);
	if (generateBranchAround) {
		jump = genoperand(Jump, ((sqInt)0));
	}
	for (i = lastDumpedLiteralIndex; i < nextLiteralIndex; i += 1) {
		litInst = literalInstructionAt(i);
		((genoperand(Literal, ((litInst->operands))[0]))->dependent = litInst);

		/* begin setLiteralOpcodeIndex: */
		assert(((litInst->opcode)) == Literal);
		((litInst->operands))[2] = opcodeIndex;
	}
	if (generateBranchAround) {
		jmpTarget(jump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	firstOpcodeIndex = opcodeIndex;
	lastDumpedLiteralIndex = nextLiteralIndex;
	return 0;
}


/*	A literal is in range if its opcode index is within
	outOfLineLiteralOpcodeLimit, or if its index has yet to be assigned. */

	/* OutOfLineLiteralsManager>>#literalInstructionInRange: */
static NoDbgRegParms sqInt
literalInstructionInRange(AbstractInstruction *litInst)
{
    sqInt opcodeIdx;

	/* begin literalOpcodeIndex */
	assert(((litInst->opcode)) == Literal);
	opcodeIdx = ((sqInt)(((litInst->operands))[2]));
	return ((((sqInt)opcodeIdx)) < 0)
	 || ((assert((getOpcodeIndex()) >= opcodeIdx),
	(opcodeIndex - opcodeIdx) < (outOfLineLiteralOpcodeLimit(backEnd))));
}


/*	Search for a Literal instruction that is in-range and answer it. Otherwise
	allocate a new sharable Literal instruction for the literal and answer it. */

	/* OutOfLineLiteralsManager>>#locateLiteral: */
static NoDbgRegParms AbstractInstruction *
locateLiteral(sqInt aLiteral)
{
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt initialNumLiterals;
    sqInt iSqInt;
    AbstractInstruction *litInst;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;

	for (i = 0; i < nextLiteralIndex; i += 1) {
		litInst = literalInstructionAt(i);
		if (((((litInst->operands))[0]) == aLiteral)
		 && (((/* begin isSharable */
			assert(((litInst->opcode)) == Literal),
		(((((litInst->operands))[1]) & 1) != 0)))
		 && (literalInstructionInRange(litInst)))) {
			return litInst;
		}
	}
	if (nextLiteralIndex >= literalsSize) {
		initialNumLiterals = literalsSize + 8;

		/* begin allocateLiterals: */
		if (initialNumLiterals > literalsSize) {
			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (literals) {
				for (iSqInt = 0; iSqInt < nextLiteralIndex; iSqInt += 1) {
					existingInst = literalInstructionAt(iSqInt);
					newInst = (&(newLiterals[iSqInt]));
					cloneLiteralFrom(newInst, existingInst);
					assert(!((existingInst->dependent)));
					(existingInst->dependent = newInst);
				}
				for (iSqInt = 0; iSqInt < opcodeIndex; iSqInt += 1) {
					existingInst = abstractInstructionAt(iSqInt);
					if ((((existingInst->dependent)))
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	litInst = literalInstructionAt(nextLiteralIndex);
	initializeSharableLiteral(litInst, aLiteral);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}
	return litInst;
}


/*	<Integer> */

	/* SimpleStackBasedCogit>>#ceClosureCopyDescriptor: */
static NoDbgRegParms sqInt
ceClosureCopyDescriptor(sqInt descriptor)
{
	return createClosureNumArgsnumCopiedstartpc(descriptor & 0x3F, (((usqInt)(descriptor)) >> 6) & 0x3F, ((usqInt)(descriptor)) >> 12);
}

	/* SimpleStackBasedCogit>>#cogMethodHasExternalPrim: */
sqInt
cogMethodHasExternalPrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (primIndex == PrimNumberExternalCall)
	 || (primIndex == PrimNumberFFICall);
}

	/* SimpleStackBasedCogit>>#cogMethodHasMachineCodePrim: */
sqInt
cogMethodHasMachineCodePrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (((primIndex >= 1) && (primIndex <= MaxCompiledPrimitiveIndex)))
	 && ((((primitiveGeneratorTable[primIndex]).primitiveGenerator)));
}


/*	Compile the jump instruction(s) at the end of the method that dispatch to
	each block body.
 */

	/* SimpleStackBasedCogit>>#compileBlockDispatch */
static sqInt
compileBlockDispatch(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSkip;

	assert(blockCount > 0);

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	blockEntryNoContextSwitch = anInstruction;

	/* Set OK to context switch flag to non-zero. */
	jumpSkip = genoperand(Jump, ((sqInt)0));
	blockEntryLabel = genoperandoperand(MoveRR, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jumpSkip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (blockCount > 1) {
		genLoadSlotsourceRegdestReg(ClosureStartPCIndex, ReceiverResultReg, TempReg);
	}
	compileBlockDispatchFromto(0, blockCount - 1);
	return 0;
}


/*	After pushing the temporaries but before the stack limit check a primitive
	method needs to fetch the error code, if any. If the primitive has failed,
	call the trampoline
	that will assign it to the last temp. */

	/* SimpleStackBasedCogit>>#compileGetErrorCode */
static void
compileGetErrorCode(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpNoError;

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(primFailCodeAddress(), genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg));
	flag("ask concrete code gen if move sets condition codes?");

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	jmpNoError = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperandoperand(MoveCwR, ((sqInt)methodLabel), ClassReg))));

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceReapAndResetErrorCodeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(jmpNoError, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}


/*	Compile a call to an interpreter primitive. Call the C routine with the
	usual stack-switching dance, test the primFailCode and then either
	return on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compileInterpreterPrimitive:flags: */
static NoDbgRegParms sqInt
compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *continueAfterProfileSample;
    AbstractInstruction *jmp;
    sqInt offset;

	assert(!((registerisInMask(VarBaseReg, ABICallerSavedRegisterMask))));
	genExternalizePointersForPrimitiveCall();

	/* begin genLoadCStackPointersForPrimCall */
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	if (recordPrimTraceForMethod(methodObj)) {
		genFastPrimTraceUsingand(ClassReg, SendNumArgsReg);
	}

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(primFailCodeAddress(), genoperandoperand(MoveRAw, TempReg, primFailCodeAddress()));
	if (methodOrBlockNumArgs) {
		/* begin AddCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(AddCqR, methodOrBlockNumArgs, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(methodOrBlockNumArgs));
		}
	}

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(argumentCountAddress(), genoperandoperand(MoveRAw, TempReg, argumentCountAddress()));
	if (((flags & PrimCallNeedsNewMethod) != 0)) {
		genLoadNewMethod();
	}

	/* #PrefetchAw: #gen:literal: */
	checkLiteralforInstruction(primFailCodeAddress(), genoperand(PrefetchAw, primFailCodeAddress()));
	if (((flags & PrimCallMayEndureCodeCompaction) != 0)) {

		/* The ceActivateFailingPrimitiveMethod: machinery can't handle framelessness. */
		needsFrame = 1;
		genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);

		/* #genSubstituteReturnAddress: #MoveCw:R: #gen:literal:operand: */
		checkLiteralforInstruction((((flags & PrimCallCollectsProfileSamples) != 0)
				? cePrimReturnEnterCogCodeProfiling
				: cePrimReturnEnterCogCode), genoperandoperand(MoveCwR, (((flags & PrimCallCollectsProfileSamples) != 0)
				? cePrimReturnEnterCogCodeProfiling
				: cePrimReturnEnterCogCode), LR));

		/* #JumpFullRT: #JumpFull: #gen:literal: */
		checkLiteralforInstruction(((sqInt)(((sqInt)primitiveRoutine))), genoperand(JumpFull, ((sqInt)(((sqInt)primitiveRoutine)))));
		return 0;
	}
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);

	/* #CallFullRT: #CallFull: #gen:literal: */
	checkLiteralforInstruction(((sqInt)primitiveRoutine), genoperand(CallFull, ((sqInt)primitiveRoutine)));

	/* genRemoveNArgsFromStack: */
	genLoadStackPointersForPrimCall(backEnd, ClassReg);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveAwR, instructionPointerAddress(), LinkReg));

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(primFailCodeAddress(), genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg));
	flag("ask concrete code gen if move sets condition codes?");

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* placing the test here attributes the tick to the primitive plus any checkForAndFollowForwardedPrimitiveState
	   scanning, but attributes all of a failing primitive to the current method (in ceStackOverflow: on frame build). */
	jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	continueAfterProfileSample = genoperandoperandoperand(MoveMwrR, 0, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(continueAfterProfileSample)) {
		(continueAfterProfileSample->dependent = locateLiteral(0));
	}

	/* RetN: */
	genoperand(RetN, BytesPerWord);
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	offset = BytesPerWord * (methodOrBlockNumArgs);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	return 0;
}


/*	Compile one method cache probe in an OpenPIC's lookup of selector.
	Answer the jump taken if the selector probe fails.
	The class tag of the receiver must be in SendNumArgsReg. ClassReg and
	TempReg are used as scratch registers.
	On a hit, the offset of the entry is in ClassReg. */

	/* SimpleStackBasedCogit>>#compileOpenPICMethodCacheProbeFor:withShift:baseRegOrNone: */
static NoDbgRegParms AbstractInstruction *
compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);
	annotateobjRef(checkLiteralforInstruction(selector, genoperandoperand(XorCwR, selector, ClassReg)), selector);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	anInstruction = genoperandoperand(AndCqR, ((int)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheMask) << (shiftForWord())))));
	}
	if (baseRegOrNone == NoReg) {
		offset = (methodCacheAddress()) + ((((usqInt)(MethodCacheSelector) << (shiftForWord()))));

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
	}
	else {
		/* AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, ((int)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheSelector) << (shiftForWord())))));
		}
	}
	annotateobjRef(checkLiteralforInstruction(selector, genoperandoperand(CmpCwR, selector, TempReg)), selector);
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		offset = (methodCacheAddress()) + ((((usqInt)(MethodCacheClass) << (shiftForWord()))));

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
	}
	else {
		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, ((int)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheClass) << (shiftForWord())))));
		}
	}

	/* begin CmpR:R: */
	assert(!(0 /* (SendNumArgsReg = SPReg) */));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile the code for an open PIC. Perform a probe of the first-level
	method lookup cache followed by a call of ceSendFromInLineCacheMiss: if
	the probe fails. */

	/* SimpleStackBasedCogit>>#compileOpenPIC:numArgs: */
static NoDbgRegParms void
compileOpenPICnumArgs(sqInt selector, sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	/* begin preenMethodLabel */
	/* setLabelOffset: */
	((methodLabel->operands))[1] = 0;
	compilePICAbort(numArgs);
	entry = genGetClassTagOfintoscratchReg(ReceiverResultReg, SendNumArgsReg, TempReg);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 0, cacheBaseReg);

	/* Fetch the method.  The interpret trampoline requires the bytecoded method in SendNumArgsReg */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = (cacheBaseReg == NoReg
				? (methodCacheAddress()) + ((((usqInt)(MethodCacheMethod) << (shiftForWord()))))
				: ((int)((usqInt)(MethodCacheMethod) << (shiftForWord()))));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	itsAHit = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(itsAHit)) {
		(itsAHit->dependent = locateLiteral(offset));
	}
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	jmpTarget(jumpBCMethod, picInterpretAbort);

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(cmNoCheckEntryOffset));
	}

	/* JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 1, cacheBaseReg);

	/* JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 2, cacheBaseReg);

	/* JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genSmalltalkToCStackSwitch(1);
	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperandoperand(MoveCwR, ((sqInt)methodLabel), SendNumArgsReg))));
	compileCallFornumArgsargargargargresultRegregsToSave(ceSendFromInLineCacheMiss, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* emptyRegisterMask */);
}


/*	Compile one method cache probe in a perform: primitive's lookup of
	selector. Answer the jump taken if the selector probe fails. */

	/* SimpleStackBasedCogit>>#compilePerformMethodCacheProbeFor:withShift:baseRegOrNone: */
static NoDbgRegParms AbstractInstruction *
compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	/* MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);

	/* XorR:R: */
	genoperandoperand(XorRR, selectorReg, ClassReg);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	anInstruction = genoperandoperand(AndCqR, ((int)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheMask) << (shiftForWord())))));
	}
	if (baseRegOrNone == NoReg) {
		offset = (methodCacheAddress()) + ((((usqInt)(MethodCacheSelector) << (shiftForWord()))));

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
	}
	else {
		/* AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, ((int)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheSelector) << (shiftForWord())))));
		}
	}

	/* begin CmpR:R: */
	assert(!((selectorReg == SPReg)));
	genoperandoperand(CmpRR, selectorReg, TempReg);
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		offset = (methodCacheAddress()) + ((((usqInt)(MethodCacheClass) << (shiftForWord()))));

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
	}
	else {
		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, ((int)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(((int)((usqInt)(MethodCacheClass) << (shiftForWord())))));
		}
	}

	/* begin CmpR:R: */
	assert(!(0 /* (SendNumArgsReg = SPReg) */));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile a primitive. If possible, performance-critical primitives will
	be generated by their own routines (primitiveGenerator). Otherwise,
	if there is a primitive at all, we call the C routine with the usual
	stack-switching dance, test the primFailCode and then either return
	on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compilePrimitive */
static sqInt
compilePrimitive(void)
{
    sqInt code;
    sqInt flags;
    sqInt opcodeIndexAtPrimitive;
    PrimitiveDescriptor *primitiveDescriptor;
    void (*primitiveRoutine)(void);

	flags = 0;
	primitiveDescriptor = ((PrimitiveDescriptor *) 0);
	if (!primitiveIndex) {
		return 0;
	}
	if ((((primitiveDescriptor = primitiveGeneratorOrNil())))
	 && ((((primitiveDescriptor->primitiveGenerator)))
	 && ((((primitiveDescriptor->primNumArgs)) < 0)
	 || (((primitiveDescriptor->primNumArgs)) == methodOrBlockNumArgs)))) {

		/* Note opcodeIndex so that any arg load instructions
		   for unimplemented primitives can be discarded. */
		opcodeIndexAtPrimitive = opcodeIndex;
		code = ((primitiveDescriptor->primitiveGenerator))();
		if ((code < 0)
		 && (code != UnimplementedPrimitive)) {

			/* Generator failed, so no point continuing... */
			return code;
		}
		if (code == UnfailingPrimitive) {
			return 0;
		}
		if ((code == CompletePrimitive)
		 && (!(/* methodUsesPrimitiveErrorCode:header: */
			((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
		 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))))) {
			return 0;
		}
		if (code == UnimplementedPrimitive) {
			opcodeIndex = opcodeIndexAtPrimitive;
		}
	}
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, (&flags));
	if ((primitiveRoutine == 0)
	 || (primitiveRoutine == (((void (*)(void)) primitiveFail)))) {
		return genFastPrimFail();
	}
	return compileInterpreterPrimitiveflags(primitiveRoutine, flags);
}

	/* SimpleStackBasedCogit>>#extendedPushBytecode */
static sqInt
extendedPushBytecode(void)
{
    usqInt variableIndex;
    usqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (!variableType) {
		return genPushReceiverVariable(variableIndex);
	}
	if (variableType == 1) {
		return genPushTemporaryVariable(variableIndex);
	}
	if (variableType == 2) {
		return genPushLiteralIndex(variableIndex);
	}
	return genPushLiteralVariable(variableIndex);
}

	/* SimpleStackBasedCogit>>#extendedStoreAndPopBytecode */
static sqInt
extendedStoreAndPopBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    usqInt variableIndex;
    usqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (!variableType) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, /* ssTopNeedsStoreCheck */
			((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(((ssTop())->constant)))
		 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(1, variableIndex);
#    if IMMUTABILITY
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY

		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, /* ssTopNeedsStoreCheck */
			((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(((ssTop())->constant)))
		 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#extendedStoreBytecode */
static sqInt
extendedStoreBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    usqInt variableIndex;
    usqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (!variableType) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, /* ssTopNeedsStoreCheck */
			((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(((ssTop())->constant)))
		 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(0, variableIndex);
#    if IMMUTABILITY
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY

		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, /* ssTopNeedsStoreCheck */
			((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(((ssTop())->constant)))
		 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#frameOffsetOfTemporary: */
static NoDbgRegParms int
frameOffsetOfTemporary(sqInt index)
{
	return /* frameOffsetOfTemporary:numArgs: */
		(index < methodOrBlockNumArgs
			? FoxCallerSavedIP + ((methodOrBlockNumArgs - index) * BytesPerWord)
			: (FoxMFReceiver - BytesPerWord) + ((methodOrBlockNumArgs - index) * BytesPerWord));
}

	/* SimpleStackBasedCogit>>#genDoubleFailIfZeroArgRcvr:arg: */
static NoDbgRegParms AbstractInstruction *
genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg)
{
    AbstractInstruction *anInstruction;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* ConvertR:Rd: */
	genoperandoperand(ConvertRRd, TempReg, DPFPReg2);

	/* CmpRd:Rd: */
	genoperandoperand(CmpRdRd, DPFPReg2, argReg);
	return gJumpFPEqual(0);
}


/*	Can use any of the first 32 literals for the selector and pass up to 7
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genExtendedSendBytecode */
static sqInt
genExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}

	/* SimpleStackBasedCogit>>#genExtendedSuperBytecode */
static sqInt
genExtendedSuperBytecode(void)
{
	return genSendSupernumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}

	/* SimpleStackBasedCogit>>#genFastPrimFail */
static sqInt
genFastPrimFail(void)
{
	primitiveIndex = 0;
	return UnfailingPrimitive;
}


/*	Support for compileInterpreterPrimitive. Generate inline code
	so as to record the primitive trace as fast as possible. */

	/* SimpleStackBasedCogit>>#genFastPrimTraceUsing:and: */
static NoDbgRegParms void
genFastPrimTraceUsingand(sqInt r1, sqInt r2)
{
    AbstractInstruction *anInstruction;
    sqInt offset;


	/* #MoveAb:R: #gen:literal:operand: */
	checkLiteralforInstruction(primTraceLogIndexAddress(), genoperandoperand(MoveAbR, primTraceLogIndexAddress(), r2));

	/* begin LoadEffectiveAddressMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, 1, r2, r1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(1));
	}

	/* #MoveR:Ab: #gen:operand:literal: */
	checkLiteralforInstruction(primTraceLogIndexAddress(), genoperandoperand(MoveRAb, r1, primTraceLogIndexAddress()));
	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperandoperand(MoveCwR, ((sqInt)methodLabel), r1))));
	offset = offsetof(CogMethod, methodObject);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, r1, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* #MoveCw:R: #gen:literal:operand: */
	checkLiteralforInstruction(((sqInt)(primTraceLogAddress())), genoperandoperand(MoveCwR, ((sqInt)(primTraceLogAddress())), r1));

	/* MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, TempReg, r2, r1);
}

	/* SimpleStackBasedCogit>>#genLoadNewMethod */
static void
genLoadNewMethod(void)
{
    sqInt address;
    AbstractInstruction *anInstruction;
    sqInt offset;

	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperandoperand(MoveCwR, ((sqInt)methodLabel), ClassReg))));
	offset = offsetof(CogMethod, methodObject);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(newMethodAddress(), genoperandoperand(MoveRAw, TempReg, newMethodAddress()));
#  if LRPCheck
	if (checkingLongRunningPrimitives()) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}

		/* #MoveR:Aw: #gen:operand:literal: */
		checkLiteralforInstruction(longRunningPrimitiveStopUsecsAddress(), genoperandoperand(MoveRAw, TempReg, longRunningPrimitiveStopUsecsAddress()));
		address = (longRunningPrimitiveStopUsecsAddress()) + 4;

		/* begin MoveR:Aw: */
		/* gen:operand:literal: */
		checkLiteralforInstruction(address, genoperandoperand(MoveRAw, TempReg, address));
	}
#  endif // LRPCheck
}

	/* SimpleStackBasedCogit>>#genLongJumpIfFalse */
static sqInt
genLongJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genLongJumpIfTrue */
static sqInt
genLongJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}


/*	237		11101101	i i i i i i i i	Pop and Store Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongStoreAndPopTemporaryVariableBytecode */
static sqInt
genLongStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte1);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalBackwardJump */
static sqInt
genLongUnconditionalBackwardJump(void)
{
    sqInt distance;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance < 0);
	return genJumpBackTo((distance + 2) + bytecodePC);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalForwardJump */
static sqInt
genLongUnconditionalForwardJump(void)
{
    sqInt distance;
    sqInt targetpc;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance >= 0);
	targetpc = (distance + 2) + bytecodePC;
	return genJumpTo(targetpc);
}


/*	Compile the code for a probe of the first-level method cache for a perform
	primitive. The selector is assumed to be in Arg0Reg. Defer to
	adjustArgumentsForPerform: to
	adjust the arguments before the jump to the method. */
/*	N.B. Can't assume TempReg already contains the tag because a method can
	of course be invoked via the unchecked entry-point, e.g. as does perform:. */

	/* SimpleStackBasedCogit>>#genLookupForPerformNumArgs: */
static NoDbgRegParms sqInt
genLookupForPerformNumArgs(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBadNumArgs;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpInterpret;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, SendNumArgsReg, 0);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 0, cacheBaseReg);

	/* Fetch the method, and check if it is cogged. */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	offset = (cacheBaseReg == NoReg
				? (methodCacheAddress()) + ((((usqInt)(MethodCacheMethod) << (shiftForWord()))))
				: ((int)((usqInt)(MethodCacheMethod) << (shiftForWord()))));

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	itsAHit = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(itsAHit)) {
		(itsAHit->dependent = locateLiteral(offset));
	}
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);

	/* check the argument count; if it's wrong fall back on the interpreter primitive. */
	jumpInterpret = genJumpImmediate(ClassReg);

	/* begin genLoadcmNumArgsOf:into: */
	anInstruction = genoperandoperandoperand(MoveMbrR, BytesPerWord, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BytesPerWord));
	}

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, numArgs, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(numArgs));
	}

	/* Adjust arguments and jump to the method's unchecked entry-point. */
	jumpBadNumArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(cmNoCheckEntryOffset));
	}
	adjustArgumentsForPerform(numArgs);

	/* JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 1, cacheBaseReg);

	/* JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 2, cacheBaseReg);

	/* JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpInterpret, jmpTarget(jumpBadNumArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return 0;
}


/*	If the objectMemory allows it, generates a quick constant move, else
	generates a word constant move
 */

	/* SimpleStackBasedCogit>>#genMoveConstant:R: */
static NoDbgRegParms AbstractInstruction *
genMoveConstantR(sqInt constant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(constant))
	 && (oopisGreaterThan(constant, trueObject()))) {
		return annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(MoveCwR, constant, reg)), constant);
	}
	else {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, constant, reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(constant));
		}
		return anInstruction;
	}
}

	/* SimpleStackBasedCogit>>#genMustBeBooleanTrampolineFor:called: */
static NoDbgRegParms usqInt
genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName)
{
    AbstractInstruction *anInstruction;

	zeroOpcodeIndex();
	assert(!(shouldAnnotateObjectReference(boolean)));

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, boolean, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(boolean));
	}
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSendMustBeBoolean, trampolineName, 1, TempReg, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 1);
}


/*	Implement 28-bit hashMultiply for SmallInteger and LargePositiveInteger
	receivers. 
 */

	/* SimpleStackBasedCogit>>#genPrimitiveHashMultiply */
static sqInt
genPrimitiveHashMultiply(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpFailImm;
    AbstractInstruction *jmpFailNotPositiveLargeInt;

	if (mclassIsSmallInteger()) {
		genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);

		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(HashMultiplyConstant));
		}

		/* MulR:R: */
		genMulRR(backEnd, TempReg, ReceiverResultReg);

		/* begin AndCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(HashMultiplyMask));
		}
		genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);

		/* RetN: */
		genoperand(RetN, 0);
		return CompletePrimitive;
	}
	jmpFailImm = genJumpImmediate(ReceiverResultReg);
	genGetCompactClassIndexNonImmOfinto(ReceiverResultReg, ClassReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, ClassLargePositiveIntegerCompactIndex, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(ClassLargePositiveIntegerCompactIndex));
	}
	jmpFailNotPositiveLargeInt = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, ReceiverResultReg, ReceiverResultReg);

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(HashMultiplyConstant));
	}

	/* MulR:R: */
	genMulRR(backEnd, TempReg, ReceiverResultReg);

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(HashMultiplyMask));
	}
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);

	/* RetN: */
	genoperand(RetN, 0);
	jmpTarget(jmpFailImm, jmpTarget(jmpFailNotPositiveLargeInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Generate the substitute return code for an external or FFI primitive call.
	On success simply return, extracting numArgs from newMethod.
	On primitive failure call ceActivateFailingPrimitiveMethod: newMethod. */

	/* SimpleStackBasedCogit>>#genPrimReturnEnterCogCodeEnilopmart: */
static NoDbgRegParms void
genPrimReturnEnterCogCodeEnilopmart(sqInt profiling)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpFail;
    sqInt quickConstant;

	zeroOpcodeIndex();
	quickConstant = varBaseAddress;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(primFailCodeAddress(), genoperandoperand(MoveAwR, primFailCodeAddress(), TempReg));
	flag("ask concrete code gen if move sets condition codes?");

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	jmpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadStackPointers(backEnd);

	/* PopR: */
	genoperand(PopR, ReceiverResultReg);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveAwR, instructionPointerAddress(), PCReg));
	jmpTarget(jmpFail, checkLiteralforInstruction(newMethodAddress(), genoperandoperand(MoveAwR, newMethodAddress(), SendNumArgsReg)));

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(cStackPointerAddress(), genoperandoperand(MoveAwR, cStackPointerAddress(), SPReg));
	compileCallFornumArgsargargargargresultRegregsToSave(ceActivateFailingPrimitiveMethod, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* emptyRegisterMask */);

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveAwR, instructionPointerAddress(), LinkReg));
	genLoadStackPointers(backEnd);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* RetN: */
	genoperand(RetN, BytesPerWord);
}

	/* SimpleStackBasedCogit>>#genPushConstantFalseBytecode */
static sqInt
genPushConstantFalseBytecode(void)
{
	return ssPushConstant(falseObject());
}

	/* SimpleStackBasedCogit>>#genPushConstantNilBytecode */
static sqInt
genPushConstantNilBytecode(void)
{
	return ssPushConstant(nilObject());
}

	/* SimpleStackBasedCogit>>#genPushConstantTrueBytecode */
static sqInt
genPushConstantTrueBytecode(void)
{
	return ssPushConstant(trueObject());
}

	/* SimpleStackBasedCogit>>#genPushLiteralConstantBytecode */
static sqInt
genPushLiteralConstantBytecode(void)
{
	return genPushLiteralIndex(byte0 & 0x1F);
}

	/* SimpleStackBasedCogit>>#genPushLiteralVariableBytecode */
static sqInt
genPushLiteralVariableBytecode(void)
{
	return genPushLiteralVariable(byte0 & 0x1F);
}

	/* SimpleStackBasedCogit>>#genPushQuickIntegerConstantBytecode */
static sqInt
genPushQuickIntegerConstantBytecode(void)
{
	return ssPushConstant((((usqInt)(byte0 - 117) << 1) | 1));
}

	/* SimpleStackBasedCogit>>#genPushReceiverVariableBytecode */
static sqInt
genPushReceiverVariableBytecode(void)
{
	return genPushReceiverVariable(byte0 & 15);
}

	/* SimpleStackBasedCogit>>#genPushTemporaryVariableBytecode */
static sqInt
genPushTemporaryVariableBytecode(void)
{
	return genPushTemporaryVariable(byte0 & 15);
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnConst */
sqInt
genQuickReturnConst(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	constant = quickPrimitiveConstantFor(primitiveIndex);

	/* begin genMoveConstant:R: */
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(constant))
	 && (oopisGreaterThan(constant, trueObject()))) {
		annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(MoveCwR, constant, ReceiverResultReg)), constant);
	}
	else {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(constant));
		}
	}
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnInstVar */
sqInt
genQuickReturnInstVar(void)
{
    sqInt index;

	index = quickPrimitiveInstVarIndexFor(primitiveIndex);
	genLoadSlotsourceRegdestReg(index, ReceiverResultReg, ReceiverResultReg);
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnSelf */
sqInt
genQuickReturnSelf(void)
{
	genUpArrowReturn();
	return UnfailingPrimitive;
}

	/* SimpleStackBasedCogit>>#genReturnFalse */
static sqInt
genReturnFalse(void)
{
    AbstractInstruction *anInstruction;

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, falseObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(falseObject()));
	}
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnNil */
static sqInt
genReturnNil(void)
{
    AbstractInstruction *anInstruction;

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, nilObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(nilObject()));
	}
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnTrue */
static sqInt
genReturnTrue(void)
{
    AbstractInstruction *anInstruction;

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(trueObject()));
	}
	return genUpArrowReturn();
}


/*	Can use any of the first 64 literals for the selector and pass up to 3
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genSecondExtendedSendBytecode */
static sqInt
genSecondExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x3F, ((usqInt)(byte1)) >> 6);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector0ArgsBytecode */
static sqInt
genSendLiteralSelector0ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 0);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector1ArgBytecode */
static sqInt
genSendLiteralSelector1ArgBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 1);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector2ArgsBytecode */
static sqInt
genSendLiteralSelector2ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 2);
}

	/* SimpleStackBasedCogit>>#genShortJumpIfFalse */
static sqInt
genShortJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genShortUnconditionalJump */
static sqInt
genShortUnconditionalJump(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpTo(target);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorEqualsEquals */
static sqInt
genSpecialSelectorEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(0);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorNotEqualsEquals */
static sqInt
genSpecialSelectorNotEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(1);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorSend */
static sqInt
genSpecialSelectorSend(void)
{
    sqInt index;
    sqInt numArgs;

	index = byte0 - FirstSpecialSelector;
	numArgs = specialSelectorNumArgs(index);
	return genSendnumArgs((-index) - 1, numArgs);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopReceiverVariableBytecode */
static sqInt
genStoreAndPopReceiverVariableBytecode(void)
{
	return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, byte0 & 7, /* ssTopNeedsStoreCheck */
		((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(((ssTop())->constant)))
	 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopRemoteTempLongBytecode */
static sqInt
genStoreAndPopRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(1, byte1, byte2, /* ssTopNeedsStoreCheck */
		((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(((ssTop())->constant)))
	 && (oopisGreaterThan(((ssTop())->constant), trueObject())))));
}

	/* SimpleStackBasedCogit>>#genStoreAndPopTemporaryVariableBytecode */
static sqInt
genStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte0 & 7);
}

	/* SimpleStackBasedCogit>>#genStoreRemoteTempLongBytecode */
static sqInt
genStoreRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(0, byte1, byte2, /* ssTopNeedsStoreCheck */
		((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(((ssTop())->constant)))
	 && (oopisGreaterThan(((ssTop())->constant), trueObject())))));
}


/*	Collect the branch and send data for cogMethod, storing it into arrayObj. */

	/* SimpleStackBasedCogit>>#mapPCDataFor:into: */
sqInt
mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj)
{
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    CogBlockMethod *cogMethod1;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    sqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt nextBcpc;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;

	introspectionDataIndex = 0;
	introspectionData = arrayObj;
	if (!((cogMethod->stackCheckOffset))) {
		assert(introspectionDataIndex == 0);
		storePointerUncheckedofObjectwithValue(0, introspectionData, nilObject());
		storePointerUncheckedofObjectwithValue(1, introspectionData, (((usqInt)cmEntryOffset << 1) | 1));
		storePointerUncheckedofObjectwithValue(2, introspectionData, nilObject());
		storePointerUncheckedofObjectwithValue(3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 1) | 1));
		return 4;
	}
	cogMethod1 = ((CogBlockMethod *) cogMethod);
	startbcpc = startPCOfMethod((cogMethod->methodObject));

	/* begin mapFor:bcpc:performUntil:arg: */
	descriptor = ((BytecodeDescriptor *) 0);
	latestContinuation = 0;
	mapByte = 0;
	nextBcpc = 0;
	assert(((cogMethod1->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogMethod1)) + ((cogMethod1->stackCheckOffset));
	result = pcDataForAnnotationMcpcBcpcMethod(null, 0 + ((((usqInt)(HasBytecodePC) << 1))), ((char *) mcpc), startbcpc, ((void *)cogMethod));
	if (result) {
		errCode = result;
		goto l1;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogMethod1->cmType)) >= CMMethod) {
		isInBlock = 0 /* cmIsFullBlock */;
		homeMethod = ((CogMethod *) cogMethod1);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;

		/* If the method has a primitive, skip it and the error code store, if any;
		   Logically. these come before the stack check and so must be ignored. */
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogMethod1->startpc)));
		homeMethod = cmHomeMethod(cogMethod1);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod1)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - BlockCreationBytecodeSize;
		bsOffset = 0 /* begin bytecodeSetOffsetForHeader: */;
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj)
		: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l1;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l1;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)
		: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
								? nExts + 1
								: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
					 && ((/* begin isBackwardBranch:at:exts:in: */
						assert(((descriptor->spanFunction))),
					(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = pcDataForAnnotationMcpcBcpcMethod(descriptor, (isBackwardBranch
							? ((((usqInt)(annotation) << 1))) + 1
							: ((sqInt)((usqInt)(annotation) << 1))), ((char *) mcpc), (isBackwardBranch
							? bcpc - (2 * nExts)
							: bcpc), ((void *)cogMethod));
				if (result) {
					errCode = result;
					goto l1;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
							? nExts + 1
							: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < ((((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += ((((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	errCode = 0;
l1:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode) {
		assert(errCode == PrimErrNoMemory);
		return -1;
	}
	if ((cogMethod->blockEntryOffset)) {
		errCode = blockDispatchTargetsForperformarg(cogMethod, pcDataForBlockEntryMethod, ((sqInt)cogMethod));
		if (errCode) {
			assert(errCode == PrimErrNoMemory);
			return -1;
		}
	}
	return introspectionDataIndex;
}

	/* SimpleStackBasedCogit>>#numSpecialSelectors */
static sqInt
numSpecialSelectors(void)
{
	return NumSpecialSelectors;
}


/*	Collect the branch and send data for the block method starting at
	blockEntryMcpc, storing it into picData.
 */

	/* SimpleStackBasedCogit>>#pcDataForBlockEntry:Method: */
static NoDbgRegParms usqInt
pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod)
{
	storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)(blockEntryMcpc - blockNoContextSwitchOffset) << 1) | 1));
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)blockEntryMcpc << 1) | 1));
	introspectionDataIndex += 4;
	return 0;
}

	/* SimpleStackBasedCogit>>#pcDataFor:Annotation:Mcpc:Bcpc:Method: */
static NoDbgRegParms sqInt
pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt actualBcpc;
    sqInt actualMcpc;

	if (!descriptor) {

		/* this is the stackCheck offset */
		assert(introspectionDataIndex == 0);
		storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)cmEntryOffset << 1) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 1) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 4, introspectionData, (((usqInt)(bcpc + 1) << 1) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 5, introspectionData, (((((((CogMethod *) cogMethodArg))->stackCheckOffset)) << 1) | 1));
		introspectionDataIndex += 6;
		return 0;
	}
	if ((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= HasBytecodePC) {
		actualBcpc = (((isBackwardBranchAndAnnotation & 1) != 0)
					? bcpc + 1
					: (bcpc + ((descriptor->numBytes))) + 1);
		actualMcpc = (((usqInt)mcpc)) - (((usqInt)cogMethodArg));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, (((usqInt)actualBcpc << 1) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)actualMcpc << 1) | 1));
		introspectionDataIndex += 2;
	}
	return 0;
}


/*	If there is a generator for the current primitive then answer it;
	otherwise answer nil. */

	/* SimpleStackBasedCogit>>#primitiveGeneratorOrNil */
static PrimitiveDescriptor *
primitiveGeneratorOrNil(void)
{
    PrimitiveDescriptor *primitiveDescriptor;

	if (isQuickPrimitiveIndex(primitiveIndex)) {
		/* an unused one */
		primitiveDescriptor = (&(primitiveGeneratorTable[0]));
		(primitiveDescriptor->primitiveGenerator = quickPrimitiveGeneratorFor(primitiveIndex));
		return primitiveDescriptor;
	}
	if (((primitiveIndex >= 1) && (primitiveIndex <= MaxCompiledPrimitiveIndex))) {
		return (&(primitiveGeneratorTable[primitiveIndex]));
	}
	return null;
}

	/* SimpleStackBasedCogit>>#register:isInMask: */
static NoDbgRegParms int
registerisInMask(sqInt reg, sqInt mask)
{
	return ((mask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1U << reg)))) != 0);
}

	/* SimpleStackBasedCogit>>#v3:Block:Code:Size: */
static NoDbgRegParms sqInt
v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts <= 0);
	return ((((usqInt)((fetchByteofObject(pc + 2, aMethodObj))) << 8))) + (fetchByteofObject(pc + 3, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:LongForward:Branch:Distance: */
static NoDbgRegParms sqInt
v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return ((((fetchByteofObject(pc, aMethodObj)) & 3) << 8)) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:Long:Branch:Distance: */
static NoDbgRegParms sqInt
v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return ((((usqInt)((((fetchByteofObject(pc, aMethodObj)) & 7) - 4)) << 8))) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	N.B. This serves for both BlueBook/V3 and V4 short jumps. */

	/* SimpleStackBasedCogit>>#v3:ShortForward:Branch:Distance: */
static NoDbgRegParms sqInt
v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return ((fetchByteofObject(pc, aMethodObj)) & 7) + 1;
}


/*	Add a blockStart for an embedded block. For a binary tree walk block
	dispatch blocks must be compiled in pc/depth-first order but are scanned
	in breadth-first
	order, so do an insertion sort (which of course is really a bubble sort
	because we
	have to move everything higher to make room). */

	/* StackToRegisterMappingCogit>>#addBlockStartAt:numArgs:numCopied:span: */
static NoDbgRegParms BlockStart *
addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span)
{
    BlockStart *blockStart;
    sqInt i;
    sqInt j;


	/* Transcript ensureCr; nextPutAll: 'addBlockStartAt: '; print: bytecodepc; cr; flush. */
	if (blockCount > 0) {
		i = blockCount - 1;
		while (1) {
			/* check for repeat addition during recompilation due to initialNil miscount. */
			blockStart = (&(blockStarts[i]));
			if (((blockStart->startpc)) == bytecodepc) {
				return blockStart;
			}
			if (!((((blockStart->startpc)) > bytecodepc)
			 && (i > 0))) break;
			i -= 1;
		}
		for (j = blockCount; j >= (i + 1); j += -1) {
			blockStarts[j] = (blockStarts[j - 1]);
		}
		blockStart = (&(blockStarts[i + 1]));
	}
	else {
		blockStart = (&(blockStarts[blockCount]));
	}
	blockCount += 1;
	(blockStart->startpc = bytecodepc);
	(blockStart->numArgs = numArgs);
	(blockStart->numCopied = numCopied);
	(blockStart->numInitialNils = 0);
	(blockStart->stackCheckLabel = null);
	(blockStart->hasInstVarRef = 0);
	(blockStart->span = span);
	return blockStart;
}


/*	e.g.	Receiver				Receiver	or	Receiver				Receiver	(RISC)
	Selector/Arg0	=>		Arg1			Selector/Arg0	=>		Arg1
	Arg1					Arg2			Arg1					Arg2
	Arg2					Arg3			Arg2			sp->	Arg3
	Arg3			sp->	retpc	sp->	Arg3
	sp->	retpc */
/*	Generate code to adjust the possibly stacked arguments immediately
	before jumping to a method looked up by a perform primitive. */

	/* StackToRegisterMappingCogit>>#adjustArgumentsForPerform: */
static NoDbgRegParms void
adjustArgumentsForPerform(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt index;
    sqInt offset;
    sqInt quickConstant;

	assert((numRegArgs()) <= 2);
	assert(numArgs >= 1);
	if (numArgs <= (numRegArgs())) {
		if (numArgs == 2) {
			/* MoveR:R: */
			genoperandoperand(MoveRR, Arg1Reg, Arg0Reg);
		}
		return;
	}
	if (((numRegArgs()) + 1) == numArgs) {
		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, 0, SPReg, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(0));
		}
		quickConstant = (numArgs + 1) * BytesPerWord;

		/* begin AddCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(AddCqR, quickConstant, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(quickConstant));
		}
		return;
	}
	for (index = (numArgs - 2); index >= 0; index += -1) {
		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, index * BytesPerWord, SPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(index * BytesPerWord));
		}
		offset = (index + 1) * BytesPerWord;

		/* begin MoveR:Mw:r: */
		/* begin gen:operand:quickConstant:operand: */
		anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, offset, SPReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteral(offset));
		}
	}

	/* begin AddCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AddCqR, BytesPerWord, SPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(BytesPerWord));
	}
}


/*	If the stack entry is already in a register not conflicting with regMask,
	answers it,
	else allocate a new register not conflicting with reg mask
 */

	/* StackToRegisterMappingCogit>>#allocateRegForStackEntryAt:notConflictingWith: */
static NoDbgRegParms sqInt
allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask)
{
    sqInt mask;
    CogSimStackEntry *stackEntry;

	stackEntry = ssValue(index);
	mask = registerMaskOrNone(stackEntry);
	if ((mask != 0)
	 && ((!(mask & regMask)))) {
		flag("TODO");
		return registerOrNone(stackEntry);
	}
	return allocateRegNotConflictingWith(regMask);
}


/*	if there's a free register, use it */

	/* StackToRegisterMappingCogit>>#allocateRegNotConflictingWith: */
static NoDbgRegParms sqInt
allocateRegNotConflictingWith(sqInt regMask)
{
    sqInt reg;

	reg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | regMask);
	if (reg == NoReg) {

		/* No free register, choose one that does not conflict with regMask */
		reg = freeAnyRegNotConflictingWith(regMask);
	}
	if (reg == ReceiverResultReg) {

		/* If we've allocated RcvrResultReg, it's not live anymore */
		voidReceiverResultRegContainsSelf();
	}
	return reg;
}

	/* StackToRegisterMappingCogit>>#anyReferencesToRegister:inTopNItems: */
static NoDbgRegParms sqInt
anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n)
{
    sqInt i;
    sqInt regMask;

	regMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1U << reg));
	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((registerMask(simStackAt(i))) & regMask) != 0)) {
			return 1;
		}
	}
	return 0;
}


/*	This is a static version of ceCallCogCodePopReceiverArg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg0Regs */
void
callCogCodePopReceiverArg0Regs(void)
{
	realCECallCogCodePopReceiverArg0Regs();
}


/*	This is a static version of ceCallCogCodePopReceiverArg1Arg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg1Arg0Regs */
void
callCogCodePopReceiverArg1Arg0Regs(void)
{
	realCECallCogCodePopReceiverArg1Arg0Regs();
}


/*	Loop over bytecodes, dispatching to the generator for each bytecode,
	handling fixups in due course.
 */

	/* StackToRegisterMappingCogit>>#compileAbstractInstructionsFrom:through: */
static NoDbgRegParms sqInt
compileAbstractInstructionsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt nextOpcodeIndex;
    sqInt result;

	traceSimStack();
	bytecodePC = start;
	nExts = (result = 0);
	descriptor = null;
	deadCode = 0;
	while (1) {
		maybeHaltIfDebugPC();
		mergeWithFixupIfRequired((fixup = fixupAt(bytecodePC)));
		descriptor = loadBytesAndGetDescriptor();
		nextOpcodeIndex = opcodeIndex;
		result = (deadCode
					? mapDeadDescriptorIfNeeded(descriptor)
					: ((descriptor->generator))());
		if (!result) {
			/* begin assertExtsAreConsumed: */
			if (!((descriptor->isExtension))) {
				assert((extA == 0)
				 && ((extB == 0)
				 && (numExtB == 0)));
			}
		}
		traceDescriptor(descriptor);
		traceSimStack();

		/* begin patchFixupTargetIfNeeded:nextOpcodeIndex: */
		if ((((((usqInt)((fixup->targetInstruction)))) >= NeedsNonMergeFixupFlag) && ((((usqInt)((fixup->targetInstruction)))) <= NeedsMergeFixupFlag))) {

			/* There is a fixup for this bytecode.  It must point to the first generated
			   instruction for this bytecode.  If there isn't one we need to add a label. */
			if (opcodeIndex == nextOpcodeIndex) {
				/* Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			(fixup->targetInstruction = abstractInstructionAt(nextOpcodeIndex));
		}

		/* begin maybeDumpLiterals: */
		if ((/* mustDumpLiterals: */
			(opcodeIndex >= firstOpcodeIndex)
		 && ((opcodeIndex - firstOpcodeIndex) >= (outOfLineLiteralOpcodeLimit(backEnd))))
		 || ((/* isUnconditionalBranch */
			(isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse)))))
		 || ((descriptor->isReturn)))) {
			dumpLiterals(!((/* isUnconditionalBranch */
				(isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse)))))
			 || ((descriptor->isReturn))));
		}
		bytecodePC = (bytecodePC + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, bytecodePC, nExts, methodObj)
		: 0));
		if (!((result == 0)
		 && (bytecodePC <= end))) break;
		nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
	}

	/* begin checkEnoughOpcodes */
	if (opcodeIndex > numAbstractOpcodes) {
		error("Cog JIT internal error. Too many abstract opcodes.  Num opcodes heuristic is too optimistic.");
	}
	return result;
}

	/* StackToRegisterMappingCogit>>#compileBlockBodies */
static sqInt
compileBlockBodies(void)
{
    BlockStart *blockStart;
    sqInt compiledBlocksCount;
    sqInt initialCounterIndex;
    sqInt initialOpcodeIndex;
    sqInt initialStackPtr;
    sqInt (* const pushNilSizeFunction)(sqInt,sqInt) = v3PushNilSizenumInitialNils;
    sqInt result;
    sqInt savedFirstOpcodeIndex;
    sqInt savedLastDumpedLiteralIndex;
    unsigned char savedNeedsFrame;
    sqInt savedNextLiteralIndex;
    sqInt savedNumArgs;
    sqInt savedNumTemps;

	initialStackPtr = 0;
	assert(blockCount > 0);
	savedNeedsFrame = needsFrame;
	savedNumArgs = methodOrBlockNumArgs;
	savedNumTemps = methodOrBlockNumTemps;
	inBlock = InVanillaBlock;
	compiledBlocksCount = 0;
	while (compiledBlocksCount < blockCount) {
		compilationPass = 1;
		blockStart = blockStartAt(compiledBlocksCount);
		if (((result = scanBlock(blockStart))) < 0) {
			return result;
		}
		initialOpcodeIndex = opcodeIndex;

		/* for SistaCogit */
		initialCounterIndex = 0 /* maybeCounterIndex */;

		/* begin saveForRecompile */
		savedFirstOpcodeIndex = firstOpcodeIndex;
		savedNextLiteralIndex = nextLiteralIndex;
		savedLastDumpedLiteralIndex = lastDumpedLiteralIndex;
		while (1) {
			compileBlockEntry(blockStart);
			initialStackPtr = simStackPtr;
			if (((result = compileAbstractInstructionsFromthrough(((blockStart->startpc)) + (pushNilSizeFunction(methodObj, ((blockStart->numInitialNils)))), (((blockStart->startpc)) + ((blockStart->span))) - 1))) < 0) {
				return result;
			}
			if (initialStackPtr == simStackPtr) break;
			assert((initialStackPtr > simStackPtr)
			 || (deadCode));

			/* for asserts */
			compilationPass += 1;
			(blockStart->numInitialNils = (((blockStart->numInitialNils)) + simStackPtr) - initialStackPtr);
			(((blockStart->fakeHeader))->dependent = null);
			reinitializeFixupsFromthrough(((blockStart->startpc)) + ((blockStart->numInitialNils)), (((blockStart->startpc)) + ((blockStart->span))) - 1);
			bzero(abstractOpcodes + initialOpcodeIndex,
									(opcodeIndex - initialOpcodeIndex) * sizeof(AbstractInstruction));
			opcodeIndex = initialOpcodeIndex;

			/* begin resetForRecompile */
			firstOpcodeIndex = savedFirstOpcodeIndex;
			nextLiteralIndex = savedNextLiteralIndex;
			lastDumpedLiteralIndex = savedLastDumpedLiteralIndex;
		}
		compiledBlocksCount += 1;
	}
	needsFrame = savedNeedsFrame;
	methodOrBlockNumArgs = savedNumArgs;
	methodOrBlockNumTemps = savedNumTemps;
	return 0;
}


/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. closure (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	Avoid use of SendNumArgsReg which is the flag determining whether
	context switch is allowed on stack-overflow. */
/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any, and to correctly
	initialize the explicitly nilled/pushed temp entries (they are /not/ of
	type constant nil). */

	/* StackToRegisterMappingCogit>>#compileBlockFrameBuild: */
static NoDbgRegParms void
compileBlockFrameBuild(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *cascade0;
    sqInt i;
    sqInt ign;

	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);

	/* PushR: */
	genoperand(PushR, LinkReg);

	/* PushR: */
	genoperand(PushR, FPReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	cascade0 = (blockStart->fakeHeader);
	addDependent(cascade0, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)((blockStart->fakeHeader))), genoperand(PushCw, ((sqInt)((blockStart->fakeHeader)))))));

	/* setLabelOffset: */
	((cascade0->operands))[1] = MFMethodFlagIsBlockFlag;

	/* begin genPushConstant: */
	/* begin PushCq: */
	/* begin gen:quickConstant: */
	anInstruction = genoperand(PushCq, nilObject());
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(nilObject()));
	}
	if ((blockStart->hasInstVarRef)) {

		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);

		/* begin genEnsureOopInRegNotForwarded:scratchReg:updatingSlot:in: */
		assert(0);

		/* genEnsureOopInRegNotForwarded:scratchReg:updatingMw:r: */

		/* MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, Arg0Reg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, Arg0Reg, ReceiverResultReg);
	}

	/* PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = 0; i < ((blockStart->numCopied)); i += 1) {
		genLoadSlotsourceRegdestReg(i + ClosureFirstCopiedValueIndex, ClassReg, TempReg);

		/* PushR: */
		genoperand(PushR, TempReg);
	}
	(blockStart->stackCheckLabel = compileStackOverflowCheck(1));
	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramefulMethod((blockStart->startpc));
	if (((blockStart->numInitialNils)) > 0) {
		if (((blockStart->numInitialNils)) > 1) {
			/* begin genMoveConstant:R: */
			/* begin MoveCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(MoveCqR, nilObject(), TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(nilObject()));
			}
			for (ign = 1; ign <= ((blockStart->numInitialNils)); ign += 1) {
				/* PushR: */
				genoperand(PushR, TempReg);
			}
		}
		else {
			/* begin genPushConstant: */
			/* begin PushCq: */
			/* begin gen:quickConstant: */
			anInstruction = genoperand(PushCq, nilObject());
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(nilObject()));
			}
		}
	}
}


/*	Make sure ReceiverResultReg holds the receiver, loaded from the closure,
	which is what is initially in ReceiverResultReg. We must annotate the
	first instruction in vanilla blocks so that
	findMethodForStartBcpc:inHomeMethod: can function. We need two annotations
	because the first is a fiducial. */
/*	Make sure ReceiverResultReg holds the receiver, loaded from
	the closure, which is what is initially in ReceiverResultReg */

	/* StackToRegisterMappingCogit>>#compileBlockFramelessEntry: */
static NoDbgRegParms void
compileBlockFramelessEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;

	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramelessBlock((blockStart->startpc));
	if ((blockStart->entryLabel)) {
		abstractInstruction = (blockStart->entryLabel);

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
	}
	if ((blockStart->hasInstVarRef)) {

		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);

		/* begin genEnsureOopInRegNotForwarded:scratchReg:updatingSlot:in: */
		assert(0);

		/* genEnsureOopInRegNotForwarded:scratchReg:updatingMw:r: */

		/* MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, TempReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, TempReg, ReceiverResultReg);
	}
}

	/* StackToRegisterMappingCogit>>#compileCogMethod: */
static NoDbgRegParms CogMethod *
compileCogMethod(sqInt selector)
{
    sqInt allocBytes;
    int extra;
    sqInt fixupBytes;
    sqInt methodFoundInvalidPostScanRV;
    sqInt numBlocks;
    sqInt numBytecodes;
    sqInt numCleanBlocks;
    sqInt opcodeBytes;
    sqInt result;

	methodOrBlockNumTemps = tempCountOf(methodObj);
	setHasMovableLiteral(0);
	setHasYoungReferent((isYoungObject(methodObj))
	 || (isYoung(selector)));
	methodOrBlockNumArgs = argumentCountOf(methodObj);
	inBlock = 0;
	maxLitIndex = -1;
	extra = ((((primitiveIndex = primitiveIndexOf(methodObj))) > 0)
		 && (!(isQuickPrimitiveIndex(primitiveIndex)))
				? 30
				: 10);

	/* initial estimate.  Actual endPC is determined in scanMethod. */
	initialPC = startPCOfMethod(methodObj);
	endPC = (isQuickPrimitiveIndex(primitiveIndex)
				? initialPC - 1
				: numBytesOf(methodObj));
	numBytecodes = (endPC - initialPC) + 1;

	/* begin allocateOpcodes:bytecodes:ifFail: */
	numAbstractOpcodes = (numBytecodes + extra) * 10 /* estimateOfAbstractOpcodesPerBytecodes */;
	opcodeBytes = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupBytes = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;

	/* Document the fact that the MaxStackAllocSize ensures that the number of abstract
	   opcodes fits in a 16 bit integer (e.g. CogBytecodeFixup's instructionIndex). */
	allocBytes = opcodeBytes + fixupBytes;
	assert((((sizeof(CogAbstractInstruction)) + (sizeof(CogBytecodeFixup))) * 0xC000) > MaxStackAllocSize);
	if (allocBytes > MaxStackAllocSize) {
		return ((CogMethod *) MethodTooBig);
		goto l1;
	}
	abstractOpcodes = alloca(allocBytes);
	bzero(abstractOpcodes, allocBytes);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeBytes));

	/* begin zeroOpcodeIndexForNewOpcodes */
	opcodeIndex = 0;
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	labelCounter = 0;
l1:	/* end allocateOpcodes:bytecodes:ifFail: */;
	if (((numBlocks = scanMethod())) < 0) {
		return ((CogMethod *) numBlocks);
	}
	numCleanBlocks = scanForCleanBlocks();

	/* begin methodFoundInvalidPostScan */
	if (!needsFrame) {
		methodFoundInvalidPostScanRV = methodOrBlockNumTemps > methodOrBlockNumArgs;
		goto l2;
	}
	methodFoundInvalidPostScanRV = 0;
l2:	/* end methodFoundInvalidPostScan */;
	if (methodFoundInvalidPostScanRV) {
		return ((CogMethod *) ShouldNotJIT);
	}
	allocateBlockStarts(numBlocks + numCleanBlocks);
	blockCount = 0;
	if (numCleanBlocks > 0) {
		addCleanBlockStarts();
	}
	blockEntryLabel = null;
	(methodLabel->dependent = null);
	if (((result = compileEntireMethod())) < 0) {
		return ((CogMethod *) result);
	}
	return generateCogMethod(selector);
}


/*	Compile the abstract instructions for the entire method, including blocks. */
/*	Compile the abstract instructions for the entire method, including blocks. */

	/* StackToRegisterMappingCogit>>#compileEntireMethod */
static sqInt
compileEntireMethod(void)
{
    sqInt result;

	regArgsHaveBeenPushed = 0;

	/* begin preenMethodLabel */
	/* setLabelOffset: */
	((methodLabel->operands))[1] = 0;
	compileAbort();
	compileEntry();
	if (((result = compilePrimitive())) < 0) {
		return result;
	}
	compileFrameBuild();
	if (((result = compileMethodBody())) < 0) {
		return result;
	}
	if (!blockCount) {
		return 0;
	}
	if (((result = compileBlockBodies())) < 0) {
		return result;
	}
	return compileBlockDispatch();
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any. */

	/* StackToRegisterMappingCogit>>#compileFrameBuild */
static void
compileFrameBuild(void)
{
    AbstractInstruction *anInstruction;
    sqInt i;
    sqInt toDoLimit;


#  if IMMUTABILITY
	if (useTwoPaths) {
		compileTwoPathFrameBuild();
		return;
	}
#  endif

	if (!needsFrame) {
		if (useTwoPaths) {
			compileTwoPathFramelessInit();
		}
		initSimStackForFramelessMethod(initialPC);
		return;
	}
	assert(!(useTwoPaths));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}

	/* PushR: */
	genoperand(PushR, LinkReg);

	/* PushR: */
	genoperand(PushR, FPReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperand(PushCw, ((sqInt)methodLabel)))));

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, nilObject(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(nilObject()));
	}

	/* PushR: */
	genoperand(PushR, SendNumArgsReg);

	/* PushR: */
	genoperand(PushR, ReceiverResultReg);
	toDoLimit = temporaryCountOfMethodHeader(methodHeader);
	for (i = (methodOrBlockNumArgs + 1); i <= toDoLimit; i += 1) {
		/* PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (/* methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	We are in a method where the frame is needed *only* for instance variable
	store, typically a setter method.
	This case has 20% overhead with Immutability compared to setter without
	immutability because of the stack
	frame creation. We compile two path, one where the object is immutable,
	one where it isn't. At the beginning 
	of the frame build, we take one path or the other depending on the
	receiver mutability.
	
	Note: this specific case happens only where there are only instance
	variabel stores. We could do something
	similar for literal variable stores, but we don't as it's too uncommon.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFrameBuild */
#if IMMUTABILITY
static void
compileTwoPathFrameBuild(void)
{
    AbstractInstruction *anInstruction;
    sqInt i;
    sqInt jumpImmutable;
    AbstractInstruction *jumpOld;
    sqInt toDoLimit;

	assert(useTwoPaths);
	assert(blockCount == 0);
	jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, TempReg);

	/* begin genJumpInOldSpace: */
	/* MoveAw:R: */
	/* gen:literal:operand: */
	checkLiteralforInstruction(youngStartAddress(), genoperandoperand(MoveAwR, youngStartAddress(), TempReg));
	assert(!(0 /* (TempReg = SPReg) */));
	genoperandoperand(CmpRR, TempReg, ReceiverResultReg);
	jumpOld = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	assert(!needsFrame);
	initSimStackForFramelessMethod(initialPC);

	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l1;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
l1:	/* end compileMethodBody */;

	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	needsFrame = 1;
	jmpTarget(jumpOld, jmpTarget(jumpImmutable, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}

	/* PushR: */
	genoperand(PushR, LinkReg);

	/* PushR: */
	genoperand(PushR, FPReg);

	/* MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(checkLiteralforInstruction(((sqInt)methodLabel), genoperand(PushCw, ((sqInt)methodLabel)))));

	/* begin genMoveConstant:R: */
	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, nilObject(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(nilObject()));
	}

	/* PushR: */
	genoperand(PushR, SendNumArgsReg);

	/* PushR: */
	genoperand(PushR, ReceiverResultReg);
	toDoLimit = temporaryCountOfMethodHeader(methodHeader);
	for (i = (methodOrBlockNumArgs + 1); i <= toDoLimit; i += 1) {
		/* PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (/* methodUsesPrimitiveErrorCode:header: */
		((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
}
#endif /* IMMUTABILITY */


/*	We are in a frameless method with at least two inst var stores. We compile
	two paths,
	one where the object is in new space, and one where it isn't. At the
	beginning 
	of the method, we take one path or the other depending on the receiver
	being in newSpace.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFramelessInit */
static void
compileTwoPathFramelessInit(void)
{
    AbstractInstruction *jumpOld;

	assert(!(IMMUTABILITY));
	assert(!(needsFrame));
	assert(useTwoPaths);

	/* begin genJumpInOldSpace: */
	/* MoveAw:R: */
	/* gen:literal:operand: */
	checkLiteralforInstruction(youngStartAddress(), genoperandoperand(MoveAwR, youngStartAddress(), TempReg));
	assert(!(0 /* (TempReg = SPReg) */));
	genoperandoperand(CmpRR, TempReg, ReceiverResultReg);
	jumpOld = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	initSimStackForFramelessMethod(initialPC);

	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l1;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
l1:	/* end compileMethodBody */;

	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	jmpTarget(jumpOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* StackToRegisterMappingCogit>>#cPICMissTrampolineFor: */
static NoDbgRegParms sqInt
cPICMissTrampolineFor(sqInt numArgs)
{
	return picMissTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}


/*	Replaces the Blue Book double-extended send [132], in which the first byte
	was wasted on 8 bits of argument count. 
	Here we use 3 bits for the operation sub-type (opType), and the remaining
	5 bits for argument count where needed. 
	The last byte give access to 256 instVars or literals. 
	See also secondExtendedSendBytecode
 */

	/* StackToRegisterMappingCogit>>#doubleExtendedDoAnythingBytecode */
static sqInt
doubleExtendedDoAnythingBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt opType;

	opType = ((usqInt)(byte1)) >> 5;
	if (!opType) {
		return genSendnumArgs(byte2, byte1 & 0x1F);
	}
	if (opType == 1) {
		return genSendSupernumArgs(byte2, byte1 & 0x1F);
	}
	switch (opType) {
	case 2:
		if (isReadMediatedContextInstVarIndex(byte2)) {
			genPushMaybeContextReceiverVariable(byte2);
		}
		else {
			genPushReceiverVariable(byte2);

			/* begin annotateInstructionForBytecode */
			abstractInstruction = (prevInstIsPCAnnotated()
						? gen(Nop)
						: genoperandoperand(Label, (labelCounter += 1), bytecodePC));

			/* begin annotateBytecode: */
			(abstractInstruction->annotation = HasBytecodePC);
			return 0;
		}
		break;
	case 3:
		genPushLiteralIndex(byte2);

		/* begin annotateInstructionForBytecode */
		abstractInstruction = (prevInstIsPCAnnotated()
					? gen(Nop)
					: genoperandoperand(Label, (labelCounter += 1), bytecodePC));

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		return 0;

	case 4:
		genPushLiteralVariable(byte2);
		break;
	case 7:
		/* genStorePop:LiteralVariable: */
		genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, byte2, /* ssTopNeedsStoreCheck */
			((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(((ssTop())->constant)))
		 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
#    if IMMUTABILITY

		/* genStorePop:LiteralVariable: annotates; don't annotate twice */
		return 0;
#    endif


	default:

		/* 5 & 6 */
		if (isWriteMediatedContextInstVarIndex(byte2)) {
			/* genStorePop:MaybeContextReceiverVariable: */
			genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, /* ssTopNeedsStoreCheck */
				((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (/* shouldAnnotateObjectReference: */
				(isNonIntegerObject(((ssTop())->constant)))
			 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
		}
		else {
			/* genStorePop:ReceiverVariable: */
			genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, /* ssTopNeedsStoreCheck */
				((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (/* shouldAnnotateObjectReference: */
				(isNonIntegerObject(((ssTop())->constant)))
			 && (oopisGreaterThan(((ssTop())->constant), trueObject())))), 1);
		}
#    if IMMUTABILITY

		/* genStorePop:...ReceiverVariable: annotate; don't annotate twice */
		return 0;
#    endif

;
	}
	assert(needsFrame);
	assert(!(prevInstIsPCAnnotated()));
	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#duplicateTopBytecode */
static sqInt
duplicateTopBytecode(void)
{
    SimStackEntry desc;

	/* begin ssTopDescriptor */
	desc = simStack[simStackPtr];
	return ssPushDesc(desc);
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureFixupAt: */
static NoDbgRegParms BytecodeFixup *
ensureFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;

	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if ((((usqInt)((fixup->targetInstruction)))) <= NeedsNonMergeFixupFlag) {

		/* convert a non-merge into a merge */
		/* begin becomeMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
		(fixup->simStackPtr = simStackPtr);
	}
	else {
		if ((fixup->isTargetOfBackwardBranch)) {

			/* this is the target of a backward branch and
			   so doesn't have a simStackPtr assigned yet. */
			(fixup->simStackPtr = simStackPtr);
		}
		else {
			assert(((fixup->simStackPtr)) == simStackPtr);
		}
	}
	return fixup;
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureNonMergeFixupAt: */
static NoDbgRegParms BytecodeFixup *
ensureNonMergeFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;

	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if (!((fixup->targetInstruction))) {
		/* begin becomeNonMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsNonMergeFixupFlag);
	}
	return fixup;
}

	/* StackToRegisterMappingCogit>>#ensureReceiverResultRegContainsSelf */
static void
ensureReceiverResultRegContainsSelf(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin ssAllocateRequiredReg: */
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
			storeToReg(simSelf(), ReceiverResultReg);
			((simSelf())->liveRegister = ReceiverResultReg);
		}
	}
	else {
		assert(((((simSelf())->type)) == SSRegister)
		 && (((((simSelf())->registerr)) == ReceiverResultReg)
		 && (receiverIsInReceiverResultReg())));
	}
}

	/* StackToRegisterMappingCogit>>#evaluate:at: */
static NoDbgRegParms void
evaluateat(BytecodeDescriptor *descriptor, sqInt pc)
{
	byte0 = fetchByteofObject(pc, methodObj);
	assert(descriptor == (generatorAt(bytecodeSetOffset + byte0)));
	loadSubsequentBytesForDescriptorat(descriptor, pc);
	((descriptor->generator))();
}


/*	Attempt to follow a branch to a pc. Handle branches to unconditional jumps
	and branches to push: aBoolean; conditional branch pairs. If the branch
	cannot be
	followed answer targetBytecodePC. It is not possible to follow jumps to
	conditional branches because the stack changes depth. That following is
	left to the genJumpIf:to:
	clients. */

	/* StackToRegisterMappingCogit>>#eventualTargetOf: */
static NoDbgRegParms sqInt
eventualTargetOf(sqInt targetBytecodePC)
{
    sqInt cond;
    sqInt currentTarget;
    BytecodeDescriptor *descriptor;
    sqInt nExts;
    sqInt nextPC;
    sqInt span;

	cond = 0;
	descriptor = ((BytecodeDescriptor *) 0);
	nextPC = (currentTarget = targetBytecodePC);
	while (1) {
		nExts = 0;
		while (1) {
			/* begin generatorForPC: */
			descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
			if ((descriptor->isReturn)) {
				return currentTarget;
			}
			if (!((descriptor->isExtension))) break;
			nExts += 1;
			nextPC += (descriptor->numBytes);
		}
		if (/* isUnconditionalBranch */
			(isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse))))) {
			span = ((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj);
			if (span < 0) {

				/* Do *not* follow backward branches; these are interrupt points and should not be elided. */
				return currentTarget;
			}
			nextPC = (nextPC + ((descriptor->numBytes))) + span;
		}
		else {
			if (((descriptor->generator)) == genPushConstantTrueBytecode) {
				cond = 1;
			}
			else {
				if (((descriptor->generator)) == genPushConstantFalseBytecode) {
					cond = 0;
				}
				else {
					return currentTarget;
				}
			}
			if (isBackwardBranchFixup(fixupAt(nextPC))) {
				return currentTarget;
			}
			nextPC = eventualTargetOf(nextPC + ((descriptor->numBytes)));
			nExts = 0;
			while (1) {
				/* begin generatorForPC: */
				descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
				if ((descriptor->isReturn)) {
					return currentTarget;
				}
				if (!((descriptor->isExtension))) break;
				nExts += 1;
				nextPC += (descriptor->numBytes);
			}
			if (!(isBranch(descriptor))) {
				return currentTarget;
			}
			if (/* isUnconditionalBranch */
				(isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse))))) {
				return currentTarget;
			}
			nextPC = (cond == ((descriptor->isBranchTrue))
						? (nextPC + ((descriptor->numBytes))) + (((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj))
						: nextPC + ((descriptor->numBytes)));
		}
		currentTarget = nextPC;
	}
	return 0;
}


/*	Spill the closest register on stack not conflicting with regMask. 
	Assertion Failure if regMask has already all the registers */

	/* StackToRegisterMappingCogit>>#freeAnyRegNotConflictingWith: */
static NoDbgRegParms sqInt
freeAnyRegNotConflictingWith(sqInt regMask)
{
    CogSimStackEntry *desc;
    sqInt index;
    sqInt reg;

	assert(needsFrame);
	reg = NoReg;
	index = ((simSpillBase < 0) ? 0 : simSpillBase);
	while ((reg == NoReg)
	 && (index < simStackPtr)) {
		desc = simStackAt(index);
		if (((desc->type)) == SSRegister) {
			if (!(((regMask & (((((desc->registerr)) < 0) ? (((usqInt)(1)) >> (-((desc->registerr)))) : (1U << ((desc->registerr)))))) != 0))) {
				reg = (desc->registerr);
			}
		}
		index += 1;
	}
	assert(!((reg == NoReg)));

	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1U << reg)), simStackPtr, simNativeStackPtr);
	return reg;
}


/*	Return from block, assuming result already loaded into ReceiverResultReg. */
/*	Return from block, assuming result already loaded into ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#genBlockReturn */
static sqInt
genBlockReturn(void)
{
	if (needsFrame) {
		/* MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);

		/* PopR: */
		genoperand(PopR, FPReg);

		/* PopR: */
		genoperand(PopR, LinkReg);
	}

	/* RetN: */
	genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);

	/* can't fall through */
	deadCode = 1;
	return 0;
}


/*	Generate special versions of the ceCallCogCodePopReceiverAndClassRegs
	enilopmart that also pop register args from the stack to undo the pushing
	of register args in the abort/miss trampolines. */

	/* StackToRegisterMappingCogit>>#genCallPICEnilopmartNumArgs: */
static NoDbgRegParms void
(*genCallPICEnilopmartNumArgs(sqInt numArgs))(void)
{
    AbstractInstruction *anInstruction;
    sqInt endAddress;
    usqInt enilopmart;
    sqInt quickConstant;
    sqInt size;

	zeroOpcodeIndex();
	quickConstant = varBaseAddress;

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	genLoadStackPointers(backEnd);

	/* PopR: */
	genoperand(PopR, ClassReg);

	/* PopR: */
	genoperand(PopR, TempReg);

	/* PopR: */
	genoperand(PopR, LinkReg);
	if (numArgs > 0) {
		if (numArgs > 1) {
			/* PopR: */
			genoperand(PopR, Arg1Reg);
			assert((numRegArgs()) == 2);
		}

		/* PopR: */
		genoperand(PopR, Arg0Reg);
	}

	/* PopR: */
	genoperand(PopR, ReceiverResultReg);

	/* JumpR: */
	genoperand(JumpR, TempReg);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineNamenumRegArgs("ceCallPIC", numArgs), enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	Override to push the register receiver and register arguments, if any. */

	/* StackToRegisterMappingCogit>>#genExternalizePointersForPrimitiveCall */
static sqInt
genExternalizePointersForPrimitiveCall(void)
{
	genPushRegisterArgs();

	/* #MoveR:Aw: #gen:operand:literal: */
	checkLiteralforInstruction(instructionPointerAddress(), genoperandoperand(MoveRAw, LinkReg, instructionPointerAddress()));
	return genSaveStackPointers(backEnd);
}


/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). 
 */
/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). Override to add version for generic and PIC-specific entry
	with reg args. */

	/* StackToRegisterMappingCogit>>#generateEnilopmarts */
static void
generateEnilopmarts(void)
{

#  if Debug
	/* begin genEnilopmartFor:forCall:called: */
	realCEEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "realCEEnterCogCodePopReceiverReg");
	ceEnterCogCodePopReceiverReg = enterCogCodePopReceiver;

	/* begin genEnilopmartFor:forCall:called: */
	realCECallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "realCECallCogCodePopReceiverReg");
	ceCallCogCodePopReceiverReg = callCogCodePopReceiver;

	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "realCECallCogCodePopReceiverAndClassRegs");
	ceCallCogCodePopReceiverAndClassRegs = callCogCodePopReceiverAndClassRegs;
#  else // Debug
	/* begin genEnilopmartFor:forCall:called: */
	ceEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "ceEnterCogCodePopReceiverReg");
	ceCallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "ceCallCogCodePopReceiverReg");
	ceCallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "ceCallCogCodePopReceiverAndClassRegs");
#  endif // Debug

	genPrimReturnEnterCogCodeEnilopmart(0);
	cePrimReturnEnterCogCode = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCode);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCode", cePrimReturnEnterCogCode);
	genPrimReturnEnterCogCodeEnilopmart(1);
	cePrimReturnEnterCogCodeProfiling = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCodeProfiling);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCodeProfiling", cePrimReturnEnterCogCodeProfiling);
#  if Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "realCECallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg0Regs = callCogCodePopReceiverArg0Regs;
	realCECallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "realCECallCogCodePopReceiverArg1Arg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = callCogCodePopReceiverArg1Arg0Regs;
#  else // Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	ceCallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "ceCallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "ceCallCogCodePopReceiverArg1Arg0Regs");
#  endif // Debug

	ceCall0ArgsPIC = genCallPICEnilopmartNumArgs(0);
	ceCall1ArgsPIC = genCallPICEnilopmartNumArgs(1);
}


/*	Size pc-dependent instructions and assign eventual addresses to all
	instructions. Answer the size of the code.
	Compute forward branches based on virtual address (abstract code starts at
	0), assuming that any branches branched over are long.
	Compute backward branches based on actual address.
	Reuse the fixups array to record the pc-dependent instructions that need
	to have
	their code generation postponed until after the others.
	
	Override to add handling for null branches (branches to the immediately
	following instruction) occasioned by StackToRegisterMapping's following of
	jumps.  */

	/* StackToRegisterMappingCogit>>#generateInstructionsAt: */
static NoDbgRegParms sqInt
generateInstructionsAt(sqInt eventualAbsoluteAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    BytecodeFixup *fixup;
    sqInt i;
    sqInt j;
    sqInt pcDependentIndex;

	absoluteAddress = eventualAbsoluteAddress;
	pcDependentIndex = 0;
	for (i = 0; i < opcodeIndex; i += 1) {
		/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
		   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
		abstractInstruction = abstractInstructionAt(i);
		maybeBreakGeneratingFromto(absoluteAddress, absoluteAddress + ((abstractInstruction->maxSize)));
		if (isPCDependent(abstractInstruction)) {
			sizePCDependentInstructionAt(abstractInstruction, absoluteAddress);
			if ((isJump(abstractInstruction))
			 && ((((i + 1) < opcodeIndex)
			 && ((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 1))))
			 || (((i + 2) < opcodeIndex)
			 && (((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 2)))
			 && ((((abstractInstructionAt(i + 1))->opcode)) == Nop))))) {
				(abstractInstruction->opcode = Nop);
				concretizeAt(abstractInstruction, absoluteAddress);
			}
			else {
				fixup = fixupAtIndex(pcDependentIndex);
				pcDependentIndex += 1;
				(fixup->instructionIndex = i);
			}
			absoluteAddress += (abstractInstruction->machineCodeSize);
		}
		else {
			/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
			   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
			absoluteAddress = concretizeAt(abstractInstruction, absoluteAddress);
			assert(((abstractInstruction->machineCodeSize)) == ((abstractInstruction->maxSize)));
		}
	}
	for (j = 0; j < pcDependentIndex; j += 1) {
		fixup = fixupAtIndex(j);
		abstractInstruction = abstractInstructionAt((fixup->instructionIndex));
		maybeBreakGeneratingFromto((abstractInstruction->address), (((abstractInstruction->address)) + ((abstractInstruction->maxSize))) - 1);
		concretizeAt(abstractInstruction, (abstractInstruction->address));
	}
	return absoluteAddress - eventualAbsoluteAddress;
}


/*	Generate the run-time entries for the various method and PIC entry misses
	and aborts.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* StackToRegisterMappingCogit>>#generateMissAbortTrampolines */
static void
generateMissAbortTrampolines(void)
{
    sqInt numArgs;
    sqInt toDoLimit;
    sqInt toDoLimit1;
    sqInt toDoLimit2;

	toDoLimit = (numRegArgs()) + 1;
	for (numArgs = 0; numArgs <= toDoLimit; numArgs += 1) {
		methodAbortTrampolines[numArgs] = (genMethodAbortTrampolineFor(numArgs));
	}
	toDoLimit1 = (numRegArgs()) + 1;
	for (numArgs = 0; numArgs <= toDoLimit1; numArgs += 1) {
		picAbortTrampolines[numArgs] = (genPICAbortTrampolineFor(numArgs));
	}
	toDoLimit2 = (numRegArgs()) + 1;
	for (numArgs = 0; numArgs <= toDoLimit2; numArgs += 1) {
		picMissTrampolines[numArgs] = (genPICMissTrampolineFor(numArgs));
	}
	ceReapAndResetErrorCodeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceReapAndResetErrorCodeFor, "ceReapAndResetErrorCodeTrampoline", 1, ClassReg, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 0);
}


/*	Override to generate code to push the register arg(s) for <= numRegArg
	arity sends.
 */

	/* StackToRegisterMappingCogit>>#generateSendTrampolines */
static void
generateSendTrampolines(void)
{
    sqInt numArgs;

	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		ordinarySendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(
	ceSendsupertonumArgs,
	numArgs,
	trampolineNamenumArgs("ceSend", numArgs),
	ClassReg,
	(/* begin trampolineArgConstant: */
		assert(0 >= 0),
	-2 - 0),
	ReceiverResultReg,
	/* numArgsOrSendNumArgsReg: */
		(numArgs <= (NumSendTrampolines - 2)
			? (/* begin trampolineArgConstant: */
				assert(numArgs >= 0),
			-2 - numArgs)
			: SendNumArgsReg)));
	}
	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		superSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(
	ceSendsupertonumArgs,
	numArgs,
	trampolineNamenumArgs("ceSuperSend", numArgs),
	ClassReg,
	(/* begin trampolineArgConstant: */
		assert(1 >= 0),
	-2 - 1),
	ReceiverResultReg,
	/* numArgsOrSendNumArgsReg: */
		(numArgs <= (NumSendTrampolines - 2)
			? (/* begin trampolineArgConstant: */
				assert(numArgs >= 0),
			-2 - numArgs)
			: SendNumArgsReg)));
	}
	firstSend = ordinarySendTrampolines[0];
	lastSend = superSendTrampolines[NumSendTrampolines - 1];
}


/*	Generate trampolines for tracing. In the simulator we can save a lot of
	time and avoid noise instructions in the lastNInstructions log by
	short-cutting these
	trampolines, but we need them in the real vm. */

	/* StackToRegisterMappingCogit>>#generateTracingTrampolines */
static void
generateTracingTrampolines(void)
{
	ceTraceLinkedSendTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceLinkedSend, "ceTraceLinkedSendTrampoline", 1, ReceiverResultReg, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceTraceBlockActivationTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceBlockActivation, "ceTraceBlockActivationTrampoline", 0, null, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceTraceStoreTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceStoreOfinto, "ceTraceStoreTrampoline", 2, TempReg, ReceiverResultReg, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
}


/*	Generates the machine code for #== in the case where the instruction is
	not followed by a branch
 */

	/* StackToRegisterMappingCogit>>#genIdenticalNoBranchArgIsConstant:rcvrIsConstant:argReg:rcvrReg:orNotIf: */
static NoDbgRegParms sqInt
genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot)
{
    AbstractInstruction *anInstruction;
    sqInt constant;
    AbstractInstruction *jumpEqual;
    AbstractInstruction *jumpNotEqual;
    AbstractInstruction *label;
    sqInt resultReg;

	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrRegOrNone != NoReg));
	if (argIsConstant) {
		constant = ((ssTop())->constant);

		/* begin genCmpConstant:R: */
		if (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(constant))
		 && (oopisGreaterThan(constant, trueObject()))) {
			annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, rcvrRegOrNone)), constant);
		}
		else {
			/* begin CmpCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(CmpCqR, constant, rcvrRegOrNone);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(constant));
			}
		}
	}
	else {
		if (rcvrIsConstant) {
			constant = ((ssValue(1))->constant);

			/* begin genCmpConstant:R: */
			if (/* shouldAnnotateObjectReference: */
				(isNonIntegerObject(constant))
			 && (oopisGreaterThan(constant, trueObject()))) {
				annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, argReg)), constant);
			}
			else {
				/* begin CmpCq:R: */
				/* begin gen:quickConstant:operand: */
				anInstruction = genoperandoperand(CmpCqR, constant, argReg);
				if (usesOutOfLineLiteral(anInstruction)) {
					(anInstruction->dependent = locateLiteral(constant));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrRegOrNone);
		}
	}
	ssPop(2);
	resultReg = (rcvrRegOrNone == NoReg
				? argReg
				: rcvrRegOrNone);
	jumpEqual = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	if (orNot) {
		/* begin genMoveConstant:R: */
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, trueObject(), resultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(trueObject()));
		}
	}
	else {
		/* begin genMoveConstant:R: */
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, falseObject(), resultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(falseObject()));
		}
	}
	jumpNotEqual = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpEqual, (orNot
			? genMoveConstantR(falseObject(), resultReg)
			: genMoveConstantR(trueObject(), resultReg)));
	jmpTarget(jumpNotEqual, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushRegister(resultReg);
	return 0;
}


/*	Decompose code generation for #== into a common constant-folding version,
	followed by a double dispatch through the objectRepresentation to a
	version that doesn't deal with forwarders and a version that does. */

	/* StackToRegisterMappingCogit>>#genInlinedIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genInlinedIdenticalOrNotIf(sqInt orNot)
{
    BytecodeDescriptor *primDescriptor;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	if ((isUnannotatableConstant(ssTop()))
	 && (isUnannotatableConstant(ssValue(1)))) {
		assert(!((primDescriptor->isMapped)));
		result = ((orNot
					? (((ssTop())->constant)) != (((ssValue(1))->constant))
					: (((ssTop())->constant)) == (((ssValue(1))->constant)))
					? trueObject()
					: falseObject());
		ssPop(2);
		return ssPushConstant(result);
	}
	return genVanillaInlinedIdenticalOrNotIf(orNot);
}

	/* StackToRegisterMappingCogit>>#genJumpBackTo: */
static NoDbgRegParms sqInt
genJumpBackTo(sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    sqInt i;
    void *jumpTarget;

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= simStackPtr) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = simStackPtr + 1;
	}

	/* can't fall through */
	deadCode = 1;

	/* #MoveAw:R: #gen:literal:operand: */
	checkLiteralforInstruction(stackLimitAddress(), genoperandoperand(MoveAwR, stackLimitAddress(), TempReg));

	/* begin CmpR:R: */
	assert(!(0 /* (TempReg = SPReg) */));
	genoperandoperand(CmpRR, TempReg, SPReg);

	/* begin fixupAt: */
	jumpTarget = fixupAtIndex(targetBytecodePC - initialPC);
	genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)jumpTarget));
	abstractInstruction = genoperand(Call, ceCheckForInterruptTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);

	/* begin fixupAt: */
	jumpTarget = fixupAtIndex(targetBytecodePC - initialPC);
	genoperand(Jump, ((sqInt)jumpTarget));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genJumpIf:to: */
static NoDbgRegParms sqInt
genJumpIfto(sqInt boolean, sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    CogSimStackEntry *desc;
    sqInt eventualTarget;
    BytecodeFixup *fixup;
    sqInt i;
    void *jumpTarget;
    AbstractInstruction *ok;
    sqInt quickConstant;

	eventualTarget = eventualTargetOf(targetBytecodePC);

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= (simStackPtr - 1)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 1)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 1))); i < simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 1) + 1;
	}
	desc = ssTop();
	ssPop(1);
	if (/* stackEntryIsBoolean: */
		(((desc->type)) == SSConstant)
	 && ((((desc->constant)) == (trueObject()))
	 || (((desc->constant)) == (falseObject())))) {

		/* Must arrange there's a fixup at the target whether it is jumped to or
		   not so that the simStackPtr can be kept correct. */
		/* Must annotate the bytecode for correct pc mapping. */
		fixup = ensureFixupAt(eventualTarget);
		abstractInstruction = (((desc->constant)) == boolean
					? genoperand(Jump, ((sqInt)fixup))
					: (prevInstIsPCAnnotated()
							? gen(Nop)
							: genoperandoperand(Label, (labelCounter += 1), bytecodePC)));

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		extA = 0;
		return 0;
	}
	popToReg(desc, TempReg);
	assert((objectAfter(falseObject())) == (trueObject()));

	/* begin genSubConstant:R: */
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(boolean))
	 && (oopisGreaterThan(boolean, trueObject()))) {
		annotateobjRef(checkLiteralforInstruction(boolean, genoperandoperand(SubCwR, boolean, TempReg)), TempReg);
	}
	else {
		/* begin SubCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(SubCqR, boolean, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(boolean));
		}
	}
	jumpTarget = ensureFixupAt(eventualTarget);

	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
	if (((extA & 1) != 0)) {
		extA = 0;
		abstractInstruction = lastOpcode();

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		return 0;
	}
	extA = 0;
	quickConstant = (boolean == (falseObject())
				? (trueObject()) - (falseObject())
				: (falseObject()) - (trueObject()));

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	ok = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genCallMustBeBooleanFor(boolean);
	jmpTarget(ok, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genJumpTo: */
static NoDbgRegParms sqInt
genJumpTo(sqInt targetBytecodePC)
{
    sqInt eventualTarget;
    BytecodeFixup *fixup;
    BytecodeDescriptor *generator;
    sqInt i;

	eventualTarget = eventualTargetOf(targetBytecodePC);
	if ((eventualTarget > bytecodePC)
	 && ((/* stackTopIsBoolean */
		(simStackPtr >= methodOrBlockNumArgs)
	 && (stackEntryIsBoolean(ssTop())))
	 && (isConditionalBranch(generator = generatorForPC(eventualTarget))))) {
		eventualTarget = (eventualTarget + ((generator->numBytes))) + ((((generator->isBranchTrue)) == ((((ssTop())->constant)) == (trueObject()))
		? ((generator->spanFunction))(generator, eventualTarget, 0, methodObj)
		: 0));
		ssPop(1);

		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
		ssPop(-1);
	}
	else {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
	}

	/* can't fall through */
	deadCode = 1;

	/* Jump: */
	genoperand(Jump, ((sqInt)fixup));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genMarshalledSend:numArgs:sendTable: */
static NoDbgRegParms sqInt
genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable)
{
    AbstractInstruction *anInstruction;
    sqInt annotation;

	assert(needsFrame);

	/* begin annotationForSendTable: */
	if (sendTable == ordinarySendTrampolines) {
		annotation = IsSendCall;
		goto l1;
	}
	assert(sendTable == superSendTrampolines);
	annotation = IsSuperSend;
l1:	/* end annotationForSendTable: */;
	if (/* annotationIsForUncheckedEntryPoint: */
		(annotation == IsSuperSend)) {
		/* genEnsureOopInRegNotForwarded:scratchReg: */
	}
	if (numArgs >= (NumSendTrampolines - 1)) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, numArgs, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(numArgs));
		}
	}
	genLoadInlineCacheWithSelector(selectorIndex);
	((genoperand(Call, sendTable[((numArgs < (NumSendTrampolines - 1)) ? numArgs : (NumSendTrampolines - 1))]))->annotation = annotation);

	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	return ssPushRegister(ReceiverResultReg);
}


/*	Generate the abort for a method. This abort performs either a call of
	ceSICMiss: to handle a single-in-line cache miss or a call of
	ceStackOverflow: to handle a
	stack overflow. It distinguishes the two by testing ResultReceiverReg. If
	the register is zero then this is a stack-overflow because a) the receiver
	has already
	been pushed and so can be set to zero before calling the abort, and b) the
	receiver must always contain an object (and hence be non-zero) on SIC
	miss.  */

	/* StackToRegisterMappingCogit>>#genMethodAbortTrampolineFor: */
static NoDbgRegParms usqInt
genMethodAbortTrampolineFor(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSICMiss;

	zeroOpcodeIndex();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* The abort sequence has pushed the LinkReg a second time - because a stack
	   overflow can only happen after building a frame, which pushes LinkReg anyway, and
	   we still need to push LinkReg in case we get to this routine from a sendMissAbort.
	   (On ARM there is a simpler way; use two separate abort calls since all instructions are 32-bits
	   but on x86 the zero receiver reg, call methodAbort sequence is smaller; we may fix this one day).
	   Overwrite that duplicate with the right one - the return address for the call to the abort trampoline.
	   The only reason it matters is an assert in ceStackOverflow: uses it */
	jumpSICMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* begin MoveR:Mw:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRMwr, LinkReg, 0, SPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceStackOverflow, 1, SendNumArgsReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpSICMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSICMiss, trampolineNamenumRegArgs("ceMethodAbort", numArgs), 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged
	target or a call of ceMNUFromPICMNUMethod:receiver: to handle an
	MNU dispatch in a closed PIC. It distinguishes the two by testing
	ClassReg. If the register is zero then this is an MNU. */

	/* StackToRegisterMappingCogit>>#genPICAbortTrampolineFor: */
static NoDbgRegParms usqInt
genPICAbortTrampolineFor(sqInt numArgs)
{
	zeroOpcodeIndex();
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genInnerPICAbortTrampoline(trampolineNamenumRegArgs("cePICAbort", numArgs));
}

	/* StackToRegisterMappingCogit>>#genPICMissTrampolineFor: */
static NoDbgRegParms usqInt
genPICMissTrampolineFor(sqInt numArgs)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCPICMissreceiver, trampolineNamenumRegArgs("cePICMiss", numArgs), 2, ClassReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genPopStackBytecode */
static sqInt
genPopStackBytecode(void)
{
    AbstractInstruction *anInstruction;

	if (((ssTop())->spilled)) {
		/* begin AddCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(AddCqR, BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(BytesPerWord));
		}
	}
	ssPop(1);
	return 0;
}


/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive. */
/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive.
	Override to push the register args first. */

	/* StackToRegisterMappingCogit>>#genPrimitiveClosureValue */
static sqInt
genPrimitiveClosureValue(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpFail1;
    AbstractInstruction *jumpFail2;
    AbstractInstruction *jumpFail3;
    AbstractInstruction *jumpFail4;
    AbstractInstruction *jumpFailNArgs;
    sqInt offset;
    void (*primitiveRoutine)(void);
    sqInt quickConstant;
    sqInt result;

	genPushRegisterArgs();
	genLoadSlotsourceRegdestReg(ClosureNumArgsIndex, ReceiverResultReg, TempReg);

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, (((usqInt)methodOrBlockNumArgs << 1) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral((((usqInt)methodOrBlockNumArgs << 1) | 1)));
	}
	jumpFailNArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ClassReg);
	jumpFail1 = genJumpImmediate(ClassReg);
	genGetCompactClassIndexNonImmOfinto(ClassReg, TempReg);
	genCmpClassMethodContextCompactIndexR(TempReg);

	/* We defer unforwarding the receiver to the prologue; scanning blocks
	   for inst var refs and only unforwarding if the block refers to inst vars. */
	jumpFail2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(MethodIndex, ClassReg, SendNumArgsReg);
	jumpFail3 = genJumpImmediate(SendNumArgsReg);

	/* begin genGetFormatOf:into: */
	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, SendNumArgsReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(0));
	}

	/* LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, instFormatFieldLSB(), TempReg);
	quickConstant = (1U << (instFormatFieldWidth())) - 1;

	/* begin AndCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	quickConstant = firstCompiledMethodFormat();

	/* begin CmpCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(quickConstant));
	}
	jumpFail4 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	offset = offsetof(CogMethod, blockEntryOffset);

	/* begin MoveM16:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveM16rR, offset, ClassReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* AddR:R: */
	genoperandoperand(AddRR, ClassReg, TempReg);
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, null);
	if (primitiveRoutine == primitiveClosureValueNoContextSwitch) {
		if (!blockNoContextSwitchOffset) {
			return NotFullyInitialized;
		}

		/* begin SubCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(SubCqR, blockNoContextSwitchOffset, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(blockNoContextSwitchOffset));
		}
	}

	/* JumpR: */
	genoperand(JumpR, TempReg);
	jmpTarget(jumpBCMethod, jmpTarget(jumpFail1, jmpTarget(jumpFail2, jmpTarget(jumpFail3, jmpTarget(jumpFail4, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	if (((result = compileInterpreterPrimitiveflags(primitiveRoutine, primitivePropertyFlagsnumArgs(primitiveIndex, methodOrBlockNumArgs)))) < 0) {
		return result;
	}
	jmpTarget(jumpFailNArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Generate an in-line perform primitive. The lookup code requires the
	selector to be in Arg0Reg.
	adjustArgumentsForPerform: adjusts the arguments once
	genLookupForPerformNumArgs: has generated the code for the lookup. */

	/* StackToRegisterMappingCogit>>#genPrimitivePerform */
static sqInt
genPrimitivePerform(void)
{
    AbstractInstruction *anInstruction;
    sqInt offset;

	if (methodOrBlockNumArgs > (numRegArgs())) {
		offset = (methodOrBlockNumArgs - 1) * BytesPerWord;

		/* begin MoveMw:r:R: */
		/* begin gen:quickConstant:operand:operand: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, SPReg, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(offset));
		}
	}
	return genLookupForPerformNumArgs(methodOrBlockNumArgs);
}

	/* StackToRegisterMappingCogit>>#genPushActiveContextBytecode */
static sqInt
genPushActiveContextBytecode(void)
{
	assert(needsFrame);
	voidReceiverResultRegContainsSelf();

	/* begin ssAllocateCallReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | ((1U << ReceiverResultReg)), simStackPtr, simNativeStackPtr);
	genGetActiveContextNumArgslargeinBlock(methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	return ssPushRegister(ReceiverResultReg);
}


/*	Block compilation. At this point in the method create the block. Note its
	start and defer generating code for it until after the method and any
	other preceding
	blocks. The block's actual code will be compiled later. */
/*	143 10001111 llllkkkk jjjjjjjj iiiiiiii	Push Closure Num Copied llll Num
	Args kkkk BlockSize jjjjjjjjiiiiiiii */

	/* StackToRegisterMappingCogit>>#genPushClosureCopyCopiedValuesBytecode */
static sqInt
genPushClosureCopyCopiedValuesBytecode(void)
{
    sqInt i;
    usqInt numArgs;
    sqInt numCopied;
    sqInt startpc;

	assert(needsFrame);
	startpc = bytecodePC + (((generatorAt(byte0))->numBytes));
	addBlockStartAtnumArgsnumCopiedspan(startpc, (numArgs = byte1 & 15), (numCopied = ((usqInt)(byte1)) >> 4), ((((usqInt)(byte2) << 8))) + byte3);

	/* begin genOutlineClosure:numArgs:numCopied: */
	if (numCopied > 0) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
	}
	voidReceiverResultRegContainsSelf();

	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << SendNumArgsReg)) | ((1U << ReceiverResultReg))), simStackPtr, simNativeStackPtr);
	genCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(startpc + 1, numArgs, numCopied, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	if (numCopied > 0) {
		ssPop(numCopied);
	}
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	<SmallInteger> */
/*	Override to avoid the BytecodeSetHasDirectedSuperSend check, which is
	unnecessary here given the simulation stack. */

	/* StackToRegisterMappingCogit>>#genPushLiteralIndex: */
static NoDbgRegParms sqInt
genPushLiteralIndex(sqInt literalIndex)
{
    sqInt literal;

	literal = getLiteral(literalIndex);
	return ssPushConstant(literal);
}

	/* StackToRegisterMappingCogit>>#genPushLiteralVariable: */
static NoDbgRegParms sqInt
genPushLiteralVariable(sqInt literalIndex)
{
    AbstractInstruction *anInstruction;
    sqInt association;
    sqInt freeReg;

	/* If followed by a directed super send bytecode, avoid generating any code yet.
	   The association will be passed to the directed send trampoline in a register
	   and fully dereferenced only when first linked.  It will be ignored in later sends. */
	association = getLiteral(literalIndex);

	/* N.B. Do _not_ use ReceiverResultReg to avoid overwriting receiver in assignment in frameless methods. */
	/* So far descriptors are not rich enough to describe the entire dereference so generate the register
	   load but don't push the result.  There is an order-of-evaluation issue if we defer the dereference. */
	freeReg = allocateRegNotConflictingWith(0);

	/* begin genMoveConstant:R: */
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(association))
	 && (oopisGreaterThan(association, trueObject()))) {
		annotateobjRef(checkLiteralforInstruction(association, genoperandoperand(MoveCwR, association, TempReg)), association);
	}
	else {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, association, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(association));
		}
	}
	genLoadSlotsourceRegdestReg(ValueIndex, TempReg, freeReg);
	ssPushRegister(freeReg);
	return 0;
}

	/* StackToRegisterMappingCogit>>#genPushMaybeContextReceiverVariable: */
static NoDbgRegParms sqInt
genPushMaybeContextReceiverVariable(sqInt slotIndex)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpDone;
    AbstractInstruction *jmpSingle;

	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ensureReceiverResultRegContainsSelf();

	/* begin genPushMaybeContextSlotIndex: */
	assert(needsFrame);
	if (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {

		/* We have no way of reloading ReceiverResultReg since we need the inst var value as the result. */
		voidReceiverResultRegContainsSelf();
	}
	if (slotIndex == InstructionPointerIndex) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(slotIndex));
		}

		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceFetchContextInstVarTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		return ssPushRegister(SendNumArgsReg);
	}
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	jmpSingle = genJumpNotSmallIntegerInScratchReg(TempReg);

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(slotIndex));
	}

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceFetchContextInstVarTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genLoadSlotsourceRegdestReg(slotIndex, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jmpDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return ssPushRegister(SendNumArgsReg);
}

	/* StackToRegisterMappingCogit>>#genPushNewArrayBytecode */
static sqInt
genPushNewArrayBytecode(void)
{
    sqInt i;
    sqInt iSqInt;
    int popValues;
    usqInt size;

	assert(needsFrame);
	voidReceiverResultRegContainsSelf();
	if ((popValues = byte1 > 0x7F)) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (iSqInt = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); iSqInt <= simStackPtr; iSqInt += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(iSqInt), frameOffsetOfTemporary(iSqInt - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
	}
	else {
		/* begin ssAllocateCallReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << SendNumArgsReg)) | ((1U << ReceiverResultReg))), simStackPtr, simNativeStackPtr);
	}
	size = byte1 & 0x7F;
	if (!popValues) {
		if (tryCollapseTempVectorInitializationOfSize(size)) {
			return 0;
		}
	}
	genNewArrayOfSizeinitialized(size, !popValues);
	if (popValues) {
		for (i = (size - 1); i >= 0; i += -1) {
			/* PopR: */
			genoperand(PopR, TempReg);
			genStoreSourceRegslotIndexintoNewObjectInDestReg(TempReg, i, ReceiverResultReg);
		}
		ssPop(size);
	}
	return ssPushRegister(ReceiverResultReg);
}

	/* StackToRegisterMappingCogit>>#genPushReceiverBytecode */
static sqInt
genPushReceiverBytecode(void)
{
	if ((((simSelf())->liveRegister)) == ReceiverResultReg) {
		return ssPushRegister(ReceiverResultReg);
	}
	return ssPushDesc(ssSelfDescriptor());
}

	/* StackToRegisterMappingCogit>>#genPushReceiverVariable: */
static NoDbgRegParms sqInt
genPushReceiverVariable(sqInt index)
{
	ensureReceiverResultRegContainsSelf();
	return ssPushBaseoffset(ReceiverResultReg, slotOffsetOfInstVarIndex(index));
}


/*	Ensure that the register args are pushed before the retpc for methods with
	arity <= self numRegArgs.
 */
/*	This isn't as clumsy on a RISC. But putting the receiver and
	args above the return address means the CoInterpreter has a
	single machine-code frame format which saves us a lot of work. */

	/* StackToRegisterMappingCogit>>#genPushRegisterArgs */
static void
genPushRegisterArgs(void)
{
	if (!(regArgsHaveBeenPushed
		 || (methodOrBlockNumArgs > (numRegArgs())))) {
		genPushRegisterArgsForNumArgsscratchReg(backEnd, methodOrBlockNumArgs, SendNumArgsReg);
		regArgsHaveBeenPushed = 1;
	}
}

	/* StackToRegisterMappingCogit>>#genPushRemoteTempLongBytecode */
static sqInt
genPushRemoteTempLongBytecode(void)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt remoteTempReg;
    sqInt tempVectReg;

	tempVectReg = allocateRegNotConflictingWith(0);
	offset = frameOffsetOfTemporary(byte2);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, FPReg, tempVectReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	remoteTempReg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | (((tempVectReg < 0) ? (((usqInt)(1)) >> (-tempVectReg)) : (1U << tempVectReg))));
	if (remoteTempReg == NoReg) {
		remoteTempReg = tempVectReg;
	}
	genLoadSlotsourceRegdestReg(byte1, tempVectReg, remoteTempReg);
	return ssPushRegister(remoteTempReg);
}


/*	If a frameless method (not a block), only argument temps can be accessed.
	This is assured by the use of needsFrameIfMod16GENumArgs: in pushTemp. */

	/* StackToRegisterMappingCogit>>#genPushTemporaryVariable: */
static NoDbgRegParms sqInt
genPushTemporaryVariable(sqInt index)
{
	assert((inBlock > 0)
	 || (needsFrame
	 || (index < methodOrBlockNumArgs)));
	return ssPushDesc(simStack[index + 1]);
}


/*	In a frameless method ReceiverResultReg already contains self.
	In a frameful method, ReceiverResultReg /may/ contain self. */

	/* StackToRegisterMappingCogit>>#genReturnReceiver */
static sqInt
genReturnReceiver(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin putSelfInReceiverResultReg */
			storeToReg(simSelf(), ReceiverResultReg);
		}
	}
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromBlock */
static sqInt
genReturnTopFromBlock(void)
{
	assert(inBlock > 0);
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genBlockReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromMethod */
static sqInt
genReturnTopFromMethod(void)
{
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genSendSuper:numArgs: */
static NoDbgRegParms sqInt
genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, superSendTrampolines);
}


/*	Generate a trampoline with four arguments.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* StackToRegisterMappingCogit>>#genSendTrampolineFor:numArgs:called:arg:arg:arg:arg: */
static NoDbgRegParms usqInt
genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3)
{
    sqInt routine;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	routine = null /* selectorIndexDereferenceRoutine */;
	if (routine) {

		/* Explicitly save LinkReg via ExtraReg2; it's presumably faster than pushing/popping */
		/* MoveR:R: */
		genoperandoperand(MoveRR, LinkReg, Extra2Reg);

		/* Call: */
		genoperand(Call, routine);

		/* MoveR:R: */
		genoperandoperand(MoveRR, Extra2Reg, LinkReg);
	}
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(aRoutine, aString, 4, regOrConst0, regOrConst1, regOrConst2, regOrConst3, 0 /* emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genSend:numArgs: */
static NoDbgRegParms sqInt
genSendnumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, ordinarySendTrampolines);
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorArithmetic */
static sqInt
genSpecialSelectorArithmetic(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    sqInt argInt;
    int argIsConst;
    sqInt argIsInt;
    sqInt i;
    sqInt index;
    AbstractInstruction *jumpContinue;
    AbstractInstruction *jumpNotSmallInts;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	argIsInt = ((argIsConst = (((ssTop())->type)) == SSConstant))
		 && ((((argInt = ((ssTop())->constant))) & 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
		 && ((((rcvrInt = ((ssValue(1))->constant))) & 1)))
		 || ((mclassIsSmallInteger())
		 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsInt
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		rcvrInt = (rcvrInt >> 1);
		argInt = (argInt >> 1);
		switch ((primDescriptor->opcode)) {
		case AddRR:
			result = rcvrInt + argInt;
			break;
		case SubRR:
			result = rcvrInt - argInt;
			break;
		case AndRR:
			result = rcvrInt & argInt;
			break;
		case OrRR:
			result = rcvrInt | argInt;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
		if (isIntegerValue(result)) {

			/* Must annotate the bytecode for correct pc mapping. */
			ssPop(2);
			return ssPushAnnotatedConstant((((usqInt)result << 1) | 1));
		}
		return genSpecialSelectorSend();
	}
	if ((rcvrIsConst
	 && (!rcvrIsInt))
	 || (argIsConst
	 && (!argIsInt))) {
		return genSpecialSelectorSend();
	}
	if (!(argIsInt
		 || (rcvrIsInt))) {
		return genSpecialSelectorSend();
	}
	if (argIsInt) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= (simStackPtr - 2)) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = (simStackPtr - 2) + 1;
		}
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsInt))
				? (argIsInt
						? genJumpNotSmallInteger(ReceiverResultReg)
						: (rcvrIsInt
								? genJumpNotSmallInteger(Arg0Reg)
								: genJumpNotSmallIntegersInandscratch(ReceiverResultReg, Arg0Reg, TempReg)))
				: 0);
	switch ((primDescriptor->opcode)) {
	case AddRR:
		if (argIsInt) {
			/* begin AddCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt - ConstZero));
			}

			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));

			/* begin SubCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt - ConstZero));
			}
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);

			/* AddR:R: */
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);

			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			if (rcvrIsInt
			 && (rcvrIsConst)) {
				/* begin MoveCq:R: */
				/* begin gen:quickConstant:operand: */
				anInstruction = genoperandoperand(MoveCqR, rcvrInt, ReceiverResultReg);
				if (usesOutOfLineLiteral(anInstruction)) {
					(anInstruction->dependent = locateLiteral(rcvrInt));
				}
			}
			else {
				/* SubR:R: */
				genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);
				genSetSmallIntegerTagsIn(ReceiverResultReg);
			}
		}
		break;
	case SubRR:
		if (argIsInt) {
			/* begin SubCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt - ConstZero));
			}

			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));

			/* begin AddCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt - ConstZero));
			}
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);

			/* SubR:R: */
			genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);

			/* overflow; must undo the damage before continuing */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));

			/* AddR:R: */
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
			genSetSmallIntegerTagsIn(Arg0Reg);
		}
		break;
	case AndRR:
		if (argIsInt) {
			/* begin AndCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(AndCqR, argInt, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt));
			}
		}
		else {
			/* AndR:R: */
			genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (jumpNotSmallInts
					? genoperand(Jump, ((sqInt)0))
					: 0);
		break;
	case OrRR:
		if (argIsInt) {
			/* begin OrCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(OrCqR, argInt, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(argInt));
			}
		}
		else {
			/* OrR:R: */
			genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (jumpNotSmallInts
					? genoperand(Jump, ((sqInt)0))
					: 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	if (jumpNotSmallInts) {
		jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	else {
		if (!jumpContinue) {

			/* overflow cannot happen */
			/* begin annotateInstructionForBytecode */
			abstractInstruction = (prevInstIsPCAnnotated()
						? gen(Nop)
						: genoperandoperand(Label, (labelCounter += 1), bytecodePC));

			/* begin annotateBytecode: */
			(abstractInstruction->annotation = HasBytecodePC);
			ssPushRegister(ReceiverResultReg);
			return 0;
		}
	}
	if (argIsInt) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, argInt, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(argInt));
		}
	}
	index = byte0 - FirstSpecialSelector;
	genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
	jmpTarget(jumpContinue, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorClass */
static sqInt
genSpecialSelectorClass(void)
{
    sqInt requiredReg1;
    sqInt topReg;

	topReg = registerOrNone(ssTop());
	ssPop(1);
	if ((topReg == NoReg)
	 || (topReg == ClassReg)) {
		requiredReg1 = (topReg = SendNumArgsReg);

		/* begin ssAllocateRequiredReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((((requiredReg1 < 0) ? (((usqInt)(1)) >> (-requiredReg1)) : (1U << requiredReg1))) | ((1U << ClassReg)), simStackPtr, simNativeStackPtr);
	}
	else {
		/* begin ssAllocateRequiredReg: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr, simNativeStackPtr);
	}
	ssPush(1);
	popToReg(ssTop(), topReg);
	genGetClassObjectOfintoscratchRegmayBeAForwarder(topReg, ClassReg, TempReg, mayBeAForwarder(ssTop()));
	ssPop(1);
	return ssPushRegister(ClassReg);
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorComparison */
static sqInt
genSpecialSelectorComparison(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    sqInt argInt;
    sqInt argIsIntConst;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt i;
    sqInt index;
    sqInt inlineCAB;
    AbstractInstruction *jumpNotSmallInts;
    void *jumpTarget;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPCSqInt;
    sqInt postBranchPC;
    sqInt postBranchPCSqInt;
    BytecodeDescriptor *primDescriptor;
    BytecodeDescriptor *primDescriptor1;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt targetBytecodePC;
    sqInt targetPC;

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= (simStackPtr - 2)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 2) + 1;
	}
	primDescriptor = generatorAt(byte0);
	argIsIntConst = ((((ssTop())->type)) == SSConstant)
		 && ((((argInt = ((ssTop())->constant))) & 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
		 && (((((ssValue(1))->constant)) & 1)))
		 || ((mclassIsSmallInteger())
		 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsIntConst
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		return genStaticallyResolvedSpecialSelectorComparison();
	}

	/* begin extractMaybeBranchDescriptorInto: */
	branchDescriptor1 = ((BytecodeDescriptor *) 0);
	primDescriptor1 = generatorAt(byte0);
	nextPCSqInt = bytecodePC + ((primDescriptor1->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPCSqInt, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPCSqInt += (branchDescriptor1->numBytes);
		}
		if (!(/* isUnconditionalBranch */
			(isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPCSqInt = eventualTargetOf((nextPCSqInt + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPCSqInt, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPCSqInt = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPCSqInt + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPCSqInt, nExts, methodObj)));
		postBranchPCSqInt = eventualTargetOf(nextPCSqInt + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPCSqInt = bytecodePC + ((primDescriptor1->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPCSqInt;
	postBranchPC = postBranchPCSqInt;
	targetPC = targetBytecodePC;

	/* Further, only interested in inlining = and ~= if there's a SmallInteger constant involved.
	   The relational operators successfully statically predict SmallIntegers; the equality operators do not. */
	inlineCAB = ((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse));
	if (inlineCAB
	 && ((((primDescriptor->opcode)) == JumpZero)
	 || (((primDescriptor->opcode)) == JumpNonZero))) {
		inlineCAB = argIsIntConst
			 || (rcvrIsInt);
	}
	if (!inlineCAB) {
		return genSpecialSelectorSend();
	}
	if (argIsIntConst) {
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsIntConst))
				? (argIsIntConst
						? genJumpNotSmallInteger(ReceiverResultReg)
						: (rcvrIsInt
								? genJumpNotSmallInteger(Arg0Reg)
								: genJumpNotSmallIntegersInandscratch(ReceiverResultReg, Arg0Reg, TempReg)))
				: 0);
	if (argIsIntConst) {
		/* begin CmpCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(CmpCqR, argInt, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(argInt));
		}
	}
	else {
		/* begin CmpR:R: */
		assert(!(0 /* (Arg0Reg = SPReg) */));
		genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	}
	genConditionalBranchoperand(((branchDescriptor->isBranchTrue)
			? (primDescriptor->opcode)
			: inverseBranchFor((primDescriptor->opcode))), ((usqInt)(ensureNonMergeFixupAt(targetPC))));
	jumpTarget = ensureNonMergeFixupAt(postBranchPC);

	/* begin Jump: */
	genoperand(Jump, ((sqInt)jumpTarget));
	if (!jumpNotSmallInts) {
		/* begin annotateInstructionForBytecode */
		abstractInstruction = (prevInstIsPCAnnotated()
					? gen(Nop)
					: genoperandoperand(Label, (labelCounter += 1), bytecodePC));

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		ensureFixupAt(postBranchPC);
		ensureFixupAt(targetPC);
		deadCode = 1;
		return 0;
	}
	jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (argIsIntConst) {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, argInt, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(argInt));
		}
	}
	index = byte0 - FirstSpecialSelector;
	return genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
}


/*	Assumes both operands are ints */

	/* StackToRegisterMappingCogit>>#genStaticallyResolvedSpecialSelectorComparison */
static sqInt
genStaticallyResolvedSpecialSelectorComparison(void)
{
    sqInt argInt;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int result;

	primDescriptor = generatorAt(byte0);
	argInt = ((ssTop())->constant);
	rcvrInt = ((ssValue(1))->constant);
	switch ((primDescriptor->opcode)) {
	case JumpLess:
		result = rcvrInt < argInt;
		break;
	case JumpLessOrEqual:
		result = rcvrInt <= argInt;
		break;
	case JumpGreater:
		result = rcvrInt > argInt;
		break;
	case JumpGreaterOrEqual:
		result = rcvrInt >= argInt;
		break;
	case JumpZero:
		result = rcvrInt == argInt;
		break;
	case JumpNonZero:
		result = rcvrInt != argInt;
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	ssPop(2);
	return ssPushAnnotatedConstant((result
			? trueObject()
			: falseObject()));
}


/*	We need a frame because the association has to be in ReceiverResultReg for
	the various trampolines
	and ReceiverResultReg holds only the receiver in frameless methods.
 */

	/* StackToRegisterMappingCogit>>#genStorePop:LiteralVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *anInstruction;
    sqInt association;
    sqInt i;
    sqInt topReg;

	assert(needsFrame);

	/* begin genLoadLiteralVariable:in: */
	association = getLiteral(litVarIndex);
	voidReceiverResultRegContainsSelf();

	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	if (/* shouldAnnotateObjectReference: */
		(isNonIntegerObject(association))
	 && (oopisGreaterThan(association, trueObject()))) {
		annotateobjRef(checkLiteralforInstruction(association, genoperandoperand(MoveCwR, association, ReceiverResultReg)), association);
	}
	else {
		/* begin MoveCq:R: */
		/* begin gen:quickConstant:operand: */
		anInstruction = genoperandoperand(MoveCqR, association, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteral(association));
		}
	}

	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#  if IMMUTABILITY
	if (needsImmCheck) {
		/* begin ssAllocateRequiredReg:upThrough: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr - 1, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);

		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, ValueIndex, ReceiverResultReg, TempReg, needsStoreCheck, 0);
	}
#  endif // IMMUTABILITY

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, ValueIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}


/*	The reason we need a frame here is that assigning to an inst var of a
	context may
	involve wholesale reorganization of stack pages, and the only way to
	preserve the
	execution state of an activation in that case is if it has a frame. */

	/* StackToRegisterMappingCogit>>#genStorePop:MaybeContextReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    sqInt i;
    AbstractInstruction *immutabilityFailure;
    AbstractInstruction *mutableJump;

	assert(needsFrame);
	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();

	/* begin genGenericStorePop:MaybeContextSlotIndex:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
	immutabilityFailure = ((AbstractInstruction *) 0);
	assert(needsFrame);
#  if IMMUTABILITY
	if (needsImmCheck) {
		mutableJump = genJumpMutablescratchReg(ReceiverResultReg, TempReg);
		genStoreTrampolineCall(slotIndex);

		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
		immutabilityFailure = genoperand(Jump, ((sqInt)0));
		jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif // IMMUTABILITY

	ssPop(1);

	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ClassReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ssPush(1);
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	ssStoreAndReplacePoptoReg(popBoolean, ClassReg);

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= simStackPtr) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = simStackPtr + 1;
	}

	/* begin MoveCq:R: */
	/* begin gen:quickConstant:operand: */
	anInstruction = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(slotIndex));
	}

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreContextInstVarTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
#  if IMMUTABILITY
	if (needsImmCheck) {
		jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif

	return 0;
}

	/* StackToRegisterMappingCogit>>#genStorePop:ReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static NoDbgRegParms sqInt
genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    sqInt i;
    sqInt needsImmCheckSqInt;
    sqInt needsStoreCheckSqInt;
    sqInt topReg;

	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();
	needsStoreCheckSqInt = (!useTwoPaths)
		 && (needsStoreCheck);
	needsImmCheckSqInt = needsImmCheck
		 && (!useTwoPaths);

	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#  if IMMUTABILITY
	if (needsImmCheckSqInt) {
		/* begin ssAllocateRequiredReg:upThrough: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr - 1, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);

		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, slotIndex, ReceiverResultReg, TempReg, needsStoreCheckSqInt, 1);
	}
#  endif // IMMUTABILITY

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheckSqInt);
}


/*	The only reason we assert needsFrame here is that in a frameless method
	ReceiverResultReg must and does contain only self, but the ceStoreCheck
	trampoline expects the target of the store to be in ReceiverResultReg. So
	in a frameless method we would have a conflict between the receiver and
	the temote temp store, unless we we smart enough to realise that
	ReceiverResultReg was unused after the literal variable store, unlikely
	given that methods return self by default. */

	/* StackToRegisterMappingCogit>>#genStorePop:RemoteTemp:At:needsStoreCheck: */
static NoDbgRegParms sqInt
genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt topReg;

	assert(needsFrame);

	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	voidReceiverResultRegContainsSelf();
	offset = frameOffsetOfTemporary(remoteTempIndex);

	/* begin MoveMw:r:R: */
	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, FPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}

	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#  if IMMUTABILITY
#  endif

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}

	/* StackToRegisterMappingCogit>>#genStorePop:TemporaryVariable: */
static NoDbgRegParms sqInt
genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt reg;

	ssFlushUpThroughTemporaryVariable(tempIndex);
	reg = ssStorePoptoPreferredReg(popBoolean, TempReg);
	offset = frameOffsetOfTemporary(tempIndex);

	/* begin MoveR:Mw:r: */
	/* begin gen:operand:quickConstant:operand: */
	anInstruction = genoperandoperandoperand(MoveRMwr, reg, offset, FPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteral(offset));
	}
	((simStackAt(tempIndex + 1))->bcptr = bytecodePC);
	return 0;
}


/*	Generate a method return from within a method or a block.
	Frameless method activation looks like
	CISCs (x86):
	receiver
	args
	sp->	ret pc.
	RISCs (ARM):
	receiver
	args
	ret pc in LR.
	A fully framed activation is described in CoInterpreter
	class>initializeFrameIndices. Return pops receiver and arguments off the
	stack. Callee pushes the result. */

	/* StackToRegisterMappingCogit>>#genUpArrowReturn */
static sqInt
genUpArrowReturn(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt offset;

	/* can't fall through */
	deadCode = 1;
	if (inBlock > 0) {
		assert(needsFrame);

		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}

		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceNonLocalReturnTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
		return 0;
	}
	if (
#  if IMMUTABILITY
		needsFrame
			 && (!useTwoPaths)
#  else
		needsFrame
#  endif
		) {

		/* MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);

		/* PopR: */
		genoperand(PopR, FPReg);

		/* PopR: */
		genoperand(PopR, LinkReg);

		/* RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	else {
		offset = ((methodOrBlockNumArgs > (numRegArgs()))
			 || (regArgsHaveBeenPushed)
					? (methodOrBlockNumArgs + 1) * BytesPerWord
					: 0);

		/* begin RetN: */
		genoperand(RetN, offset);
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#genVanillaInlinedIdenticalOrNotIf: */
static NoDbgRegParms sqInt
genVanillaInlinedIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *anInstruction;
    int argIsConstant;
    sqInt argNeedsReg;
    sqInt argReg;
    sqInt argRegSqInt;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt constant;
    sqInt i;
    void *jumpTarget;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPCSqInt;
    sqInt postBranchPC;
    sqInt postBranchPCSqInt;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrIsConstant;
    sqInt rcvrNeedsReg;
    sqInt rcvrReg;
    sqInt rcvrRegSqInt;
    sqInt reg;
    sqInt rNextSqInt;
    sqInt rTopSqInt;
    BytecodeFixup *self_in_CogBytecodeFixup;
    sqInt targetBytecodePC;
    sqInt targetPC;
    sqInt topRegistersMask;

	/* begin extractMaybeBranchDescriptorInto: */
	branchDescriptor1 = ((BytecodeDescriptor *) 0);
	primDescriptor = generatorAt(byte0);
	nextPCSqInt = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPCSqInt, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPCSqInt += (branchDescriptor1->numBytes);
		}
		if (!(/* isUnconditionalBranch */
			(isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPCSqInt = eventualTargetOf((nextPCSqInt + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPCSqInt, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPCSqInt = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPCSqInt + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPCSqInt, nExts, methodObj)));
		postBranchPCSqInt = eventualTargetOf(nextPCSqInt + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPCSqInt = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPCSqInt;
	postBranchPC = postBranchPCSqInt;
	targetPC = targetBytecodePC;

	/* They can't be both constants to use correct machine opcodes.
	   However annotable constants can't be resolved statically, hence we need to careful. */
	argIsConstant = (((ssTop())->type)) == SSConstant;
	rcvrIsConstant = (!argIsConstant)
		 && ((((ssValue(1))->type)) == SSConstant);
	argNeedsReg = !argIsConstant;
	rcvrNeedsReg = !rcvrIsConstant;

	/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
	assert(argNeedsReg
	 || (rcvrNeedsReg));
	argRegSqInt = (rcvrRegSqInt = NoReg);
	if (argNeedsReg) {
		if (rcvrNeedsReg) {
			/* begin allocateRegForStackTopTwoEntriesInto: */
			topRegistersMask = 0;
			rTopSqInt = (rNextSqInt = NoReg);
			if ((registerOrNone(ssTop())) != NoReg) {
				rTopSqInt = registerOrNone(ssTop());
			}
			if ((registerOrNone(ssValue(1))) != NoReg) {
				reg = (rNextSqInt = registerOrNone(ssValue(1)));

				/* begin registerMaskFor: */
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1U << reg));
			}
			if (rTopSqInt == NoReg) {
				rTopSqInt = allocateRegNotConflictingWith(topRegistersMask);
			}
			if (rNextSqInt == NoReg) {
				rNextSqInt = allocateRegNotConflictingWith(((rTopSqInt < 0) ? (((usqInt)(1)) >> (-rTopSqInt)) : (1U << rTopSqInt)));
			}
			assert(!(((rTopSqInt == NoReg)
 || (rNextSqInt == NoReg))));
			argRegSqInt = rTopSqInt;
			rcvrRegSqInt = rNextSqInt;
			popToReg(ssTop(), argRegSqInt);
			popToReg(ssValue(1), rcvrRegSqInt);
		}
		else {
			argRegSqInt = allocateRegForStackEntryAtnotConflictingWith(0, 0);
			popToReg(ssTop(), argRegSqInt);
			if (((ssValue(1))->spilled)) {
				/* begin AddCq:R: */
				/* begin gen:quickConstant:operand: */
				anInstruction = genoperandoperand(AddCqR, BytesPerWord, SPReg);
				if (usesOutOfLineLiteral(anInstruction)) {
					(anInstruction->dependent = locateLiteral(BytesPerWord));
				}
			}
		}
	}
	else {
		assert(rcvrNeedsReg);
		assert(!((((ssTop())->spilled))));
		rcvrRegSqInt = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), rcvrRegSqInt);
	}
	assert(!((argNeedsReg
 && (argRegSqInt == NoReg))));
	assert(!((rcvrNeedsReg
 && (rcvrRegSqInt == NoReg))));
	rcvrReg = rcvrRegSqInt;
	argReg = argRegSqInt;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argIsConstant, rcvrIsConstant, argReg, rcvrReg, orNot);
	}

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= (simStackPtr - 2)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < (simStackPtr - 2)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : (simStackPtr - 2))); i <= (simStackPtr - 2); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = (simStackPtr - 2) + 1;
	}

	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (argIsConstant) {
		constant = ((ssTop())->constant);

		/* begin genCmpConstant:R: */
		if (/* shouldAnnotateObjectReference: */
			(isNonIntegerObject(constant))
		 && (oopisGreaterThan(constant, trueObject()))) {
			annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, rcvrReg)), constant);
		}
		else {
			/* begin CmpCq:R: */
			/* begin gen:quickConstant:operand: */
			anInstruction = genoperandoperand(CmpCqR, constant, rcvrReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteral(constant));
			}
		}
	}
	else {
		if (rcvrIsConstant) {
			constant = ((ssValue(1))->constant);

			/* begin genCmpConstant:R: */
			if (/* shouldAnnotateObjectReference: */
				(isNonIntegerObject(constant))
			 && (oopisGreaterThan(constant, trueObject()))) {
				annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, argReg)), constant);
			}
			else {
				/* begin CmpCq:R: */
				/* begin gen:quickConstant:operand: */
				anInstruction = genoperandoperand(CmpCqR, constant, argReg);
				if (usesOutOfLineLiteral(anInstruction)) {
					(anInstruction->dependent = locateLiteral(constant));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);

	/* begin fixupAt: */
	self_in_CogBytecodeFixup = fixupAtIndex(nextPC - initialPC);
	if ((self_in_CogBytecodeFixup->targetInstruction)) {
		assert(!(deadCode));
	}
	else {

		/* The next instruction is dead.  we can skip it. */
		deadCode = 1;
		ensureFixupAt(targetPC);
		ensureFixupAt(postBranchPC);
	}
	if (orNot == ((branchDescriptor->isBranchTrue))) {

		/* a == b ifFalse: ... or a ~~ b ifTrue: ... jump on equal to post-branch pc */
		ensureNonMergeFixupAt(targetPC);
		jumpTarget = ensureNonMergeFixupAt(postBranchPC);

		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		jumpTarget = ensureNonMergeFixupAt(targetPC);

		/* begin Jump: */
		genoperand(Jump, ((sqInt)jumpTarget));
	}
	else {

		/* orNot is true for ~~ */
		/* a == b ifTrue: ... or a ~~ b ifFalse: ... jump on equal to target pc */
		jumpTarget = ensureNonMergeFixupAt(targetPC);

		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		jumpTarget = ensureNonMergeFixupAt(postBranchPC);

		/* begin Jump: */
		genoperand(Jump, ((sqInt)jumpTarget));
	}
	if (!deadCode) {
		ssPushConstant(trueObject());
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#initSimStackForFramefulMethod: */
static NoDbgRegParms void
initSimStackForFramefulMethod(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
	cascade0 = simSelf();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 1);
	(cascade0->registerr = FPReg);
	(cascade0->offset = FoxMFReceiver);
	(cascade0->liveRegister = NoReg);
	for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxCallerSavedIP + (((methodOrBlockNumArgs - i) + 1) * BytesPerWord));
		(desc->bcptr = startpc);
	}
	for (i = (methodOrBlockNumArgs + 1); i <= simStackPtr; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxMFReceiver - ((i - methodOrBlockNumArgs) * BytesPerWord));
		(desc->bcptr = startpc);
	}
}


/*	The register receiver (the closure itself) and args are pushed by the
	closure value primitive(s)
	and hence a frameless block has all arguments and copied values pushed to
	the stack. However,
	the method receiver (self) is put in the ReceiverResultReg by the block
	entry. 
 */

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessBlock: */
static NoDbgRegParms void
initSimStackForFramelessBlock(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps >= methodOrBlockNumArgs);
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = SPReg);
		(desc->offset = (methodOrBlockNumArgs - i) * BytesPerWord);
		(desc->bcptr = startpc);
	}

	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
}

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessMethod: */
static NoDbgRegParms void
initSimStackForFramelessMethod(sqInt startpc)
{
    CogSimStackEntry *cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps == methodOrBlockNumArgs);
	assert((numRegArgs()) <= 2);
	if (((methodOrBlockNumArgs >= 1) && (methodOrBlockNumArgs <= (numRegArgs())))) {
		desc = simStackAt(1);
		(desc->type = SSRegister);
		(desc->spilled = 0);
		(desc->registerr = Arg0Reg);
		(desc->bcptr = startpc);
		if (methodOrBlockNumArgs > 1) {
			desc = simStackAt(2);
			(desc->type = SSRegister);
			(desc->spilled = 0);
			(desc->registerr = Arg1Reg);
			(desc->bcptr = startpc);
		}
	}
	else {
		for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
			desc = simStackAt(i);
			(desc->type = SSBaseOffset);
			(desc->registerr = SPReg);
			(desc->spilled = 1);
			(desc->offset = (methodOrBlockNumArgs - i) * BytesPerWord);
			(desc->bcptr = startpc);
		}
	}
	simStackPtr = methodOrBlockNumArgs;
	simSpillBase = methodOrBlockNumArgs + 1;
}


/*	Do not inline (inBlock access) */

	/* StackToRegisterMappingCogit>>#isNonForwarderReceiver: */
static NoDbgRegParms sqInt
isNonForwarderReceiver(sqInt reg)
{
	return ((((simSelf())->liveRegister)) == ReceiverResultReg)
	 && ((inBlock == 0)
	 && (reg == ReceiverResultReg));
}

	/* StackToRegisterMappingCogit>>#liveRegisters */
static sqInt
liveRegisters(void)
{
    sqInt i;
    sqInt regsSet;

	if (needsFrame) {
		regsSet = 0;
	}
	else {
		regsSet = (1U << ReceiverResultReg);
		if ((methodOrBlockNumArgs <= (numRegArgs()))
		 && (methodOrBlockNumArgs > 0)) {
			regsSet = regsSet | ((1U << Arg0Reg));
		}
	}
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= simStackPtr; i += 1) {
		regsSet = regsSet | (registerMask(simStackAt(i)));
	}
	return regsSet;
}


/*	insert nops for dead code that is mapped so that bc 
	to mc mapping is not many to one */

	/* StackToRegisterMappingCogit>>#mapDeadDescriptorIfNeeded: */
static NoDbgRegParms sqInt
mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor)
{
    AbstractInstruction *abstractInstruction;

	flag("annotateInstruction");
	if (((descriptor->isMapped))
	 || ((inBlock > 0)
	 && ((descriptor->isMappedInBlock)))) {
		abstractInstruction = gen(Nop);

		/* begin annotateBytecode: */
		(abstractInstruction->annotation = HasBytecodePC);
	}
	return 0;
}


/*	Spill everything on the simulated stack that needs spilling (that below
	receiver and arguments).
	Marshall receiver and arguments to stack and/or registers depending on arg
	count. If the args don't fit in registers push receiver and args (spill
	everything), but still assign
	the receiver to ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#marshallSendArguments: */
static NoDbgRegParms void
marshallSendArguments(sqInt numArgs)
{
    sqInt anyRefs;
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt numSpilled;

	/* begin ssFlushTo: */
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= ((simStackPtr - numArgs) - 1)) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < ((simStackPtr - numArgs) - 1)) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : ((simStackPtr - numArgs) - 1))); i < (simStackPtr - numArgs); i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = ((simStackPtr - numArgs) - 1) + 1;
	}
	if (numArgs > (numRegArgs())) {

		/* If there are no spills and no references to ReceiverResultReg
		   the fetch of ReceiverResultReg from the stack can be avoided
		   by assigning directly to ReceiverResultReg and pushing it. */
		numSpilled = numberOfSpillsInTopNItems(numArgs + 1);
		anyRefs = anyReferencesToRegisterinTopNItems(ReceiverResultReg, numArgs + 1);
		if ((numSpilled > 0)
		 || (anyRefs)) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= simStackPtr) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = simStackPtr + 1;
			}
			storeToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
		}
		else {
			cascade0 = simStackAt(simStackPtr - numArgs);
			storeToReg(cascade0, ReceiverResultReg);
			(cascade0->type = SSRegister);
			(cascade0->registerr = ReceiverResultReg);

			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= simStackPtr) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = simStackPtr + 1;
			}
		}
	}
	else {

		/* Move the args to the register arguments, being careful to do
		   so last to first so e.g. previous contents don't get overwritten.
		   Also check for any arg registers in use by other args. */
		if (numArgs > 0) {
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg0Reg), simStackPtr - 1, simNativeStackPtr);
		}
		if (numArgs > 0) {
			popToReg(simStackAt((simStackPtr - numArgs) + 1), Arg0Reg);
		}
		popToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
	}
	ssPop(numArgs + 1);
}


/*	For assert checking; or rather for avoiding assert fails when dealing with
	the hack for block temps in the SqueakV3PlusClosures bytecode set.
 */

	/* StackToRegisterMappingCogit>>#maybeCompilingFirstPassOfBlockWithInitialPushNil */
static sqInt
maybeCompilingFirstPassOfBlockWithInitialPushNil(void)
{
	return (inBlock == InVanillaBlock)
	 && ((methodOrBlockNumTemps > methodOrBlockNumArgs)
	 && (compilationPass == 1));
}


/*	If this bytecode has a fixup, some kind of merge needs to be done. There
	are 4 cases:
	1) the bytecode has no fixup (fixup isNotAFixup)
	do nothing
	2) the bytecode has a non merge fixup
	the fixup has needsNonMergeFixup.
	The code generating non merge fixup (currently only special selector code)
	is responsible
	for the merge so no need to do it.
	We set deadCode to false as the instruction can be reached from jumps.
	3) the bytecode has a merge fixup, but execution flow *cannot* fall
	through to the merge point.
	the fixup has needsMergeFixup and deadCode = true.
	ignores the current simStack as it does not mean anything 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	4) the bytecode has a merge fixup and execution flow *can* fall through to
	the merge point.
	the fixup has needsMergeFixup and deadCode = false.
	flushes the stack to the stack pointer so the fall through execution path
	simStack is 
	in the state the merge point expects it to be. 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	
	In addition, if this is a backjump merge point, we patch the fixup to hold
	the current simStackPtr 
	for later assertions. */

	/* StackToRegisterMappingCogit>>#mergeWithFixupIfRequired: */
static NoDbgRegParms sqInt
mergeWithFixupIfRequired(BytecodeFixup *fixup)
{
    CogSimStackEntry *cascade0;
    sqInt i;

	/* begin assertCorrectSimStackPtr */
	assert((simSpillBase >= methodOrBlockNumTemps)
	 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
	 && (simSpillBase > methodOrBlockNumArgs)));
	if (needsFrame
	 && (simSpillBase > 0)) {
		assert(((((simStackAt(simSpillBase - 1))->spilled)) == 1)
		 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
		 && (simSpillBase > methodOrBlockNumArgs)));
		assert((simSpillBase > simStackPtr)
		 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	}
	if (!((fixup->targetInstruction))) {
		return 0;
	}
	if ((((usqInt)((fixup->targetInstruction)))) == NeedsNonMergeFixupFlag) {
		deadCode = 0;
		return 0;
	}
	assert(isMergeFixup(fixup));
	traceMerge(fixup);
	if (deadCode) {

		/* case 3 */
		/* Would like to assert fixup simStackPtr >= methodOrBlockNumTemps
		   but can't because of the initialNils hack. */
		assert((((fixup->simStackPtr)) >= methodOrBlockNumTemps)
		 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
		simStackPtr = (fixup->simStackPtr);
	}
	else {

		/* case 4 */
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= simStackPtr) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < simStackPtr) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : simStackPtr)); i <= simStackPtr; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = simStackPtr + 1;
		}
	}
	deadCode = 0;
	if ((fixup->isTargetOfBackwardBranch)) {
		(fixup->simStackPtr = simStackPtr);
	}
	(fixup->targetInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	assert(simStackPtr == ((fixup->simStackPtr)));

	/* begin restoreSimStackAtMergePoint: */
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	for (i = (methodOrBlockNumTemps + 1); i <= simStackPtr; i += 1) {
		cascade0 = simStackAt(i);
		(cascade0->type = SSSpill);
		(cascade0->offset = FoxMFReceiver - ((i - methodOrBlockNumArgs) * BytesPerOop));
		(cascade0->registerr = FPReg);
		(cascade0->spilled = 1);
	}
	simSpillBase = simStackPtr + 1;
	return 0;
}

	/* StackToRegisterMappingCogit>>#methodAbortTrampolineFor: */
static NoDbgRegParms sqInt
methodAbortTrampolineFor(sqInt numArgs)
{
	return methodAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}

	/* StackToRegisterMappingCogit>>#needsFrameIfMod16GENumArgs: */
static NoDbgRegParms sqInt
needsFrameIfMod16GENumArgs(sqInt stackDelta)
{
	return (byte0 % 16) >= methodOrBlockNumArgs;
}


/*	As of August 2013, the code generator can't deal with spills in frameless
	methods (the
	issue is to do with the stack offset to get at an argument, which is
	changed when there's a spill).
	In e.g. TextColor>>#dominates: other ^other class == self class the second
	send of class
	needs also rto allocate a register that the first one used, but the first
	one's register can't be
	spilled. So avoid this by only allowing class to be sent if the stack
	contains a single element. */

	/* StackToRegisterMappingCogit>>#needsFrameIfStackGreaterThanOne: */
static NoDbgRegParms sqInt
needsFrameIfStackGreaterThanOne(sqInt stackDelta)
{
	return stackDelta > 1;
}

	/* StackToRegisterMappingCogit>>#numberOfSpillsInTopNItems: */
static NoDbgRegParms sqInt
numberOfSpillsInTopNItems(sqInt n)
{
    sqInt i;

	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((simStackAt(i))->type)) == SSSpill) {
			return n - (simStackPtr - i);
		}
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#picAbortTrampolineFor: */
static NoDbgRegParms sqInt
picAbortTrampolineFor(sqInt numArgs)
{
	return picAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}

	/* StackToRegisterMappingCogit>>#prevInstIsPCAnnotated */
static sqInt
prevInstIsPCAnnotated(void)
{
    sqInt prevIndex;
    AbstractInstruction *prevInst;

	if (!(opcodeIndex > 0)) {
		return 0;
	}
	prevIndex = opcodeIndex - 1;
	while (1) {
		if (prevIndex <= 0) {
			return 0;
		}
		prevInst = abstractInstructionAt(prevIndex);
		if (((!((prevInst->annotation))
			? 0
			: (prevInst->annotation))) >= HasBytecodePC) {
			return 1;
		}
		if (!(((prevInst->opcode)) == Label)) break;
		prevIndex -= 1;
	}
	return 0;
}


/*	Used to mark ReceiverResultReg as dead or not containing simSelf.
	Used when the simStack has already been flushed, e.g. for sends. */

	/* StackToRegisterMappingCogit>>#receiverIsInReceiverResultReg */
static int
receiverIsInReceiverResultReg(void)
{
	return (((simSelf())->liveRegister)) == ReceiverResultReg;
}


/*	When a block must be recompiled due to overestimating the
	numInitialNils fixups must be restored, which means rescannning
	since backward branches need their targets initialized. */

	/* StackToRegisterMappingCogit>>#reinitializeFixupsFrom:through: */
static NoDbgRegParms void
reinitializeFixupsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt pc;
    BytecodeFixup *self_in_CogSSBytecodeFixup;
    sqInt targetPC;

	pc = start;
	nExts = 0;
	while (pc <= end) {
		/* begin fixupAt: */
		self_in_CogSSBytecodeFixup = fixupAtIndex(pc - initialPC);
		(self_in_CogSSBytecodeFixup->targetInstruction) = 0;
		(self_in_CogSSBytecodeFixup->simStackPtr) = 0;
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((isBranch(descriptor))
		 && ((/* begin isBackwardBranch:at:exts:in: */
			assert(((descriptor->spanFunction))),
		(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0))) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;

			/* begin initializeFixupAt: */
			/* begin fixupAt: */
			fixup = fixupAtIndex(targetPC - initialPC);
			(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);

			/* begin setIsBackwardBranchFixup */
			(fixup->isTargetOfBackwardBranch) = 1;
		}
		if ((descriptor->isBlockCreation)) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			pc = (pc + ((descriptor->numBytes))) + distance;
		}
		else {
			pc += (descriptor->numBytes);
		}
		nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
	}
}


/*	Scan the block to determine if the block needs a frame or not */

	/* StackToRegisterMappingCogit>>#scanBlock: */
static NoDbgRegParms sqInt
scanBlock(BlockStart *blockStart)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt framelessStackDelta;
    sqInt nExts;
    sqInt numPushNils;
    sqInt (* const numPushNilsFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt) = v3NumPushNils;
    sqInt pc;
    sqInt pushingNils;

	needsFrame = 0;
	prevBCDescriptor = null;
	methodOrBlockNumArgs = (blockStart->numArgs);
	inBlock = InVanillaBlock;
	pc = (blockStart->startpc);
	end = ((blockStart->startpc)) + ((blockStart->span));
	framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0))));
	pushingNils = 1;
	while (pc < end) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (!needsFrame) {
			if ((!((descriptor->needsFrameFunction)))
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {
				needsFrame = 1;
			}
			else {
				framelessStackDelta += (descriptor->stackDelta);
			}
		}
		if (pushingNils
		 && (!((descriptor->isExtension)))) {

			/* Count the initial number of pushed nils acting as temp initializers.  We can't tell
			   whether an initial pushNil is an operand reference or a temp initializer, except
			   when the pushNil is a jump target (has a fixup), which never happens:
			   self systemNavigation browseAllSelect:
			   [:m| | ebc |
			   (ebc := m embeddedBlockClosures
			   select: [:ea| ea decompile statements first isMessage]
			   thenCollect: [:ea| ea decompile statements first selector]) notEmpty
			   and: [(#(whileTrue whileFalse whileTrue: whileFalse:) intersection: ebc) notEmpty]]
			   or if the bytecode set has a push multiple nils bytecode.  We simply count initial nils.
			   Rarely we may end up over-estimating.  We will correct by checking the stack depth
			   at the end of the block in compileBlockBodies. */
			if (((numPushNils = numPushNilsFunction(descriptor, pc, nExts, methodObj))) > 0) {
				assert(((descriptor->numBytes)) == 1);
				(blockStart->numInitialNils = ((blockStart->numInitialNils)) + numPushNils);
			}
			else {
				pushingNils = 0;
			}
		}
		pc = (pc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
		? ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)
		: 0));
		if ((descriptor->isExtension)) {
			nExts += 1;
		}
		else {
			nExts = (extA = (numExtB = (extB = 0)));
		}
		prevBCDescriptor = descriptor;
	}
	if (!needsFrame) {
		assert((framelessStackDelta >= 0)
		 && (((blockStart->numInitialNils)) >= framelessStackDelta));
		(blockStart->numInitialNils = ((blockStart->numInitialNils)) - framelessStackDelta);
	}
	return 0;
}


/*	Scan the method (and all embedded blocks) to determine
	- what the last bytecode is; extra bytes at the end of a method are used
	to encode things like source pointers or temp names
	- if the method needs a frame or not
	- what are the targets of any backward branches.
	- how many blocks it creates
	Answer the block count or on error a negative error code */

	/* StackToRegisterMappingCogit>>#scanMethod */
static sqInt
scanMethod(void)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt framelessStackDelta;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt numBlocks;
    sqInt pc;
    sqInt seenInstVarStore;
    sqInt targetPC;

	needsFrame = (useTwoPaths = (seenInstVarStore = 0));
	prevBCDescriptor = null;
	if ((primitiveIndex > 0)
	 && (isQuickPrimitiveIndex(primitiveIndex))) {
		return 0;
	}
	pc = (latestContinuation = initialPC);
	numBlocks = (framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0)))));
	while (pc <= endPC) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			if (((descriptor->opcode)) == Nop) {

				/* unknown bytecode tag; see Cogit class>>#generatorTableFrom: */
				return EncounteredUnknownBytecode;
			}
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			endPC = pc;
		}
		if (!needsFrame) {
			if ((!((descriptor->needsFrameFunction)))
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {

				/* With immutability we win simply by avoiding a frame build if the receiver is young and not immutable. */
#        if IMMUTABILITY
				if ((descriptor->is1ByteInstVarStore)) {
					useTwoPaths = 1;
				}
				else {
					needsFrame = 1;
					useTwoPaths = 0;
				}
#        else // IMMUTABILITY
				needsFrame = 1;
				useTwoPaths = 0;
#        endif

			}
			else {
				/* Without immutability we win if there are two or more stores and the receiver is new. */
				framelessStackDelta += (descriptor->stackDelta);
#        if IMMUTABILITY
#        else
				if ((descriptor->is1ByteInstVarStore)) {
					if (seenInstVarStore) {
						useTwoPaths = 1;
					}
					else {
						seenInstVarStore = 1;
					}
				}
#        endif // IMMUTABILITY

			}
		}
		if (isBranch(descriptor)) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			if ((/* begin isBackwardBranch:at:exts:in: */
				assert(((descriptor->spanFunction))),
			(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0)) {
				/* begin initializeFixupAt: */
				/* begin fixupAt: */
				fixup = fixupAtIndex(targetPC - initialPC);
				(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);

				/* begin setIsBackwardBranchFixup */
				(fixup->isTargetOfBackwardBranch) = 1;
			}
			else {
				latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
			}
		}
		if ((descriptor->isBlockCreation)) {
			numBlocks += 1;
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
		}
		pc += (descriptor->numBytes);
		nExts = ((descriptor->isExtension)
					? nExts + 1
					: (extA = (numExtB = (extB = 0))));
		prevBCDescriptor = descriptor;
	}
	return numBlocks;
}

	/* StackToRegisterMappingCogit>>#ssAllocateRequiredRegMask:upThrough:upThroughNative: */
static NoDbgRegParms void
ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr)
{
    sqInt i;
    sqInt iSqInt;
    sqInt lastRequired;
    sqInt lastRequiredNative;
    sqInt liveRegs;

	lastRequired = -1;

	/* compute live regs while noting the last occurrence of required regs.
	   If these are not free we must spill from simSpillBase to last occurrence.
	   Note we are conservative here; we could allocate FPReg in frameless methods. */
	lastRequiredNative = -1;
	liveRegs = (1U << FPReg) | (1U << SPReg);
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= stackPtr; i += 1) {
		liveRegs = liveRegs | (registerMask(simStackAt(i)));
		if ((((registerMask(simStackAt(i))) & requiredRegsMask) != 0)) {
			lastRequired = i;
		}
	}
	if (((liveRegs & requiredRegsMask) != 0)) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= lastRequired) {
			for (iSqInt = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < lastRequired) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : lastRequired)); iSqInt <= lastRequired; iSqInt += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(iSqInt), frameOffsetOfTemporary(iSqInt - 1), FPReg);
			}
			simSpillBase = lastRequired + 1;
		}
		assert(!(((((liveRegisters()) & requiredRegsMask) != 0))));
	}
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughReceiverVariable: */
static NoDbgRegParms void
ssFlushUpThroughReceiverVariable(sqInt slotIndex)
{
    CogSimStackEntry *desc;
    sqInt i;
    sqInt index;


	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((desc = simStackAt(index)),
		(((desc->type)) == SSBaseOffset)
			 && ((((desc->registerr)) == ReceiverResultReg)
			 && (((desc->offset)) == (slotOffsetOfInstVarIndex(slotIndex)))))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
l1:	/* end ssFlushUpThrough: */;
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughTemporaryVariable: */
static NoDbgRegParms void
ssFlushUpThroughTemporaryVariable(sqInt tempIndex)
{
    CogSimStackEntry *desc;
    sqInt i;
    sqInt index;
    sqInt offset;

	offset = ((simStackAt(tempIndex + 1))->offset);
	assert(offset == (frameOffsetOfTemporary(tempIndex)));

	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((desc = simStackAt(index)),
		(((desc->type)) == SSBaseOffset)
			 && ((((desc->registerr)) == FPReg)
			 && (((desc->offset)) == offset)))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
l1:	/* end ssFlushUpThrough: */;
}

	/* StackToRegisterMappingCogit>>#ssPop: */
static NoDbgRegParms void
ssPop(sqInt n)
{
    sqInt i;
    sqInt toDoLimit;

	assert(((simStackPtr - n) >= methodOrBlockNumTemps)
	 || (((!needsFrame)
	 && ((simStackPtr - n) >= 0))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil())));
	simStackPtr -= n;

	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	toDoLimit = (((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr);
	for (i = (methodOrBlockNumTemps + 1); i <= toDoLimit; i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
}

	/* StackToRegisterMappingCogit>>#ssPushAnnotatedConstant: */
static NoDbgRegParms sqInt
ssPushAnnotatedConstant(sqInt literal)
{
    AbstractInstruction *abstractInstruction;

	ssPushConstant(literal);

	/* begin annotateInstructionForBytecode */
	abstractInstruction = (prevInstIsPCAnnotated()
				? gen(Nop)
				: genoperandoperand(Label, (labelCounter += 1), bytecodePC));

	/* begin annotateBytecode: */
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushBase:offset: */
static NoDbgRegParms sqInt
ssPushBaseoffset(sqInt reg, sqInt offset)
{
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt toDoLimit;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->offset = offset);
	(cascade0->bcptr = bytecodePC);

	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	toDoLimit = (((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr);
	for (i = (methodOrBlockNumTemps + 1); i <= toDoLimit; i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushConstant: */
static NoDbgRegParms sqInt
ssPushConstant(sqInt literal)
{
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt toDoLimit;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSConstant);
	(cascade0->spilled = 0);
	(cascade0->constant = literal);
	(cascade0->bcptr = bytecodePC);

	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	toDoLimit = (((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr);
	for (i = (methodOrBlockNumTemps + 1); i <= toDoLimit; i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushDesc: */
static NoDbgRegParms sqInt
ssPushDesc(SimStackEntry simStackEntry)
{
    sqInt i;
    sqInt toDoLimit;

	if (((simStackEntry.type)) == SSSpill) {
		(simStackEntry.type = SSBaseOffset);
	}
	(simStackEntry.spilled = 0);
	(simStackEntry.bcptr = bytecodePC);
	simStack[(simStackPtr += 1)] = simStackEntry;

	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	toDoLimit = (((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr);
	for (i = (methodOrBlockNumTemps + 1); i <= toDoLimit; i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushRegister: */
static NoDbgRegParms sqInt
ssPushRegister(sqInt reg)
{
    CogSimStackEntry *cascade0;
    sqInt i;
    sqInt toDoLimit;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);

	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	toDoLimit = (((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr);
	for (i = (methodOrBlockNumTemps + 1); i <= toDoLimit; i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPush: */
static NoDbgRegParms void
ssPush(sqInt n)
{
	simStackPtr += n;
}

	/* StackToRegisterMappingCogit>>#ssSelfDescriptor */
static SimStackEntry
ssSelfDescriptor(void)
{
	return simStack[0];
}


/*	In addition to ssStorePop:toReg:, if this is a store and not
	a popInto I change the simulated stack to use the register 
	for the top value */

	/* StackToRegisterMappingCogit>>#ssStoreAndReplacePop:toReg: */
static NoDbgRegParms void
ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg)
{
    char topSpilled;

	topSpilled = ((ssTop())->spilled);
	ssStorePoptoReg(popBoolean
	 || (topSpilled), reg);
	if (!popBoolean) {
		if (!topSpilled) {
			ssPop(1);
		}
		ssPushRegister(reg);
	}
}


/*	Store or pop the top simulated stack entry to a register.
	Use preferredReg if the entry is not itself a register.
	Answer the actual register the result ends up in. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toPreferredReg: */
static NoDbgRegParms sqInt
ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg)
{
    sqInt actualReg;

	actualReg = preferredReg;
	if ((((ssTop())->type)) == SSRegister) {
		assert(!(((ssTop())->spilled)));
		actualReg = ((ssTop())->registerr);
	}
	ssStorePoptoReg(popBoolean, actualReg);
	return actualReg;
}


/*	Store or pop the top simulated stack entry to a register.
	N.B.: popToReg: and storeToReg: does not generate anything if 
	it moves a register to the same register. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toReg: */
static NoDbgRegParms void
ssStorePoptoReg(sqInt popBoolean, sqInt reg)
{
	if (popBoolean) {
		popToReg(ssTop(), reg);
		ssPop(1);
	}
	else {
		storeToReg(ssTop(), reg);
	}
}

	/* StackToRegisterMappingCogit>>#ssTop */
static CogSimStackEntry *
ssTop(void)
{
	return simStackAt(simStackPtr);
}

	/* StackToRegisterMappingCogit>>#ssValue: */
static NoDbgRegParms CogSimStackEntry *
ssValue(sqInt n)
{
	return simStackAt(simStackPtr - n);
}

	/* StackToRegisterMappingCogit>>#stackEntryIsBoolean: */
static NoDbgRegParms sqInt
stackEntryIsBoolean(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((((simStackEntry->constant)) == (trueObject()))
	 || (((simStackEntry->constant)) == (falseObject())));
}


/*	Answer if the stack is valid up to, but not including, simSpillBase. */

	/* StackToRegisterMappingCogit>>#tempsValidAndVolatileEntriesSpilled */
static sqInt
tempsValidAndVolatileEntriesSpilled(void)
{
    sqInt culprit;
    sqInt i;

	culprit = 0;
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		if (!(((((simStackAt(i))->type)) == SSBaseOffset)
			 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i < simSpillBase; i += 1) {
		if (!(((simStackAt(i))->spilled))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	return 1;
}


/*	If the sequence of bytecodes is
	push: (Array new: 1)
	popIntoTemp: tempIndex
	pushConstant: const or pushTemp: n
	popIntoTemp: 0 inVectorAt: tempIndex
	collapse this into
	tempAt: tempIndex put: {const or temp}
	and answer true, otherwise answer false.
	One might think that we should look for a sequence of more than
	one pushes and pops but this is extremely rare.
	Exclude pushRcvr: n to avoid potential complications with context inst
	vars.  */

	/* StackToRegisterMappingCogit>>#tryCollapseTempVectorInitializationOfSize: */
static NoDbgRegParms sqInt
tryCollapseTempVectorInitializationOfSize(sqInt slots)
{
    sqInt pc;
    BytecodeDescriptor *pushArrayDesc;
    BytecodeDescriptor *pushValueDesc;
    sqInt reg;
    sqInt remoteTempIndex;
    BytecodeDescriptor *storeArrayDesc;
    BytecodeDescriptor *storeValueDesc;
    usqInt tempIndex;

	if (slots != 1) {
		return 0;
	}

	/* begin generatorForPC: */
	pushArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(bytecodePC, methodObj)));
	assert(((pushArrayDesc->generator)) == genPushNewArrayBytecode);
	pc = bytecodePC + ((pushArrayDesc->numBytes));

	/* begin generatorForPC: */
	storeArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
	if (((storeArrayDesc->generator)) == genStoreAndPopTemporaryVariableBytecode) {
		tempIndex = (fetchByteofObject(bytecodePC + ((pushArrayDesc->numBytes)), methodObj)) & 7;
	}
	else {
		if (!(((storeArrayDesc->generator)) == genLongStoreAndPopTemporaryVariableBytecode)) {
			return 0;
		}
		tempIndex = fetchByteofObject((bytecodePC + ((pushArrayDesc->numBytes))) + 1, methodObj);
	}
	pc = (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes));

	/* begin generatorForPC: */
	pushValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
	if (!((((pushValueDesc->generator)) == genPushLiteralConstantBytecode)
		 || ((((pushValueDesc->generator)) == genPushQuickIntegerConstantBytecode)
		 || (((pushValueDesc->generator)) == genPushTemporaryVariableBytecode)))) {
		return 0;
	}
	pc = ((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes));

	/* begin generatorForPC: */
	storeValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
	remoteTempIndex = fetchByteofObject((((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + 2, methodObj);
	if (!((((storeValueDesc->generator)) == genStoreAndPopRemoteTempLongBytecode)
		 && (tempIndex == remoteTempIndex))) {
		return 0;
	}
	genNewArrayOfSizeinitialized(1, 0);
	evaluateat(pushValueDesc, (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes)));
	reg = ssStorePoptoPreferredReg(1, TempReg);
	genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, 0, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
	evaluateat(storeArrayDesc, bytecodePC + ((pushArrayDesc->numBytes)));

	/* + pushArrayDesc numBytes this gets added by nextBytecodePCFor:at:exts:in: */
	bytecodePC = ((bytecodePC + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + ((storeValueDesc->numBytes));
	return 1;
}

	/* StackToRegisterMappingCogit>>#v3PushNilSize:numInitialNils: */
static NoDbgRegParms sqInt
v3PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils)
{
	return numInitialNils;
}

	/* StackToRegisterMappingCogit>>#v3:Num:Push:Nils: */
static NoDbgRegParms sqInt
v3NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	return (((descriptor->generator)) == genPushConstantNilBytecode
			? 1
			: 0);
}

	/* StackToRegisterMappingCogit>>#violatesEnsureSpilledSpillAssert */
static sqInt
violatesEnsureSpilledSpillAssert(void)
{
	return 1;
}


/*	Used when ReceiverResultReg is allocated for other than simSelf, and
	there may be references to ReceiverResultReg which need to be spilled. */

	/* StackToRegisterMappingCogit>>#voidReceiverResultRegContainsSelf */
static void
voidReceiverResultRegContainsSelf(void)
{
    sqInt i;
    sqInt iSqInt;
    sqInt spillIndex;

	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	spillIndex = 0;
	for (i = ((((methodOrBlockNumTemps + 1) < simSpillBase) ? simSpillBase : (methodOrBlockNumTemps + 1))); i <= simStackPtr; i += 1) {
		if ((registerOrNone(simStackAt(i))) == ReceiverResultReg) {
			spillIndex = i;
		}
	}
	if (spillIndex > 0) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= spillIndex) {
			for (iSqInt = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < spillIndex) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : spillIndex)); iSqInt <= spillIndex; iSqInt += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(iSqInt), frameOffsetOfTemporary(iSqInt - 1), FPReg);
			}
			simSpillBase = spillIndex + 1;
		}
	}
}
