/* Automatically generated by
	CCodeGenerator VMMaker.oscog-eem.3408 uuid: 99ace54d-4adf-4a36-9064-0f4fe0bbaae2
   from
	SistaCogit VMMaker.oscog-eem.3408 uuid: 99ace54d-4adf-4a36-9064-0f4fe0bbaae2
 */
static char __buildInfo[] = "SistaCogit VMMaker.oscog-eem.3408 uuid: 99ace54d-4adf-4a36-9064-0f4fe0bbaae2 " __DATE__ ;
char *__cogitBuildInfo = __buildInfo;



#include "sqConfig.h"
#include <stddef.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include "sqPlatformSpecific.h"
#include "sqMemoryAccess.h"
#include "sqCogStackAlignment.h"
#include "dispdbg.h"
#include "cogmethod.h"
#if COGMTVM
#include "cointerpmt.h"
#else
#include "cointerp.h"
#endif
#include "cogit.h"


/* Cogit class>>preambleCCode */
#if __APPLE__ && __MACH__ // Mac OS X
# include <libkern/OSCacheControl.h>
#endif
#if __linux__
# include <sys/auxv.h>
#endif
/* end Cogit class>>preambleCCode */


/*** Constants ***/
#define ABICalleeSavedRegisterMask 0x1FF80000
#define ABICallerSavedRegisterMask 0x3FFFF
#define ABIResultReg 0
#define AddCqR 106
#define AddCqRR 123
#define AddCwR 114
#define AddRdRd 132
#define AddRR 100
#define AddRRR 126
#define AL 14
#define AlignmentNops 3
#define AltBlockCreationBytecodeSize 3
#define AltFirstSpecialSelector 96
#define AltNumSpecialSelectors 32
#define AndCqR 108
#define AndCqRR 124
#define AndCwR 116
#define AndRR 102
#define AnnotationShift 5
#define Arg0Reg 7
#define Arg1Reg 8
#define ArithmeticAdd 0
#define ArithmeticAddS 1
#define ArithmeticShiftRightCqR 91
#define ArithmeticShiftRightCqRR 128
#define ArithmeticShiftRightRR 92
#define ArithmeticSub 2
#define ArithmeticSubS 3
#define BadRegisterSet 1
#define BlockCreationBytecodeSize 4
#define BytecodeSetHasDirectedSuperSend 1
#define CArg0Reg 0
#define CArg1Reg 1
#define CArg2Reg 2
#define CArg3Reg 3
#define CASAL 176
#define Call 6
#define CallerSavedRegisterMask 0x1F0
#define CallFull 7
#define CBNZ 177
#define CBZ 178
#define CC 3
#define CCMPNE 179
#if !defined(CheckRememberedInTrampoline) /* Allow this to be overridden on the compiler command line */
# define CheckRememberedInTrampoline 0
#endif
#define CLREX 181
#define ClassArray 7
#define ClassArrayCompactIndex 51
#define ClassBlockClosureCompactIndex 37
#define ClassFloatCompactIndex 34
#define ClassFullBlockClosureCompactIndex 38
#define ClassLargeNegativeIntegerCompactIndex 32
#define ClassLargePositiveIntegerCompactIndex 33
#define ClassMethodContextCompactIndex 36
#define ClassPointCompactIndex 54
#define ClassReg 4
#define ClosureFirstCopiedValueIndex 3
#define ClosureIndex 4
#define ClosureNumArgsIndex 2
#define ClosureOuterContextIndex 0
#define ClosureStartPCIndex 1
#define ClzRR 157
#define CMBlock 4
#define CMClosedPIC 2
#define CMFree 1
#define CMMaxUsageCount 7
#define CMMethod 5
#define CMMethodFlaggedForBecome 6
#define CMOpenPIC 3
#define CmpC32R 113
#define CmpCqR 105
#define CmpCwR 112
#define CmpRdRd 131
#define CmpRR 99
#define CompletePrimitive 4
#define ConstZero 1
#define ConvertRdR 146
#define ConvertRRd 145
#define CounterBytes 4
#define CS 2
#define CSET 180
#define DC 169
#define DC_CIVAC 14
#define DC_CVAC 16
#define DC_CVAU 19
#if !defined(Debug) /* Allow this to be overridden on the compiler command line */
# define Debug DEBUGVM
#endif
#define DisplacementMask 0x1F
#define DisplacementX2N 0
#define DivRdRd 135
#define DivRRR 163
#define DPFPReg0 0
#define DPFPReg1 1
#define DPFPReg2 2
#define DPFPReg3 3
#define DPFPReg4 4
#define DPFPReg5 5
#define DPFPReg6 6
#define DPFPReg7 7
#define DSB 172
#define DSB_ALL 3
#define DSB_ISH 2
#define EncounteredUnknownBytecode -6
#define EQ 0
#define Extra0Reg 19
#define Extra1Reg 20
#define Extra2Reg 21
#define Fill32 4
#define FirstAnnotation 64
#define FirstJump 12
#define FirstSpecialSelector 176
#define FoxCallerSavedIP 8
#define FoxIFSavedIP -32
#define FoxMethod -8
#define FoxMFReceiver -24
#define FoxSavedFP 0
#define FoxThisContext -16
#define FP 29
#define FPReg 29
#define FullClosureCompiledBlockIndex 1
#define FullClosureFirstCopiedValueIndex 4
#define FullClosureReceiverIndex 3
#define GCModeBecome 8
#define GCModeFull 1
#define GCModeNewSpace 2
#define GE 10
#define GT 12
#define HasBytecodePC 5
#define HashMultiplyConstant 1664525
#define HashMultiplyMask 0xFFFFFFF
#define HeaderIndex 0
#define HI 8
#define IC 170
#define IC_IALLU 0
#define IC_IALLUIS 1
#define IC_IVAU 2
#if !defined(IMMUTABILITY) /* Allow this to be overridden on the compiler command line */
# define IMMUTABILITY 1
#endif
#define InFullBlock 2
#define InstanceSpecificationIndex 2
#define InstructionPointerIndex 1
#define InsufficientCodeSpace -2
#define InVanillaBlock 1
#define ISB 173
#define IsAbsPCReference 3
#define IsAnnotationExtension 1
#define IsDirectedSuperBindingSend 10
#define IsDirectedSuperSend 9
#define IsDisplacementX2N 0
#define IsNSDynamicSuperSend null
#define IsNSSelfSend null
#define IsNSSendCall null
#define IsObjectReference 2
#define IsRelativeCall 4
#define IsSendCall 7
#define IsSuperSend 8
#define Jump 16
#define JumpAbove 33
#define JumpAboveOrEqual 32
#define JumpBelow 31
#define JumpBelowOrEqual 34
#define JumpCarry 25
#define JumpFPEqual 35
#define JumpFPGreater 39
#define JumpFPGreaterOrEqual 40
#define JumpFPLess 37
#define JumpFPLessOrEqual 38
#define JumpFPNotEqual 36
#define JumpFPOrdered 41
#define JumpFPUnordered 42
#define JumpFull 12
#define JumpGreater 29
#define JumpGreaterOrEqual 28
#define JumpLess 27
#define JumpLessOrEqual 30
#define JumpLong 13
#define JumpLongNonZero 15
#define JumpLongZero 14
#define JumpMulOverflow 23
#define JumpNegative 19
#define JumpNoCarry 26
#define JumpNoMulOverflow 24
#define JumpNonNegative 20
#define JumpNonZero 18
#define JumpNoOverflow 22
#define JumpOverflow 21
#define JumpR 10
#define JumpZero 17
#define Label 1
#define LargeContextSlots 62
#define LastJump 42
#define LDAXR 182
#define LE 13
#define LinkReg 30
#define Literal 2
#define LiteralStart 1
#define LoadEffectiveAddressMwrR 88
#define LogicalAndS 3
#define LogicalOr 1
#define LogicalShiftLeftCqR 95
#define LogicalShiftLeftCqRR 129
#define LogicalShiftLeftRR 96
#define LogicalShiftRightCqR 93
#define LogicalShiftRightCqRR 130
#define LogicalShiftRightRR 94
#define LogicalXor 2
#define LR 30
#define LS 9
#define LT 11
#define MapEnd 0
#define MaxCompiledPrimitiveIndex 582
#define MaxCounterValue 0xFFFF
#define MaxCPICCases 6
#define MaxMethodSize 65535
#define MaxNegativeErrorCode -8
#define MaxStackAllocSize 1572864
#define MaxStackCheckOffset 0xFFF
#define MaxX2NDisplacement 992
#define MethodCacheClass 2
#define MethodCacheMask 0xFFC
#define MethodCacheMethod 3
#define MethodCacheSelector 1
#define MethodIndex 3
#define MethodTooBig -4
#define MFMethodFlagHasContextFlag 1
#define MFMethodFlagIsBlockFlag 2
#define MI 4
#define MoveA32R 45
#define MoveAbR 48
#define MoveAwR 44
#define MoveAwRR 165
#define MoveC32R 71
#define MoveCqR 69
#define MoveCwR 70
#define MoveM16rR 57
#define MoveM32rR 61
#define MoveM64rRd 75
#define MoveMbrR 65
#define MoveMwrR 50
#define MovePerfCnt64RL 159
#define MovePerfCnt64RRL 158
#define MoveRA32 47
#define MoveRAb 49
#define MoveRAw 46
#define MoveRdM64r 76
#define MoveRdR 73
#define MoveRM16r 58
#define MoveRM32r 62
#define MoveRMbr 66
#define MoveRMwr 51
#define MoveRR 43
#define MoveRRAw 166
#define MoveRRd 72
#define MoveRX16rR 60
#define MoveRX32rR 64
#define MoveRXbrR 68
#define MoveRXwrR 53
#define MoveX16rRR 59
#define MoveX32rRR 63
#define MoveXbrRR 67
#define MoveXwrRR 52
#define MRS_CTR_EL0 174
#define MRS_ID_AA64ISAR0_EL1 175
#define MSubRRR 164
#define MulOverflowRRR 161
#define MulRdRd 134
#define MulRRR 160
#define NativePopR 84
#define NativePopRR 168
#define NativePushR 85
#define NativePushRR 167
#define NativeRetN 86
#define NativeSPReg 31
#define NE 1
#define NeedsMergeFixupFlag 2
#define NeedsNonMergeFixupFlag 1
#define NegateR 89
#define NOP 3573751839U
#define Nop 5
#define NoReg -1
#define NotFullyInitialized -1
#define NumObjRefsInRuntime 0
#define NumOopsPerNSC 6
#define NumSendTrampolines 4
#define NumSpecialSelectors 32
#define NumStoreTrampolines 5
#define NumTrampolines (71 + (IMMUTABILITY ? 5 : 0))
#define OrCqR 109
#define OrCqRR 125
#define OrCwR 117
#define OrRR 103
#undef PCReg
#define PL 5
#define PopR 80
#define PrefetchAw 87
#define PrimCallCollectsProfileSamples 16
#define PrimCallIsExternalCall 32
#define PrimCallMayEndureCodeCompaction 8
#define PrimCallNeedsNewMethod 4
#define PrimCallOnSmalltalkStack 1
#define PrimCallOnSmalltalkStackAlign2x 2
#define PrimNumberExternalCall 117
#define PrimNumberFFICall 120
#define PushCq 82
#define PushCw 83
#define PushR 81
#define R0 0
#define R1 1
#define R17 17
#define ReceiverIndex 5
#define ReceiverResultReg 5
#define RetN 9
#define RISCTempReg 9
#define RotateLeftCqR 97
#define RotateRightCqR 98
#define SelectorCannotInterpret 34
#define SelectorDoesNotUnderstand 20
#define SenderIndex 0
#define SendNumArgsReg 6
#define ShouldNotJIT -8
#define SignExtend16RR 152
#define SignExtend32RR 153
#define SignExtend8RR 151
#define SistaV1BytecodeSet 1
#define SmallContextSlots 22
#define SP 31
#define SPReg 16
#define SpecialSelectors 23
#define SqrtRd 136
#define SSBaseOffset 1
#define SSConstant 2
#define SSRegister 3
#define SSSpill 4
#define STLR 184
#define STLXR 183
#define StackPointerIndex 2
#define Stop 11
#define SubbRR 121
#define SubCqR 107
#define SubCwR 115
#define SubRdRd 133
#define SubRR 101
#define SubRRR 127
#define SXTX 7
#define TempReg 0
#define TstCqR 110
#define UnfailingPrimitive 3
#define UnimplementedOperation 2
#define UnimplementedPrimitive -7
#define UXTX 3
#define ValueIndex 1
#define VarBaseReg 28
#define VC 7
#define VS 6
#define XorCqR 111
#define XorCwR 118
#define XorRdRd 137
#define XorRR 104
#define XZR 31
#define YoungSelectorInPIC -5
#define ZeroExtend16RR 155
#define ZeroExtend32RR 156
#define ZeroExtend8RR 154

typedef struct _AbstractInstruction {
	unsigned char	opcode;
	unsigned char	machineCodeSize;
	unsigned char	maxSize;
	unsigned char	annotation;
	unsigned int		machineCode[3];
	usqInt		operands[3];
	usqInt	address;
	struct _AbstractInstruction *dependent;
 } AbstractInstruction;

#define CogARMv8Compiler AbstractInstruction
#define CogAbstractInstruction AbstractInstruction


typedef struct {
	AbstractInstruction *fakeHeader;
	AbstractInstruction *fillInstruction;
	sqInt	numArgs;
	sqInt	numCopied;
	sqInt	numInitialNils;
	sqInt	startpc;
	AbstractInstruction *entryLabel;
	AbstractInstruction *stackCheckLabel;
	sqInt	span;
	sqInt	hasInstVarRef;
 } BlockStart;

#define CogBlockStart BlockStart


typedef struct _BytecodeDescriptor {
	sqInt (*generator)(void);
	sqInt NoDbgRegParms (*spanFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt);
	sqInt NoDbgRegParms (*needsFrameFunction)(sqInt);
	signed char	stackDelta;
	unsigned char	opcode;
	unsigned char	numBytes;
	unsigned		isBranchTrue : 1;
	unsigned		isBranchFalse : 1;
	unsigned		isReturn : 1;
	unsigned		isBlockCreation : 1;
	unsigned		isMapped : 1;
	unsigned		isMappedInBlock : 1;
	unsigned		isExtension : 1;
	unsigned		isInstVarRef : 1;
	unsigned		is1ByteInstVarStore : 1;
	unsigned		hasUnsafeJump : 1;
 } BytecodeDescriptor;

#define CogBytecodeDescriptor BytecodeDescriptor


typedef struct {
	sqInt (*primitiveGenerator)(void);
	sqInt	primNumArgs;
 } PrimitiveDescriptor;

#define CogPrimitiveDescriptor PrimitiveDescriptor


typedef struct {
	char	type;
	char	spilled;
	signed char	liveRegister;
	signed char	registerr;
	sqInt	offset;
	sqInt	constant;
	sqInt	bcptr;
 } SimStackEntry;

#define CogSimStackEntry SimStackEntry


typedef struct {
	AbstractInstruction *targetInstruction;
	unsigned char	simStackPtr;
	char	isTargetOfBackwardBranch;
	unsigned short	instructionIndex;
 } BytecodeFixup;

#define CogSSBytecodeFixup BytecodeFixup
#define CogBytecodeFixup BytecodeFixup


typedef struct {
	sqInt	isReceiverResultRegLive;
	CogSimStackEntry *ssEntry;
 } CogSSOptStatus;



/*** Function Prototypes ***/


#if !PRODUCTION && defined(PlatformNoDbgRegParms)
# define NoDbgRegParms PlatformNoDbgRegParms
#endif

#if !defined(NoDbgRegParms)
# define NoDbgRegParms /*empty*/
#endif



#if !defined(NeverInline)
# define NeverInline /*empty*/
#endif

static AbstractInstruction * NoDbgRegParms addDependent(AbstractInstruction * self_in_addDependent, AbstractInstruction *anInstruction);
static sqInt NoDbgRegParms availableFloatRegisterOrNoneFor(AbstractInstruction * self_in_availableFloatRegisterOrNoneFor, sqInt liveRegsMask);
static sqInt NoDbgRegParms availableRegisterOrNoneFor(AbstractInstruction * self_in_availableRegisterOrNoneFor, sqInt liveRegsMask);
static void NoDbgRegParms cloneLiteralFrom(AbstractInstruction * self_in_cloneLiteralFrom, AbstractInstruction *existingLiteral);
static sqInt NoDbgRegParms concretizeAt(AbstractInstruction * self_in_concretizeAt, sqInt actualAddress);
static sqInt NoDbgRegParms genLoadCStackPointer(AbstractInstruction * self_in_genLoadCStackPointer);
static sqInt NoDbgRegParms genLoadStackPointerForPrimCall(AbstractInstruction * self_in_genLoadStackPointerForPrimCall, sqInt spareReg);
static sqInt NoDbgRegParms genLoadStackPointersForPrimCall(AbstractInstruction * self_in_genLoadStackPointersForPrimCall, sqInt spareReg);
static AbstractInstruction * NoDbgRegParms genSwapRRScratch(AbstractInstruction * self_in_genSwapRRScratch, sqInt regA, sqInt regB, sqInt regTmp);
static void NoDbgRegParms genWriteCResultIntoReg(AbstractInstruction * self_in_genWriteCResultIntoReg, sqInt abstractRegister);
static void NoDbgRegParms initializeSharableLiteral(AbstractInstruction * self_in_initializeSharableLiteral, sqInt literal);
static void NoDbgRegParms initializeUniqueLiteral(AbstractInstruction * self_in_initializeUniqueLiteral, sqInt literal);
static sqInt NoDbgRegParms isWithinMwOffsetRange(AbstractInstruction * self_in_isWithinMwOffsetRange, sqInt anAddress);
static AbstractInstruction * NoDbgRegParms jmpTarget(AbstractInstruction * self_in_jmpTarget, AbstractInstruction *anAbstractInstruction);
static void NoDbgRegParms relocateJumpLongBeforeFollowingAddressby(AbstractInstruction * self_in_relocateJumpLongBeforeFollowingAddressby, sqInt pc, sqInt delta);
static void NoDbgRegParms relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction * self_in_relocateJumpLongConditionalBeforeFollowingAddressby, sqInt pc, sqInt delta);
static void NoDbgRegParms resolveJumpTarget(AbstractInstruction * self_in_resolveJumpTarget);
static sqInt NoDbgRegParms rewriteCallFullAttarget(AbstractInstruction * self_in_rewriteCallFullAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static sqInt NoDbgRegParms rewriteConditionalJumpLongAttarget(AbstractInstruction * self_in_rewriteConditionalJumpLongAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static sqInt NoDbgRegParms addrnrdimmshiftBy12(AbstractInstruction * self_in_addrnrdimmshiftBy12, sqInt rn, sqInt rd, sqInt offset, sqInt shiftBy12);
static sqInt NoDbgRegParms brlinkreg(AbstractInstruction * self_in_brlinkreg, sqInt link, sqInt reg);
static sqInt NoDbgRegParms callFullInstructionByteSize(AbstractInstruction * self_in_callFullInstructionByteSize);
static sqInt NoDbgRegParms callInstructionByteSize(AbstractInstruction * self_in_callInstructionByteSize);
static sqInt NoDbgRegParms callTargetFromReturnAddress(AbstractInstruction * self_in_callTargetFromReturnAddress, sqInt mcpc);
static sqInt NoDbgRegParms canDivQuoRem(AbstractInstruction * self_in_canDivQuoRem);
static sqInt NoDbgRegParms cmpC32RTempByteSize(AbstractInstruction * self_in_cmpC32RTempByteSize);
static sqInt NoDbgRegParms computeJumpTargetOffset(AbstractInstruction * self_in_computeJumpTargetOffset);
static sqInt NoDbgRegParms computeLowBit(AbstractInstruction * self_in_computeLowBit, sqInt nArg);
static sqInt NoDbgRegParms computeMaximumSize(AbstractInstruction * self_in_computeMaximumSize);
static sqInt NoDbgRegParms concretizeConditionalJump(AbstractInstruction * self_in_concretizeConditionalJump, sqInt conditionCode);
static sqInt NoDbgRegParms concretizeCwRArithmeticRd(AbstractInstruction * self_in_concretizeCwRArithmeticRd, sqInt arithOp, sqInt destRegOrXZR);
static sqInt NoDbgRegParms concretizeCwRLogical(AbstractInstruction * self_in_concretizeCwRLogical, sqInt op);
static sqInt NoDbgRegParms concretizeDataCacheControl(AbstractInstruction * self_in_concretizeDataCacheControl);
static sqInt NoDbgRegParms concretizeFill32(AbstractInstruction * self_in_concretizeFill32);
static sqInt NoDbgRegParms concretizeLiteral(AbstractInstruction * self_in_concretizeLiteral);
static sqInt NoDbgRegParms concretizeLoadEffectiveAddressMwrR(AbstractInstruction * self_in_concretizeLoadEffectiveAddressMwrR);
static sqInt NoDbgRegParms concretizeLogicalOpCqRDest(AbstractInstruction * self_in_concretizeLogicalOpCqRDest, sqInt op, sqInt destReg);
static sqInt NoDbgRegParms concretizeMoveCqR(AbstractInstruction * self_in_concretizeMoveCqR);
static sqInt NoDbgRegParms concretizeMoveMSrR(AbstractInstruction * self_in_concretizeMoveMSrR, sqInt unitSizeLog2MinusOne);
static sqInt NoDbgRegParms concretizeMovePerfCnt64RL(AbstractInstruction * self_in_concretizeMovePerfCnt64RL);
static sqInt NoDbgRegParms concretizeMoveRMSr(AbstractInstruction * self_in_concretizeMoveRMSr, sqInt unitSizeLog2MinusOne);
static sqInt NoDbgRegParms concretizeMoveRXSrR(AbstractInstruction * self_in_concretizeMoveRXSrR, sqInt unitSizeLog2MinusOne);
static sqInt NoDbgRegParms concretizeMoveXSrRR(AbstractInstruction * self_in_concretizeMoveXSrRR, sqInt unitSizeLog2MinusOne);
static sqInt NoDbgRegParms concretizeMulOverflowJump(AbstractInstruction * self_in_concretizeMulOverflowJump);
static sqInt NoDbgRegParms concretizeRRArithmeticRd(AbstractInstruction * self_in_concretizeRRArithmeticRd, sqInt arithOp, sqInt rd);
static sqInt NoDbgRegParms concretizeRRLogical(AbstractInstruction * self_in_concretizeRRLogical, sqInt logicalOp);
static sqInt NoDbgRegParms concretizeSignExtendRR(AbstractInstruction * self_in_concretizeSignExtendRR, sqInt width);
static sqInt NoDbgRegParms concretizeSTLR(AbstractInstruction * self_in_concretizeSTLR);
static sqInt NoDbgRegParms concretizeZeroExtendRR(AbstractInstruction * self_in_concretizeZeroExtendRR, sqInt width);
static sqInt NoDbgRegParms condoffset(AbstractInstruction * self_in_condoffset, sqInt cond, sqInt offset);
static sqInt NoDbgRegParms countLeadingOnes(AbstractInstruction * self_in_countLeadingOnes, sqInt anInteger);
static sqInt NoDbgRegParms countTrailingOnes(AbstractInstruction * self_in_countTrailingOnes, sqInt anInteger);
static sqInt NoDbgRegParms countTrailingZeros(AbstractInstruction * self_in_countTrailingZeros, sqInt anInteger);
static usqInt NoDbgRegParms decodeNimmsimmr(AbstractInstruction * self_in_decodeNimmsimmr, sqInt n, sqInt imms, sqInt immr);
#if __linux__
static void NoDbgRegParms detectFeaturesOnLinux(AbstractInstruction * self_in_detectFeaturesOnLinux);
#endif /* __linux__ */
#if !__APPLE__ && !__linux__
static void NoDbgRegParms detectFeaturesOnRawMachine(AbstractInstruction * self_in_detectFeaturesOnRawMachine);
#endif /* !__APPLE__ && !__linux__ */
static sqInt NoDbgRegParms dispatchConcretize(AbstractInstruction * self_in_dispatchConcretize);
static sqInt NoDbgRegParms emitLdfprnrtimmshiftBy12at(AbstractInstruction * self_in_emitLdfprnrtimmshiftBy12at, sqInt baseReg, sqInt targetDPReg, sqInt offset, sqInt shiftBy12, sqInt instrOffset);
static sqInt NoDbgRegParms emitLdrnrtimmshiftBy12at(AbstractInstruction * self_in_emitLdrnrtimmshiftBy12at, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt targetReg, sqInt offset, sqInt shiftBy12, sqInt instrOffset);
static sqInt NoDbgRegParms emitMoveCwintoRat(AbstractInstruction * self_in_emitMoveCwintoRat, usqInt constantArg, sqInt destReg, sqInt offsetBytes);
static sqInt NoDbgRegParms emitStrnrtimmshiftBy12(AbstractInstruction * self_in_emitStrnrtimmshiftBy12, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt sourceReg, sqInt offset, sqInt shiftBy12);
static sqInt NoDbgRegParms fullCallsAreRelative(AbstractInstruction * self_in_fullCallsAreRelative);
static AbstractInstruction * NoDbgRegParms genDivRRQuoRem(AbstractInstruction * self_in_genDivRRQuoRem, sqInt regDivisor, sqInt regDividend, sqInt regQuotient, sqInt regRemainder);
static void NoDbgRegParms generateDCacheFlush(AbstractInstruction * self_in_generateDCacheFlush);
static void NoDbgRegParms generateICacheFlush(AbstractInstruction * self_in_generateICacheFlush);
static sqInt NoDbgRegParms genLoadCStackPointers(AbstractInstruction * self_in_genLoadCStackPointers);
static void NoDbgRegParms genLoadNativeSPRegWithAlignedSPReg(AbstractInstruction * self_in_genLoadNativeSPRegWithAlignedSPReg);
static sqInt NoDbgRegParms genLoadStackPointers(AbstractInstruction * self_in_genLoadStackPointers);
static AbstractInstruction * NoDbgRegParms genMarshallNArgsargargargarg(AbstractInstruction * self_in_genMarshallNArgsargargargarg, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3);
static AbstractInstruction * NoDbgRegParms genMulOverflowRR(AbstractInstruction * self_in_genMulOverflowRR, sqInt regSource, sqInt regDest);
static AbstractInstruction * NoDbgRegParms genMulRR(AbstractInstruction * self_in_genMulRR, sqInt regSource, sqInt regDest);
static void NoDbgRegParms genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction * self_in_genPushRegisterArgsForAbortMissNumArgs, sqInt numArgs);
static void NoDbgRegParms genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction * self_in_genPushRegisterArgsForNumArgsscratchReg, sqInt numArgs, sqInt ignored);
static sqInt NoDbgRegParms genRemoveNArgsFromStack(AbstractInstruction * self_in_genRemoveNArgsFromStack, sqInt n);
static sqInt NoDbgRegParms genRestoreRegs(AbstractInstruction * self_in_genRestoreRegs, sqInt regMask);
static sqInt NoDbgRegParms genSaveRegs(AbstractInstruction * self_in_genSaveRegs, sqInt regMask);
static sqInt NoDbgRegParms genSaveStackPointers(AbstractInstruction * self_in_genSaveStackPointers);
static AbstractInstruction * NoDbgRegParms genSubstituteReturnAddress(AbstractInstruction * self_in_genSubstituteReturnAddress, sqInt retpc);
static sqInt NoDbgRegParms hasVarBaseRegister(AbstractInstruction * self_in_hasVarBaseRegister);
static usqInt NoDbgRegParms inlineCacheTagAt(AbstractInstruction * self_in_inlineCacheTagAt, sqInt callSiteReturnAddress);
static sqInt NoDbgRegParms instructionAddressBefore(AbstractInstruction * self_in_instructionAddressBefore, sqInt mcpc);
static unsigned int NoDbgRegParms instructionAt(AbstractInstruction * self_in_instructionAt, sqInt pc);
static usqInt NoDbgRegParms instructionBeforeAddress(AbstractInstruction * self_in_instructionBeforeAddress, sqInt followingAddress);
static sqInt NoDbgRegParms instructionIsADR(AbstractInstruction * self_in_instructionIsADR, sqInt word);
static sqInt NoDbgRegParms instructionIsImm26BorBL(AbstractInstruction * self_in_instructionIsImm26BorBL, usqInt word);
static sqInt NoDbgRegParms instructionIsLoadStore(AbstractInstruction * self_in_instructionIsLoadStore, sqInt instr);
static sqInt NoDbgRegParms instructionIsPCRelativeLoad(AbstractInstruction * self_in_instructionIsPCRelativeLoad, sqInt instr);
static sqInt NoDbgRegParms instructionSizeAt(AbstractInstruction * self_in_instructionSizeAt, sqInt pc);
static sqInt NoDbgRegParms inverseForArithOp(AbstractInstruction * self_in_inverseForArithOp, sqInt arithOp);
static sqInt NoDbgRegParms isAddressRelativeToVarBase(AbstractInstruction * self_in_isAddressRelativeToVarBase, usqInt varAddress);
static sqInt NoDbgRegParms isCallPrecedingReturnPC(AbstractInstruction * self_in_isCallPrecedingReturnPC, sqInt mcpc);
static sqInt NoDbgRegParms isFullJumpAtPC(AbstractInstruction * self_in_isFullJumpAtPC, sqInt mcpc);
static sqInt NoDbgRegParms isImm12orImm9offset(AbstractInstruction * self_in_isImm12orImm9offset, sqInt offset);
static sqInt NoDbgRegParms isInImmediateBranchAndLinkRange(AbstractInstruction * self_in_isInImmediateBranchAndLinkRange, sqIntptr_t offset);
static sqInt NoDbgRegParms isInImmediateBranchRange(AbstractInstruction * self_in_isInImmediateBranchRange, usqIntptr_t offset);
static sqInt NoDbgRegParms isInImmediateJumpRange(AbstractInstruction * self_in_isInImmediateJumpRange, usqIntptr_t operand);
static sqInt NoDbgRegParms isInImmediateOffsetRange(AbstractInstruction * self_in_isInImmediateOffsetRange, sqInt offset);
static sqInt NoDbgRegParms isJump(AbstractInstruction * self_in_isJump);
static sqInt NoDbgRegParms isJumpAt(AbstractInstruction * self_in_isJumpAt, sqInt pc);
static sqInt NoDbgRegParms isPCDependent(AbstractInstruction * self_in_isPCDependent);
static sqInt NoDbgRegParms isShiftedMask(AbstractInstruction * self_in_isShiftedMask, sqInt anInteger);
static sqInt NoDbgRegParms isUnsigned12BitMultipleOf8(AbstractInstruction * self_in_isUnsigned12BitMultipleOf8, sqInt anInteger);
static sqInt NoDbgRegParms jumpLongByteSize(AbstractInstruction * self_in_jumpLongByteSize);
static sqInt NoDbgRegParms jumpLongConditionalByteSize(AbstractInstruction * self_in_jumpLongConditionalByteSize);
static sqInt NoDbgRegParms jumpLongTargetBeforeFollowingAddress(AbstractInstruction * self_in_jumpLongTargetBeforeFollowingAddress, sqInt mcpc);
static usqInt NoDbgRegParms jumpTargetPCAt(AbstractInstruction * self_in_jumpTargetPCAt, sqInt pc);
static usqInt NoDbgRegParms literal32BeforeFollowingAddress(AbstractInstruction * self_in_literal32BeforeFollowingAddress, sqInt followingAddress);
static sqInt NoDbgRegParms literalBeforeFollowingAddress(AbstractInstruction * self_in_literalBeforeFollowingAddress, sqInt followingAddress);
static sqInt NoDbgRegParms literalLoadInstructionBytes(AbstractInstruction * self_in_literalLoadInstructionBytes);
static sqInt NoDbgRegParms loadLiteralByteSize(AbstractInstruction * self_in_loadLiteralByteSize);
static sqInt NoDbgRegParms loadPICLiteralByteSize(AbstractInstruction * self_in_loadPICLiteralByteSize);
static sqInt NoDbgRegParms machineCodeBytes(AbstractInstruction * self_in_machineCodeBytes);
static sqInt NoDbgRegParms machineCodeWords(AbstractInstruction * self_in_machineCodeWords);
static sqInt NoDbgRegParms movernrd(AbstractInstruction * self_in_movernrd, sqInt srcReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms noteFollowingConditionalBranch(AbstractInstruction * self_in_noteFollowingConditionalBranch, AbstractInstruction *branch);
static sqInt NoDbgRegParms numDCacheFlushOpcodes(AbstractInstruction * self_in_numDCacheFlushOpcodes);
static sqInt NoDbgRegParms numICacheFlushOpcodes(AbstractInstruction * self_in_numICacheFlushOpcodes);
static sqInt NoDbgRegParms numIntRegArgs(AbstractInstruction * self_in_numIntRegArgs);
static sqInt NoDbgRegParms outOfLineLiteralOpcodeLimit(AbstractInstruction * self_in_outOfLineLiteralOpcodeLimit);
static void NoDbgRegParms padIfPossibleWithStopsFromto(AbstractInstruction * self_in_padIfPossibleWithStopsFromto, sqInt startAddr, sqInt endAddr);
static sqInt NoDbgRegParms pcRelativeAddressAt(AbstractInstruction * self_in_pcRelativeAddressAt, sqInt mcpc);
static sqInt NoDbgRegParms prnimmshiftBy12(AbstractInstruction * self_in_prnimmshiftBy12, sqInt baseReg, sqInt offset, sqInt shiftBy12);
static sqInt NoDbgRegParms pushLinkRegisterByteSize(AbstractInstruction * self_in_pushLinkRegisterByteSize);
static void NoDbgRegParms relocateCallBeforeReturnPCby(AbstractInstruction * self_in_relocateCallBeforeReturnPCby, sqInt retpc, sqInt delta);
static void NoDbgRegParms relocateMethodReferenceBeforeAddressby(AbstractInstruction * self_in_relocateMethodReferenceBeforeAddressby, sqInt pc, sqInt delta);
static sqInt NoDbgRegParms rewriteCallAttarget(AbstractInstruction * self_in_rewriteCallAttarget, usqInt callSiteReturnAddress, usqInt callTargetAddress);
static sqInt NoDbgRegParms rewriteImm19JumpBeforetarget(AbstractInstruction * self_in_rewriteImm19JumpBeforetarget, sqInt followingAddress, sqInt targetAddress);
static sqInt NoDbgRegParms rewriteImm26JumpBeforetarget(AbstractInstruction * self_in_rewriteImm26JumpBeforetarget, sqInt followingAddress, sqInt targetAddress);
static sqInt NoDbgRegParms rewriteInlineCacheAttagtarget(AbstractInstruction * self_in_rewriteInlineCacheAttagtarget, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress);
static void NoDbgRegParms rewriteInlineCacheTagat(AbstractInstruction * self_in_rewriteInlineCacheTagat, sqInt cacheTag, sqInt callSiteReturnAddress);
static sqInt NoDbgRegParms rewriteJumpFullAttarget(AbstractInstruction * self_in_rewriteJumpFullAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress);
static void NoDbgRegParms setLiteralSize(AbstractInstruction * self_in_setLiteralSize, sqInt sizeOfLiteral);
static sqInt NoDbgRegParms setsConditionCodesFor(AbstractInstruction * self_in_setsConditionCodesFor, sqInt aConditionalJumpOpcode);
static usqInt NoDbgRegParms sizePCDependentInstructionAt(AbstractInstruction * self_in_sizePCDependentInstructionAt, sqInt eventualAbsoluteAddress);
static void NoDbgRegParms stopsFromto(AbstractInstruction * self_in_stopsFromto, sqInt startAddr, sqInt endAddr);
static void NoDbgRegParms storeLiteral32beforeFollowingAddress(AbstractInstruction * self_in_storeLiteral32beforeFollowingAddress, sqInt literal, sqInt followingAddress);
static void NoDbgRegParms storeLiteralbeforeFollowingAddress(AbstractInstruction * self_in_storeLiteralbeforeFollowingAddress, sqInt literal, sqInt followingAddress);
static sqInt NoDbgRegParms strnrtimmshiftBy12(AbstractInstruction * self_in_strnrtimmshiftBy12, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt targetReg, sqInt offset, sqInt shiftBy12);
static sqInt NoDbgRegParms usesOutOfLineLiteral(AbstractInstruction * self_in_usesOutOfLineLiteral);
static sqInt NoDbgRegParms zoneCallsAreRelative(AbstractInstruction * self_in_zoneCallsAreRelative);
static CogMethod * NoDbgRegParms cmHomeMethod(CogBlockMethod * self_in_cmHomeMethod);
static sqInt NoDbgRegParms isCMBlock(CogBlockMethod * self_in_isCMBlock);
static sqInt NoDbgRegParms isCMClosedPIC(CogBlockMethod * self_in_isCMClosedPIC);
static sqInt NoDbgRegParms isCMFree(CogBlockMethod * self_in_isCMFree);
static sqInt NoDbgRegParms isCMMethodEtAl(CogBlockMethod * self_in_isCMMethodEtAl);
static sqInt NoDbgRegParms isCMOpenPIC(CogBlockMethod * self_in_isCMOpenPIC);
static sqInt NoDbgRegParms isBranch(BytecodeDescriptor * self_in_isBranch);
static AbstractInstruction * NoDbgRegParms gAddCqR(sqInt quickConstant, sqInt reg);
static AbstractInstruction * NoDbgRegParms gAddCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms gAddRRR(sqInt addendReg, sqInt badendReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms gAndCqR(sqInt quickConstant, sqInt reg);
static AbstractInstruction * NoDbgRegParms gAndCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
extern sqInt abortOffset(void);
static void addCleanBlockStarts(void);
extern void addCogMethodsToHeapMap(void);
static sqInt NoDbgRegParms addressIsInCurrentCompilation(sqInt address);
static sqInt NoDbgRegParms addressIsInFixups(BytecodeFixup *address);
static sqInt NoDbgRegParms addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC);
static void alignMethodZoneBase(void);
static sqInt NoDbgRegParms alignUptoRoutineBoundary(sqInt anAddress);
static sqInt allMachineCodeObjectReferencesValid(void);
static sqInt allMethodsHaveCorrectHeader(void);
static AbstractInstruction * NoDbgRegParms annotateAbsolutePCRef(AbstractInstruction *abstractInstruction);
static AbstractInstruction * NoDbgRegParms annotateBytecode(AbstractInstruction *abstractInstruction);
static AbstractInstruction * NoDbgRegParms annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop);
static void NoDbgRegParms assertSaneJumpTarget(AbstractInstruction *jumpTarget);
static sqInt NoDbgRegParms availableRegisterOrNoneIn(sqInt liveRegsMask);
static sqInt NoDbgRegParms blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg);
extern sqInt bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static AbstractInstruction * NoDbgRegParms CallRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved);
static AbstractInstruction * NoDbgRegParms gCall(sqInt callTarget);
static AbstractInstruction * NoDbgRegParms gCmpCqR(sqInt quickConstant, sqInt reg);
static void callCogCodePopReceiver(void);
static void callCogCodePopReceiverAndClassRegs(void);
static sqInt NoDbgRegParms ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver);
static void NoDbgRegParms ceFree(void *pointer);
static void* NoDbgRegParms ceMalloc(size_t size);
static sqInt NoDbgRegParms ceSICMiss(sqInt receiver);
static sqInt NoDbgRegParms checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
static sqInt NoDbgRegParms checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern sqInt checkIntegrityOfObjectReferencesInCode(sqInt gcModes);
static sqInt NoDbgRegParms checkMaybeObjRefInClosedPIC(sqInt maybeObject);
static sqInt NoDbgRegParms checkValidObjectReferencesInClosedPIC(CogMethod *cPIC);
static sqInt NoDbgRegParms NeverInline cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg);
static sqInt NoDbgRegParms closedPICRefersToUnmarkedObject(CogMethod *cPIC);
extern char * codeEntryFor(char *address);
extern char * codeEntryNameFor(char *address);
extern sqInt cogCodeBase(void);
extern sqInt cogCodeConstituents(sqInt withDetails);
static void NoDbgRegParms cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase);
extern CogMethod * cogFullBlockMethodnumCopied(sqInt aMethodObj, sqInt numCopied);
extern void cogitPostGCAction(sqInt gcMode);
static sqInt NoDbgRegParms cogMethodDoesntLookKosher(CogMethod *cogMethod);
extern CogMethod * cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs);
static CogMethod * NoDbgRegParms cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs);
static CogMethod * NoDbgRegParms cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase);
extern CogMethod * cogselector(sqInt aMethodObj, sqInt aSelectorOop);
static sqInt NoDbgRegParms collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static sqInt NoDbgRegParms collectCogMethodConstituent(CogMethod *cogMethod);
extern void compactCogCompiledCode(void);
static void compactPICsWithFreedTargets(void);
static AbstractInstruction * compileAbort(void);
static sqInt NoDbgRegParms compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex);
static void NoDbgRegParms compileBlockEntry(BlockStart *blockStart);
static void NoDbgRegParms compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask);
static AbstractInstruction * compileCPICEntry(void);
static sqInt NoDbgRegParms compileEntireFullBlockMethod(sqInt numCopied);
static void compileEntry(void);
static sqInt compileFullBlockEntry(void);
static sqInt compileMethodBody(void);
static sqInt NoDbgRegParms compilePICAbort(sqInt numArgs);
static AbstractInstruction * NoDbgRegParms compileStackOverflowCheck(sqInt canContextSwitch);
static void NoDbgRegParms compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone);
static void computeEntryOffsets(void);
static void computeFullBlockEntryOffsets(void);
static usqInt computeGoodVarBaseAddress(void);
static void computeMaximumSizes(void);
static sqInt NoDbgRegParms configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta);
static sqInt NoDbgRegParms configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta);
static sqInt NoDbgRegParms cPICCompactAndIsNowEmpty(CogMethod *cPIC);
static sqInt NoDbgRegParms cPICHasForwardedClass(CogMethod *cPIC);
static sqInt NoDbgRegParms cPICHasFreedTargets(CogMethod *cPIC);
static usqInt cPICPrototypeCaseOffset(void);
static sqInt NoDbgRegParms cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod);
static sqInt NoDbgRegParms createCPICData(CogMethod *cPIC);
static AbstractInstruction * NoDbgRegParms gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder);
static sqInt NoDbgRegParms deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader);
static sqInt NoDbgRegParms endPCOf(sqInt aMethod);
static void enterCogCodePopReceiver(void);
static sqInt NoDbgRegParms entryPointTagIsSelector(sqInt entryPoint);
static sqInt NoDbgRegParms expectedClosedPICPrototype(CogMethod *cPIC);
static sqInt extABytecode(void);
static sqInt extBBytecode(void);
static sqInt NoDbgRegParms fillInBlockHeadersAt(sqInt startAddress);
static sqInt NoDbgRegParms findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc);
static usqInt NoDbgRegParms findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc);
static usqInt NoDbgRegParms findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod);
extern CogBlockMethod * findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod);
static sqInt NoDbgRegParms findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc);
static sqInt NoDbgRegParms firstMappedPCFor(CogMethod *cogMethod);
static sqInt firstPrototypeMethodOop(void);
static BytecodeFixup * NoDbgRegParms fixupAt(sqInt fixupPC);
extern void flagCogMethodForBecome(CogMethod *cogMethod);
static void NoDbgRegParms followForwardedLiteralsImplementationIn(CogMethod *cogMethod);
extern void followForwardedLiteralsIn(CogMethod *cogMethod);
extern void followMovableLiteralsAndUpdateYoungReferrers(void);
extern void freeBecomeFlaggedMethods(void);
extern void freeCogMethod(CogMethod *cogMethod);
extern void freeUnmarkedMachineCode(void);
static AbstractInstruction * NoDbgRegParms genCallMustBeBooleanFor(sqInt boolean);
static AbstractInstruction * NoDbgRegParms genConditionalBranchoperand(sqInt opcode, sqInt operandOne);
static void (*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void);
static void NoDbgRegParms genEnilopmartReturn(sqInt forCall);
static void NoDbgRegParms NeverInline generateCaptureCStackPointers(sqInt captureFramePointer);
static void generateClosedPICPrototype(void);
static CogMethod * generateCogFullBlock(void);
static CogMethod * NoDbgRegParms generateCogMethod(sqInt selector);
static sqInt NoDbgRegParms generateMapAtstart(usqInt addressOrNull, usqInt startAddress);
static void generateOpenPICPrototype(void);
static void generateRunTimeTrampolines(void);
static void generateStackPointerCapture(void);
static void generateTrampolines(void);
static BytecodeDescriptor * NoDbgRegParms generatorForPC(sqInt pc);
static void genGetLeafCallStackPointers(void);
static usqInt NoDbgRegParms genInnerPICAbortTrampoline(char *name);
static void (*genInvokeInterpretTrampoline(void))(void);
static void NoDbgRegParms genLoadInlineCacheWithSelector(sqInt selectorIndex);
static usqInt genReturnToInterpreterTrampoline(void);
static sqInt NoDbgRegParms genSmalltalkToCStackSwitch(sqInt pushLinkReg);
static usqInt NoDbgRegParms genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean);
static void NoDbgRegParms genTrampolineReturn(sqInt lnkRegWasPushed);
static AbstractInstruction * NoDbgRegParms gen(sqInt opcode);
static AbstractInstruction * NoDbgRegParms genoperand(sqInt opcode, sqInt operand);
static AbstractInstruction * NoDbgRegParms genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo);
static AbstractInstruction * NoDbgRegParms genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree);
static sqInt NoDbgRegParms getLiteral(sqInt litIndex);
static sqInt getOpcodeIndex(void);
static sqInt NoDbgRegParms incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static sqInt NoDbgRegParms indexForSelectorin(sqInt selector, CogMethod *cogMethod);
static sqInt initialClosedPICUsageCount(void);
static void initializeBackend(void);
static sqInt initialMethodUsageCount(void);
static sqInt initialOpenPICUsageCount(void);
static sqInt NoDbgRegParms inverseBranchFor(sqInt opcode);
static sqInt NoDbgRegParms isPCMappedAnnotation(sqInt annotation);
static sqInt NoDbgRegParms isPCWithinMethodZone(void *address);
extern sqInt isSendReturnPC(sqInt retpc);
static AbstractInstruction * NoDbgRegParms gJumpFPEqual(void *jumpTarget);
static AbstractInstruction * NoDbgRegParms gJumpFPGreaterOrEqual(void *jumpTarget);
static AbstractInstruction * NoDbgRegParms gJumpFPGreater(void *jumpTarget);
static AbstractInstruction * NoDbgRegParms gJumpFPNotEqual(void *jumpTarget);
static AbstractInstruction * NoDbgRegParms gLogicalShiftLeftCqR(sqInt quickConstant, sqInt reg);
static AbstractInstruction * NoDbgRegParms gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms gLogicalShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static AbstractInstruction * lastOpcode(void);
extern void linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver);
static BytecodeDescriptor * loadBytesAndGetDescriptor(void);
static void NoDbgRegParms loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc);
static AbstractInstruction * NoDbgRegParms gMoveAwR(sqInt address, sqInt reg);
static AbstractInstruction * NoDbgRegParms gMoveCwR(sqInt wordConstant, sqInt reg);
static AbstractInstruction * NoDbgRegParms gMovePerfCnt64RL(sqInt destReg, sqInt liveRegisterMask);
static AbstractInstruction * NoDbgRegParms gMovePerfCnt64RRL(sqInt destRegLo, sqInt destRegHi, sqInt liveRegisterMask);
static usqInt NoDbgRegParms mapEndFor(CogMethod *cogMethod);
static sqInt NoDbgRegParms mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg);
static sqInt NoDbgRegParms mapObjectReferencesInClosedPIC(CogMethod *cPIC);
static void mapObjectReferencesInGeneratedRuntime(void);
static void mapObjectReferencesInMachineCodeForBecome(void);
static void mapObjectReferencesInMachineCodeForFullGC(void);
static void mapObjectReferencesInMachineCodeForYoungGC(void);
extern void mapObjectReferencesInMachineCode(sqInt gcMode);
extern void markAndTraceMachineCodeOfMarkedMethods(void);
static void markAndTraceObjectReferencesInGeneratedRuntime(void);
static sqInt NoDbgRegParms markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit);
static sqInt NoDbgRegParms markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC);
static sqInt NoDbgRegParms markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
static sqInt NoDbgRegParms markLiteralspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod);
extern void markMethodAndReferents(CogBlockMethod *aCogMethod);
extern usqInt maxCogMethodAddress(void);
static sqInt NoDbgRegParms maximumDistanceFromCodeZone(sqInt anAddress);
static sqInt maybeAllocAndInitIRCs(void);
static sqInt NoDbgRegParms maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod);
static sqInt mclassCouldBeContext(void);
static sqInt mclassIsSmallInteger(void);
extern usqInt mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod);
static sqInt NoDbgRegParms methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral);
extern sqInt mnuOffset(void);
static AbstractInstruction * NoDbgRegParms gNativePopR(sqInt reg);
static AbstractInstruction * NoDbgRegParms gNativePushR(sqInt reg);
static AbstractInstruction * NoDbgRegParms gNativeRetN(sqInt offset);
static sqInt NoDbgRegParms needsFrameIfImmutability(sqInt stackDelta);
static sqInt NoDbgRegParms needsFrameIfInBlock(sqInt stackDelta);
static sqInt NoDbgRegParms needsFrameNever(sqInt stackDelta);
static sqInt NoDbgRegParms noAssertMethodClassAssociationOf(sqInt methodPointer);
static sqInt noCogMethodsMaximallyMarked(void);
static sqInt NoDbgRegParms noTargetsFreeInClosedPIC(CogMethod *cPIC);
static AbstractInstruction * NoDbgRegParms gOrCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms outputInstructionsAt(sqInt startAddress);
static sqInt NoDbgRegParms outputInstructionsForGeneratedRuntimeAt(sqInt startAddress);
static AbstractInstruction * NoDbgRegParms gPushCw(sqInt wordConstant);
extern sqInt patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver);
static sqInt picAbortDiscriminatorValue(void);
static sqInt picInterpretAbortOffset(void);
static AbstractInstruction * previousInstruction(void);
extern void printCogMethodFor(void *address);
extern void printTrampolineTable(void);
static sqInt processorHasDivQuoRemAndMClassIsSmallInteger(void);
static sqInt processorHasDoublePrecisionFloatingPointSupport(void);
static sqInt processorHasMultiplyAndMClassIsSmallInteger(void);
static void NoDbgRegParms recordGeneratedRunTimeaddress(char *aString, sqInt address);
extern sqInt recordPrimTraceFunc(void);
static void recordRunTimeObjectReferences(void);
static sqInt NoDbgRegParms registerMaskFor(sqInt reg);
static sqInt NoDbgRegParms registerMaskForand(sqInt reg1, sqInt reg2);
static sqInt NoDbgRegParms registerMaskForandandand(sqInt reg1, sqInt reg2, sqInt reg3, sqInt reg4);
static void NoDbgRegParms relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod);
static void NoDbgRegParms relocateCallsInClosedPIC(CogMethod *cPIC);
static sqInt NoDbgRegParms relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg);
static sqInt NoDbgRegParms remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr);
static sqInt NoDbgRegParms remapMaybeObjRefInClosedPICAt(sqInt mcpc);
static void NoDbgRegParms rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget);
static AbstractInstruction * NoDbgRegParms gSubCwR(sqInt wordConstant, sqInt reg);
static AbstractInstruction * NoDbgRegParms gSubRRR(sqInt subReg, sqInt fromReg, sqInt destReg);
static sqInt scanForCleanBlocks(void);
extern void setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop);
static sqInt NoDbgRegParms spanForCleanBlockStartingAt(sqInt startPC);
static usqInt NoDbgRegParms stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc);
static sqInt subsequentPrototypeMethodOop(void);
static AbstractInstruction * NoDbgRegParms gTstCqR(sqInt quickConstant, sqInt reg);
extern sqInt traceLinkedSendOffset(void);
static char * NoDbgRegParms trampolineNamenumArgs(char *routinePrefix, sqInt numArgs);
static char * NoDbgRegParms trampolineNamenumArgslimit(char *routinePrefix, int numArgs, sqInt argsLimit);
static char * NoDbgRegParms trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs);
extern void unflagBecomeFlaggedMethods(void);
static sqInt unknownBytecode(void);
extern void unlinkAllSends(void);
static sqInt NoDbgRegParms unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector);
static sqInt NoDbgRegParms unlinkIfInvalidClassSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static sqInt NoDbgRegParms unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static sqInt NoDbgRegParms unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg);
static sqInt NoDbgRegParms unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity);
static sqInt NoDbgRegParms unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod);
extern void unlinkSendsLinkedForInvalidClasses(void);
extern void unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector);
static void unlinkSendsToFree(void);
extern void unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue);
extern void unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue);
extern void voidCogCompiledCode(void);
static AbstractInstruction * NoDbgRegParms gXorCwR(sqInt wordConstant, sqInt reg);
static void zeroOpcodeIndex(void);
static void zeroOpcodeIndexForNewOpcodes(void);
static void NoDbgRegParms addToOpenPICList(CogMethod *anOpenPIC);
static void NoDbgRegParms addToYoungReferrers(CogMethod *writableCogMethod);
static usqInt NoDbgRegParms allocate(sqInt numBytes);
extern CogMethod * cogMethodContaining(usqInt mcpc);
static void compactCompiledCode(void);
static void NoDbgRegParms ensureInYoungReferrers(CogMethod *cogMethod);
static void followForwardedLiteralsInOpenPICList(void);
static void NoDbgRegParms freeMethod(CogMethod *cogMethod);
static void freeOlderMethodsForCompaction(void);
extern sqInt kosherYoungReferrers(void);
static sqInt NoDbgRegParms mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod);
extern CogMethod * methodFor(void *address);
extern sqInt methodsCompiledToMachineCodeInto(sqInt arrayObj);
extern sqInt numMethods(void);
extern sqInt numMethodsOfType(sqInt cogMethodType);
static sqInt NoDbgRegParms occurrencesInYoungReferrers(CogMethod *cogMethod);
static CogMethod * NoDbgRegParms openPICWithSelector(sqInt aSelector);
static void planCompaction(void);
extern void printCogMethods(void);
extern void printCogMethodsOfType(sqInt cmType);
extern void printCogMethodsWithMethod(sqInt methodOop);
extern void printCogMethodsWithPrimitive(sqInt primIdx);
extern void printCogMethodsWithSelector(sqInt selectorOop);
extern void printCogYoungReferrers(void);
extern sqInt printOpenPICList(void);
static sqInt pruneYoungReferrers(void);
static sqInt relocateAndPruneYoungReferrers(void);
static sqInt relocateMethodsPreCompaction(void);
static sqInt NoDbgRegParms removeFromOpenPICList(CogMethod *anOpenPIC);
static void NoDbgRegParms restorePICUsageCount(CogMethod *cogMethod);
static sqInt NoDbgRegParms roundUpLength(sqInt numBytes);
static void NoDbgRegParms savePICUsageCount(CogMethod *cogMethod);
static void voidOpenPICList(void);
static void voidUnpairedMethodList(void);
static void voidYoungReferrersPostTenureAll(void);
extern char * whereIsMaybeCodeThing(sqInt anOop);
static sqInt zoneAlignment(void);
static sqInt NoDbgRegParms checkValidObjectReference(sqInt anOop);
static AbstractInstruction * NoDbgRegParms genCmpClassFloatCompactIndexR(sqInt reg);
static AbstractInstruction * NoDbgRegParms genCmpClassMethodContextCompactIndexR(sqInt reg);
static sqInt NoDbgRegParms genGetMethodHeaderOfintoscratch(sqInt methodReg, sqInt headerReg, sqInt scratchReg);
static sqInt NoDbgRegParms genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg);
static sqInt genPrimitiveAdd(void);
static sqInt genPrimitiveAsFloat(void);
static sqInt genPrimitiveBitAnd(void);
static sqInt genPrimitiveBitOr(void);
static sqInt genPrimitiveBitShift(void);
static sqInt genPrimitiveBitXor(void);
static sqInt genPrimitiveClass(void);
static sqInt genPrimitiveDiv(void);
static sqInt genPrimitiveDivide(void);
static sqInt genPrimitiveEqual(void);
static sqInt genPrimitiveFloatAdd(void);
static sqInt genPrimitiveFloatDivide(void);
static sqInt genPrimitiveFloatMultiply(void);
static sqInt genPrimitiveFloatSquareRoot(void);
static sqInt genPrimitiveFloatSubtract(void);
static sqInt genPrimitiveGreaterOrEqual(void);
static sqInt genPrimitiveGreaterThan(void);
static sqInt genPrimitiveHighBit(void);
static sqInt genPrimitiveIdentical(void);
static sqInt genPrimitiveLessOrEqual(void);
static sqInt genPrimitiveLessThan(void);
static sqInt genPrimitiveMod(void);
static sqInt genPrimitiveMultiply(void);
static sqInt genPrimitiveNewMethod(void);
static sqInt genPrimitiveNotEqual(void);
static sqInt genPrimitiveNotIdentical(void);
static sqInt genPrimitiveQuo(void);
static sqInt genPrimitiveSmallFloatAdd(void);
static sqInt genPrimitiveSmallFloatDivide(void);
static sqInt genPrimitiveSmallFloatEqual(void);
static sqInt genPrimitiveSmallFloatGreaterOrEqual(void);
static sqInt genPrimitiveSmallFloatGreaterThan(void);
static sqInt genPrimitiveSmallFloatLessOrEqual(void);
static sqInt genPrimitiveSmallFloatLessThan(void);
static sqInt genPrimitiveSmallFloatMultiply(void);
static sqInt genPrimitiveSmallFloatNotEqual(void);
static sqInt genPrimitiveSmallFloatSquareRoot(void);
static sqInt genPrimitiveSmallFloatSubtract(void);
static sqInt genPrimitiveSubtract(void);
static sqInt NoDbgRegParms genSmallIntegerComparison(sqInt jumpOpcode);
static sqInt NoDbgRegParms isUnannotatableConstant(CogSimStackEntry *simStackEntry);
static sqInt NoDbgRegParms allImmediatebranchIfinstanceOfBehaviorstarget(sqInt immediateMask, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms allImmediatebranchIfnotInstanceOfBehaviorstarget(sqInt immediateMask, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms classForInlineCacheTag(sqInt classIndex);
static sqInt NoDbgRegParms genAddSmallIntegerTagsTo(sqInt aRegister);
static AbstractInstruction * NoDbgRegParms genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static AbstractInstruction * NoDbgRegParms genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static AbstractInstruction * NoDbgRegParms genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2);
static sqInt NoDbgRegParms genClearAndSetSmallIntegerTagsIn(sqInt scratchReg);
static sqInt NoDbgRegParms genConvertBitsToSmallFloatInscratch(sqInt reg, sqInt scratch);
static void NoDbgRegParms genConvertCharacterToSmallIntegerInReg(sqInt reg);
static sqInt NoDbgRegParms genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms genConvertIntegerToSmallIntegerInReg(sqInt reg);
static sqInt NoDbgRegParms genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(sqInt reg, sqInt scratch);
static void NoDbgRegParms genConvertSmallIntegerToCharacterInReg(sqInt reg);
static sqInt NoDbgRegParms genConvertSmallIntegerToIntegerInReg(sqInt reg);
static sqInt NoDbgRegParms genFetchRegArgsForPerformWithArguments(sqInt sizeReg);
static sqInt NoDbgRegParms genFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed);
static sqInt NoDbgRegParms genFloatComparisonorIntegerComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt jumpOpcode, sqInt invertComparison, sqInt rcvrBoxed);
static sqInt NoDbgRegParms genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg);
static sqInt NoDbgRegParms genGetHashFieldNonImmOfinto(sqInt instReg, sqInt destReg);
static AbstractInstruction * NoDbgRegParms genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry);
static sqInt NoDbgRegParms genGetNumBytesOfinto(sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms genGetOverflowSlotsOfinto(sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms genGetSmallFloatValueOfscratchinto(sqInt oopReg, sqInt scratch, sqInt dpReg);
static AbstractInstruction * NoDbgRegParms genJumpCharacterInScratchReg(sqInt aRegister);
static AbstractInstruction * NoDbgRegParms genJumpCharacter(sqInt reg);
static AbstractInstruction * NoDbgRegParms genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static AbstractInstruction * NoDbgRegParms genJumpNotCharacter(sqInt reg);
static AbstractInstruction * NoDbgRegParms genJumpNotSmallFloatValueBitsscratch(sqInt reg, sqInt exponent);
static AbstractInstruction * NoDbgRegParms genJumpNotSmallFloat(sqInt reg);
static AbstractInstruction * NoDbgRegParms genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg);
static AbstractInstruction * NoDbgRegParms genJumpNotSmallInteger(sqInt reg);
static AbstractInstruction * NoDbgRegParms genJumpSmallFloatInScratchReg(sqInt aRegister);
static AbstractInstruction * NoDbgRegParms genJumpSmallFloat(sqInt aRegister);
static AbstractInstruction * NoDbgRegParms genJumpSmallIntegerInScratchReg(sqInt aRegister);
static AbstractInstruction * NoDbgRegParms genJumpSmallInteger(sqInt aRegister);
static sqInt NoDbgRegParms genPrimitiveAtPutSigned(sqInt signedVersion);
static sqInt NoDbgRegParms genPrimitiveAtSigned(sqInt signedVersion);
static sqInt genPrimitiveFloatEqual(void);
static sqInt genPrimitiveFloatGreaterOrEqual(void);
static sqInt genPrimitiveFloatGreaterThan(void);
static sqInt genPrimitiveFloatLessOrEqual(void);
static sqInt genPrimitiveFloatLessThan(void);
static sqInt genPrimitiveFloatNotEqual(void);
static sqInt genPrimitiveIdentityHash(void);
static sqInt genPrimitiveImmediateAsInteger(void);
static sqInt genPrimitiveNew(void);
static sqInt genPrimitiveNewWithArg(void);
static sqInt genPrimitiveShallowCopy(void);
static sqInt genPrimitiveSlotAt(void);
static sqInt genPrimitiveSlotAtPut(void);
static sqInt genPrimitiveStringAt(void);
static sqInt genPrimitiveStringAtPut(void);
static sqInt genPrimitiveUninitializedNewWithArg(void);
static sqInt NoDbgRegParms genPureFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed);
static sqInt NoDbgRegParms genPureFloatComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt invertComparison, sqInt rcvrBoxed);
static sqInt NoDbgRegParms genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg);
static sqInt NoDbgRegParms genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg);
static sqInt NoDbgRegParms genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison);
static sqInt NoDbgRegParms getLiteralCountOfplusOneinBytesintoscratch(sqInt methodReg, sqInt plusOne, sqInt inBytes, sqInt litCountReg, sqInt scratchReg);
static sqInt NoDbgRegParms inlineCacheTagForInstance(sqInt oop);
static sqInt log2BytesPerWord(void);
static void maybeGenerateSelectorIndexDereferenceRoutine(void);
static usqInt NoDbgRegParms numCountersFor(usqInt theCounters);
static sqInt numSmallIntegerBits(void);
static sqInt numSmallIntegerTagBits(void);
static sqInt NoDbgRegParms validInlineCacheTag(sqInt classIndexOrTagPattern);
static sqInt NoDbgRegParms branchIfinstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms branchIfnotInstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static void callStoreCheckTrampoline(void);
static sqInt NoDbgRegParms checkValidDerivedObjectReference(sqInt bodyAddress);
static sqInt NoDbgRegParms checkValidOopReference(sqInt anOop);
static sqInt NoDbgRegParms couldBeDerivedObject(sqInt bodyAddress);
static sqInt NoDbgRegParms couldBeObject(sqInt literal);
static usqInt NoDbgRegParms genActiveContextTrampolineLargeinBlockcalled(sqInt isLarge, sqInt isInBlock, char *aString);
static AbstractInstruction * NoDbgRegParms genCheckRememberedBitOfscratch(sqInt objReg, sqInt scratchReg);
static void NoDbgRegParms genCmpClassIndexR(sqInt classIndex, sqInt reg);
static sqInt NoDbgRegParms genConvertCharacterToCodeInReg(sqInt reg);
static sqInt NoDbgRegParms genConvertIntegerToCharacterInReg(sqInt reg);
static sqInt NoDbgRegParms genCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock);
static sqInt NoDbgRegParms genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(sqInt compiledBlock, sqInt numArgs, sqInt numCopied, sqInt ignoreContext, sqInt contextNumArgs, sqInt contextIsLarge, sqInt contextIsBlock);
static sqInt NoDbgRegParms genEnsureObjInRegNotForwardedscratchReg(sqInt reg, sqInt scratch);
static sqInt NoDbgRegParms genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(sqInt reg, sqInt scratch, void *fwdJumpTarget, void *nonFwdJumpTargetOrZero);
static sqInt NoDbgRegParms genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(sqInt reg, sqInt scratch, sqInt index, sqInt objReg);
static void generateObjectRepresentationTrampolines(void);
static sqInt NoDbgRegParms genGetActiveContextLargeinBlock(sqInt isLarge, sqInt isInBlock);
static sqInt NoDbgRegParms genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock);
static sqInt NoDbgRegParms genGetBitsofFormatByteOfinto(sqInt mask, sqInt sourceReg, sqInt destReg);
static sqInt NoDbgRegParms genGetClassIndexOfNonImminto(sqInt sourceReg, sqInt destReg);
static sqInt NoDbgRegParms genGetClassObjectOfClassIndexintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static sqInt NoDbgRegParms genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeForwarder);
static AbstractInstruction * NoDbgRegParms genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg);
static sqInt NoDbgRegParms genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg);
static sqInt NoDbgRegParms genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg);
static sqInt NoDbgRegParms genGetFormatOfinto(sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(sqInt sourceReg, sqInt destReg, sqInt scratchRegOrNone);
static void NoDbgRegParms genGetIdentityHashresultReg(sqInt rcvrReg, sqInt resultReg);
static sqInt NoDbgRegParms genGetInstanceOfByteClassintoinitializingIfnumBytes(sqInt classObj, sqInt destReg, sqInt initializeInstance, sqInt numBytes);
static sqInt NoDbgRegParms genGetInstanceOfPointerClassintoinitializingIfnumVariableSlots(sqInt classObj, sqInt destReg, sqInt initializeInstance, sqInt varSlots);
static sqInt NoDbgRegParms genGetNumSlotsOfinto(sqInt srcReg, sqInt destReg);
static sqInt NoDbgRegParms genGetRawSlotSizeOfNonImminto(sqInt sourceReg, sqInt destReg);
static sqInt NoDbgRegParms genGetUninitializedInstanceWithClassIndexnumSlotsformatinto(sqInt classIndex, sqInt numSlots, sqInt format, sqInt destReg);
static AbstractInstruction * NoDbgRegParms genIfRequiredCheckRememberedBitOfscratch(sqInt rr, sqInt scratchReg);
static AbstractInstruction * NoDbgRegParms genJumpImmediate(sqInt aRegister);
#if IMMUTABILITY
static AbstractInstruction * NoDbgRegParms genJumpImmutablescratchReg(sqInt sourceReg, sqInt scratchReg);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static AbstractInstruction * NoDbgRegParms genJumpMutablescratchReg(sqInt sourceReg, sqInt scratchReg);
#endif /* IMMUTABILITY */
static AbstractInstruction * NoDbgRegParms genJumpNotImmediate(sqInt aRegister);
static sqInt NoDbgRegParms genNewArrayOfSizeinitialized(sqInt size, sqInt initialize);
static sqInt NoDbgRegParms genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock);
static sqInt genPrimitiveAsCharacter(void);
static sqInt genPrimitiveAt(void);
static sqInt genPrimitiveAtPut(void);
static sqInt NoDbgRegParms genPrimitiveIdenticalOrNotIf(sqInt orNot);
static sqInt genPrimitiveIntegerAt(void);
static sqInt genPrimitiveIntegerAtPut(void);
static sqInt genPrimitiveMakePoint(void);
static sqInt genPrimitiveObjectAt(void);
static sqInt genPrimitiveSize(void);
static sqInt genPrimitiveStringCompareWith(void);
static sqInt genPrimitiveStringReplace(void);
static sqInt NoDbgRegParms genSetSmallIntegerTagsIn(sqInt scratchReg);
static usqInt genStoreCheckContextReceiverTrampoline(void);
static sqInt NoDbgRegParms genStoreCheckReceiverRegvalueRegscratchReginFrame(sqInt destReg, sqInt valueReg, sqInt scratchReg, sqInt inFrame);
static sqInt NoDbgRegParms genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck);
static sqInt NoDbgRegParms genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg);
#if IMMUTABILITY
static usqInt NoDbgRegParms genStoreTrampolineCalledinstVarIndex(char *trampolineName, sqInt instVarIndex);
#endif /* IMMUTABILITY */
static sqInt NoDbgRegParms genStoreValueinstancenumSlots(sqInt value, sqInt destReg, sqInt numSlots);
#if IMMUTABILITY
static sqInt NoDbgRegParms genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static sqInt NoDbgRegParms genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
#if IMMUTABILITY
static sqInt NoDbgRegParms genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needsStoreCheck, sqInt needRestoreRcvr);
#endif /* IMMUTABILITY */
static void genVarIndexCallStoreTrampoline(void);
static sqInt getActiveContextAllocatesInMachineCode(void);
static sqInt NoDbgRegParms inlineCacheTagIsYoung(sqInt cacheTag);
static AbstractInstruction * NoDbgRegParms jumpNotCharacterUnsignedValueInRegister(sqInt reg);
static sqInt NoDbgRegParms markAndTraceLiteralinatpc(sqInt literal, CogMethod *cogMethodOrNil, usqInt address);
static void NoDbgRegParms markAndTraceLiteralinat(sqInt literal, CogMethod *cogMethod, sqInt *address);
static void NoDbgRegParms markAndTraceUpdatedLiteralin(sqInt objOop, CogMethod *cogMethodOrNil);
static sqInt NoDbgRegParms maybeCompileRetryOfonPrimitiveFailflags(void (*primitiveRoutine)(void), sqInt primIndex, sqInt flags);
static sqInt NoDbgRegParms maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg);
static sqInt NoDbgRegParms mixedbranchIfinstanceOfBehaviorstarget(sqInt numNonImmediates, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms mixedbranchIfnotInstanceOfBehaviorstarget(sqInt numNonImmediates, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms noneImmediateBranchIfinstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt NoDbgRegParms noneImmediateBranchIfnotInstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp);
static sqInt numCharacterBits(void);
static sqInt NoDbgRegParms remapObject(sqInt objOop);
static sqInt NoDbgRegParms remapOop(sqInt objOop);
extern void resetCountersIn(CogMethod *cogMethod);
static sqInt NoDbgRegParms shouldAnnotateObjectReference(sqInt anOop);
static sqInt NoDbgRegParms slotOffsetOfInstVarIndex(sqInt index);
static sqInt NoDbgRegParms valueOfAssociation(sqInt associationOop);
static SimStackEntry * NoDbgRegParms ensureSpilledAtfrom(SimStackEntry * self_in_ensureSpilledAtfrom, sqInt baseOffset, sqInt baseRegister);
static sqInt NoDbgRegParms isSameEntryAs(SimStackEntry * self_in_isSameEntryAs, CogSimStackEntry *ssEntry);
static sqInt NoDbgRegParms mayBeAForwarder(SimStackEntry * self_in_mayBeAForwarder);
static void NoDbgRegParms popToReg(SimStackEntry * self_in_popToReg, sqInt reg);
static sqInt NoDbgRegParms registerMask(SimStackEntry * self_in_registerMask);
static sqInt NoDbgRegParms registerMaskOrNone(SimStackEntry * self_in_registerMaskOrNone);
static sqInt NoDbgRegParms registerOrNone(SimStackEntry * self_in_registerOrNone);
static void NoDbgRegParms storeToReg(SimStackEntry * self_in_storeToReg, sqInt reg);
static sqInt NoDbgRegParms isMergeFixup(BytecodeFixup * self_in_isMergeFixup);
static AbstractInstruction * NoDbgRegParms allocateLiteral(sqInt aLiteral);
static AbstractInstruction * NoDbgRegParms checkQuickConstantforInstruction(sqInt literal, AbstractInstruction *anInstruction);
static sqInt NoDbgRegParms literalInstructionInRange(AbstractInstruction *litInst);
static sqInt NoDbgRegParms mustDumpLiterals(sqInt currentOpcodeIndex);
static AbstractInstruction * NoDbgRegParms checkLiteral32forInstruction(sqInt literal, AbstractInstruction *anInstruction);
static AbstractInstruction * NoDbgRegParms checkLiteralforInstruction(sqInt literal, AbstractInstruction *anInstruction);
static sqInt NoDbgRegParms dumpLiterals(sqInt generateBranchAround);
static AbstractInstruction * NoDbgRegParms locateLiteralsize(sqInt aLiteral, sqInt litSize);
extern sqInt cogMethodHasExternalPrim(CogMethod *aCogMethod);
extern sqInt cogMethodHasMachineCodePrim(CogMethod *aCogMethod);
static sqInt compileBlockDispatch(void);
static void compileGetErrorCode(void);
static sqInt compileInterpreterPrimitive(void);
static sqInt NoDbgRegParms compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags);
static sqInt NoDbgRegParms compileOnStackExternalPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags);
static AbstractInstruction * NoDbgRegParms compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone);
static void NoDbgRegParms compileOpenPICnumArgs(sqInt selector, sqInt numArgs);
static AbstractInstruction * NoDbgRegParms compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone);
static sqInt compilePrimitive(void);
static sqInt extendedPushBytecode(void);
static sqInt extendedStoreAndPopBytecode(void);
static sqInt extendedStoreBytecode(void);
static sqInt NoDbgRegParms frameOffsetOfTemporary(sqInt index);
static AbstractInstruction * NoDbgRegParms genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg);
static sqInt genExtendedSendBytecode(void);
static sqInt genExtendedSuperBytecode(void);
static sqInt genExtJumpIfFalse(void);
static sqInt genExtJumpIfTrue(void);
static sqInt genExtNopBytecode(void);
static sqInt genExtPushCharacterBytecode(void);
static sqInt genExtPushIntegerBytecode(void);
static sqInt genExtPushLiteralBytecode(void);
static sqInt genExtPushLiteralVariableBytecode(void);
static sqInt genExtPushPseudoVariable(void);
static sqInt genExtPushReceiverVariableBytecode(void);
static sqInt genExtSendBytecode(void);
static sqInt genExtSendSuperBytecode(void);
static sqInt genExtStoreAndPopLiteralVariableBytecode(void);
static sqInt genExtStoreAndPopReceiverVariableBytecode(void);
static sqInt genExtStoreLiteralVariableBytecode(void);
static sqInt genExtStoreReceiverVariableBytecode(void);
static sqInt genExtUnconditionalJump(void);
static sqInt genFastPrimFail(void);
static void NoDbgRegParms genFastPrimTraceUsingand(sqInt r1, sqInt r2);
static void genLoadNewMethod(void);
static sqInt genLongJumpIfFalse(void);
static sqInt genLongJumpIfTrue(void);
static sqInt genLongPushTemporaryVariableBytecode(void);
static sqInt genLongStoreAndPopTemporaryVariableBytecode(void);
static sqInt genLongStoreTemporaryVariableBytecode(void);
static sqInt genLongUnconditionalBackwardJump(void);
static sqInt genLongUnconditionalForwardJump(void);
static sqInt NoDbgRegParms genLookupForPerformNumArgs(sqInt numArgs);
static AbstractInstruction * NoDbgRegParms genMoveFalseR(sqInt reg);
static AbstractInstruction * NoDbgRegParms genMoveTrueR(sqInt reg);
static sqInt genPrimitiveHashMultiply(void);
static void NoDbgRegParms genPrimReturnEnterCogCodeEnilopmart(sqInt profiling);
static sqInt genPushConstantFalseBytecode(void);
static sqInt genPushConstantNilBytecode(void);
static sqInt genPushConstantOneBytecode(void);
static sqInt genPushConstantTrueBytecode(void);
static sqInt genPushConstantZeroBytecode(void);
static sqInt genPushLiteralConstantBytecode(void);
static sqInt genPushLiteralVariable16CasesBytecode(void);
static sqInt genPushLiteralVariableBytecode(void);
static sqInt genPushQuickIntegerConstantBytecode(void);
static sqInt genPushReceiverVariableBytecode(void);
static sqInt genPushTemporaryVariableBytecode(void);
extern sqInt genQuickReturnConst(void);
extern sqInt genQuickReturnInstVar(void);
extern sqInt genQuickReturnSelf(void);
static sqInt genReturnFalse(void);
static sqInt genReturnNil(void);
static sqInt genReturnNilFromBlock(void);
static sqInt genReturnTrue(void);
static sqInt genSecondExtendedSendBytecode(void);
static sqInt genSendLiteralSelector0ArgsBytecode(void);
static sqInt genSendLiteralSelector1ArgBytecode(void);
static sqInt genSendLiteralSelector2ArgsBytecode(void);
static sqInt genShortJumpIfFalse(void);
static sqInt genShortJumpIfTrue(void);
static sqInt genShortUnconditionalJump(void);
static sqInt genSpecialSelectorEqualsEquals(void);
static sqInt genSpecialSelectorNotEqualsEquals(void);
static sqInt genSpecialSelectorSend(void);
static sqInt genStoreAndPopReceiverVariableBytecode(void);
static sqInt genStoreAndPopRemoteTempLongBytecode(void);
static sqInt genStoreAndPopTemporaryVariableBytecode(void);
static sqInt genStoreRemoteTempLongBytecode(void);
static void genTakeProfileSample(void);
extern sqInt mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj);
static sqInt numSpecialSelectors(void);
static usqInt NoDbgRegParms pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod);
static sqInt NoDbgRegParms pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
static PrimitiveDescriptor * primitiveGeneratorOrNil(void);
static sqInt NoDbgRegParms registerisInMask(sqInt reg, sqInt mask);
static sqInt NoDbgRegParms registerisNotInMask(sqInt reg, sqInt mask);
static sqInt NoDbgRegParms v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v4BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v4LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static sqInt NoDbgRegParms v4LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static CogMethod * NoDbgRegParms compileCogFullBlockMethod(sqInt numCopied);
static CogMethod * NoDbgRegParms compileCogMethod(sqInt selector);
static void compileFrameBuild(void);
static void NoDbgRegParms compileFullBlockMethodFrameBuild(sqInt numCopied);
extern sqInt defaultCogCodeSize(void);
static void NoDbgRegParms fillInCountersatStartAddress(sqInt nCounters, sqInt startAddress);
static void NoDbgRegParms fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector);
static sqInt NoDbgRegParms genAtPutInlinePrimitive(sqInt prim);
static sqInt NoDbgRegParms genBinaryAtConstInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryAtInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryCompInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryConstOpVarSmiInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryInlineComparisonopFalsedestReg(sqInt opTrue, sqInt opFalse, sqInt destReg);
static sqInt NoDbgRegParms genBinaryInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryNewInlinePrimitive(sqInt primIndex);
static sqInt genBinarySmiBitShiftLeftInlinePrimitive(void);
static sqInt genBinarySmiBitShiftRightInlinePrimitive(void);
static sqInt NoDbgRegParms genBinaryVarOpConstSmiInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genBinaryVarOpVarSmiInlinePrimitive(sqInt primIndex);
static sqInt genByteAtPut(void);
static sqInt genByteAtPutImmutabilityCheck(void);
static sqInt NoDbgRegParms genByteEqualsInlinePrimitiveResultreturnReg(AbstractInstruction *jmp, sqInt reg);
static sqInt NoDbgRegParms genByteEqualsInlinePrimitive(sqInt prim);
static sqInt genCallMappedInlinedPrimitive(void);
static sqInt genCallPrimitiveBytecode(void);
static sqInt NoDbgRegParms genCounterTripOnlyJumpIfto(sqInt boolean, sqInt targetBytecodePC);
static sqInt genDirectCall(void);
static sqInt NoDbgRegParms genDivInlinePrimitive(sqInt primIndex);
static sqInt genEnsureEnoughSlots(void);
static void generateSistaRuntime(void);
static sqInt NoDbgRegParms genForwardersInlinedIdenticalOrNotIf(sqInt orNot);
static sqInt NoDbgRegParms genJumpBinaryInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genJumpIfto(sqInt boolean, sqInt targetBytecodePC);
static sqInt NoDbgRegParms genJumpTrinaryInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genJumpUnaryInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genMappedInlinePrimitive(sqInt primIndex);
static usqInt NoDbgRegParms genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName);
static sqInt NoDbgRegParms genPointerAtPutConstantMaybeContextstoreCheckimmutabilityCheck(sqInt maybeContext, sqInt needsStoreCheck, sqInt needsImmCheck);
static sqInt genPointerAtPutImmCheckAndStoreCheck(void);
static sqInt genPointerAtPutImmCheckButNoStoreCheck(void);
static sqInt NoDbgRegParms genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(sqInt maybeContext, sqInt needsStoreCheck, sqInt needsImmCheck);
static sqInt NoDbgRegParms genPointerAtPutStoreCheck(sqInt needsStoreCheck);
static sqInt NoDbgRegParms genSistaInlinePrimitive(sqInt prim);
static sqInt genSpecialSelectorComparison(void);
static sqInt genSpecialSelectorComparisonWithoutCounters(void);
static sqInt NoDbgRegParms genTrinaryInlinePrimitive(sqInt prim);
static sqInt genUnaryClassPrimitive(void);
static sqInt NoDbgRegParms genUnaryConvertInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genUnaryHashInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genUnaryInlinePrimitive(sqInt primIndex);
static sqInt NoDbgRegParms genUnaryNewInlinePrimitive(sqInt primIndex);
static sqInt genUnaryPossibleRootInlinePrimitive(void);
static sqInt NoDbgRegParms genUnarySizeInlinePrimitive(sqInt primIndex);
static sqInt genUnaryUnforwardInlinePrimitive(void);
static sqInt genUnaryUnforwardNonImmediateInlinePrimitive(void);
static sqInt genUnconditionalTrapBytecode(void);
extern usqInt getJumpTargetPCAt(sqInt pc);
extern void initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress);
static sqInt maybeAllocAndInitCounters(void);
static sqInt NoDbgRegParms maybeDealWithUnsafeJumpForDescriptorpclatestContinuation(BytecodeDescriptor *descriptor, sqInt pc, sqInt latestContinuation);
static usqInt NoDbgRegParms picDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod);
static sqInt NoDbgRegParms picDataForCounterat(unsigned int counter, sqInt bcpc);
static sqInt NoDbgRegParms picDataForSendTomethodClassIfSuperatbcpc(CogMethod *cogMethod, sqInt methodClassOrNil, char *sendMcpc, sqInt sendBcpc);
static sqInt NoDbgRegParms picDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg);
extern sqInt picDataForinto(CogMethod *cogMethod, sqInt arrayObj);
static void NoDbgRegParms populatewithPICInfoForfirstCacheTag(sqInt tuple, CogMethod *cPIC, sqInt firstCacheTag);
extern double getCogCodeZoneThreshold(void);
extern sqInt setCogCodeZoneThreshold(double ratio);
static BlockStart * NoDbgRegParms addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span);
static void NoDbgRegParms adjustArgumentsForPerform(sqInt numArgs);
static sqInt NoDbgRegParms allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask);
static sqInt NoDbgRegParms allocateRegNotConflictingWith(sqInt regMask);
static sqInt NoDbgRegParms anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n);
extern void callCogCodePopReceiverArg0Regs(void);
extern void callCogCodePopReceiverArg1Arg0Regs(void);
static sqInt NoDbgRegParms compileAbstractInstructionsFromthrough(sqInt start, sqInt end);
static sqInt compileBlockBodies(void);
static void NoDbgRegParms compileBlockFrameBuild(BlockStart *blockStart);
static void NoDbgRegParms compileBlockFramelessEntry(BlockStart *blockStart);
static sqInt compileEntireMethod(void);
static void NoDbgRegParms compileFullBlockFramelessEntry(sqInt numCopied);
#if IMMUTABILITY
static void compileTwoPathFrameBuild(void);
#endif /* IMMUTABILITY */
static void compileTwoPathFramelessInit(void);
static sqInt NoDbgRegParms cPICMissTrampolineFor(sqInt numArgs);
static sqInt doubleExtendedDoAnythingBytecode(void);
static sqInt duplicateTopBytecode(void);
static BytecodeFixup * NoDbgRegParms ensureFixupAt(sqInt targetPC);
static BytecodeFixup * NoDbgRegParms ensureNonMergeFixupAt(sqInt targetPC);
static void ensureReceiverResultRegContainsSelf(void);
static void NoDbgRegParms evaluateat(BytecodeDescriptor *descriptor, sqInt pc);
static sqInt NoDbgRegParms eventualTargetOf(sqInt targetBytecodePC);
static sqInt NoDbgRegParms freeAnyRegNotConflictingWith(sqInt regMask);
static sqInt genBlockReturn(void);
static void (*genCallPICEnilopmartNumArgs(sqInt numArgs))(void);
static sqInt genExternalizePointersForPrimitiveCall(void);
static AbstractInstruction * genExternalizeStackPointerForFastPrimitiveCall(void);
static sqInt genExtPushClosureBytecode(void);
static sqInt genExtPushFullClosureBytecode(void);
static void generateEnilopmarts(void);
static sqInt NoDbgRegParms generateInstructionsAt(sqInt eventualAbsoluteAddress);
static void generateMissAbortTrampolines(void);
static void generateSendTrampolines(void);
static void generateTracingTrampolines(void);
static sqInt NoDbgRegParms genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot);
static sqInt NoDbgRegParms genInlinedIdenticalOrNotIf(sqInt orNot);
static sqInt NoDbgRegParms genJumpBackTo(sqInt targetBytecodePC);
static sqInt NoDbgRegParms genJumpTo(sqInt targetBytecodePC);
static sqInt NoDbgRegParms genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable);
static usqInt NoDbgRegParms genMethodAbortTrampolineFor(sqInt numArgs);
static usqInt NoDbgRegParms genPICAbortTrampolineFor(sqInt numArgs);
static usqInt NoDbgRegParms genPICMissTrampolineFor(sqInt numArgs);
static sqInt genPopStackBytecode(void);
static sqInt genPrimitiveClosureValue(void);
static sqInt genPrimitiveFullClosureValue(void);
static sqInt genPrimitivePerform(void);
static sqInt genPrimitivePerformWithArguments(void);
static sqInt genPushActiveContextBytecode(void);
static sqInt genPushClosureCopyCopiedValuesBytecode(void);
static sqInt NoDbgRegParms genPushLiteralIndex(sqInt literalIndex);
static sqInt NoDbgRegParms genPushLiteralVariable(sqInt literalIndex);
static sqInt NoDbgRegParms genPushMaybeContextReceiverVariable(sqInt slotIndex);
static sqInt genPushNewArrayBytecode(void);
static sqInt genPushReceiverBytecode(void);
static sqInt NoDbgRegParms genPushReceiverVariable(sqInt index);
static void genPushRegisterArgs(void);
static sqInt genPushRemoteTempLongBytecode(void);
static sqInt NoDbgRegParms genPushTemporaryVariable(sqInt index);
static sqInt genReturnReceiver(void);
static sqInt genReturnTopFromBlock(void);
static sqInt genReturnTopFromMethod(void);
static sqInt NoDbgRegParms genSendDirectedSupernumArgs(sqInt selectorIndex, sqInt numArgs);
static sqInt NoDbgRegParms genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs);
static usqInt NoDbgRegParms genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3);
static sqInt NoDbgRegParms genSendnumArgs(sqInt selectorIndex, sqInt numArgs);
static sqInt genSpecialSelectorArithmetic(void);
static sqInt genSpecialSelectorClass(void);
static sqInt genStaticallyResolvedSpecialSelectorComparison(void);
static sqInt NoDbgRegParms genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static sqInt NoDbgRegParms genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static sqInt NoDbgRegParms genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck);
static sqInt NoDbgRegParms genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck);
static sqInt NoDbgRegParms genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex);
static sqInt genUpArrowReturn(void);
static sqInt NoDbgRegParms genVanillaInlinedIdenticalOrNotIf(sqInt orNot);
static void NoDbgRegParms initSimStackForFramefulMethod(sqInt startpc);
static void NoDbgRegParms initSimStackForFramelessBlock(sqInt startpc);
static void NoDbgRegParms initSimStackForFramelessMethod(sqInt startpc);
static sqInt NoDbgRegParms isNonForwarderReceiver(sqInt reg);
static sqInt liveRegisters(void);
static sqInt NoDbgRegParms mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor);
static void NoDbgRegParms marshallSendArguments(sqInt numArgs);
static sqInt maybeCompilingFirstPassOfBlockWithInitialPushNil(void);
static sqInt NoDbgRegParms mergeWithFixupIfRequired(BytecodeFixup *fixup);
static sqInt NoDbgRegParms methodAbortTrampolineFor(sqInt numArgs);
static sqInt methodFoundInvalidPostScan(void);
static sqInt NoDbgRegParms needsFrameIfMod16GENumArgs(sqInt stackDelta);
static sqInt NoDbgRegParms needsFrameIfStackGreaterThanOne(sqInt stackDelta);
static sqInt NoDbgRegParms numberOfSpillsInTopNItems(sqInt n);
static sqInt NoDbgRegParms picAbortTrampolineFor(sqInt numArgs);
static sqInt prevInstIsPCAnnotated(void);
static sqInt receiverIsInReceiverResultReg(void);
static void NoDbgRegParms reinitializeFixupsFromthrough(sqInt start, sqInt end);
static sqInt NoDbgRegParms scanBlock(BlockStart *blockStart);
static sqInt scanMethod(void);
static sqInt NoDbgRegParms squeakV3orSistaV1PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils);
static sqInt NoDbgRegParms squeakV3orSistaV1NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj);
static void NoDbgRegParms ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr);
static void NoDbgRegParms ssFlushUpThroughReceiverVariable(sqInt slotIndex);
static void NoDbgRegParms ssFlushUpThroughTemporaryVariable(sqInt tempIndex);
static void NoDbgRegParms ssPop(sqInt n);
static sqInt NoDbgRegParms ssPushAnnotatedConstant(sqInt literal);
static sqInt NoDbgRegParms ssPushBaseoffset(sqInt reg, sqInt offset);
static sqInt NoDbgRegParms ssPushConstant(sqInt literal);
static sqInt NoDbgRegParms ssPushDesc(SimStackEntry simStackEntry);
static sqInt NoDbgRegParms ssPushRegister(sqInt reg);
static void NoDbgRegParms ssPush(sqInt n);
static SimStackEntry ssSelfDescriptor(void);
static void NoDbgRegParms ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg);
static sqInt NoDbgRegParms ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg);
static void NoDbgRegParms ssStorePoptoReg(sqInt popBoolean, sqInt reg);
static CogSimStackEntry * ssTop(void);
static CogSimStackEntry * NoDbgRegParms ssValue(sqInt n);
static sqInt NoDbgRegParms stackEntryIsBoolean(CogSimStackEntry *simStackEntry);
static sqInt tempsValidAndVolatileEntriesSpilled(void);
static sqInt NoDbgRegParms tryCollapseTempVectorInitializationOfSize(sqInt slots);
static sqInt violatesEnsureSpilledSpillAssert(void);
static void voidReceiverResultRegContainsSelf(void);


/*** Variables ***/
static AbstractInstruction * abstractOpcodes;
static usqInt allocationThreshold;
static usqInt baseAddress;
static sqInt blockCount;
static AbstractInstruction * blockEntryLabel;
static AbstractInstruction * blockEntryNoContextSwitch;
static BlockStart * blockStarts;
static sqInt branchReachedOnlyForCounterTrip;
static sqInt breakBlock;
static sqInt breakMethod;
static sqInt byte0;
static sqInt byte1;
static sqInt byte2;
static sqInt byte3;
static sqInt bytecodePC;
static sqInt bytecodeSetOffset;
static sqInt ceByteSizeOfTrampoline;
static sqInt ceCPICMissTrampoline;
static sqInt ceDereferenceSelectorIndex;
static sqInt ceFetchContextInstVarTrampoline;
static sqInt ceFFICalloutTrampoline;
static sqInt ceFloatObjectOfTrampoline;
static sqInt ceFloatValueOfTrampoline;
static sqInt ceFreeTrampoline;
static sqInt ceInlineNewHashTrampoline;
static sqInt ceInstantiateClassIndexableSizeTrampoline;
static sqInt ceInstantiateClassTrampoline;
static sqInt ceLargeActiveContextInBlockTrampoline;
static sqInt ceLargeActiveContextInFullBlockTrampoline;
static sqInt ceLargeActiveContextInMethodTrampoline;
static sqInt ceMallocTrampoline;
static sqInt ceMethodAbortTrampoline;
static sqInt ceNewHashTrampoline;
static sqInt ceNonLocalReturnTrampoline;
static sqInt cePICAbortTrampoline;
static sqInt cePositive64BitIntegerTrampoline;
static sqInt cePositive64BitValueOfTrampoline;
static sqInt cePrimReturnEnterCogCode;
static sqInt cePrimReturnEnterCogCodeProfiling;
static sqInt ceReapAndResetErrorCodeTrampoline;
static sqInt ceScheduleScavengeTrampoline;
static sqInt ceSendMustBeBooleanAddFalseTrampoline;
static sqInt ceSendMustBeBooleanAddTrueTrampoline;
static sqInt ceSigned64BitIntegerTrampoline;
static sqInt ceSigned64BitValueOfTrampoline;
static sqInt ceSmallActiveContextInBlockTrampoline;
static sqInt ceSmallActiveContextInFullBlockTrampoline;
static sqInt ceSmallActiveContextInMethodTrampoline;
static sqInt ceStoreCheckContextReceiverTrampoline;
static sqInt ceStoreCheckTrampoline;
static sqInt ceStoreContextInstVarTrampoline;
static sqInt ceTraceBlockActivationTrampoline;
static sqInt ceTraceLinkedSendTrampoline;
static sqInt ceTraceStoreTrampoline;
static sqInt ceTrapTrampoline;
static sqInt checkedEntryAlignment;
static sqInt closedPICSize;
static sqInt codeBase;
#if DUAL_MAPPED_CODE_ZONE
static sqInt codeToDataDelta;
#else
# define codeToDataDelta 0
#endif
static sqInt cogConstituentIndex;
static sqInt compactionInProgress;
static sqInt compilationPass;
static sqInt compilationTrace;
static sqInt counterIndex;
static usqInt counters;
static sqInt cPICCaseSize;
static sqInt cPICEndOfCodeOffset;
static sqInt cPICEndSize;
static CogMethod * cPICPrototype;
static sqInt currentCallCleanUpSize;
static sqInt debugBytecodePointers;
static sqInt debugFixupBreaks;
static sqInt debugOpcodeIndices;
static sqInt debugStackPointers;
static sqInt directedSuperBindingSendTrampolines[NumSendTrampolines];
static sqInt directedSuperSendTrampolines[NumSendTrampolines];
static sqInt disassemblingMethod;
static AbstractInstruction * endCPICCase0;
static sqInt endPC;
static AbstractInstruction * entry;
static sqInt entryPointMask;
static CogMethod * enumeratingCogMethod;
static sqInt expectedFPAlignment;
static sqInt expectedSPAlignment;
static sqInt extA;
static sqInt extB;
static sqInt firstCPICCaseOffset;
static sqInt firstOpcodeIndex;
static sqInt firstSend;
static BytecodeFixup * fixups;
static AbstractInstruction * fullBlockEntry;
static AbstractInstruction * fullBlockNoContextSwitchEntry;
static BytecodeDescriptor generatorTable[512] = {
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushReceiverBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantTrueBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantFalseBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantNilBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushQuickIntegerConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genReturnReceiver, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTrue, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnFalse, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNil, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromMethod, 0, needsFrameIfInBlock, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extendedPushBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ extendedStoreBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ extendedStoreAndPopBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ doubleExtendedDoAnythingBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtendedSuperBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0 },
	{ genSecondExtendedSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genPopStackBytecode, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ duplicateTopBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushActiveContextBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushNewArrayBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genCallPrimitiveBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushClosureCopyCopiedValuesBytecode, v3BlockCodeSize, 0, 0, 0, 4, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalBackwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongUnconditionalForwardJump, v3LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfTrue, v3LongForwardBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genLongJumpIfFalse, v3LongForwardBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AddRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, SubRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLess, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreater, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLessOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreaterOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpNonZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AndRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, OrRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorClass, 0, needsFrameIfStackGreaterThanOne, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorNotEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushReceiverVariableBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralVariable16CasesBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushLiteralConstantBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushTemporaryVariableBytecode, 0, needsFrameIfMod16GENumArgs, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushReceiverBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantTrueBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantFalseBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantNilBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantZeroBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPushConstantOneBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushPseudoVariable, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ duplicateTopBytecode, 0, needsFrameNever, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genReturnReceiver, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTrue, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnFalse, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNil, 0, needsFrameIfInBlock, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnTopFromMethod, 0, needsFrameIfInBlock, -1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0 },
	{ genReturnNilFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ genReturnTopFromBlock, 0, needsFrameNever, -1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtNopBytecode, 0, needsFrameNever, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AddRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, SubRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLess, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreater, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpLessOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpGreaterOrEqual, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorComparison, 0, 0, 0, JumpNonZero, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, AndRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorArithmetic, 0, 0, 0, OrRR, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorClass, 0, needsFrameIfStackGreaterThanOne, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorNotEqualsEquals, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSpecialSelectorSend, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector0ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector1ArgBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genSendLiteralSelector2ArgsBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortUnconditionalJump, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfTrue, v3ShortForwardBranchDistance, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genShortJumpIfFalse, v3ShortForwardBranchDistance, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopReceiverVariableBytecode, 0, needsFrameIfImmutability, -1, 0, 1, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 1, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genPopStackBytecode, 0, needsFrameNever, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genUnconditionalTrapBytecode, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extABytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ extBBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genExtPushReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0 },
	{ genExtPushLiteralVariableBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushLiteralBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genLongPushTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genPushNewArrayBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushIntegerBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushCharacterBytecode, 0, needsFrameNever, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtSendBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtSendSuperBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genCallMappedInlinedPrimitive, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1 },
	{ genExtUnconditionalJump, v4LongBranchDistance, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtJumpIfTrue, v4LongBranchDistance, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtJumpIfFalse, v4LongBranchDistance, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0 },
	{ genExtStoreAndPopReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtStoreAndPopLiteralVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 0, 0, 0 },
	{ genLongStoreAndPopTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtStoreReceiverVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 1, 0, 0 },
	{ genExtStoreLiteralVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, IMMUTABILITY, 0, 0, 0, 0, 0 },
	{ genLongStoreTemporaryVariableBytecode, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ genCallPrimitiveBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1 },
	{ genExtPushFullClosureBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genExtPushClosureBytecode, v4BlockCodeSize, 0, 0, 0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0 },
	{ genPushRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ genStoreAndPopRemoteTempLongBytecode, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 },
	{ unknownBytecode, 0, 0, 0, Nop, 3, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0 }
};
static sqInt guardPageSize;
static sqInt initialCounterValue;
static sqInt initialPC;
static sqInt introspectionData;
static sqInt introspectionDataIndex;
static sqInt lastDumpedLiteralIndex;
static sqInt lastSend;
static usqInt limitAddress;
static AbstractInstruction * literals;
static sqInt literalsSize;
static sqInt maxLitIndex;
static sqInt methodAbortTrampolines[4];
static sqInt methodBytesFreedSinceLastCompaction;
static sqInt methodCount;
static sqInt methodHeader;
static sqInt methodObj;
static sqInt methodOrBlockNumArgs;
static sqInt methodOrBlockNumTemps;
static usqIntptr_t minValidCallAddress;
static usqInt mzFreeStart;
static sqInt nextLiteralIndex;
static AbstractInstruction * noCheckEntry;
static sqInt numAbstractOpcodes;
static sqInt numCounters;
static sqInt numExtB;
static usqInt objectReferencesInRuntime[NumObjRefsInRuntime+1];
static sqInt opcodeIndex;
static CogMethod *openPICList = 0;
static sqInt openPICSize;
static sqInt ordinarySendTrampolines[NumSendTrampolines];
static sqInt picAbortTrampolines[4];
static AbstractInstruction * picInterpretAbort;
static sqInt picMissTrampolines[4];
static AbstractInstruction * picMNUAbort;
static BytecodeDescriptor * prevBCDescriptor;
static PrimitiveDescriptor primitiveGeneratorTable[MaxCompiledPrimitiveIndex+1] = {
	{ 0, -1 },
	{ genPrimitiveAdd, 1 },
	{ genPrimitiveSubtract, 1 },
	{ genPrimitiveLessThan, 1 },
	{ genPrimitiveGreaterThan, 1 },
	{ genPrimitiveLessOrEqual, 1 },
	{ genPrimitiveGreaterOrEqual, 1 },
	{ genPrimitiveEqual, 1 },
	{ genPrimitiveNotEqual, 1 },
	{ genPrimitiveMultiply, 1 },
	{ genPrimitiveDivide, 1 },
	{ genPrimitiveMod, 1 },
	{ genPrimitiveDiv, 1 },
	{ genPrimitiveQuo, 1 },
	{ genPrimitiveBitAnd, 1 },
	{ genPrimitiveBitOr, 1 },
	{ genPrimitiveBitXor, 1 },
	{ genPrimitiveBitShift, 1 },
	{ genPrimitiveMakePoint, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAsFloat, 0 },
	{ genPrimitiveFloatAdd, 1 },
	{ genPrimitiveFloatSubtract, 1 },
	{ genPrimitiveFloatLessThan, 1 },
	{ genPrimitiveFloatGreaterThan, 1 },
	{ genPrimitiveFloatLessOrEqual, 1 },
	{ genPrimitiveFloatGreaterOrEqual, 1 },
	{ genPrimitiveFloatEqual, 1 },
	{ genPrimitiveFloatNotEqual, 1 },
	{ genPrimitiveFloatMultiply, 1 },
	{ genPrimitiveFloatDivide, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveFloatSquareRoot, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveAt, 1 },
	{ genPrimitiveAtPut, 2 },
	{ genPrimitiveSize, 0 },
	{ genPrimitiveStringAt, 1 },
	{ genPrimitiveStringAtPut, 2 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ genPrimitiveObjectAt, 1 },
	{ 0, -1 },
	{ genPrimitiveNew, 0 },
	{ genPrimitiveNewWithArg, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNewMethod, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitivePerform, -1 },
	{ genPrimitivePerformWithArguments, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringReplace, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIdentical, 1 },
	{ genPrimitiveClass, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveShallowCopy, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveStringCompareWith, 1 },
	{ genPrimitiveHashMultiply, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveIntegerAt, 1 },
	{ genPrimitiveIntegerAtPut, 2 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveNotIdentical, 1 },
	{ genPrimitiveAsCharacter, -1 },
	{ genPrimitiveImmediateAsInteger, 0 },
	{ 0, -1 },
	{ genPrimitiveSlotAt, 1 },
	{ genPrimitiveSlotAtPut, 2 },
	{ genPrimitiveIdentityHash, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genFastPrimFail, -1 },
	{ genFastPrimFail, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ genPrimitiveClosureValue, 2 },
	{ genPrimitiveClosureValue, 3 },
	{ genPrimitiveClosureValue, 4 },
	{ 0, -1 },
	{ genPrimitiveFullClosureValue, -1 },
	{ 0, -1 },
	{ genPrimitiveFullClosureValue, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveClosureValue, 0 },
	{ genPrimitiveClosureValue, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveSmallFloatAdd, 1 },
	{ genPrimitiveSmallFloatSubtract, 1 },
	{ genPrimitiveSmallFloatLessThan, 1 },
	{ genPrimitiveSmallFloatGreaterThan, 1 },
	{ genPrimitiveSmallFloatLessOrEqual, 1 },
	{ genPrimitiveSmallFloatGreaterOrEqual, 1 },
	{ genPrimitiveSmallFloatEqual, 1 },
	{ genPrimitiveSmallFloatNotEqual, 1 },
	{ genPrimitiveSmallFloatMultiply, 1 },
	{ genPrimitiveSmallFloatDivide, 1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveSmallFloatSquareRoot, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveHighBit, 0 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ 0, -1 },
	{ genPrimitiveUninitializedNewWithArg, 1 }
};
static sqInt primitiveIndex;
static sqInt processorLock;
static sqInt receiverTags;
static sqInt runtimeObjectRefIndex;
static AbstractInstruction * sendMiss;
static sqInt simNativeStackPtr;
static sqInt simSpillBase;
static SimStackEntry simStack[70];
static sqInt simStackPtr;
static AbstractInstruction * stackCheckLabel;
static AbstractInstruction * stackOverflowCall;
static sqInt superSendTrampolines[NumSendTrampolines];
static sqInt tempOop;
static char *trampolineAddresses[NumTrampolines*2];
static sqInt trampolineTableIndex;
static sqInt uncheckedEntryAlignment;
static usqInt unpairedMethodList;
static sqInt varBaseAddress;
static usqInt youngReferrers;
static int labelCounter;
static unsigned char codeModified;
static unsigned char dataCacheFlushRequired;
static unsigned char dataCacheLineLength;
static unsigned char deadCode;
static unsigned char directedSendUsesBinding;
static unsigned char hasAtomicInstructions;
static unsigned char hasMovableLiteral;
static unsigned char hasYoungReferent;
static unsigned char inBlock;
static unsigned char instructionCacheFlushRequired;
static unsigned char instructionCacheLineLength;
static unsigned char needsFrame;
static unsigned char regArgsHaveBeenPushed;
static unsigned char useTwoPaths;
static AbstractInstruction aMethodLabel;
static AbstractInstruction * const backEnd = &aMethodLabel;
static void (*ceFlushDCache)(usqIntptr_t from, usqIntptr_t to);
static void (*ceFlushICache)(usqIntptr_t from, usqIntptr_t to);
#if IMMUTABILITY
static sqInt ceStoreTrampolines[5];;
#endif
static AbstractInstruction * const methodLabel = &aMethodLabel;
static double thresholdRatio = 0.5;
sqInt blockNoContextSwitchOffset;
sqInt breakPC;
sqInt cbEntryOffset;
sqInt cbNoSwitchEntryOffset;
sqInt ceBaseFrameReturnTrampoline;
sqInt ceCannotResumeTrampoline;
sqInt ceCheckForInterruptTrampoline;
sqInt ceReturnToInterpreterTrampoline;
#if !defined(cFramePointerInUse)
sqInt cFramePointerInUse;
#endif
sqInt cmEntryOffset;
sqInt cmNoCheckEntryOffset;
usqInt methodZoneBase;
sqInt missOffset;
const char * traceFlagsMeanings[] = {
		"1: print trace", "2: trace sends", "4: trace block activations", "8: trace interpreter primitives",
		"16: trace events (context switches, GCs, etc)", "32: trace stack overflow (poll for events hook)",
		"64: trace linked sends", "128: trace fast C call interpreter primitives", null
	};
sqInt traceStores;
int PJWPNChange;
int PJWPNClear;
int PJWPNSet;
int PJWPNState;
int traceFlags = 8 /* prim trace log on by default */;
void (*ceCall0ArgsPIC)(void);
void (*ceCall1ArgsPIC)(void);
void (*ceCall2ArgsPIC)(void);
void (*ceCallCogCodePopReceiverAndClassRegs)(void);
void (*ceCallCogCodePopReceiverArg0Regs)(void);
void (*ceCallCogCodePopReceiverArg1Arg0Regs)(void);
void (*ceCallCogCodePopReceiverReg)(void);
void (*ceCaptureCStackPointers)(void);
void (*ceEnterCogCodePopReceiverReg)(void);
usqIntptr_t (*ceGetFP)(void);
usqIntptr_t (*ceGetSP)(void);
void (*ceInvokeInterpret)(void);
void (*realCECallCogCodePopReceiverAndClassRegs)(void);
void (*realCECallCogCodePopReceiverArg0Regs)(void);
void (*realCECallCogCodePopReceiverArg1Arg0Regs)(void);
void (*realCECallCogCodePopReceiverReg)(void);
void (*realCEEnterCogCodePopReceiverReg)(void);


/*** Macros ***/
#define dataCacheFlushRequired(ign) dataCacheFlushRequired
#define dataCacheLineLength(ign) dataCacheLineLength
#define flushDCacheFromto(me,startAddress,endAddress) ceFlushDCache(startAddress,endAddress)
#define inlineCacheValueForSelectorin(backEnd,selector,aCogMethod) indexForSelectorin(selector,aCogMethod)
#define instructionCacheFlushRequired(ign) instructionCacheFlushRequired
#define instructionCacheLineLength(ign) instructionCacheLineLength
#define roundUpToMethodAlignment(ignored,numBytes) (((numBytes) + 15) & -16)
#define setDataCacheFlushRequired(ign,b) dataCacheFlushRequired = b
#define setDataCacheLineLength(ign,n) dataCacheLineLength = n
#define setInstructionCacheFlushRequired(ign,b) instructionCacheFlushRequired = b
#define setInstructionCacheLineLength(ign,n) instructionCacheLineLength = n
#define cPICNumCases stackCheckOffset
#define cPICNumCasesHack hack hack hack i.e. the getter macro does all the work
#define abstractInstructionAt(index) (&abstractOpcodes[index])
#define addressIsInInstructions(address) (!((usqInt)(address) & (BytesPerWord-1)) \
							&& (address) >= &abstractOpcodes[0] \
							&& (address) < &abstractOpcodes[opcodeIndex])
#define allocateBlockStarts(numBlocks) do { \
		blockStarts = (numBlocks) ? alloca(sizeof(BlockStart) * (numBlocks)) : 0; \
} while (0)
#define assertValidDualZone() true
#define assertValidDualZoneReadAddress(address) 0
#define assertValidDualZoneWriteAddress(address) 0
#define backEnd() backEnd
#define blockAlignment() 8
#define blockStartAt(index) (&blockStarts[index])
#define ceBaseFrameReturnPC() ceBaseFrameReturnTrampoline
#define ceCannotResumePC() ((usqInt)ceCannotResumeTrampoline)
#define ceCheckForInterruptTrampoline() ceCheckForInterruptTrampoline
#define ceReturnToInterpreterPC() ((usqInt)ceReturnToInterpreterTrampoline)
#define codeByteAtput(address,value) byteAtput((address) + codeToDataDelta, value)
#define codeLong32Atput(address,value) long32Atput((address) + codeToDataDelta, value)
#define codeLong64Atput(address,value) long64Atput((address) + codeToDataDelta, value)
#define codeLongAtput(address,value) longAtput((address) + codeToDataDelta, value)
#define codeMemcpy(dest,src,bytes) memcpy(dest,src,bytes)
#define codeMemmove(dest,src,bytes) memmove((char *)(dest)+codeToDataDelta,src,bytes)
#define cr() putchar('\n')
#define entryOffset() cmEntryOffset
#define generatorAt(index) (&generatorTable[index])
#define getCodeToDataDelta() codeToDataDelta
#define getIsObjectReference() 2
#define halt() warning("halt")
#define haltmsg(msg) warning("halt: " msg)
#define interpretOffset() missOffset
#define mapPerMethodProfile() 0
#define maxCogCodeSize() (16*1024*1024)
#define maybeBreakGeneratingFromto(address,end) 0
#define maybeBreakGeneratingInstructionWithIndex(i) 0
#define maybeHaltIfDebugPC() 0
#define methodLabel() methodLabel
#define methodZoneBase() methodZoneBase
#define minCogMethodAddress() methodZoneBase
#define moveProfileToMethods() 0
#define noCheckEntryOffset() cmNoCheckEntryOffset
#define noContextSwitchBlockEntryOffset() blockNoContextSwitchOffset
#define notYetImplemented() warning("not yet implemented")
#define null 0
#define printNum(n) printf("%" PRIdSQINT, (sqInt) (n))
#define printOnTrace() (traceFlags & 1)
#define recordBlockTrace() (traceFlags & 4)
#define recordEventTrace() (traceFlags & 16)
#define recordFastCCallPrimTrace() (traceFlags & 128)
#define recordOverflowTrace() (traceFlags & 32)
#define recordPrimTrace() (traceFlags & 8)
#define recordSendTrace() (traceFlags & 2)
#define reportError(n) warning("compilation error")
#define setHasMovableLiteral(b) (hasMovableLiteral = (b))
#define setHasYoungReferent(b) (hasYoungReferent = (b))
#define varBaseAddress() varBaseAddress
#define nextOpenPIC methodObject
#define nextOpenPICHack hack hack hack i.e. the getter macro does all the work
#define freeStart() mzFreeStart
#define limitZony() ((CogMethod *)mzFreeStart)
#define methodBytesFreedSinceLastCompaction() methodBytesFreedSinceLastCompaction
#define youngReferrers() youngReferrers
#define numRegArgs() 2
#define maybeConstant(sse) ((sse)->constant)
#define literalInstructionAt(index) (&literals[index])
#define fullBlockEntryOffset() cbEntryOffset
#define fullBlockNoContextSwitchEntryOffset() cbNoSwitchEntryOffset
#define needsFrame() needsFrame
#define fixupAtIndex(index) (&fixups[index])
#define simNativeStackAt(index) (simNativeStack + (index))
#define simSelf() simStack
#define simStackAt(index) (simStack + (index))
#define traceDescriptor(ign) 0
#define traceFixupmerge(igu,ana) 0
#define traceMerge(ign) 0
#define traceSimStack() 0
#define traceSpill(ign) 0
#define allocatype(numElements, elementType) alloca((numElements)*sizeof(elementType))
#define numElementsIn(anArray) (sizeof(anArray)/sizeof(anArray[0]))
#define oopisGreaterThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) >= (usqInt)(otherOop))
#define oopisGreaterThanOrEqualToandLessThanOrEqualTo(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) <= (usqInt)(limitOop))
#define oopisGreaterThanOrEqualToandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) >= (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisGreaterThan(anOop,otherOop) ((usqInt)(anOop) > (usqInt)(otherOop))
#define oopisGreaterThanandLessThan(anOop,baseOop,limitOop) ((usqInt)(anOop) > (usqInt)(baseOop) && (usqInt)(anOop) < (usqInt)(limitOop))
#define oopisLessThanOrEqualTo(anOop,otherOop) ((usqInt)(anOop) <= (usqInt)(otherOop))
#define oopisLessThan(anOop,otherOop) ((usqInt)(anOop) < (usqInt)(otherOop))


	/* CogAbstractInstruction>>#addDependent: */
static AbstractInstruction * NoDbgRegParms
addDependent(AbstractInstruction * self_in_addDependent, AbstractInstruction *anInstruction)
{
	if (!(((self_in_addDependent->dependent)) == null)) {
		(anInstruction->dependent = (self_in_addDependent->dependent));
	}
	return ((self_in_addDependent->dependent) = anInstruction);
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them. */

	/* CogAbstractInstruction>>#availableFloatRegisterOrNoneFor: */
static sqInt NoDbgRegParms
availableFloatRegisterOrNoneFor(AbstractInstruction * self_in_availableFloatRegisterOrNoneFor, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << DPFPReg0))) != 0))) {
		return DPFPReg0;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg1))) != 0))) {
		return DPFPReg1;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg2))) != 0))) {
		return DPFPReg2;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg3))) != 0))) {
		return DPFPReg3;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg4))) != 0))) {
		return DPFPReg4;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg5))) != 0))) {
		return DPFPReg5;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg6))) != 0))) {
		return DPFPReg6;
	}
	if (!(((liveRegsMask & ((1U << DPFPReg7))) != 0))) {
		return DPFPReg7;
	}
	return NoReg;
}


/*	Answer an unused abstract register in the liveRegMask.
	Subclasses with more registers can override to answer them.
	N.B. Do /not/ allocate TempReg. */

	/* CogAbstractInstruction>>#availableRegisterOrNoneFor: */
static sqInt NoDbgRegParms
availableRegisterOrNoneFor(AbstractInstruction * self_in_availableRegisterOrNoneFor, sqInt liveRegsMask)
{
	if (!(((liveRegsMask & ((1U << Arg1Reg))) != 0))) {
		return Arg1Reg;
	}
	if (!(((liveRegsMask & ((1U << Arg0Reg))) != 0))) {
		return Arg0Reg;
	}
	if (!(((liveRegsMask & ((1U << SendNumArgsReg))) != 0))) {
		return SendNumArgsReg;
	}
	if (!(((liveRegsMask & ((1U << ClassReg))) != 0))) {
		return ClassReg;
	}
	if (!(((liveRegsMask & ((1U << ReceiverResultReg))) != 0))) {
		return ReceiverResultReg;
	}
	return NoReg;
}


/*	For out-of-line literal support, clone a literal from a literal. */

	/* CogAbstractInstruction>>#cloneLiteralFrom: */
static void NoDbgRegParms
cloneLiteralFrom(AbstractInstruction * self_in_cloneLiteralFrom, AbstractInstruction *existingLiteral)
{
	assert((((existingLiteral->opcode)) == Literal)
	 && ((((self_in_cloneLiteralFrom->dependent)) == null)
	 && (((self_in_cloneLiteralFrom->address)) == null)));
	(self_in_cloneLiteralFrom->opcode) = Literal;
	(self_in_cloneLiteralFrom->annotation) = (existingLiteral->annotation);
	((self_in_cloneLiteralFrom->operands))[0] = (((existingLiteral->operands))[0]);
	((self_in_cloneLiteralFrom->operands))[1] = (((existingLiteral->operands))[1]);
	((self_in_cloneLiteralFrom->operands))[2] = (((existingLiteral->operands))[2]);
}


/*	Generate concrete machine code for the instruction at actualAddress,
	setting machineCodeSize, and answer the following address. */

	/* CogAbstractInstruction>>#concretizeAt: */
static sqInt NoDbgRegParms
concretizeAt(AbstractInstruction * self_in_concretizeAt, sqInt actualAddress)
{
	(self_in_concretizeAt->address) = actualAddress;
	(self_in_concretizeAt->machineCodeSize) = dispatchConcretize(self_in_concretizeAt);
	assert((((self_in_concretizeAt->maxSize)) == null)
	 || (((self_in_concretizeAt->maxSize)) >= ((self_in_concretizeAt->machineCodeSize))));
	return actualAddress + ((self_in_concretizeAt->machineCodeSize));
}


/*	Load the stack pointer register with that of the C stack, effecting
	a switch to the C stack. Used when machine code calls into the
	CoInterpreter run-time (e.g. to invoke interpreter primitives). */

	/* CogAbstractInstruction>>#genLoadCStackPointer */
static sqInt NoDbgRegParms
genLoadCStackPointer(AbstractInstruction * self_in_genLoadCStackPointer)
{
    sqInt operandOne;

	/* begin gen:literal:operand: */
	operandOne = cStackPointerAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, NativeSPReg));
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointerForPrimCall: */
static sqInt NoDbgRegParms
genLoadStackPointerForPrimCall(AbstractInstruction * self_in_genLoadStackPointerForPrimCall, sqInt spareReg)
{
    sqInt operandOne;

	/* begin gen:literal:operand: */
	operandOne = stackPointerAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, SPReg));
	return 0;
}


/*	Switch back to the Smalltalk stack where there may be a C return address
	on top of stack below
	the last primitive argument. Assign SPReg first because typically it is
	used immediately afterwards.
 */

	/* CogAbstractInstruction>>#genLoadStackPointersForPrimCall: */
static sqInt NoDbgRegParms
genLoadStackPointersForPrimCall(AbstractInstruction * self_in_genLoadStackPointersForPrimCall, sqInt spareReg)
{
	genLoadStackPointers(self_in_genLoadStackPointersForPrimCall);
	return 0;
}


/*	Generic register swap code. Subclasses for processors that have a true
	exchange operation will override to use it. */

	/* CogAbstractInstruction>>#genSwapR:R:Scratch: */
static AbstractInstruction * NoDbgRegParms
genSwapRRScratch(AbstractInstruction * self_in_genSwapRRScratch, sqInt regA, sqInt regB, sqInt regTmp)
{
    AbstractInstruction *first;

	first = genoperandoperand(MoveRR, regA, regTmp);
	genoperandoperand(MoveRR, regB, regA);
	genoperandoperand(MoveRR, TempReg, regB);
	return first;
}

	/* CogAbstractInstruction>>#genWriteCResultIntoReg: */
static void NoDbgRegParms
genWriteCResultIntoReg(AbstractInstruction * self_in_genWriteCResultIntoReg, sqInt abstractRegister)
{
	if ((abstractRegister != NoReg)
	 && (abstractRegister != ABIResultReg)) {
		genoperandoperand(MoveRR, ABIResultReg, abstractRegister);
	}
}


/*	For out-of-line literal support, initialize a sharable literal. */

	/* CogAbstractInstruction>>#initializeSharableLiteral: */
static void NoDbgRegParms
initializeSharableLiteral(AbstractInstruction * self_in_initializeSharableLiteral, sqInt literal)
{
	(self_in_initializeSharableLiteral->opcode) = Literal;

	/* separate := nil for Slang */
	(self_in_initializeSharableLiteral->annotation) = null;
	(self_in_initializeSharableLiteral->address) = null;
	(self_in_initializeSharableLiteral->dependent) = null;
	((self_in_initializeSharableLiteral->operands))[0] = literal;
	((self_in_initializeSharableLiteral->operands))[1] = (1 + (((sqInt)((usqInt)(BytesPerOop) << 1))));
	((self_in_initializeSharableLiteral->operands))[2] = -1;
}


/*	For out-of-line literal support, initialize an unsharable literal. */

	/* CogAbstractInstruction>>#initializeUniqueLiteral: */
static void NoDbgRegParms
initializeUniqueLiteral(AbstractInstruction * self_in_initializeUniqueLiteral, sqInt literal)
{
	(self_in_initializeUniqueLiteral->opcode) = Literal;

	/* separate := nil for Slang */
	(self_in_initializeUniqueLiteral->annotation) = null;
	(self_in_initializeUniqueLiteral->address) = null;
	(self_in_initializeUniqueLiteral->dependent) = null;
	((self_in_initializeUniqueLiteral->operands))[0] = literal;
	((self_in_initializeUniqueLiteral->operands))[1] = (0 + (((sqInt)((usqInt)(BytesPerOop) << 1))));
	((self_in_initializeUniqueLiteral->operands))[2] = -1;
}


/*	Answer if an address can be accessed using the offset in a MoveMw:r:R: or
	similar instruction.
	We assume this is true for 32-bit processors and expect 64-bit processors
	to answer false
	for values in the interpreter or the object memory. */

	/* CogAbstractInstruction>>#isWithinMwOffsetRange: */
static sqInt NoDbgRegParms
isWithinMwOffsetRange(AbstractInstruction * self_in_isWithinMwOffsetRange, sqInt anAddress)
{
	return 1;
}


/*	Set the target of a jump instruction. These all have the target in the
	first operand. */

	/* CogAbstractInstruction>>#jmpTarget: */
static AbstractInstruction * NoDbgRegParms
jmpTarget(AbstractInstruction * self_in_jmpTarget, AbstractInstruction *anAbstractInstruction)
{
	((self_in_jmpTarget->operands))[0] = (((usqInt)anAbstractInstruction));
	return anAbstractInstruction;
}


/*	We assume here that calls and jumps look the same as regards their
	displacement. This works on at least x86, ARM and x86_64. Processors on
	which that isn't the
	case can override as necessary. */

	/* CogAbstractInstruction>>#relocateJumpLongBeforeFollowingAddress:by: */
static void NoDbgRegParms
relocateJumpLongBeforeFollowingAddressby(AbstractInstruction * self_in_relocateJumpLongBeforeFollowingAddressby, sqInt pc, sqInt delta)
{
	relocateCallBeforeReturnPCby(self_in_relocateJumpLongBeforeFollowingAddressby, pc, delta);
}


/*	Relocate a long conditional jump before pc. Default to relocating a
	non-conditional jump.
	Processors that have different formats for conditional and unconditional
	jumps override. */

	/* CogAbstractInstruction>>#relocateJumpLongConditionalBeforeFollowingAddress:by: */
static void NoDbgRegParms
relocateJumpLongConditionalBeforeFollowingAddressby(AbstractInstruction * self_in_relocateJumpLongConditionalBeforeFollowingAddressby, sqInt pc, sqInt delta)
{
	relocateJumpLongBeforeFollowingAddressby(self_in_relocateJumpLongConditionalBeforeFollowingAddressby, pc, delta);
}

	/* CogAbstractInstruction>>#resolveJumpTarget */
static void NoDbgRegParms
resolveJumpTarget(AbstractInstruction * self_in_resolveJumpTarget)
{
    BytecodeFixup *fixup;

	assert(isJump(self_in_resolveJumpTarget));
	fixup = ((BytecodeFixup *) (((self_in_resolveJumpTarget->operands))[0]));
	if (addressIsInFixups(fixup)) {
		assert(addressIsInInstructions((fixup->targetInstruction)));
		jmpTarget(self_in_resolveJumpTarget, (fixup->targetInstruction));
	}
}


/*	Rewrite a CallFull instruction to call a different target. This variant is
	used to rewrite cached primitive calls.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush.
	This defaults to rewriteCallAt:target:; processors that differentiate
	between Call and CallFull will override. */

	/* CogAbstractInstruction>>#rewriteCallFullAt:target: */
static sqInt NoDbgRegParms
rewriteCallFullAttarget(AbstractInstruction * self_in_rewriteCallFullAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteCallAttarget(self_in_rewriteCallFullAttarget, callSiteReturnAddress, callTargetAddress);
}


/*	Rewrite a conditional jump long to jump to target. This version defaults
	to using
	rewriteJumpLongAt:, which works for many ISAs. Subclasses override if
	necessary.  */

	/* CogAbstractInstruction>>#rewriteConditionalJumpLongAt:target: */
static sqInt NoDbgRegParms
rewriteConditionalJumpLongAttarget(AbstractInstruction * self_in_rewriteConditionalJumpLongAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	return rewriteImm26JumpBeforetarget(self_in_rewriteConditionalJumpLongAttarget, callSiteReturnAddress, callTargetAddress);
}


/*	C6.2.4 ADD (immediate) p761 */

	/* CogARMv8Compiler>>#addrn:rd:imm:shiftBy12: */
static sqInt NoDbgRegParms
addrnrdimmshiftBy12(AbstractInstruction * self_in_addrnrdimmshiftBy12, sqInt rn, sqInt rd, sqInt offset, sqInt shiftBy12)
{
	assert(((offset >= 0) && (offset <= (0xFFF))));
	return (((0x91000000U + ((shiftBy12
	? 0x400000
	: 0))) + (((sqInt)((usqInt)(offset) << 10)))) + (((sqInt)((usqInt)(rn) << 5)))) + rd;
}

	/* CogARMv8Compiler>>#brlink:reg: */
static sqInt NoDbgRegParms
brlinkreg(AbstractInstruction * self_in_brlinkreg, sqInt link, sqInt reg)
{
	return ((0xD61F0000U) + ((link
	? 0x200000
	: 0))) + (((sqInt)((usqInt)(reg) << 5)));
}

	/* CogARMv8Compiler>>#callFullInstructionByteSize */
static sqInt NoDbgRegParms
callFullInstructionByteSize(AbstractInstruction * self_in_callFullInstructionByteSize)
{
	return 8;
}


/*	ARM calls and jumps span +/- 32 mb, more than enough for intra-zone calls
	and jumps.
 */

	/* CogARMv8Compiler>>#callInstructionByteSize */
static sqInt NoDbgRegParms
callInstructionByteSize(AbstractInstruction * self_in_callInstructionByteSize)
{
	return 4;
}

	/* CogARMv8Compiler>>#callTargetFromReturnAddress: */
static sqInt NoDbgRegParms
callTargetFromReturnAddress(AbstractInstruction * self_in_callTargetFromReturnAddress, sqInt mcpc)
{
    unsigned int instr;


	/* C6.2.26 	B		C6-799
	   C6.2.33	BL		C6-812 */
	instr = long32At(mcpc - 4);
	assert(((((usqInt)(instr)) >> 26) == 37)
	 || ((((usqInt)(instr)) >> 26) == 5));
	return (((((sqLong) (((sqInt)((usqInt)((instr & (0x3FFFFFF))) << 38)))))) >> 36) + (mcpc - 4);
}

	/* CogARMv8Compiler>>#canDivQuoRem */
static sqInt NoDbgRegParms
canDivQuoRem(AbstractInstruction * self_in_canDivQuoRem)
{
	return 1;
}

	/* CogARMv8Compiler>>#cmpC32RTempByteSize */
static sqInt NoDbgRegParms
cmpC32RTempByteSize(AbstractInstruction * self_in_cmpC32RTempByteSize)
{
	return 8;
}

	/* CogARMv8Compiler>>#computeJumpTargetOffset */
static sqInt NoDbgRegParms
computeJumpTargetOffset(AbstractInstruction * self_in_computeJumpTargetOffset)
{
    AbstractInstruction *jumpTarget;
    AbstractInstruction *jumpTarget1;

	/* begin jumpTargetAddress */
	jumpTarget1 = ((AbstractInstruction *) (((self_in_computeJumpTargetOffset->operands))[0]));
	assertSaneJumpTarget(jumpTarget1);
	if ((addressIsInInstructions(jumpTarget1))
	 || (jumpTarget1 == (methodLabel()))) {
		jumpTarget1 = ((AbstractInstruction *) ((jumpTarget1->address)));
	}
	assert(jumpTarget1 != 0);
	jumpTarget = jumpTarget1;
	return (((sqLong) jumpTarget)) - (((sqLong) ((self_in_computeJumpTargetOffset->address))));
}


/*	Answer the index of the low order one bit.
	2r00101000 lowBit (Answers: 4)
	2r-00101000 lowBit (Answers: 4)
	This is an implementation of SmallInteger>>lowBit */

	/* CogARMv8Compiler>>#computeLowBit: */
static sqInt NoDbgRegParms
computeLowBit(AbstractInstruction * self_in_computeLowBit, sqInt nArg)
{
    sqInt n;
    sqInt result;

	if (nArg == 0) {
		return 0;
	}
	n = nArg;
	result = 1;
	while ((!(n & 0xFF))) {
		result += 8;
		n = (((usqInt)(n)) >> 8);
	}
	if ((!(n & 15))) {
		result += 4;
		n = (((usqInt)(n)) >> 4);
	}
	if ((!(n & 3))) {
		result += 2;
		n = (((usqInt)(n)) >> 2);
	}
	return (((n & 1) != 0)
		? result
		: result + 1);
}


/*	Because we don't use Thumb, each ARMv8 instruction has 4 bytes. Several
	abstract opcodes need more than one instruction. Instructions that refer
	to constants and/or literals depend on literals being stored out-of-line
	or encoded
	in immediate instruction fields (i.e. we only support
	OutOfLineLiteralsManager. 
	N.B. The ^N forms are to get around the old bytecode compiler's long
	branch limits which are exceeded when each case jumps around the
	otherwise.  */

	/* CogARMv8Compiler>>#computeMaximumSize */
static sqInt NoDbgRegParms
computeMaximumSize(AbstractInstruction * self_in_computeMaximumSize)
{
    sqInt constant;
    sqInt constant1;
    sqInt imm;
    sqInt imm1;
    sqInt immediate;
    sqInt immr1;
    sqInt immr2;
    usqInt mask;
    usqInt mask1;
    usqInt n1;
    usqInt n2;
    usqInt nImms;
    usqInt nImms1;
    sqInt numLeadingOnes;
    sqInt numLeadingOnes1;
    sqInt numTrailingOnes;
    sqInt numTrailingOnes1;
    sqInt rotateCount;
    sqInt rotateCount1;
    sqInt size;
    sqInt size1;

	switch ((self_in_computeMaximumSize->opcode)) {
	case Label:
		return 0;

	case Literal:
		return (assert(((self_in_computeMaximumSize->opcode)) == Literal),
			((((self_in_computeMaximumSize->operands))[1]) == null
					? BytesPerOop
					: (((((self_in_computeMaximumSize->operands))[1])) >> 1) & 15));

	case AlignmentNops:
		return (((self_in_computeMaximumSize->operands))[0]) - 4;

	case CallFull:
	case JumpFull:
	case JumpLongZero:
	case JumpLongNonZero:
	case JumpMulOverflow:
	case JumpNoMulOverflow:
	case JumpFPOrdered:
	case JumpFPUnordered:
	case AddCwR:
	case AndCwR:
	case CmpCwR:
	case CmpC32R:
	case OrCwR:
	case SubCwR:
	case XorCwR:
	case PushCw:
	case PushCq:
		return 8;

	case RetN:
	case NativeRetN:
		return ((((self_in_computeMaximumSize->operands))[0]) == 0
			? 4
			: 8);

	case AddCqR:
	case AddCqRR:
	case CmpCqR:
	case SubCqR:
	case LoadEffectiveAddressMwrR:
		/* begin isPossiblyShiftableNegatableImm12:ifTrue:ifFalse: */
		immediate = ((sqLong) (((self_in_computeMaximumSize->operands))[0]));
		if (((immediate >= (-4096)) && (immediate <= (0xFFF)))) {
			return 4;
		}
		if (((!(immediate & (0xFFF))))
		 && (((((immediate) >> 12) >= (-4096)) && (((immediate) >> 12) <= (0xFFF))))) {
			return 4;
		}
		return 8;

	case AndCqR:
	case OrCqR:
	case OrCqRR:
	case TstCqR:
	case XorCqR:
		/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
		constant = ((self_in_computeMaximumSize->operands))[0];
		if (((constant >= -1) && (constant <= 0))) {
			return 8;
		}

		/* First, determine the element size. */
		imm = constant;
		size = 32;
		while (1) {
			mask = (1ULL << size) - 1;
			if (!(((imm & mask) != ((((usqInt)(imm)) >> size) & mask)
				? ((size = size * 2),
					0)
				: size > 2))) break;
			size = size / 2;
		}
		mask = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size);
		imm = imm & mask;
		if (isShiftedMask(self_in_computeMaximumSize, imm)) {
			rotateCount = countTrailingZeros(self_in_computeMaximumSize, imm);
			numTrailingOnes = countTrailingOnes(self_in_computeMaximumSize, ((usqInt)(imm)) >> rotateCount);
		}
		else {
			imm = imm | (~(usqIntptr_t)mask);
			if (!(isShiftedMask(self_in_computeMaximumSize, imm))) {
				return 8;
			}
			numLeadingOnes = countLeadingOnes(self_in_computeMaximumSize, imm);
			rotateCount = 64 - numLeadingOnes;
			numTrailingOnes = (numLeadingOnes + (countTrailingOnes(self_in_computeMaximumSize, imm))) - (64 - size);
		}
		assert(size > rotateCount);

		/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
		immr1 = (size - rotateCount) & (size - 1);

		/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
		nImms = ((sqInt)((usqInt)((~(usqIntptr_t)(size - 1))) << 1));

		/* Extract the seventh bit and toggle it to create the N field. */
		nImms = nImms | (numTrailingOnes - 1);
		n1 = (((nImms) >> 6) & 1) ^ 1;
		nImms = nImms & 0x3F;
		assert((decodeNimmsimmr(self_in_computeMaximumSize, n1, nImms, immr1)) == (((usqInt) constant)));
		return 4;

	case AndCqRR:
		
		/* N.B. For three operand logical ops only support AndCqRR with a NativeSPReg target, used for alignment purposes. */
		/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
		constant1 = ((self_in_computeMaximumSize->operands))[0];
		if (((constant1 >= -1) && (constant1 <= 0))) {
			return ((((self_in_computeMaximumSize->operands))[2]) == SP
				? 12
				: 8);
		}

		/* First, determine the element size. */
		imm1 = constant1;
		size1 = 32;
		while (1) {
			mask1 = (1ULL << size1) - 1;
			if (!(((imm1 & mask1) != ((((usqInt)(imm1)) >> size1) & mask1)
				? ((size1 = size1 * 2),
					0)
				: size1 > 2))) break;
			size1 = size1 / 2;
		}
		mask1 = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size1);
		imm1 = imm1 & mask1;
		if (isShiftedMask(self_in_computeMaximumSize, imm1)) {
			rotateCount1 = countTrailingZeros(self_in_computeMaximumSize, imm1);
			numTrailingOnes1 = countTrailingOnes(self_in_computeMaximumSize, ((usqInt)(imm1)) >> rotateCount1);
		}
		else {
			imm1 = imm1 | (~(usqIntptr_t)mask1);
			if (!(isShiftedMask(self_in_computeMaximumSize, imm1))) {
				return ((((self_in_computeMaximumSize->operands))[2]) == SP
					? 12
					: 8);
			}
			numLeadingOnes1 = countLeadingOnes(self_in_computeMaximumSize, imm1);
			rotateCount1 = 64 - numLeadingOnes1;
			numTrailingOnes1 = (numLeadingOnes1 + (countTrailingOnes(self_in_computeMaximumSize, imm1))) - (64 - size1);
		}
		assert(size1 > rotateCount1);

		/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
		immr2 = (size1 - rotateCount1) & (size1 - 1);

		/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
		nImms1 = ((sqInt)((usqInt)((~(usqIntptr_t)(size1 - 1))) << 1));

		/* Extract the seventh bit and toggle it to create the N field. */
		nImms1 = nImms1 | (numTrailingOnes1 - 1);
		n2 = (((nImms1) >> 6) & 1) ^ 1;
		nImms1 = nImms1 & 0x3F;
		assert((decodeNimmsimmr(self_in_computeMaximumSize, n2, nImms1, immr2)) == (((usqInt) constant1)));
		return ((((self_in_computeMaximumSize->operands))[2]) == SP
			? 8
			: 4);

	case SubRR:
	case SubRRR:
		return ((((self_in_computeMaximumSize->operands))[0]) == SP
			? 8
			: 4);

	case MulOverflowRRR:
		return 12;

	case MoveAwR:
		return ((((((self_in_computeMaximumSize->operands))[0]) != null)
		 && ((((((self_in_computeMaximumSize->operands))[0]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_computeMaximumSize->operands))[0]) <= (((varBaseAddress()) + (0x1000)) - 1)))))
		 || (((((usqInt)(((self_in_computeMaximumSize->operands))[0]))) >= ((methodLabel->address)))
		 && ((((usqInt)(((self_in_computeMaximumSize->operands))[0]))) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize)))))
			? ((((self_in_computeMaximumSize->operands))[1]) != SP
					? 4
					: 8)
			: ((((self_in_computeMaximumSize->operands))[1]) != SP
					? 8
					: 12));

	case MoveRAw:
		return ((((((self_in_computeMaximumSize->operands))[1]) != null)
		 && ((((((self_in_computeMaximumSize->operands))[1]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_computeMaximumSize->operands))[1]) <= (((varBaseAddress()) + (0x1000)) - 1)))))
		 || (((((usqInt)(((self_in_computeMaximumSize->operands))[1]))) >= ((methodLabel->address)))
		 && ((((usqInt)(((self_in_computeMaximumSize->operands))[1]))) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize)))))
			? ((((self_in_computeMaximumSize->operands))[0]) != SP
					? 4
					: 8)
			: ((((self_in_computeMaximumSize->operands))[0]) != SP
					? 8
					: 12));

	case MoveAwRR:
		assert(isAddressRelativeToVarBase(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[0]));
		return (((((self_in_computeMaximumSize->operands))[1]) == SP)
		 || ((((self_in_computeMaximumSize->operands))[2]) == SP)
			? 8
			: 4);

	case MoveRRAw:
		assert(isAddressRelativeToVarBase(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[2]));
		return (((((self_in_computeMaximumSize->operands))[0]) == SP)
		 || ((((self_in_computeMaximumSize->operands))[1]) == SP)
			? 8
			: 4);

	case MoveAbR:
		return (((((self_in_computeMaximumSize->operands))[0]) != null)
		 && ((((((self_in_computeMaximumSize->operands))[0]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_computeMaximumSize->operands))[0]) <= (((varBaseAddress()) + (0x1000)) - 1))))
			? 4
			: 8);

	case MoveRAb:
		return (((((self_in_computeMaximumSize->operands))[1]) != null)
		 && ((((((self_in_computeMaximumSize->operands))[1]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_computeMaximumSize->operands))[1]) <= (((varBaseAddress()) + (0x1000)) - 1))))
			? 4
			: 8);

	case MoveMwrR:
	case MoveM32rR:
	case MoveM16rR:
	case MoveMbrR:
		return (isImm12orImm9offset(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[0])
			? 4
			: 8);

	case MoveRMwr:
	case MoveRM32r:
	case MoveRM16r:
	case MoveRMbr:
		return (isImm12orImm9offset(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[1])
			? 4
			: 8);

	case MoveM64rRd:
		return (isUnsigned12BitMultipleOf8(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[0])
			? 4
			: 8);

	case MoveRdM64r:
		return (isUnsigned12BitMultipleOf8(self_in_computeMaximumSize, ((self_in_computeMaximumSize->operands))[1])
			? 4
			: 8);

	default:
		return 4;

	}
	return 0;
}


/*	Will get inlined into concretizeAt: switch. */
/*	Sizing/generating jumps.
	Jump targets can be to absolute addresses or other abstract instructions.
	Generating initial trampolines instructions may have no maxSize and be to
	absolute addresses.
	Otherwise instructions must have a machineCodeSize which must be kept to. */

	/* CogARMv8Compiler>>#concretizeConditionalJump: */
static sqInt NoDbgRegParms
concretizeConditionalJump(AbstractInstruction * self_in_concretizeConditionalJump, sqInt conditionCode)
{
    AbstractInstruction *jumpTarget;
    AbstractInstruction *jumpTarget1;
    sqInt offset;

	/* begin computeJumpTargetOffset */
	jumpTarget1 = ((AbstractInstruction *) (((self_in_concretizeConditionalJump->operands))[0]));
	assertSaneJumpTarget(jumpTarget1);
	if ((addressIsInInstructions(jumpTarget1))
	 || (jumpTarget1 == (methodLabel()))) {
		jumpTarget1 = ((AbstractInstruction *) ((jumpTarget1->address)));
	}
	assert(jumpTarget1 != 0);
	jumpTarget = jumpTarget1;
	offset = (((sqLong) jumpTarget)) - (((sqLong) ((self_in_concretizeConditionalJump->address))));
	assert((offset != 0)
	 && (isInImmediateBranchRange(self_in_concretizeConditionalJump, offset)));
	((self_in_concretizeConditionalJump->machineCode))[0] = ((assert((offset & 3) == 0),
((0x54000000) + (((sqInt)((usqInt)((offset & (0x1FFFFF))) << 3)))) + conditionCode));
	return 4;
}


/*	cogit processor disassembleInstructionAt: 0 In: machineCode object */
/*	cogit processor disassembleInstructionAt: 4 In: machineCode object */

	/* CogARMv8Compiler>>#concretizeCwRArithmetic:Rd: */
static sqInt NoDbgRegParms
concretizeCwRArithmeticRd(AbstractInstruction * self_in_concretizeCwRArithmeticRd, sqInt arithOp, sqInt destRegOrXZR)
{
    sqInt instrBytes;


	/* ADD (extended register) on page C6-758
	   ADDS (extended register) on page C6-766
	   SUB (extended register) on page C6-1308
	   SUBS (extended register) on page C6-1318 */
	instrBytes = emitMoveCwintoRat(self_in_concretizeCwRArithmeticRd, ((self_in_concretizeCwRArithmeticRd->operands))[0], RISCTempReg, 0);
	((self_in_concretizeCwRArithmeticRd->machineCode))[instrBytes / 4] = ((((((0x8B200000U) + (((sqInt)((usqInt)(arithOp) << 29)))) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + ((((self_in_concretizeCwRArithmeticRd->operands))[1]) << 5)) + destRegOrXZR);
	return instrBytes + 4;
}

	/* CogARMv8Compiler>>#concretizeCwRLogical: */
static sqInt NoDbgRegParms
concretizeCwRLogical(AbstractInstruction * self_in_concretizeCwRLogical, sqInt op)
{
    usqInt constant;
    usqInt destReg;
    sqInt offset;

	constant = ((self_in_concretizeCwRLogical->operands))[0];
	destReg = ((self_in_concretizeCwRLogical->operands))[1];

	/* AND	(shifted register) - 64-bit variant on page C6-777
	   BIC	(shifted register) - 64-bit variant on page C6-808
	   ORR	(shifted register) - 64-bit variant on page C6-1127
	   ORN	(shifted register) - 64-bit variant on page C6-1123
	   EOR	(shifted register) - 64-bit variant on page C6-898
	   EON	(shifted register) - 64-bit variant on page C6-894
	   ANDS	(shifted register) - 64-bit variant on page C6-781
	   BICS	(shifted register) - 64-bit variant on page C6-810 */
	offset = emitMoveCwintoRat(self_in_concretizeCwRLogical, constant, RISCTempReg, 0);
	((self_in_concretizeCwRLogical->machineCode))[offset / 4] = (((((0x8A000000U) + (((sqInt)((usqInt)(op) << 29)))) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (destReg << 5)) + destReg);
	return offset + 4;
}


/*	Issue a DC CIVAC, CVAC or CVAU
	C5.3.14	DC CIVAC, Data or unified Cache line Clean and Invalidate by VA to
	PoC		C5-486 Clean and Invalidate data cache by address to Point of
	Coherency. C5.3.16	DC CVAC, Data or unified Cache line Clean by VA to
	PoC						C5-490 Clean data cache by address to Point of Coherency.
	C5.3.19	DC CVAU, Clean data cache by address to Point of
	Unification				C5-496 Clean data cache by address to Point of Unification. */
/*	(operands at: 0) is the target register, accessed within
	concretizeCacheControlOp1:CRm:Op2: 
 */

	/* CogARMv8Compiler>>#concretizeDataCacheControl */
static sqInt NoDbgRegParms
concretizeDataCacheControl(AbstractInstruction * self_in_concretizeDataCacheControl)
{
    int cacheOpcode;

	switch (((self_in_concretizeDataCacheControl->operands))[1]) {
	case DC_CIVAC:
		cacheOpcode = 14;
		break;

	case DC_CVAC:
		cacheOpcode = 10;
		break;

	case DC_CVAU:
		cacheOpcode = 11;
		break;

	default:
		error("Case not found and no otherwise clause");
		cacheOpcode = -1;
	}
	/* begin concretizeCacheControlOp1:CRm:Op2: */
	((self_in_concretizeDataCacheControl->machineCode))[0] = ((((0xD50B7000U) + (((sqInt)((usqInt)(cacheOpcode) << 8)))) + (32)) + (((self_in_concretizeDataCacheControl->operands))[0]));
	return 4;
}


/*	fill with operand 0 according to the processor's endianness */

	/* CogARMv8Compiler>>#concretizeFill32 */
static sqInt NoDbgRegParms
concretizeFill32(AbstractInstruction * self_in_concretizeFill32)
{
	((self_in_concretizeFill32->machineCode))[0] = (((self_in_concretizeFill32->operands))[0]);
	return 4;
}


/*	Generate an out-of-line literal. Copy the value and any annotation from
	the stand-in in the literals manager. */

	/* CogARMv8Compiler>>#concretizeLiteral */
static sqInt NoDbgRegParms
concretizeLiteral(AbstractInstruction * self_in_concretizeLiteral)
{
    usqInt literal;
    AbstractInstruction *literalAsInstruction;

	literalAsInstruction = ((AbstractInstruction *) (((self_in_concretizeLiteral->operands))[0]));
	literal = ((addressIsInInstructions(literalAsInstruction))
	 || (literalAsInstruction == (methodLabel()))
		? (literalAsInstruction->address)
		: ((usqInt)literalAsInstruction));
	assert((((self_in_concretizeLiteral->dependent)) != null)
	 && (((((self_in_concretizeLiteral->dependent))->opcode)) == Literal));
	if (!(((((self_in_concretizeLiteral->dependent))->annotation)) == null)) {
		assert(((self_in_concretizeLiteral->annotation)) == null);
		(self_in_concretizeLiteral->annotation) = (((self_in_concretizeLiteral->dependent))->annotation);
	}
	if (!(((((self_in_concretizeLiteral->dependent))->address)) == null)) {
		assert(((((self_in_concretizeLiteral->dependent))->address)) == ((self_in_concretizeLiteral->address)));
	}
	(((self_in_concretizeLiteral->dependent))->address = (self_in_concretizeLiteral->address));
	((self_in_concretizeLiteral->machineCode))[0] = (literal & 0xFFFFFFFFU);
	if (((assert(((self_in_concretizeLiteral->opcode)) == Literal),
	((((self_in_concretizeLiteral->operands))[1]) == null
			? BytesPerOop
			: (((((self_in_concretizeLiteral->operands))[1])) >> 1) & 15))) == 4) {
		return 4;
	}
	((self_in_concretizeLiteral->machineCode))[1] = ((literal) >> 32);
	return 8;
}

	/* CogARMv8Compiler>>#concretizeLoadEffectiveAddressMwrR */
static sqInt NoDbgRegParms
concretizeLoadEffectiveAddressMwrR(AbstractInstruction * self_in_concretizeLoadEffectiveAddressMwrR)
{
    usqInt baseReg;
    usqInt destReg;
    sqInt instrBytes;
    sqInt offset;

	offset = ((self_in_concretizeLoadEffectiveAddressMwrR->operands))[0];
	baseReg = ((self_in_concretizeLoadEffectiveAddressMwrR->operands))[1];
	destReg = ((self_in_concretizeLoadEffectiveAddressMwrR->operands))[2];
	/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
	if (((offset >= 0) && (offset <= (0xFFF)))) {
		
		/* C6.2.4		ADD (immediate)	C6-761 */
		((self_in_concretizeLoadEffectiveAddressMwrR->machineCode))[0] = ((((0x91000000U) + (((sqInt)((usqInt)(offset) << 10)))) + (baseReg << 5)) + destReg);
		return 4;
		goto l1;
	}
	if (((!(offset & (0xFFF))))
	 && (((((offset) >> 12) >= 0) && (((offset) >> 12) <= (0xFFF))))) {
		
		/* C6.2.4		ADD (immediate)	C6-761 */
		((self_in_concretizeLoadEffectiveAddressMwrR->machineCode))[0] = ((((0x91000000U) + (((usqInt)(offset)) >> 2)) + (baseReg << 5)) + destReg);
		return 4;
		goto l1;
	}
	l1:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
	/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
	if ((((-offset) >= 0) && ((-offset) <= (0xFFF)))) {
		
		/* C6.2.308		SUB (immediate)	C6-1311 */
		((self_in_concretizeLoadEffectiveAddressMwrR->machineCode))[0] = ((((0x91000000U) + (((sqInt)((usqInt)((-offset)) << 10)))) + (baseReg << 5)) + destReg);
		return 4;
		goto l2;
	}
	if (((!((-offset) & (0xFFF))))
	 && ((((((-offset)) >> 12) >= 0) && ((((-offset)) >> 12) <= (0xFFF))))) {
		
		/* C6.2.308		SUB (immediate)	C6-1311 */
		((self_in_concretizeLoadEffectiveAddressMwrR->machineCode))[0] = ((((0x91000000U) + (((usqInt)((-offset))) >> 2)) + (baseReg << 5)) + destReg);
		return 4;
		goto l2;
	}
	l2:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;

	/* C6.2.3		ADD (extended register) C6-758 */
	instrBytes = emitMoveCwintoRat(self_in_concretizeLoadEffectiveAddressMwrR, offset, RISCTempReg, 0);
	((self_in_concretizeLoadEffectiveAddressMwrR->machineCode))[instrBytes / 4] = (((((0x8B200000U) + (destReg << 16)) + (((sqInt)((usqInt)(UXTX) << 13)))) + (((sqInt)((usqInt)(RISCTempReg) << 5)))) + destReg);
	return instrBytes + 4;
}


/*	AND	(immediate) - 64-bit variant on page C6-775
	ORR	(immediate) - 64-bit variant on page C6-1125
	EOR	(immediate) - 64-bit variant on page C6-896
	ANDS	(immediate) - 64-bit variant on page C6-779
	C6.2.329	TST (immediate)	C6-1346 */

	/* CogARMv8Compiler>>#concretizeLogicalOp:CqRDest: */
static sqInt NoDbgRegParms
concretizeLogicalOpCqRDest(AbstractInstruction * self_in_concretizeLogicalOpCqRDest, sqInt op, sqInt destReg)
{
    usqInt constant;
    sqInt effectiveDestReg;
    sqInt imm;
    sqInt immr1;
    usqInt mask;
    usqInt n1;
    usqInt nImms;
    sqInt numLeadingOnes;
    sqInt numTrailingOnes;
    sqInt offset;
    sqInt rotateCount;
    sqInt size;
    usqInt srcReg;

	constant = ((self_in_concretizeLogicalOpCqRDest->operands))[0];

	/* N.B. For three operand logical ops only support AndCq: const R: reg R: NativeSPReg, which is used for alignment. */
	srcReg = ((self_in_concretizeLogicalOpCqRDest->operands))[1];
	effectiveDestReg = ((((self_in_concretizeLogicalOpCqRDest->opcode)) == AndCqRR)
	 && (destReg == SP)
		? RISCTempReg
		: destReg);
	/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
	if (((constant >= -1) && (constant <= 0))) {
		
		/* OPC	N
		   00		0	AND (shifted register) - 64-bit variant on page C6-777
		   00		1	BIC (shifted register) - 64-bit variant on page C6-808
		   01		0	ORR (shifted register) - 64-bit variant on page C6-1127
		   01		1	ORN (shifted register) - 64-bit variant on page C6-1123
		   10		0	EOR (shifted register) - 64-bit variant on page C6-898
		   10		1	EON (shifted register) - 64-bit variant on page C6-894
		   11		0	ANDS (shifted register) - 64-bit variant on page C6-781
		   11		0	BICS (shifted register) - 64-bit variant on page C6-810 */
		offset = emitMoveCwintoRat(self_in_concretizeLogicalOpCqRDest, constant, RISCTempReg, 0);
		((self_in_concretizeLogicalOpCqRDest->machineCode))[offset / 4] = (((((0x8A000000U) + (((sqInt)((usqInt)(op) << 29)))) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (srcReg << 5)) + effectiveDestReg);
		offset += 4;
		goto l1;
	}

	/* First, determine the element size. */
	imm = constant;
	size = 32;
	while (1) {
		mask = (1ULL << size) - 1;
		if (!(((imm & mask) != ((((usqInt)(imm)) >> size) & mask)
			? ((size = size * 2),
				0)
			: size > 2))) break;
		size = size / 2;
	}
	mask = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size);
	imm = imm & mask;
	if (isShiftedMask(self_in_concretizeLogicalOpCqRDest, imm)) {
		rotateCount = countTrailingZeros(self_in_concretizeLogicalOpCqRDest, imm);
		numTrailingOnes = countTrailingOnes(self_in_concretizeLogicalOpCqRDest, ((usqInt)(imm)) >> rotateCount);
	}
	else {
		imm = imm | (~(usqIntptr_t)mask);
		if (!(isShiftedMask(self_in_concretizeLogicalOpCqRDest, imm))) {
			
			/* OPC	N
			   00		0	AND (shifted register) - 64-bit variant on page C6-777
			   00		1	BIC (shifted register) - 64-bit variant on page C6-808
			   01		0	ORR (shifted register) - 64-bit variant on page C6-1127
			   01		1	ORN (shifted register) - 64-bit variant on page C6-1123
			   10		0	EOR (shifted register) - 64-bit variant on page C6-898
			   10		1	EON (shifted register) - 64-bit variant on page C6-894
			   11		0	ANDS (shifted register) - 64-bit variant on page C6-781
			   11		0	BICS (shifted register) - 64-bit variant on page C6-810 */
			offset = emitMoveCwintoRat(self_in_concretizeLogicalOpCqRDest, constant, RISCTempReg, 0);
			((self_in_concretizeLogicalOpCqRDest->machineCode))[offset / 4] = (((((0x8A000000U) + (((sqInt)((usqInt)(op) << 29)))) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (srcReg << 5)) + effectiveDestReg);
			offset += 4;
			goto l1;
		}
		numLeadingOnes = countLeadingOnes(self_in_concretizeLogicalOpCqRDest, imm);
		rotateCount = 64 - numLeadingOnes;
		numTrailingOnes = (numLeadingOnes + (countTrailingOnes(self_in_concretizeLogicalOpCqRDest, imm))) - (64 - size);
	}
	assert(size > rotateCount);

	/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
	immr1 = (size - rotateCount) & (size - 1);

	/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
	nImms = ((sqInt)((usqInt)((~(usqIntptr_t)(size - 1))) << 1));

	/* Extract the seventh bit and toggle it to create the N field. */
	nImms = nImms | (numTrailingOnes - 1);
	n1 = (((nImms) >> 6) & 1) ^ 1;
	nImms = nImms & 0x3F;
	assert((decodeNimmsimmr(self_in_concretizeLogicalOpCqRDest, n1, nImms, immr1)) == (((usqInt) constant)));
	((self_in_concretizeLogicalOpCqRDest->machineCode))[0] = (((((((0x92000000U) + (((sqInt)((usqInt)(op) << 29)))) + (n1 << 22)) + (((sqInt)((usqInt)(immr1) << 16)))) + (nImms << 10)) + (srcReg << 5)) + effectiveDestReg);
	offset = 4;
	l1:	/* end isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */;
	if (!((((self_in_concretizeLogicalOpCqRDest->opcode)) == AndCqRR)
		 && (destReg == SP))) {
		return offset;
	}
	((self_in_concretizeLogicalOpCqRDest->machineCode))[offset / 4] = (movernrd(self_in_concretizeLogicalOpCqRDest, effectiveDestReg, destReg));
	return offset + 4;
}


/*	C3.3.4		Move (immediate)	C3-215
	
	The Move (immediate) instructions are aliases for a single MOVZ, MOVN, or
	ORR (immediate with zero register),
	instruction to load an immediate value into the destination register. An
	assembler must permit a signed or
	unsigned immediate, as long as its binary representation can be generated
	using one of these instructions,
	and an assembler error results if the immediate cannot be generated in
	this way. On disassembly, it is
	unspecified whether the immediate is output as a signed or an unsigned
	value. 
	C6.2.191	MOVZ	C6-1102	Move wide with zero moves an optionally-shifted
	16-bit immediate value to a register.
	C6.2.190	MOVN	C6-1100	Move wide with NOT moves the inverse of an
	optionally-shifted 16-bit immediate value to a register.
	C6.2.204	ORR (immediate)	C6-1125
	Bitwise OR (immediate) performs a bitwise (inclusive) OR of a register
	value and an immediate
	register value, and writes the result to the destination register. */

	/* CogARMv8Compiler>>#concretizeMoveCqR */
static sqInt NoDbgRegParms
concretizeMoveCqR(AbstractInstruction * self_in_concretizeMoveCqR)
{
    sqInt constant;
    usqInt destReg;
    sqInt imm;
    sqInt immr1;
    sqInt lowBit;
    sqInt lowBitMod16;
    sqInt mask;
    usqInt mask1;
    usqInt n1;
    usqInt nImms;
    sqInt numLeadingOnes;
    sqInt numTrailingOnes;
    sqInt rotateCount;
    sqInt size;

	constant = ((self_in_concretizeMoveCqR->operands))[0];
	destReg = ((self_in_concretizeMoveCqR->operands))[1];
	if (destReg != SP) {
		lowBit = (constant > 0
			? computeLowBit(self_in_concretizeMoveCqR, constant)
			: 0);
		lowBitMod16 = (lowBit / 16) * 16;
		mask = ((sqInt)((usqInt)((0xFFFF)) << lowBitMod16));
		if ((constant & mask) == constant) {

			/* Use MOVZ */
			((self_in_concretizeMoveCqR->machineCode))[0] = ((((0xD2800000U) + (((sqInt)((usqInt)((lowBitMod16 / 16)) << 21)))) + (((sqInt)((usqInt)((((usqInt)(constant)) >> lowBitMod16)) << 5)))) + destReg);
			return 4;
		}
		lowBit = ((((sqLong) constant)) < -1
			? computeLowBit(self_in_concretizeMoveCqR, ~(usqIntptr_t)constant)
			: 0);
		if ((((sqLong) (constant | mask))) == -1) {

			/* Use MOVN */
			assert((((usqInt)((~(usqIntptr_t)constant))) >> lowBitMod16) == ((((usqInt)((~(usqIntptr_t)constant))) >> lowBitMod16) & mask));
			((self_in_concretizeMoveCqR->machineCode))[0] = ((((0x92800000U) + (((sqInt)((usqInt)((lowBitMod16 / 16)) << 21)))) + (((sqInt)((usqInt)((((usqInt)((~(usqIntptr_t)constant))) >> lowBitMod16)) << 5)))) + destReg);
			return 4;
		}
	}
	/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
	if (((constant >= -1) && (constant <= 0))) {
		return emitMoveCwintoRat(self_in_concretizeMoveCqR, constant, destReg, 0);
	}

	/* First, determine the element size. */
	imm = constant;
	size = 32;
	while (1) {
		mask1 = (1ULL << size) - 1;
		if (!(((imm & mask1) != ((((usqInt)(imm)) >> size) & mask1)
			? ((size = size * 2),
				0)
			: size > 2))) break;
		size = size / 2;
	}
	mask1 = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size);
	imm = imm & mask1;
	if (isShiftedMask(self_in_concretizeMoveCqR, imm)) {
		rotateCount = countTrailingZeros(self_in_concretizeMoveCqR, imm);
		numTrailingOnes = countTrailingOnes(self_in_concretizeMoveCqR, ((usqInt)(imm)) >> rotateCount);
	}
	else {
		imm = imm | (~(usqIntptr_t)mask1);
		if (!(isShiftedMask(self_in_concretizeMoveCqR, imm))) {
			return emitMoveCwintoRat(self_in_concretizeMoveCqR, constant, destReg, 0);
		}
		numLeadingOnes = countLeadingOnes(self_in_concretizeMoveCqR, imm);
		rotateCount = 64 - numLeadingOnes;
		numTrailingOnes = (numLeadingOnes + (countTrailingOnes(self_in_concretizeMoveCqR, imm))) - (64 - size);
	}
	assert(size > rotateCount);

	/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
	immr1 = (size - rotateCount) & (size - 1);

	/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
	nImms = ((sqInt)((usqInt)((~(usqIntptr_t)(size - 1))) << 1));

	/* Extract the seventh bit and toggle it to create the N field. */
	nImms = nImms | (numTrailingOnes - 1);
	n1 = (((nImms) >> 6) & 1) ^ 1;
	nImms = nImms & 0x3F;
	assert((decodeNimmsimmr(self_in_concretizeMoveCqR, n1, nImms, immr1)) == (((usqInt) constant)));
	
	/* Use ORR */
	((self_in_concretizeMoveCqR->machineCode))[0] = ((((((0xB2000000U) + (n1 << 22)) + (((sqInt)((usqInt)(immr1) << 16)))) + (nImms << 10)) + (((sqInt)((usqInt)(XZR) << 5)))) + destReg);
	return 4;
}


/*	Mwr/M32r/M16r/Mbr - memory unit whose address is a constant M away from an
	address in a register
 */

	/* CogARMv8Compiler>>#concretizeMoveMSrR: */
static sqInt NoDbgRegParms
concretizeMoveMSrR(AbstractInstruction * self_in_concretizeMoveMSrR, sqInt unitSizeLog2MinusOne)
{
	return emitLdrnrtimmshiftBy12at(self_in_concretizeMoveMSrR, unitSizeLog2MinusOne, ((self_in_concretizeMoveMSrR->operands))[1], ((sqLong) (((self_in_concretizeMoveMSrR->operands))[2])), ((self_in_concretizeMoveMSrR->operands))[0], 0, 0);
}


/*	D13.8.26		CNTVCT_EL0, Counter-timer Virtual Count register		p D13-3774 */
/*	MRS <Xt>, CNTVCT_EL0		op0:0b11 op1:0b011 CRn:0b1110 CRm:0b0000 op2:0b010 */

	/* CogARMv8Compiler>>#concretizeMovePerfCnt64RL */
static sqInt NoDbgRegParms
concretizeMovePerfCnt64RL(AbstractInstruction * self_in_concretizeMovePerfCnt64RL)
{
	((self_in_concretizeMovePerfCnt64RL->machineCode))[0] = ((3577471040U) + (((self_in_concretizeMovePerfCnt64RL->operands))[0]));
	return 4;
}


/*	Mwr/M32r/M16r/Mbr - memory unit whose address is a constant M away from an
	address in a register
 */

	/* CogARMv8Compiler>>#concretizeMoveRMSr: */
static sqInt NoDbgRegParms
concretizeMoveRMSr(AbstractInstruction * self_in_concretizeMoveRMSr, sqInt unitSizeLog2MinusOne)
{
	return emitStrnrtimmshiftBy12(self_in_concretizeMoveRMSr, unitSizeLog2MinusOne, ((self_in_concretizeMoveRMSr->operands))[2], ((self_in_concretizeMoveRMSr->operands))[0], ((sqLong) (((self_in_concretizeMoveRMSr->operands))[1])), 0);
}


/*	Xwr/X32r/X16r/Xbr - memory unit whose address is r * unit size away from
	an address in a register
 */
/*	C6.2.274	STR (register)	C6-1242 */

	/* CogARMv8Compiler>>#concretizeMoveRXSrR: */
static sqInt NoDbgRegParms
concretizeMoveRXSrR(AbstractInstruction * self_in_concretizeMoveRXSrR, sqInt unitSizeLog2MinusOne)
{
    usqInt base;
    usqInt index;
    usqInt src;

	src = ((self_in_concretizeMoveRXSrR->operands))[0];
	index = ((self_in_concretizeMoveRXSrR->operands))[1];
	base = ((self_in_concretizeMoveRXSrR->operands))[2];
	assert(!(((SP == src)
 || (SP == index))));
	((self_in_concretizeMoveRXSrR->machineCode))[0] = (((((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38200000)) + (index << 16)) + (((sqInt)((usqInt)(UXTX) << 13)))) + (6144)) + (base << 5)) + src);
	return 4;
}


/*	Xwr/X32r/X16r/Xbr - memory unit whose address is r * unit size away from
	an address in a register
 */
/*	C6.2.132	LDR (register)	C6-981 */

	/* CogARMv8Compiler>>#concretizeMoveXSrRR: */
static sqInt NoDbgRegParms
concretizeMoveXSrRR(AbstractInstruction * self_in_concretizeMoveXSrRR, sqInt unitSizeLog2MinusOne)
{
    usqInt base;
    usqInt dest;
    usqInt index;

	index = ((self_in_concretizeMoveXSrRR->operands))[0];
	base = ((self_in_concretizeMoveXSrRR->operands))[1];
	dest = ((self_in_concretizeMoveXSrRR->operands))[2];
	assert(!((SP == dest)));
	((self_in_concretizeMoveXSrRR->machineCode))[0] = (((((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38600000)) + (index << 16)) + (((sqInt)((usqInt)(UXTX) << 13)))) + (6144)) + (base << 5)) + dest);
	return 4;
}


/*	Sizing/generating jumps.
	Jump targets can be to absolute addresses or other abstract instructions.
	Generating initial trampolines instructions may have no maxSize and be to
	absolute addresses.
	Otherwise instructions must have a machineCodeSize which must be kept to. */

	/* CogARMv8Compiler>>#concretizeMulOverflowJump */
static sqInt NoDbgRegParms
concretizeMulOverflowJump(AbstractInstruction * self_in_concretizeMulOverflowJump)
{
    sqInt offset;


	/* -4 because the jump is from the second word... */
	offset = (computeJumpTargetOffset(self_in_concretizeMulOverflowJump)) - 4;
	assert((offset != 0)
	 && (isInImmediateBranchRange(self_in_concretizeMulOverflowJump, offset)));
	((self_in_concretizeMulOverflowJump->machineCode))[0] = (((((0x8B000000U) + (((sqInt)((usqInt)(ArithmeticAddS) << 29)))) + (((sqInt)((usqInt)(CArg1Reg) << 16)))) + (((sqInt)((usqInt)(RISCTempReg) << 5)))) + RISCTempReg);
	((self_in_concretizeMulOverflowJump->machineCode))[1] = (condoffset(self_in_concretizeMulOverflowJump, (((self_in_concretizeMulOverflowJump->opcode)) == JumpMulOverflow
	? NE
	: EQ), offset));
	return 8;
}


/*	rd := regA op regB */

	/* CogARMv8Compiler>>#concretizeRRArithmetic:Rd: */
static sqInt NoDbgRegParms
concretizeRRArithmeticRd(AbstractInstruction * self_in_concretizeRRArithmeticRd, sqInt arithOp, sqInt rd)
{
    usqInt regA;
    usqInt regB;

	regB = ((self_in_concretizeRRArithmeticRd->operands))[0];

	/* cogit processor disassembleInstructionAt: 0 In: machineCode object */
	/* cogit processor disassembleInstructionAt: 4 In: machineCode object */
	regA = ((self_in_concretizeRRArithmeticRd->operands))[1];
	if (SP == regB) {

		/* Arithmetic with the sp; we must use the extended register forms, and negate
		   on subtract because we can't have Rm as SP, Rm = 31 is interoreted as XZR. */
		/* ADD (extended register) on page C6-758
		   ADDS (extended register) on page C6-766
		   SUB (extended register) on page C6-1308
		   SUBS (extended register) on page C6-1318 */
		((self_in_concretizeRRArithmeticRd->machineCode))[0] = ((((((0x8B200000U) + (((sqInt)((usqInt)(arithOp) << 29)))) + (regA << 16)) + (((sqInt)((usqInt)(UXTX) << 13)))) + (regB << 5)) + rd);
		if (!((arithOp == ArithmeticSub)
			 || (arithOp == ArithmeticSubS))) {
			return 4;
		}
		((self_in_concretizeRRArithmeticRd->machineCode))[1] = (((((0x8B000000U) + (((sqInt)((usqInt)(arithOp) << 29)))) + (((sqInt)((usqInt)(rd) << 16)))) + (((sqInt)((usqInt)(XZR) << 5)))) + rd);
		return 8;
	}
	if (SP == regA) {

		/* Arithmetic with the sp; we must use the extended register forms. */
		/* ADD (extended register) on page C6-758
		   ADDS (extended register) on page C6-766
		   SUB (extended register) on page C6-1308
		   SUBS (extended register) on page C6-1318
		   CMN (extended register) on page C6-850
		   CMP (extended register) on page C6-856 */
		((self_in_concretizeRRArithmeticRd->machineCode))[0] = ((((((0x8B200000U) + (((sqInt)((usqInt)(arithOp) << 29)))) + (regB << 16)) + (((sqInt)((usqInt)(UXTX) << 13)))) + (regA << 5)) + rd);
		if (rd != XZR) {
		}
		return 4;
	}
	assert((regA != SP)
	 && (regB != SP));
	((self_in_concretizeRRArithmeticRd->machineCode))[0] = (((((0x8B000000U) + (((sqInt)((usqInt)(arithOp) << 29)))) + (regB << 16)) + (regA << 5)) + rd);
	return 4;
}


/*	AND (shifted register) - 64-bit variant on page C6-777
	BIC (shifted register) - 64-bit variant on page C6-808
	ORR (shifted register) - 64-bit variant on page C6-1127
	ORN (shifted register) - 64-bit variant on page C6-1123
	EOR (shifted register) - 64-bit variant on page C6-898
	EON (shifted register) - 64-bit variant on page C6-894
	ANDS (shifted register) - 64-bit variant on page C6-781
	BICS (shifted register) - 64-bit variant on page C6-810 */

	/* CogARMv8Compiler>>#concretizeRRLogical: */
static sqInt NoDbgRegParms
concretizeRRLogical(AbstractInstruction * self_in_concretizeRRLogical, sqInt logicalOp)
{
    usqInt destReg;
    usqInt srcReg;

	srcReg = ((self_in_concretizeRRLogical->operands))[0];
	destReg = ((self_in_concretizeRRLogical->operands))[1];
	((self_in_concretizeRRLogical->machineCode))[0] = (((((0x8A000000U) + (((sqInt)((usqInt)(logicalOp) << 29)))) + (srcReg << 16)) + (destReg << 5)) + destReg);
	return 4;
}


/*	C6.2.320 SXTB	C6-1332
	C6.2.321 SXTH	C6-1334
	C6.2.322 SXTW	C6-1336 */

	/* CogARMv8Compiler>>#concretizeSignExtendRR: */
static sqInt NoDbgRegParms
concretizeSignExtendRR(AbstractInstruction * self_in_concretizeSignExtendRR, sqInt width)
{
    usqInt rd;
    usqInt rn;

	rn = ((self_in_concretizeSignExtendRR->operands))[0];
	rd = ((self_in_concretizeSignExtendRR->operands))[1];
	((self_in_concretizeSignExtendRR->machineCode))[0] = ((((0x93400000U) + (((sqInt)((usqInt)((width - 1)) << 10)))) + (rn << 5)) + rd);
	return 4;
}


/*	Xt */
/*	Xn */

	/* CogARMv8Compiler>>#concretizeSTLR */
static sqInt NoDbgRegParms
concretizeSTLR(AbstractInstruction * self_in_concretizeSTLR)
{
    usqInt baseReg;
    usqInt valueReg;

	valueReg = ((self_in_concretizeSTLR->operands))[0];
	baseReg = ((self_in_concretizeSTLR->operands))[1];
	((self_in_concretizeSTLR->machineCode))[0] = (((0xC89FFC00U) + (baseReg << 5)) + valueReg);
	return 4;
}


/*	C6.2.333	UBFX	C6-1353
	C6.2.341 UXTB	C6-1364
	C6.2.342 UXTH	C6-1366 */

	/* CogARMv8Compiler>>#concretizeZeroExtendRR: */
static sqInt NoDbgRegParms
concretizeZeroExtendRR(AbstractInstruction * self_in_concretizeZeroExtendRR, sqInt width)
{
    usqInt rd;
    usqInt rn;

	rn = ((self_in_concretizeZeroExtendRR->operands))[0];
	rd = ((self_in_concretizeZeroExtendRR->operands))[1];
	((self_in_concretizeZeroExtendRR->machineCode))[0] = ((((0xD3400000U) + (((sqInt)((usqInt)((width - 1)) << 10)))) + (rn << 5)) + rd);
	return 4;
}


/*	Conditional branch (immediate) C4-257 */

	/* CogARMv8Compiler>>#cond:offset: */
static sqInt NoDbgRegParms
condoffset(AbstractInstruction * self_in_condoffset, sqInt cond, sqInt offset)
{
	assert((offset & 3) == 0);
	return ((0x54000000) + (((sqInt)((usqInt)((offset & (0x1FFFFF))) << 3)))) + cond;
}

	/* CogARMv8Compiler>>#countLeadingOnes: */
static sqInt NoDbgRegParms
countLeadingOnes(AbstractInstruction * self_in_countLeadingOnes, sqInt anInteger)
{
    sqInt count;
    sqInt logBase2Shift;
    sqInt shift;
    sqInt theInteger;

	if (anInteger >= 0) {
		return 0;
	}
	theInteger = anInteger;
	count = 0;
	for (logBase2Shift = 5; logBase2Shift >= 0; logBase2Shift += -1) {
		shift = 1ULL << logBase2Shift;
		if (((theInteger) >> shift) == -1) {
			count += shift;
		}
		else {
			theInteger = (theInteger) >> shift;
		}
	}
	return (theInteger == -1
		? count + 1
		: count);
}

	/* CogARMv8Compiler>>#countTrailingOnes: */
static sqInt NoDbgRegParms
countTrailingOnes(AbstractInstruction * self_in_countTrailingOnes, sqInt anInteger)
{
    sqInt bits;
    sqInt count;

	bits = anInteger;
	count = 0;
	while (((bits & 0xFFFF) == 0xFFFF)) {
		bits = (((usqInt)(bits)) >> 16);
		count += 16;
	}
	while (((bits & 0xFF) == 0xFF)) {
		bits = (((usqInt)(bits)) >> 8);
		count += 8;
	}
	if (((bits & 15) == 15)) {
		bits = (((usqInt)(bits)) >> 4);
		count += 4;
	}
	if (((bits & 3) == 3)) {
		bits = (((usqInt)(bits)) >> 2);
		count += 2;
	}
	return (((bits & 1) == 1)
		? count + 1
		: count);
}


/*	a.k.a. anInteger lowBit - 1 */

	/* CogARMv8Compiler>>#countTrailingZeros: */
static sqInt NoDbgRegParms
countTrailingZeros(AbstractInstruction * self_in_countTrailingZeros, sqInt anInteger)
{
    sqInt n;
    sqInt result;

	assert(anInteger != 0);
	n = anInteger;
	result = 0;
	while ((!(n & 0xFF))) {
		result += 8;
		n = (((usqInt)(n)) >> 8);
	}
	if ((!(n & 15))) {
		result += 4;
		n = (((usqInt)(n)) >> 4);
	}
	if ((!(n & 3))) {
		result += 2;
		n = (((usqInt)(n)) >> 2);
	}
	return (((n & 1) != 0)
		? result
		: result + 1);
}


/*	See aarch64/instrs/integer/bitmasks/DecodeBitMasks J1-7389. */

	/* CogARMv8Compiler>>#decodeN:imms:immr: */
static usqInt NoDbgRegParms
decodeNimmsimmr(AbstractInstruction * self_in_decodeNimmsimmr, sqInt n, sqInt imms, sqInt immr)
{
    sqInt bits;
    sqInt immediate;
    usqInt mask;
    sqInt rotation;
    sqInt width;

	bits = 0;
	width = 0;
	assert((((n >= 0) && (n <= 1)))
	 && ((((imms >= 0) && (imms <= 0x3F)))
	 && (((immr >= 0) && (immr <= 0x3F)))));
	if (imms == 0x3F) {
		return 0;
	}
	if (n == 1) {
		width = 64;
		mask = 0xFFFFFFFFFFFFFFFFULL;
		bits = imms;
		rotation = immr;
	}
	else {
		if (imms < 32) {
			width = 32;
			bits = imms;
		}
		else {
			if (imms < 48) {
				width = 16;
				bits = imms & 15;
			}
			else {
				if (imms < 56) {
					width = 8;
					bits = imms & 7;
				}
				else {
					if (imms < 60) {
						width = 4;
						bits = imms & 3;
					}
					else {
						if (imms < 0x3E) {
							width = 2;
							bits = imms & 1;
						}
						else {
							error("invalid logical immediate");
						}
					}
				}
			}
		}
		mask = (1ULL << width) - 1;
		rotation = immr & (width - 1);
	}
	if ((width - 1) == bits) {
		return 0;
	}
	immediate = (1ULL << (bits + 1)) - 1;
	if (rotation > 0) {
		immediate = ((sqInt)((usqInt)(immediate) << (width - rotation)));
	}
	if (((width >= 2) && (width <= 32))) {
		immediate = (((width < 0) ? (((usqInt)(immediate)) >> (-width)) : (((sqInt)((usqInt)(immediate) << width))))) | immediate;
	}
	return immediate;
}


/*	Do a throw-away compilation to read CTR_EL0 and initialize ctrEl0.
	Some linux kernels trap and synthesize access to ID_AA64ISAR0_EL1,
	and some do not, so use getauxval(3) to access value(s) derived
	there-from, i.e. whether the processor has atomic instructions. */

	/* CogARMv8Compiler>>#detectFeaturesOnLinux */
#if __linux__
static void NoDbgRegParms
detectFeaturesOnLinux(AbstractInstruction * self_in_detectFeaturesOnLinux)
{
    sqInt ctrEL0;
    sqInt fixupSize;
    usqIntptr_t (*getFeatureReg)(void);
    sqInt opcodeSize;
    usqInt startAddress;

	startAddress = methodZoneBase();
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 4;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;

	/* Return the value of CTR_EL0; that's the control register that defines the vital statistics of the processor's caches. */
	getFeatureReg = ((usqIntptr_t (*)(void)) startAddress);
	gen(Nop);
	genoperand(MRS_CTR_EL0, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	/* begin resetMethodZoneBase: */
	methodZoneBase = startAddress;
	/* begin ensureExecutableCodeZoneWithin: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	
	/* see e.g. CogARMv8Compiler class>>printCTR_EL0:, concretizeCacheControlOp1:CRm:Op2: &
	   http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.100403_0200_00_en/lau1443435580346.html
	   DminLine & IminLine are Log2 words; 16 words miniumum */
	ctrEL0 = getFeatureReg();
	setDataCacheFlushRequired(self_in_detectFeaturesOnLinux, (!(ctrEL0 & (0x10000000))));
	setDataCacheLineLength(self_in_detectFeaturesOnLinux, 4ULL << ((((usqInt)(ctrEL0)) >> 16) & 15));
	if ((dataCacheLineLength(self_in_detectFeaturesOnLinux)) == 0) {
		setDataCacheLineLength(self_in_detectFeaturesOnLinux, 64);
	}
	setInstructionCacheFlushRequired(self_in_detectFeaturesOnLinux, (!(ctrEL0 & (0x20000000))));
	setInstructionCacheLineLength(self_in_detectFeaturesOnLinux, 4ULL << (ctrEL0 & 15));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	if ((instructionCacheLineLength(self_in_detectFeaturesOnLinux)) == 0) {
		setInstructionCacheLineLength(self_in_detectFeaturesOnLinux, 64);
	}
}
#endif /* __linux__ */


/*	Do throw-away compilations to read CTR_EL0 & ID_AA64ISAR0_EL1 and
	initialize ctrEl0 & idISAR0
 */

	/* CogARMv8Compiler>>#detectFeaturesOnRawMachine */
#if !__APPLE__ && !__linux__
static void NoDbgRegParms
detectFeaturesOnRawMachine(AbstractInstruction * self_in_detectFeaturesOnRawMachine)
{
    sqInt ctrEL0;
    sqInt fixupSize;
    usqIntptr_t (*getFeatureReg)(void);
    sqInt opcodeSize;
    usqInt startAddress;

	startAddress = methodZoneBase();
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 4;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;

	/* Return the value of CTR_EL0; that's the control register that defines the vital statistics of the processor's caches. */
	getFeatureReg = ((usqIntptr_t (*)(void)) startAddress);
	gen(Nop);
	genoperand(MRS_CTR_EL0, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	/* begin resetMethodZoneBase: */
	methodZoneBase = startAddress;
	/* begin ensureExecutableCodeZoneWithin: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	
	/* see e.g. CogARMv8Compiler class>>printCTR_EL0:, concretizeCacheControlOp1:CRm:Op2: &
	   http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.100403_0200_00_en/lau1443435580346.html
	   DminLine & IminLine are Log2 words; 16 words miniumum */
	ctrEL0 = getFeatureReg();
	setDataCacheFlushRequired(self_in_detectFeaturesOnRawMachine, (!(ctrEL0 & (0x10000000))));
	setDataCacheLineLength(self_in_detectFeaturesOnRawMachine, 4ULL << ((((usqInt)(ctrEL0)) >> 16) & 15));
	if ((dataCacheLineLength(self_in_detectFeaturesOnRawMachine)) == 0) {
		setDataCacheLineLength(self_in_detectFeaturesOnRawMachine, 64);
	}
	setInstructionCacheFlushRequired(self_in_detectFeaturesOnRawMachine, (!(ctrEL0 & (0x20000000))));
	setInstructionCacheLineLength(self_in_detectFeaturesOnRawMachine, 4ULL << (ctrEL0 & 15));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	if ((instructionCacheLineLength(self_in_detectFeaturesOnRawMachine)) == 0) {
		setInstructionCacheLineLength(self_in_detectFeaturesOnRawMachine, 64);
	}
	zeroOpcodeIndexForNewOpcodes();
	gen(Nop);
	genoperand(MRS_ID_AA64ISAR0_EL1, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
}
#endif /* !__APPLE__ && !__linux__ */


/*	Attempt to generate concrete machine code for the instruction at address.
	This is the inner dispatch of concretizeAt: actualAddress which exists
	only to get around the branch size limits in the SqueakV3 (blue book
	derived) bytecode set. */

	/* CogARMv8Compiler>>#dispatchConcretize */
static sqInt NoDbgRegParms
dispatchConcretize(AbstractInstruction * self_in_dispatchConcretize)
{
    usqInt addressReg;
    usqInt addressReg1;
    usqInt baseReg;
    usqInt baseReg1;
    usqInt baseReg2;
    usqInt CRm;
    sqInt CRm1;
    usqInt condition;
    usqInt constant;
    usqInt constant1;
    sqInt constant10;
    sqInt constant11;
    sqInt constant12;
    usqInt constant2;
    usqInt constant3;
    usqInt constant4;
    usqInt constant5;
    usqInt constant6;
    usqInt constant7;
    usqInt constant8;
    usqInt constantReg;
    sqInt constant9;
    AbstractInstruction *dependentChain;
    usqInt dest;
    usqInt destAddr;
    usqInt destAddr1;
    usqInt destReg;
    usqInt destReg1;
    usqInt destReg10;
    sqInt destReg11;
    sqInt destReg12;
    usqInt destReg2;
    usqInt destReg3;
    usqInt destReg4;
    usqInt destReg5;
    usqInt destReg6;
    usqInt destReg7;
    usqInt destReg8;
    usqInt destReg9;
    usqInt flags;
    sqInt instrBytes;
    sqInt instrBytes1;
    sqInt instrOffset;
    sqInt instrOffset1;
    sqInt instrOffset2;
    sqInt instrOffset3;
    sqInt instrOffset4;
    sqInt instrOffset5;
    AbstractInstruction *jumpTarget;
    AbstractInstruction *jumpTarget1;
    AbstractInstruction *jumpTarget11;
    AbstractInstruction *jumpTarget12;
    AbstractInstruction *jumpTarget2;
    AbstractInstruction *jumpTarget3;
    usqInt maskReg;
    sqInt offset;
    sqInt offset1;
    sqInt offset10;
    sqInt offset11;
    sqInt offset12;
    sqInt offset13;
    sqInt offset2;
    sqInt offset3;
    sqInt offset4;
    sqInt offset5;
    sqInt offset6;
    usqInt offset7;
    usqInt offset8;
    sqInt offset9;
    usqInt pairAddress;
    usqInt pairAddress1;
    usqInt rA;
    usqInt rD;
    usqInt rd;
    usqInt rd1;
    usqInt reg;
    usqInt reg1;
    usqInt reg10;
    usqInt reg11;
    usqInt reg12;
    usqInt reg13;
    usqInt reg14;
    usqInt reg15;
    usqInt reg16;
    usqInt reg17;
    usqInt reg2;
    usqInt reg21;
    usqInt reg22;
    usqInt reg23;
    usqInt reg24;
    usqInt reg25;
    usqInt reg26;
    usqInt reg27;
    usqInt reg3;
    usqInt reg31;
    usqInt reg32;
    usqInt reg33;
    usqInt reg4;
    usqInt reg5;
    usqInt reg6;
    usqInt reg7;
    usqInt reg8;
    usqInt reg9;
    usqInt rM;
    usqInt rM1;
    usqInt rN;
    usqInt rN1;
    usqInt rn;
    usqInt rn1;
    usqInt rn2;
    usqInt shiftCountReg;
    usqInt shiftCountReg1;
    usqInt shiftCountReg2;
    usqInt shiftedReg;
    usqInt shiftedReg1;
    usqInt shiftedReg2;
    usqInt srcAddr;
    usqInt srcAddr1;
    usqInt srcAddr2;
    usqInt srcReg;
    usqInt srcReg1;
    usqInt srcReg10;
    usqInt srcReg11;
    usqInt srcReg12;
    usqInt srcReg2;
    usqInt srcReg3;
    usqInt srcReg4;
    usqInt srcReg5;
    usqInt srcReg6;
    usqInt srcReg7;
    usqInt srcReg8;
    usqInt srcReg9;
    usqInt statusReg;
    usqInt targetReg;
    usqInt valueReg;
    usqInt valueReg1;

	switch ((self_in_dispatchConcretize->opcode)) {
	case Label:
		/* begin concretizeLabel */
		dependentChain = (self_in_dispatchConcretize->dependent);
		while (!(dependentChain == null)) {
			/* begin updateLabel: */
			assert((((dependentChain->opcode)) == MoveCwR)
			 || (((dependentChain->opcode)) == PushCw));
			((dependentChain->operands))[0] = (((self_in_dispatchConcretize->address)) + (((self_in_dispatchConcretize->operands))[1]));
			dependentChain = (dependentChain->dependent);
		}
		return 0;

	case Literal:
		return concretizeLiteral(self_in_dispatchConcretize);

	case AlignmentNops:
		/* begin concretizeAlignmentNops */
		assert((((self_in_dispatchConcretize->machineCodeSize)) % 4) == 0);
		((self_in_dispatchConcretize->machineCode))[0] = NOP;
		return ((usqInt) ((self_in_dispatchConcretize->machineCodeSize)));

	case Fill32:
		/* begin concretizeFill32 */
		((self_in_dispatchConcretize->machineCode))[0] = (((self_in_dispatchConcretize->operands))[0]);
		return 4;

	case Nop:
		/* begin concretizeNop */
		((self_in_dispatchConcretize->machineCode))[0] = NOP;
		return 4;

	case Call:
		/* begin concretizeCall */
		offset = ((sqLong) ((((self_in_dispatchConcretize->operands))[0]) - ((self_in_dispatchConcretize->address))));
		assert((!(offset & 3)));
		assert(isInImmediateBranchAndLinkRange(self_in_dispatchConcretize, offset));
		((self_in_dispatchConcretize->machineCode))[0] = ((assert((!(offset & 3))),
(0x94000000U) + (((offset) >> 2) & (0x3FFFFFF))));
		return 4;

	case CallFull:
		/* begin concretizeCallJumpFull: */
		jumpTarget1 = ((AbstractInstruction *) (((self_in_dispatchConcretize->operands))[0]));
		if ((addressIsInInstructions(jumpTarget1))
		 || (jumpTarget1 == (methodLabel()))) {
			jumpTarget1 = ((AbstractInstruction *) ((jumpTarget1->address)));
		}
		assert(jumpTarget1 != 0);
		jumpTarget = jumpTarget1;
		instrOffset = emitMoveCwintoRat(self_in_dispatchConcretize, ((usqInt)jumpTarget), RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[instrOffset / 4] = (brlinkreg(self_in_dispatchConcretize, 1, RISCTempReg));
		assert(instrOffset == (literalLoadInstructionBytes(self_in_dispatchConcretize)));
		return instrOffset + 4;

	case JumpR:
		/* begin concretizeJumpR */
		((self_in_dispatchConcretize->machineCode))[0] = (((0xD6000000U) + (((sqInt)((usqInt)(XZR) << 16)))) + ((((self_in_dispatchConcretize->operands))[0]) << 5));
		return 4;

	case JumpFull:
		/* begin concretizeCallJumpFull: */
		jumpTarget11 = ((AbstractInstruction *) (((self_in_dispatchConcretize->operands))[0]));
		if ((addressIsInInstructions(jumpTarget11))
		 || (jumpTarget11 == (methodLabel()))) {
			jumpTarget11 = ((AbstractInstruction *) ((jumpTarget11->address)));
		}
		assert(jumpTarget11 != 0);
		jumpTarget2 = jumpTarget11;
		instrOffset1 = emitMoveCwintoRat(self_in_dispatchConcretize, ((usqInt)jumpTarget2), RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[instrOffset1 / 4] = (brlinkreg(self_in_dispatchConcretize, 0, RISCTempReg));
		assert(instrOffset1 == (literalLoadInstructionBytes(self_in_dispatchConcretize)));
		return instrOffset1 + 4;

	case JumpLong:
		/* begin concretizeJumpLong */
		offset1 = (((self_in_dispatchConcretize->operands))[0]) - ((self_in_dispatchConcretize->address));
		assert((!(offset1 & 3)));
		assert(((((offset1) >> 2) >= (-67108864)) && (((offset1) >> 2) <= (0x3FFFFFF))));
		((self_in_dispatchConcretize->machineCode))[0] = ((0x14000000) + (((offset1) >> 2) & (0x3FFFFFF)));
		return 4;

	case JumpLongZero:
		/* begin concretizeConditionalLongJump: */
		assert(1);
		offset2 = (((self_in_dispatchConcretize->operands))[0]) - (((self_in_dispatchConcretize->address)) + 4);
		assert((!(offset2 & 3)));
		assert(((((offset2) >> 2) >= (-67108864)) && (((offset2) >> 2) <= (0x3FFFFFF))));
		((self_in_dispatchConcretize->machineCode))[0] = ((0x54000040) + (NE));
		((self_in_dispatchConcretize->machineCode))[1] = ((0x14000000) + (((offset2) >> 2) & (0x3FFFFFF)));
		return 8;

	case JumpLongNonZero:
		/* begin concretizeConditionalLongJump: */
		assert(NE == NE);
		offset3 = (((self_in_dispatchConcretize->operands))[0]) - (((self_in_dispatchConcretize->address)) + 4);
		assert((!(offset3 & 3)));
		assert(((((offset3) >> 2) >= (-67108864)) && (((offset3) >> 2) <= (0x3FFFFFF))));
		((self_in_dispatchConcretize->machineCode))[0] = ((0x54000040) + (EQ));
		((self_in_dispatchConcretize->machineCode))[1] = ((0x14000000) + (((offset3) >> 2) & (0x3FFFFFF)));
		return 8;

	case Jump:
		return concretizeConditionalJump(self_in_dispatchConcretize, AL);

	case JumpZero:
	case JumpFPEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, EQ);

	case JumpNonZero:
	case JumpFPNotEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, NE);

	case JumpNegative:
		return concretizeConditionalJump(self_in_dispatchConcretize, MI);

	case JumpNonNegative:
		return concretizeConditionalJump(self_in_dispatchConcretize, PL);

	case JumpOverflow:
	case JumpFPUnordered:
		return concretizeConditionalJump(self_in_dispatchConcretize, VS);

	case JumpNoOverflow:
	case JumpFPOrdered:
		return concretizeConditionalJump(self_in_dispatchConcretize, VC);

	case JumpMulOverflow:
	case JumpNoMulOverflow:
		return concretizeMulOverflowJump(self_in_dispatchConcretize);

	case JumpCarry:
	case JumpAboveOrEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, CS);

	case JumpNoCarry:
	case JumpBelow:
		return concretizeConditionalJump(self_in_dispatchConcretize, CC);

	case JumpLess:
	case JumpFPLess:
		return concretizeConditionalJump(self_in_dispatchConcretize, LT);

	case JumpGreaterOrEqual:
	case JumpFPGreaterOrEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, GE);

	case JumpGreater:
	case JumpFPGreater:
		return concretizeConditionalJump(self_in_dispatchConcretize, GT);

	case JumpLessOrEqual:
	case JumpFPLessOrEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, LE);

	case JumpAbove:
		return concretizeConditionalJump(self_in_dispatchConcretize, HI);

	case JumpBelowOrEqual:
		return concretizeConditionalJump(self_in_dispatchConcretize, LS);

	case RetN:
		/* begin concretizeRetN */

		/* C6.2.218 RET p1147 */
		offset4 = ((self_in_dispatchConcretize->operands))[0];
		if (offset4 == 0) {
			((self_in_dispatchConcretize->machineCode))[0] = (((0xD6400000U) + (((sqInt)((usqInt)(XZR) << 16)))) + (((sqInt)((usqInt)(LR) << 5))));
			return 4;
		}
		((self_in_dispatchConcretize->machineCode))[0] = (addrnrdimmshiftBy12(self_in_dispatchConcretize, SPReg, SPReg, offset4, 0));
		((self_in_dispatchConcretize->machineCode))[1] = (((0xD6400000U) + (((sqInt)((usqInt)(XZR) << 16)))) + (((sqInt)((usqInt)(LR) << 5))));
		return 8;

	case NativeRetN:
		/* begin concretizeNativeRetN */

		/* C6.2.218 RET p1147 */
		offset5 = ((self_in_dispatchConcretize->operands))[0];
		if (offset5 == 0) {
			((self_in_dispatchConcretize->machineCode))[0] = (((0xD6400000U) + (((sqInt)((usqInt)(XZR) << 16)))) + (((sqInt)((usqInt)(LR) << 5))));
			return 4;
		}
		((self_in_dispatchConcretize->machineCode))[0] = (addrnrdimmshiftBy12(self_in_dispatchConcretize, SP, SP, offset5, 0));
		((self_in_dispatchConcretize->machineCode))[1] = (((0xD6400000U) + (((sqInt)((usqInt)(XZR) << 16)))) + (((sqInt)((usqInt)(LR) << 5))));
		return 8;

	case Stop:
		/* begin concretizeStop */
		((self_in_dispatchConcretize->machineCode))[0] = 0xD4400000U /* stop */;
		return 4;

	case AddCqR:
		/* begin concretizeAddCqRDest: */
		destReg11 = ((self_in_dispatchConcretize->operands))[1];
		constant9 = ((sqLong) (((self_in_dispatchConcretize->operands))[0]));
		srcReg11 = ((self_in_dispatchConcretize->operands))[1];
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if (((constant9 >= 0) && (constant9 <= (0xFFF)))) {
			
			/* C6.2.4		ADD (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg11 == SP
	? 0x122
	: 354))) << 23))) + (((sqInt)((usqInt)(constant9) << 10)))) + (srcReg11 << 5)) + destReg11);
			return 4;
			goto l8;
		}
		if (((!(constant9 & (0xFFF))))
		 && (((((constant9) >> 12) >= 0) && (((constant9) >> 12) <= (0xFFF))))) {
			
			/* C6.2.4		ADD (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg11 == SP
	? 0x122
	: 354))) << 23))) + ((((usqInt)(constant9)) >> 2) + (0x400000))) + (srcReg11 << 5)) + destReg11);
			return 4;
			goto l8;
		}
	l8:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if ((((-constant9) >= 0) && ((-constant9) <= (0xFFF)))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg11 == SP
	? 418
	: 482))) << 23))) + (((sqInt)((usqInt)((-constant9)) << 10)))) + (srcReg11 << 5)) + destReg11);
			return 4;
			goto l9;
		}
		if (((!((-constant9) & (0xFFF))))
		 && ((((((-constant9)) >> 12) >= 0) && ((((-constant9)) >> 12) <= (0xFFF))))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg11 == SP
	? 418
	: 482))) << 23))) + ((((usqInt)((-constant9))) >> 2) + (0x400000))) + (srcReg11 << 5)) + destReg11);
			return 4;
			goto l9;
		}
	l9:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;

		/* C6.2.7		ADDS (extended register)		C6-766 */
		offset10 = emitMoveCwintoRat(self_in_dispatchConcretize, constant9, RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[offset10 / 4] = (((((0xAB200000U) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(SXTX) << 13)))) + (srcReg11 << 5)) + destReg11);
		return offset10 + 4;

	case AddCqRR:
		/* begin concretizeAddCqRDest: */
		destReg12 = ((self_in_dispatchConcretize->operands))[2];
		constant10 = ((sqLong) (((self_in_dispatchConcretize->operands))[0]));
		srcReg12 = ((self_in_dispatchConcretize->operands))[1];
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if (((constant10 >= 0) && (constant10 <= (0xFFF)))) {
			
			/* C6.2.4		ADD (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg12 == SP
	? 0x122
	: 354))) << 23))) + (((sqInt)((usqInt)(constant10) << 10)))) + (srcReg12 << 5)) + destReg12);
			return 4;
			goto l10;
		}
		if (((!(constant10 & (0xFFF))))
		 && (((((constant10) >> 12) >= 0) && (((constant10) >> 12) <= (0xFFF))))) {
			
			/* C6.2.4		ADD (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg12 == SP
	? 0x122
	: 354))) << 23))) + ((((usqInt)(constant10)) >> 2) + (0x400000))) + (srcReg12 << 5)) + destReg12);
			return 4;
			goto l10;
		}
	l10:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if ((((-constant10) >= 0) && ((-constant10) <= (0xFFF)))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg12 == SP
	? 418
	: 482))) << 23))) + (((sqInt)((usqInt)((-constant10)) << 10)))) + (srcReg12 << 5)) + destReg12);
			return 4;
			goto l11;
		}
		if (((!((-constant10) & (0xFFF))))
		 && ((((((-constant10)) >> 12) >= 0) && ((((-constant10)) >> 12) <= (0xFFF))))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((srcReg12 == SP
	? 418
	: 482))) << 23))) + ((((usqInt)((-constant10))) >> 2) + (0x400000))) + (srcReg12 << 5)) + destReg12);
			return 4;
			goto l11;
		}
	l11:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;

		/* C6.2.7		ADDS (extended register)		C6-766 */
		offset11 = emitMoveCwintoRat(self_in_dispatchConcretize, constant10, RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[offset11 / 4] = (((((0xAB200000U) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(SXTX) << 13)))) + (srcReg12 << 5)) + destReg12);
		return offset11 + 4;

	case AndCqR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalAndS, ((self_in_dispatchConcretize->operands))[1]);

	case AndCqRR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalAndS, ((self_in_dispatchConcretize->operands))[2]);

	case OrCqR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalOr, ((self_in_dispatchConcretize->operands))[1]);

	case OrCqRR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalOr, ((self_in_dispatchConcretize->operands))[2]);

	case CmpCqR:
		/* begin concretizeCmpCqR */
		constant11 = ((sqLong) (((self_in_dispatchConcretize->operands))[0]));

		/* cogit processor disassembleInstructionAt: 0 In: machineCode object */
		rn2 = ((self_in_dispatchConcretize->operands))[1];
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if (((constant11 >= 0) && (constant11 <= (0xFFF)))) {
			
			/* C6.2.61	CMP (immediate)	C6-858
			   C6.2.314	SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((0xF1000000U) + (((sqInt)((usqInt)(constant11) << 10)))) + (rn2 << 5)) + XZR);
			return 4;
			goto l12;
		}
		if (((!(constant11 & (0xFFF))))
		 && (((((constant11) >> 12) >= 0) && (((constant11) >> 12) <= (0xFFF))))) {
			
			/* C6.2.61	CMP (immediate)	C6-858
			   C6.2.314	SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((0xF1000000U) + ((((usqInt)(constant11)) >> 2) + (0x400000))) + (rn2 << 5)) + XZR);
			return 4;
			goto l12;
		}
	l12:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if ((((-constant11) >= 0) && ((-constant11) <= (0xFFF)))) {
			
			/* C6.2.58	CMN (immediate)	C6-852
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((0xB1000000U) + (((sqInt)((usqInt)((-constant11)) << 10)))) + (rn2 << 5)) + XZR);
			return 4;
			goto l13;
		}
		if (((!((-constant11) & (0xFFF))))
		 && ((((((-constant11)) >> 12) >= 0) && ((((-constant11)) >> 12) <= (0xFFF))))) {
			
			/* C6.2.58	CMN (immediate)	C6-852
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((0xB1000000U) + ((((usqInt)((-constant11))) >> 2) + (0x400000))) + (rn2 << 5)) + XZR);
			return 4;
			goto l13;
		}
	l13:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;

		/* C6.2.60	CMP (extended register)	C6-856 */
		offset12 = emitMoveCwintoRat(self_in_dispatchConcretize, constant11, RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[offset12 / 4] = (((((0xEB200000U) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (rn2 << 5)) + XZR);
		return offset12 + 4;

	case SubCqR:
		/* begin concretizeSubCqR */
		constant12 = ((sqLong) (((self_in_dispatchConcretize->operands))[0]));
		reg10 = ((self_in_dispatchConcretize->operands))[1];
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if (((constant12 >= 0) && (constant12 <= (0xFFF)))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((reg10 == SP
	? 418
	: 482))) << 23))) + (((sqInt)((usqInt)(constant12) << 10)))) + (reg10 << 5)) + reg10);
			return 4;
			goto l14;
		}
		if (((!(constant12 & (0xFFF))))
		 && (((((constant12) >> 12) >= 0) && (((constant12) >> 12) <= (0xFFF))))) {
			
			/* C6.2.308		SUB (immediate)	C6-1311
			   C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((reg10 == SP
	? 418
	: 482))) << 23))) + ((((usqInt)(constant12)) >> 2) + (0x400000))) + (reg10 << 5)) + reg10);
			return 4;
			goto l14;
		}
	l14:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if ((((-constant12) >= 0) && ((-constant12) <= (0xFFF)))) {
			
			/* C6.2.4		ADDS (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((reg10 == SP
	? 0x122
	: 354))) << 23))) + (((sqInt)((usqInt)((-constant12)) << 10)))) + (reg10 << 5)) + reg10);
			return 4;
			goto l15;
		}
		if (((!((-constant12) & (0xFFF))))
		 && ((((((-constant12)) >> 12) >= 0) && ((((-constant12)) >> 12) <= (0xFFF))))) {
			
			/* C6.2.4		ADDS (immediate)	C6-761
			   C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_dispatchConcretize->machineCode))[0] = ((((((sqInt)((usqInt)(((reg10 == SP
	? 0x122
	: 354))) << 23))) + ((((usqInt)((-constant12))) >> 2) + (0x400000))) + (reg10 << 5)) + reg10);
			return 4;
			goto l15;
		}
	l15:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		offset13 = emitMoveCwintoRat(self_in_dispatchConcretize, constant12, RISCTempReg, 0);
		assert(!((SP == reg10)));
		((self_in_dispatchConcretize->machineCode))[offset13 / 4] = (((((0x8B000000U) + (((sqInt)((usqInt)(ArithmeticSubS) << 29)))) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (reg10 << 5)) + reg10);
		return offset13 + 4;

	case TstCqR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalAndS, XZR);

	case XorCqR:
		return concretizeLogicalOpCqRDest(self_in_dispatchConcretize, LogicalXor, ((self_in_dispatchConcretize->operands))[1]);

	case AddCwR:
		return concretizeCwRArithmeticRd(self_in_dispatchConcretize, ArithmeticAddS, ((self_in_dispatchConcretize->operands))[1]);

	case AndCwR:
		return concretizeCwRLogical(self_in_dispatchConcretize, LogicalAndS);

	case CmpCwR:
		return concretizeCwRArithmeticRd(self_in_dispatchConcretize, ArithmeticSubS, XZR);

	case CmpC32R:
		/* begin concretizeCmpC32R */
		constant = ((self_in_dispatchConcretize->operands))[0];
		rn = ((self_in_dispatchConcretize->operands))[1];
		assert(addressIsInCurrentCompilation((((self_in_dispatchConcretize->dependent))->address)));
		assert((((((self_in_dispatchConcretize->dependent))->address)) % 4) == 0);
		assert((SQABS(((((usqInt)((((self_in_dispatchConcretize->dependent))->address)))) - (((usqInt)((self_in_dispatchConcretize->address))))))) < (0x40000));
		((self_in_dispatchConcretize->machineCode))[0] = (((0x18000000) + (((sqInt)((usqInt)((((((self_in_dispatchConcretize->dependent))->address)) - ((self_in_dispatchConcretize->address)))) << 3)))) + RISCTempReg);
		((self_in_dispatchConcretize->machineCode))[1] = (((((0x6B200000) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (rn << 5)) + XZR);
		return 8;

	case OrCwR:
		return concretizeCwRLogical(self_in_dispatchConcretize, LogicalOr);

	case SubCwR:
		return concretizeCwRArithmeticRd(self_in_dispatchConcretize, ArithmeticSubS, ((self_in_dispatchConcretize->operands))[1]);

	case XorCwR:
		return concretizeCwRLogical(self_in_dispatchConcretize, LogicalXor);

	case AddRR:
		return concretizeRRArithmeticRd(self_in_dispatchConcretize, ArithmeticAddS, ((self_in_dispatchConcretize->operands))[1]);

	case AndRR:
		return concretizeRRLogical(self_in_dispatchConcretize, LogicalAndS);

	case CmpRR:
		return concretizeRRArithmeticRd(self_in_dispatchConcretize, ArithmeticSubS, XZR);

	case OrRR:
		return concretizeRRLogical(self_in_dispatchConcretize, LogicalOr);

	case SubRR:
		return concretizeRRArithmeticRd(self_in_dispatchConcretize, ArithmeticSubS, ((self_in_dispatchConcretize->operands))[1]);

	case XorRR:
		return concretizeRRLogical(self_in_dispatchConcretize, LogicalXor);

	case AddRRR:
		return concretizeRRArithmeticRd(self_in_dispatchConcretize, ArithmeticAddS, ((self_in_dispatchConcretize->operands))[2]);

	case SubRRR:
		return concretizeRRArithmeticRd(self_in_dispatchConcretize, ArithmeticSubS, ((self_in_dispatchConcretize->operands))[2]);

	case AddRdRd:
		/* begin concretizeMathRdRd: */
		srcReg = ((self_in_dispatchConcretize->operands))[0];
		destReg = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x1E600000) + (srcReg << 16)) + (0x2800)) + (destReg << 5)) + destReg);
		return 4;

	case CmpRdRd:
		/* begin concretizeCmpRdRd */
		((self_in_dispatchConcretize->machineCode))[0] = ((((0x1E600000) + ((((self_in_dispatchConcretize->operands))[0]) << 16)) + (0x2000)) + ((((self_in_dispatchConcretize->operands))[1]) << 5));
		return 4;

	case DivRdRd:
		/* begin concretizeMathRdRd: */
		srcReg1 = ((self_in_dispatchConcretize->operands))[0];
		destReg1 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x1E600000) + (srcReg1 << 16)) + (6144)) + (destReg1 << 5)) + destReg1);
		return 4;

	case MulRdRd:
		/* begin concretizeMathRdRd: */
		srcReg2 = ((self_in_dispatchConcretize->operands))[0];
		destReg2 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x1E600000) + (srcReg2 << 16)) + (0x800)) + (destReg2 << 5)) + destReg2);
		return 4;

	case SubRdRd:
		/* begin concretizeMathRdRd: */
		srcReg3 = ((self_in_dispatchConcretize->operands))[0];
		destReg3 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x1E600000) + (srcReg3 << 16)) + (14336)) + (destReg3 << 5)) + destReg3);
		return 4;

	case XorRdRd:
		/* begin concretizeXorRdRd */
		srcReg4 = ((self_in_dispatchConcretize->operands))[0];
		destReg4 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x2E200000) + (srcReg4 << 16)) + (7168)) + (destReg4 << 5)) + destReg4);
		return 4;

	case SqrtRd:
		/* begin concretizeSqrtRd */
		reg = ((self_in_dispatchConcretize->operands))[0];
		((self_in_dispatchConcretize->machineCode))[0] = (((509722624) + (reg << 5)) + reg);
		return 4;

	case NegateR:
		/* begin concretizeNegateR */
		reg1 = ((self_in_dispatchConcretize->operands))[0];
		assert(!((SP == reg1)));
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xCB000000U) + (reg1 << 16)) + (((sqInt)((usqInt)(XZR) << 5)))) + reg1);
		return 4;

	case LoadEffectiveAddressMwrR:
		return concretizeLoadEffectiveAddressMwrR(self_in_dispatchConcretize);

	case ArithmeticShiftRightCqR:
		/* begin concretizeArithmeticShiftRightCqR */
		constant1 = ((self_in_dispatchConcretize->operands))[0];
		reg2 = ((self_in_dispatchConcretize->operands))[1];
		assert(((constant1 >= 1) && (constant1 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x93400000U) + (constant1 << 16)) + (0xFC00)) + (reg2 << 5)) + reg2);
		return 4;

	case ArithmeticShiftRightCqRR:
		/* begin concretizeArithmeticShiftRightCqRR */
		constant2 = ((self_in_dispatchConcretize->operands))[0];
		srcReg5 = ((self_in_dispatchConcretize->operands))[1];
		destReg5 = ((self_in_dispatchConcretize->operands))[2];
		assert(((constant2 >= 1) && (constant2 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x93400000U) + (constant2 << 16)) + (0xFC00)) + (srcReg5 << 5)) + destReg5);
		return 4;

	case LogicalShiftRightCqR:
		/* begin concretizeLogicalShiftRightCqR */
		constant3 = ((self_in_dispatchConcretize->operands))[0];
		reg3 = ((self_in_dispatchConcretize->operands))[1];
		assert(((constant3 >= 1) && (constant3 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xD3400000U) + (constant3 << 16)) + (0xFC00)) + (reg3 << 5)) + reg3);
		return 4;

	case LogicalShiftRightCqRR:
		/* begin concretizeLogicalShiftRightCqRR */
		constant4 = ((self_in_dispatchConcretize->operands))[0];
		srcReg6 = ((self_in_dispatchConcretize->operands))[1];
		destReg6 = ((self_in_dispatchConcretize->operands))[2];
		assert(((constant4 >= 1) && (constant4 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xD3400000U) + (constant4 << 16)) + (0xFC00)) + (srcReg6 << 5)) + destReg6);
		return 4;

	case LogicalShiftLeftCqR:
		/* begin concretizeLogicalShiftLeftCqR */
		constant5 = ((self_in_dispatchConcretize->operands))[0];
		reg4 = ((self_in_dispatchConcretize->operands))[1];
		assert(((constant5 >= 1) && (constant5 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xD3400000U) + (((sqInt)((usqInt)((64 - constant5)) << 16)))) + (((sqInt)((usqInt)((0x3F - constant5)) << 10)))) + (reg4 << 5)) + reg4);
		return 4;

	case LogicalShiftLeftCqRR:
		/* begin concretizeLogicalShiftLeftCqRR */
		constant6 = ((self_in_dispatchConcretize->operands))[0];
		srcReg7 = ((self_in_dispatchConcretize->operands))[1];
		destReg7 = ((self_in_dispatchConcretize->operands))[2];
		assert(((constant6 >= 1) && (constant6 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xD3400000U) + (((sqInt)((usqInt)((64 - constant6)) << 16)))) + (((sqInt)((usqInt)((0x3F - constant6)) << 10)))) + (srcReg7 << 5)) + destReg7);
		return 4;

	case ArithmeticShiftRightRR:
		/* begin concretizeShiftRR: */
		shiftCountReg = ((self_in_dispatchConcretize->operands))[0];
		shiftedReg = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9AC00000U) + (shiftCountReg << 16)) + (0x2800)) + (shiftedReg << 5)) + shiftedReg);
		return 4;

	case LogicalShiftLeftRR:
		/* begin concretizeShiftRR: */
		shiftCountReg1 = ((self_in_dispatchConcretize->operands))[0];
		shiftedReg1 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9AC00000U) + (shiftCountReg1 << 16)) + (0x2000)) + (shiftedReg1 << 5)) + shiftedReg1);
		return 4;

	case LogicalShiftRightRR:
		/* begin concretizeShiftRR: */
		shiftCountReg2 = ((self_in_dispatchConcretize->operands))[0];
		shiftedReg2 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9AC00000U) + (shiftCountReg2 << 16)) + (9216)) + (shiftedReg2 << 5)) + shiftedReg2);
		return 4;

	case RotateRightCqR:
	case RotateLeftCqR:
		/* begin concretizeRotateCqR */
		constant7 = ((self_in_dispatchConcretize->operands))[0];
		reg5 = ((self_in_dispatchConcretize->operands))[1];
		assert(((constant7 >= 1) && (constant7 <= 0x3F)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x93C00000U) + (reg5 << 16)) + (((RotateRightCqR == ((self_in_dispatchConcretize->opcode))
	? constant7
	: 64 - constant7)) << 10)) + (reg5 << 5)) + reg5);
		return 4;

	case ClzRR:
		/* begin concretizeClzRR */
		maskReg = ((self_in_dispatchConcretize->operands))[0];
		dest = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((3670020096U) + (maskReg << 5)) + dest);
		return 4;

	case MulRRR:
		/* begin concretizeMulRRR */
		reg11 = ((self_in_dispatchConcretize->operands))[0];
		reg21 = ((self_in_dispatchConcretize->operands))[1];
		reg31 = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9B000000U) + (reg11 << 16)) + (((sqInt)((usqInt)(XZR) << 10)))) + (reg21 << 5)) + reg31);
		return 4;

	case MulOverflowRRR:
		/* begin concretizeMulOverflowRRR */
		reg12 = ((self_in_dispatchConcretize->operands))[0];
		reg22 = ((self_in_dispatchConcretize->operands))[1];

		/* RISCTempReg := high(reg1 * reg2); must precede destructive MUL */
		reg32 = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9B400000U) + (reg12 << 16)) + (((sqInt)((usqInt)(XZR) << 10)))) + (reg22 << 5)) + RISCTempReg);
		((self_in_dispatchConcretize->machineCode))[1] = (((((0x9B000000U) + (reg12 << 16)) + (((sqInt)((usqInt)(XZR) << 10)))) + (reg22 << 5)) + reg32);
		((self_in_dispatchConcretize->machineCode))[2] = (((0xD37FFC00U) + (reg32 << 5)) + CArg1Reg);
		return 12;

	case DivRRR:
		/* begin concretizeDivRRR */
		reg13 = ((self_in_dispatchConcretize->operands))[0];
		reg23 = ((self_in_dispatchConcretize->operands))[1];
		reg33 = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0x9AC00000U) + (reg23 << 16)) + (0xC00)) + (reg13 << 5)) + reg33);
		return 4;

	case MSubRRR:
		/* begin concretizeMSubRRR */
		rM = ((((self_in_dispatchConcretize->operands))[0])) >> 5;
		rN = (((self_in_dispatchConcretize->operands))[0]) & 0x1F;
		rA = ((self_in_dispatchConcretize->operands))[1];
		rD = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = ((((((0x9B000000U) + (rM << 16)) + (0x8000)) + (rA << 10)) + (rN << 5)) + rD);
		return 4;

	case DC:
		return concretizeDataCacheControl(self_in_dispatchConcretize);

	case DSB:
		/* begin concretizeDataSynchronizationBarrier */
		CRm = ((((self_in_dispatchConcretize->operands))[0]) << 2) + (((self_in_dispatchConcretize->operands))[1]);
		((self_in_dispatchConcretize->machineCode))[0] = (0xD503309FU + (CRm << 8));
		return 4;

	case IC:
		/* begin concretizeInstructionCacheControl */
		return (/* begin concretizeCacheControlOp1:CRm:Op2: */
		((self_in_dispatchConcretize->machineCode))[0] = ((3574297888U) + (((self_in_dispatchConcretize->operands))[0])),
		4);

	case ISB:
		/* begin concretizeInstructionSynchronizationBarrier */
		CRm1 = 15;
		((self_in_dispatchConcretize->machineCode))[0] = (0xD50330DFU + (((sqInt)((usqInt)(CRm1) << 8))));
		return 4;

	case MRS_CTR_EL0:
		/* begin concretizeMRS_CTR_EL0 */
		((self_in_dispatchConcretize->machineCode))[0] = ((3577413664U) + (((self_in_dispatchConcretize->operands))[0]));
		return 4;

	case MRS_ID_AA64ISAR0_EL1:
		/* begin concretizeMRS_ID_AA64ISAR0_EL1 */
		((self_in_dispatchConcretize->machineCode))[0] = ((3577218560U) + (((self_in_dispatchConcretize->operands))[0]));
		return 4;

	case MoveCqR:
		return concretizeMoveCqR(self_in_dispatchConcretize);

	case MoveCwR:
		/* begin concretizeMoveCwR */
		return emitMoveCwintoRat(self_in_dispatchConcretize, ((self_in_dispatchConcretize->operands))[0], ((self_in_dispatchConcretize->operands))[1], 0);

	case MoveC32R:
		/* begin concretizeMoveC32R */
		constant8 = ((self_in_dispatchConcretize->operands))[0];
		rn1 = ((self_in_dispatchConcretize->operands))[1];
		assert(addressIsInCurrentCompilation((((self_in_dispatchConcretize->dependent))->address)));
		assert((((((self_in_dispatchConcretize->dependent))->address)) % 4) == 0);
		assert((((((self_in_dispatchConcretize->dependent))->address)) - ((self_in_dispatchConcretize->address))) < (0x100000));
		((self_in_dispatchConcretize->machineCode))[0] = (((0x98000000U) + (((sqInt)((usqInt)(((((((self_in_dispatchConcretize->dependent))->address)) - ((self_in_dispatchConcretize->address))) & (0x1FFFFF))) << 3)))) + rn1);
		return 4;

	case MoveRR:
		/* begin concretizeMoveRR */
		srcReg8 = ((self_in_dispatchConcretize->operands))[0];

		/* C6.2.184 MOV (to/from SP) p1089
		   C6.2.188 MOV (register) p1096 */
		destReg8 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((srcReg8 == SP)
 || (destReg8 == SP)
	? ((0x91000000U) + (srcReg8 << 5)) + destReg8
	: (((0xAA000000U) + (srcReg8 << 16)) + (((sqInt)((usqInt)(XZR) << 5)))) + destReg8));
		return 4;

	case MoveRRd:
		/* begin concretizeMoveRRd */
		((self_in_dispatchConcretize->machineCode))[0] = (((0x9E670000U) + ((((self_in_dispatchConcretize->operands))[0]) << 5)) + (((self_in_dispatchConcretize->operands))[1]));
		return 4;

	case MoveRdR:
		/* begin concretizeMoveRdR */
		((self_in_dispatchConcretize->machineCode))[0] = (((0x9E660000U) + ((((self_in_dispatchConcretize->operands))[0]) << 5)) + (((self_in_dispatchConcretize->operands))[1]));
		return 4;

	case MoveAwR:
		/* begin concretizeMoveAwR */
		srcAddr = ((self_in_dispatchConcretize->operands))[0];

		/* ldr srcReg, [VarBaseReg, #offset] except that this is illegal for SP/X31 */
		destReg9 = ((self_in_dispatchConcretize->operands))[1];
		if ((srcAddr != null)
		 && (((srcAddr >= ((varBaseAddress()) - (0x100))) && (srcAddr <= (((varBaseAddress()) + (0x1000)) - 1))))) {
			if (srcAddr < (varBaseAddress())) {
				error("shouldBeImplemented");
				return 4;
			}
			if (destReg9 != SP) {
				return emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 3, VarBaseReg, destReg9, srcAddr - (varBaseAddress()), 0, 0);
			}
			instrOffset2 = emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 3, VarBaseReg, RISCTempReg, srcAddr - (varBaseAddress()), 0, 0);
			((self_in_dispatchConcretize->machineCode))[instrOffset2 / 4] = (movernrd(self_in_dispatchConcretize, RISCTempReg, destReg9));
			return instrOffset2 + 4;
		}
		instrOffset2 = emitMoveCwintoRat(self_in_dispatchConcretize, srcAddr, RISCTempReg, 0);
		if (SP != destReg9) {
			return emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 3, RISCTempReg, destReg9, 0, 0, instrOffset2);
		}
		instrOffset2 = emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 3, RISCTempReg, RISCTempReg, 0, 0, instrOffset2);
		((self_in_dispatchConcretize->machineCode))[instrOffset2 / 4] = (movernrd(self_in_dispatchConcretize, RISCTempReg, destReg9));
		return instrOffset2 + 4;

	case MoveRAw:
		/* begin concretizeMoveRAw */
		srcReg9 = ((self_in_dispatchConcretize->operands))[0];

		/* str srcReg, [VarBaseReg, #offset] except that this is illegal for srcReg = SP/X31 */
		destAddr = ((self_in_dispatchConcretize->operands))[1];
		if ((destAddr != null)
		 && (((destAddr >= ((varBaseAddress()) - (0x100))) && (destAddr <= (((varBaseAddress()) + (0x1000)) - 1))))) {
			if (destAddr < (varBaseAddress())) {
				error("shouldBeImplemented");
				return 4;
			}
			if (srcReg9 != SP) {
				((self_in_dispatchConcretize->machineCode))[0] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 3, VarBaseReg, srcReg9, destAddr - (varBaseAddress()), 0));
				return 4;
			}
			((self_in_dispatchConcretize->machineCode))[0] = (movernrd(self_in_dispatchConcretize, srcReg9, RISCTempReg));
			((self_in_dispatchConcretize->machineCode))[1] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 3, VarBaseReg, RISCTempReg, destAddr - (varBaseAddress()), 0));
			return 8;
		}
		instrOffset3 = emitMoveCwintoRat(self_in_dispatchConcretize, destAddr, RISCTempReg, 0);
		if (SP != srcReg9) {
			((self_in_dispatchConcretize->machineCode))[instrOffset3 / 4] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 3, RISCTempReg, srcReg9, 0, 0));
			return instrOffset3 + 4;
		}
		((self_in_dispatchConcretize->machineCode))[instrOffset3 / 4] = (movernrd(self_in_dispatchConcretize, srcReg9, CArg1Reg));
		((self_in_dispatchConcretize->machineCode))[instrOffset3 / 4] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 3, RISCTempReg, CArg1Reg, 0, 0));
		return instrOffset3 + 8;

	case MoveAwRR:
		/* begin concretizeMoveAwRR */
		pairAddress = ((self_in_dispatchConcretize->operands))[0];
		reg14 = ((self_in_dispatchConcretize->operands))[1];
		reg24 = ((self_in_dispatchConcretize->operands))[2];
		assert(reg14 != reg24);
		assert((reg14 != RISCTempReg)
		 && (reg24 != RISCTempReg));
		assert(isAddressRelativeToVarBase(self_in_dispatchConcretize, pairAddress));
		assert(((pairAddress - (varBaseAddress())) / 8) < (64));
		if (reg14 == SP) {
			reg14 = RISCTempReg;
		}
		if (reg24 == SP) {
			reg24 = RISCTempReg;
		}
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xA9400000U) + (((sqInt)((usqInt)(((pairAddress - (varBaseAddress())) / 8)) << 15)))) + (reg24 << 10)) + (((sqInt)((usqInt)(VarBaseReg) << 5)))) + reg14);
		if ((reg14 != RISCTempReg)
		 && (reg24 != RISCTempReg)) {
			return 4;
		}
		((self_in_dispatchConcretize->machineCode))[1] = (movernrd(self_in_dispatchConcretize, RISCTempReg, SP));
		return 8;

	case MoveRRAw:
		/* begin concretizeMoveRRAw */
		reg15 = ((self_in_dispatchConcretize->operands))[0];
		reg25 = ((self_in_dispatchConcretize->operands))[1];
		pairAddress1 = ((self_in_dispatchConcretize->operands))[2];
		assert(reg15 != reg25);
		assert((reg15 != RISCTempReg)
		 && (reg25 != RISCTempReg));
		assert(isAddressRelativeToVarBase(self_in_dispatchConcretize, pairAddress1));
		assert(((pairAddress1 - (varBaseAddress())) / 8) < (64));
		if (reg15 == SP) {
			reg15 = RISCTempReg;
		}
		if (reg25 == SP) {
			reg25 = RISCTempReg;
		}
		if ((reg15 == RISCTempReg)
		 || (reg25 == RISCTempReg)) {
			((self_in_dispatchConcretize->machineCode))[0] = (movernrd(self_in_dispatchConcretize, SP, RISCTempReg));
			offset6 = 1;
		}
		else {
			offset6 = 0;
		}
		((self_in_dispatchConcretize->machineCode))[offset6] = (((((0xA9000000U) + (((sqInt)((usqInt)(((pairAddress1 - (varBaseAddress())) / 8)) << 15)))) + (reg25 << 10)) + (((sqInt)((usqInt)(VarBaseReg) << 5)))) + reg15);
		return (offset6 + 1) * 4;

	case MoveAbR:
		/* begin concretizeMoveAbR */
		srcAddr1 = ((self_in_dispatchConcretize->operands))[0];
		destReg10 = ((self_in_dispatchConcretize->operands))[1];
		assert(!((SP == destReg10)));
		if ((srcAddr1 != null)
		 && (((srcAddr1 >= ((varBaseAddress()) - (0x100))) && (srcAddr1 <= (((varBaseAddress()) + (0x1000)) - 1))))) {
			if (srcAddr1 < (varBaseAddress())) {
				error("shouldBeImplemented");
				return 4;
			}
			return emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 0, VarBaseReg, destReg10, srcAddr1 - (varBaseAddress()), 0, 0);
		}
		instrOffset4 = emitMoveCwintoRat(self_in_dispatchConcretize, srcAddr1, RISCTempReg, 0);
		return emitLdrnrtimmshiftBy12at(self_in_dispatchConcretize, 0, RISCTempReg, destReg10, 0, 0, instrOffset4);

	case MoveRAb:
		/* begin concretizeMoveRAb */
		srcReg10 = ((self_in_dispatchConcretize->operands))[0];
		destAddr1 = ((self_in_dispatchConcretize->operands))[1];
		assert(!((SP == srcReg10)));
		if ((destAddr1 != null)
		 && (((destAddr1 >= ((varBaseAddress()) - (0x100))) && (destAddr1 <= (((varBaseAddress()) + (0x1000)) - 1))))) {
			if (destAddr1 < (varBaseAddress())) {
				error("shouldBeImplemented");
				return 4;
			}
			((self_in_dispatchConcretize->machineCode))[0] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 0, VarBaseReg, srcReg10, destAddr1 - (varBaseAddress()), 0));
			return 4;
		}
		instrOffset5 = emitMoveCwintoRat(self_in_dispatchConcretize, destAddr1, RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[instrOffset5 / 4] = (strnrtimmshiftBy12(self_in_dispatchConcretize, 0, RISCTempReg, srcReg10, 0, 0));
		return instrOffset5 + 4;

	case MoveMwrR:
		return concretizeMoveMSrR(self_in_dispatchConcretize, 3);

	case MoveM32rR:
		return concretizeMoveMSrR(self_in_dispatchConcretize, 2);

	case MoveM16rR:
		return concretizeMoveMSrR(self_in_dispatchConcretize, 1);

	case MoveMbrR:
		return concretizeMoveMSrR(self_in_dispatchConcretize, 0);

	case MoveRMwr:
		return concretizeMoveRMSr(self_in_dispatchConcretize, 3);

	case MoveRM32r:
		return concretizeMoveRMSr(self_in_dispatchConcretize, 2);

	case MoveRM16r:
		return concretizeMoveRMSr(self_in_dispatchConcretize, 1);

	case MoveRMbr:
		return concretizeMoveRMSr(self_in_dispatchConcretize, 0);

	case MoveXwrRR:
		return concretizeMoveXSrRR(self_in_dispatchConcretize, 3);

	case MoveX32rRR:
		return concretizeMoveXSrRR(self_in_dispatchConcretize, 2);

	case MoveX16rRR:
		return concretizeMoveXSrRR(self_in_dispatchConcretize, 1);

	case MoveXbrRR:
		return concretizeMoveXSrRR(self_in_dispatchConcretize, 0);

	case MoveRXwrR:
		return concretizeMoveRXSrR(self_in_dispatchConcretize, 3);

	case MoveRX32rR:
		return concretizeMoveRXSrR(self_in_dispatchConcretize, 2);

	case MoveRX16rR:
		return concretizeMoveRXSrR(self_in_dispatchConcretize, 1);

	case MoveRXbrR:
		return concretizeMoveRXSrR(self_in_dispatchConcretize, 0);

	case MoveM64rRd:
		/* begin concretizeMoveM64rRd */
		offset7 = ((self_in_dispatchConcretize->operands))[0];
		baseReg = ((self_in_dispatchConcretize->operands))[1];
		rd = ((self_in_dispatchConcretize->operands))[2];
		assert((!(offset7 & 7)));
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xFD400000U) + (offset7 << 7)) + (baseReg << 5)) + rd);
		return 4;

	case MoveRdM64r:
		/* begin concretizeMoveRdM64r */
		rd1 = ((self_in_dispatchConcretize->operands))[0];
		offset8 = ((self_in_dispatchConcretize->operands))[1];
		baseReg1 = ((self_in_dispatchConcretize->operands))[2];
		assert((!(offset8 & 7)));
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xFD000000U) + (offset8 << 7)) + (baseReg1 << 5)) + rd1);
		return 4;

	case PopR:
		/* begin concretizePopR */
		reg6 = ((self_in_dispatchConcretize->operands))[0];
		assert(!((SP == reg6)));
		((self_in_dispatchConcretize->machineCode))[0] = (((4164977664U) + (((sqInt)((usqInt)(SPReg) << 5)))) + reg6);
		return 4;

	case PushR:
		/* begin concretizePushR */
		reg7 = ((self_in_dispatchConcretize->operands))[0];
		assert(!((SP == reg7)));
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xF8000000U) + (((sqInt)((usqInt)((-8 & (0x1FF))) << 12)))) + (0xC00)) + (((sqInt)((usqInt)(SPReg) << 5)))) + reg7);
		return 4;

	case NativePopRR:
		/* begin concretizeNativePopRR */
		reg16 = ((self_in_dispatchConcretize->operands))[0];

		/* Post-index */
		reg26 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xA8C10000U) + (reg26 << 10)) + (((sqInt)((usqInt)(SP) << 5)))) + reg16);
		return 4;

	case NativePushRR:
		/* begin concretizeNativePushRR */
		reg17 = ((self_in_dispatchConcretize->operands))[0];

		/* Pre-index */
		reg27 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xA9BF0000U) + (reg27 << 10)) + (((sqInt)((usqInt)(SP) << 5)))) + reg17);
		return 4;

	case PushCq:
		/* begin concretizePushCq */
		((self_in_dispatchConcretize->operands))[1] = RISCTempReg;

		/* C6.2.273	STR (immediate)	Pre-index	C6-1239 */
		instrBytes = concretizeMoveCqR(self_in_dispatchConcretize);
		((self_in_dispatchConcretize->machineCode))[instrBytes / 4] = (((((0xF8000000U) + (((sqInt)((usqInt)((-8 & (0x1FF))) << 12)))) + (0xC00)) + (((sqInt)((usqInt)(SPReg) << 5)))) + RISCTempReg);
		return instrBytes + 4;

	case PushCw:
		/* begin concretizePushCw */

		/* C6.2.273	STR (immediate)	Pre-index	C6-1239 */
		instrBytes1 = emitMoveCwintoRat(self_in_dispatchConcretize, ((self_in_dispatchConcretize->operands))[0], RISCTempReg, 0);
		((self_in_dispatchConcretize->machineCode))[instrBytes1 / 4] = (((((0xF8000000U) + (((sqInt)((usqInt)((-8 & (0x1FF))) << 12)))) + (0xC00)) + (((sqInt)((usqInt)(SPReg) << 5)))) + RISCTempReg);
		return instrBytes1 + 4;

	case PrefetchAw:
		/* begin concretizePrefetchAw */
		srcAddr2 = ((self_in_dispatchConcretize->operands))[0];
		if ((srcAddr2 != null)
		 && (((srcAddr2 >= ((varBaseAddress()) - (0x100))) && (srcAddr2 <= (((varBaseAddress()) + (0x1000)) - 1))))) {
			if (srcAddr2 < (varBaseAddress())) {
				error("shouldBeImplemented");
				return 4;
			}
			((self_in_dispatchConcretize->machineCode))[0] = (prnimmshiftBy12(self_in_dispatchConcretize, VarBaseReg, srcAddr2 - (varBaseAddress()), 0));
			return 4;
		}
		return 0;

	case ConvertRRd:
		/* begin concretizeConvertRRd */
		((self_in_dispatchConcretize->machineCode))[0] = (((0x9E620000U) + ((((self_in_dispatchConcretize->operands))[0]) << 5)) + (((self_in_dispatchConcretize->operands))[1]));
		return 4;

	case ConvertRdR:
		/* begin concretizeConvertRdR */
		((self_in_dispatchConcretize->machineCode))[0] = (((0x9E780000U) + ((((self_in_dispatchConcretize->operands))[0]) << 5)) + (((self_in_dispatchConcretize->operands))[1]));
		return 4;

	case SignExtend8RR:
		return concretizeSignExtendRR(self_in_dispatchConcretize, 8);

	case SignExtend16RR:
		return concretizeSignExtendRR(self_in_dispatchConcretize, 16);

	case SignExtend32RR:
		return concretizeSignExtendRR(self_in_dispatchConcretize, 32);

	case ZeroExtend8RR:
		return concretizeZeroExtendRR(self_in_dispatchConcretize, 8);

	case ZeroExtend16RR:
		return concretizeZeroExtendRR(self_in_dispatchConcretize, 16);

	case ZeroExtend32RR:
		return concretizeZeroExtendRR(self_in_dispatchConcretize, 32);

	case LDAXR:
		/* begin concretizeLDAXR */
		targetReg = ((self_in_dispatchConcretize->operands))[0];
		addressReg = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = (((3361733632U) + (addressReg << 5)) + targetReg);
		return 4;

	case STLXR:
		/* begin concretizeSTLXR */
		valueReg = ((self_in_dispatchConcretize->operands))[0];
		addressReg1 = ((self_in_dispatchConcretize->operands))[1];
		statusReg = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xC8000000U) + (statusReg << 16)) + (0xFC00)) + (addressReg1 << 5)) + valueReg);
		return 4;

	case CLREX:
		/* begin concretizeCLREX */
		((self_in_dispatchConcretize->machineCode))[0] = 0xD5033F5FU;
		return 4;

	case STLR:
		return concretizeSTLR(self_in_dispatchConcretize);

	case CASAL:
		/* begin concretizeCASAL */
		constantReg = ((self_in_dispatchConcretize->operands))[0];
		valueReg1 = ((self_in_dispatchConcretize->operands))[1];
		baseReg2 = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xC8E00000U) + (constantReg << 16)) + (0xFC00)) + (baseReg2 << 5)) + valueReg1);
		return 4;

	case CCMPNE:
		/* begin concretizeCCMPNE */
		rM1 = ((self_in_dispatchConcretize->operands))[0];
		rN1 = ((self_in_dispatchConcretize->operands))[1];
		flags = ((self_in_dispatchConcretize->operands))[2];
		((self_in_dispatchConcretize->machineCode))[0] = (((((0xFA400000U) + (rM1 << 16)) + (((sqInt)((usqInt)(NE) << 12)))) + (rN1 << 5)) + flags);
		return 4;

	case CSET:
		/* begin concretizeCSET */
		reg8 = ((self_in_dispatchConcretize->operands))[0];
		condition = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = ((((0x9A9F0000U) + ((condition ^ 1) << 12)) + (0x7E0)) + reg8);
		return 4;

	case CBNZ:
	case CBZ:
		/* begin concretizeCB */
		jumpTarget12 = ((AbstractInstruction *) (((self_in_dispatchConcretize->operands))[0]));
		assertSaneJumpTarget(jumpTarget12);
		if ((addressIsInInstructions(jumpTarget12))
		 || (jumpTarget12 == (methodLabel()))) {
			jumpTarget12 = ((AbstractInstruction *) ((jumpTarget12->address)));
		}
		assert(jumpTarget12 != 0);
		jumpTarget3 = jumpTarget12;
		offset9 = (((sqLong) jumpTarget3)) - (((sqLong) ((self_in_dispatchConcretize->address))));
		assert((offset9 != 0)
		 && (isInImmediateBranchRange(self_in_dispatchConcretize, offset9)));
		reg9 = ((self_in_dispatchConcretize->operands))[1];
		((self_in_dispatchConcretize->machineCode))[0] = ((((0xB4000000U) + ((((self_in_dispatchConcretize->opcode)) == CBNZ
	? 0x1000000
	: 0))) + (((sqInt)((usqInt)((offset9 & (0x1FFFFF))) << 3)))) + reg9);
		return 4;

	case MovePerfCnt64RL:
		return concretizeMovePerfCnt64RL(self_in_dispatchConcretize);

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	C7.2.184	LDR (immediate, SIMD&FP)	C7-1800
	C7.2.186	LDR (register, SIMD&FP)	C7-1806 */
/*	cogit processor disassembleInstructionAt: instrOffset In: machineCode
	object 
 */
/*	Unsigned offset, C7-1801 */

	/* CogARMv8Compiler>>#emitLdfprn:rt:imm:shiftBy12:at: */
static sqInt NoDbgRegParms
emitLdfprnrtimmshiftBy12at(AbstractInstruction * self_in_emitLdfprnrtimmshiftBy12at, sqInt baseReg, sqInt targetDPReg, sqInt offset, sqInt shiftBy12, sqInt instrOffset)
{
    sqInt instrBytes;

	if (((offset % 8) == 0)
	 && ((((offset / 8) >= 0) && ((offset / 8) <= (0xFFF))))) {
		((self_in_emitLdfprnrtimmshiftBy12at->machineCode))[instrOffset / 4] = ((((0xFD400000U) + (((sqInt)((usqInt)(offset) << 7)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetDPReg);
		return instrOffset + 4;
	}

	/* C7.2.186	LDR (register, SIMD&FP)	C7-1806 */
	instrBytes = emitMoveCwintoRat(self_in_emitLdfprnrtimmshiftBy12at, offset, RISCTempReg, instrOffset);
	((self_in_emitLdfprnrtimmshiftBy12at->machineCode))[instrBytes / 4] = ((((((0xFC600000U) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (0x800)) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetDPReg);
	return instrBytes + 4;
}


/*	C6.2.130	LDR (immediate)	C6-976
	C6.2.132	LDR (register)		C6-981
	C6.2.136	LDRH (immediate)	C6-990
	C6.2.166	LDUR				C6-1058
	C6.2.134	LDRB (immediate)	C6-985
	C6.2.135	LDRB (register)		C6-988 */
/*	cogit processor disassembleInstructionAt: instrOffset In: machineCode
	object 
 */

	/* CogARMv8Compiler>>#emitLd:rn:rt:imm:shiftBy12:at: */
static sqInt NoDbgRegParms
emitLdrnrtimmshiftBy12at(AbstractInstruction * self_in_emitLdrnrtimmshiftBy12at, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt targetReg, sqInt offset, sqInt shiftBy12, sqInt instrOffset)
{
    sqInt instrBytes;
    sqInt unitSize;


	/* Unsigned offset, C6-977 */
	unitSize = 1ULL << unitSizeLog2MinusOne;
	if (((offset % unitSize) == 0)
	 && ((((offset / unitSize) >= 0) && ((offset / unitSize) <= (0xFFF))))) {
		((self_in_emitLdrnrtimmshiftBy12at->machineCode))[instrOffset / 4] = (((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x39400000)) + (((sqInt)((usqInt)(offset) << (10 - unitSizeLog2MinusOne))))) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetReg);
		return instrOffset + 4;
	}
	if (((offset >= -256) && (offset <= 0xFF))) {

		/* Unscaled signed 9-bit offset, C6-1058 */
		((self_in_emitLdrnrtimmshiftBy12at->machineCode))[instrOffset / 4] = (((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38400000)) + (((sqInt)((usqInt)((offset & 0x1FF)) << 12)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetReg);
		return instrOffset + 4;
	}
	instrBytes = emitMoveCwintoRat(self_in_emitLdrnrtimmshiftBy12at, offset, RISCTempReg, instrOffset);
	((self_in_emitLdrnrtimmshiftBy12at->machineCode))[instrBytes / 4] = (((((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38600000)) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (0x800)) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetReg);
	return instrBytes + 4;
}


/*	Emit a load of constant into destReg. Answer the number of bytes of
	machine code
	generated. Literals are stored out-of-line; emit a LDR (literal) with the
	relevant offset. */

	/* CogARMv8Compiler>>#emitMoveCw:intoR:at: */
static sqInt NoDbgRegParms
emitMoveCwintoRat(AbstractInstruction * self_in_emitMoveCwintoRat, usqInt constantArg, sqInt destReg, sqInt offsetBytes)
{
    sqInt aligned;
    usqInt constant;
    sqInt pcRelativeOffset;
    usqInt unaligned;

	assert(!((destReg == SP)));
	constant = ((addressIsInInstructions(((AbstractInstruction *) constantArg)))
	 || ((((AbstractInstruction *) constantArg)) == (methodLabel()))
		? ((((AbstractInstruction *) constantArg))->address)
		: constantArg);
	if (((((self_in_emitMoveCwintoRat->opcode)) == MoveCwR)
	 || (((self_in_emitMoveCwintoRat->opcode)) == PushCw))
	 && (((addressIsInInstructions(((AbstractInstruction *) constant)))
	 || ((((AbstractInstruction *) constant)) == (methodLabel())))
	 || (((((usqInt)constant)) >= ((methodLabel->address)))
	 && ((((usqInt)constant)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))))) {

		/* C6.2.10 	ADR	C6-773 */
		unaligned = constant & 3;
		aligned = constant - unaligned;
		pcRelativeOffset = ((aligned - (((sqInt)((self_in_emitMoveCwintoRat->address)))))) >> 2;
		assert(((pcRelativeOffset >= (-262144)) && (pcRelativeOffset <= (0x3FFFF))));
		((self_in_emitMoveCwintoRat->machineCode))[offsetBytes / 4] = ((((unaligned << 29) + (0x10000000)) + (((sqInt)((usqInt)((pcRelativeOffset & (0x7FFFF))) << 5)))) + destReg);
		return offsetBytes + 4;
	}
	assert(addressIsInCurrentCompilation((((self_in_emitMoveCwintoRat->dependent))->address)));
	assert((((((self_in_emitMoveCwintoRat->dependent))->address)) % 4) == 0);
	assert((SQABS(((((usqInt)((((self_in_emitMoveCwintoRat->dependent))->address)))) - (((usqInt)(((self_in_emitMoveCwintoRat->address)) + offsetBytes)))))) < (0x100000));
	((self_in_emitMoveCwintoRat->machineCode))[offsetBytes / 4] = (((0x58000000) + (((sqInt)((usqInt)(((((((self_in_emitMoveCwintoRat->dependent))->address)) - (((self_in_emitMoveCwintoRat->address)) + offsetBytes)) & (0x1FFFFF))) << 3)))) + destReg);
	return offsetBytes + 4;
}


/*	C6.2.273	STR (immediate)	C6-1239
	C6.2.274	STR (register)		C6-1242
	C6.2.275	STRB (immediate)	C6-1244 */

	/* CogARMv8Compiler>>#emitSt:rn:rt:imm:shiftBy12: */
static sqInt NoDbgRegParms
emitStrnrtimmshiftBy12(AbstractInstruction * self_in_emitStrnrtimmshiftBy12, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt sourceReg, sqInt offset, sqInt shiftBy12)
{
    sqInt instrBytes;
    sqInt unitSize;

	unitSize = 1ULL << unitSizeLog2MinusOne;
	assert(!((SP == sourceReg)));
	assert(!((baseReg == sourceReg)));
	if (((offset % unitSize) == 0)
	 && ((((offset / unitSize) >= 0) && ((offset / unitSize) <= (0xFFF))))) {
		((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = (((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x39000000)) + (((sqInt)((usqInt)(offset) << (10 - unitSizeLog2MinusOne))))) + (((sqInt)((usqInt)(baseReg) << 5)))) + sourceReg);
		return 4;
	}
	if (((offset >= -256) && (offset <= 0xFF))) {

		/* Unscaled signed 9-bit offset, C6-1244 */
		((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = (((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38000000)) + (((sqInt)((usqInt)((offset & 0x1FF)) << 12)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + sourceReg);
		return 4;
	}
	if ((((offset >= (-4096)) && (offset <= (0xFFF))))
	 || (((!(offset & (0xFFF))))
	 && (((((offset) >> 12) >= (-4096)) && (((offset) >> 12) <= (0xFFF)))))) {
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if (((offset >= 0) && (offset <= (0xFFF)))) {
			
			/* C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = ((((0xB1000000U) + (((sqInt)((usqInt)(offset) << 10)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + RISCTempReg);
			goto l2;
		}
		if (((!(offset & (0xFFF))))
		 && (((((offset) >> 12) >= 0) && (((offset) >> 12) <= (0xFFF))))) {
			
			/* C6.2.8		ADDS (immediate)	C6-769 */
			((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = ((((0xB1000000U) + ((((usqInt)(offset)) >> 2) + (0x400000))) + (((sqInt)((usqInt)(baseReg) << 5)))) + RISCTempReg);
			goto l2;
		}
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		if ((((-offset) >= 0) && ((-offset) <= (0xFFF)))) {
			
			/* C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = ((((0xF1000000U) + (((sqInt)((usqInt)((-offset)) << 10)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + RISCTempReg);
			goto l3;
		}
		if (((!((-offset) & (0xFFF))))
		 && ((((((-offset)) >> 12) >= 0) && ((((-offset)) >> 12) <= (0xFFF))))) {
			
			/* C6.2.314		SUBS (immediate)	C6-1321 */
			((self_in_emitStrnrtimmshiftBy12->machineCode))[0] = ((((0xF1000000U) + ((((usqInt)((-offset))) >> 2) + (0x400000))) + (((sqInt)((usqInt)(baseReg) << 5)))) + RISCTempReg);
			goto l3;
		}
		error("cannot happen");
	l3:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
	l2:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		((self_in_emitStrnrtimmshiftBy12->machineCode))[1] = (((((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38200000)) + (((sqInt)((usqInt)(XZR) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (0x800)) + (((sqInt)((usqInt)(RISCTempReg) << 5)))) + sourceReg);
		return 8;
	}
	instrBytes = emitMoveCwintoRat(self_in_emitStrnrtimmshiftBy12, offset, RISCTempReg, 0);
	assert(instrBytes == 4);
	((self_in_emitStrnrtimmshiftBy12->machineCode))[1] = (((((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38200000)) + (((sqInt)((usqInt)(RISCTempReg) << 16)))) + (((sqInt)((usqInt)(UXTX) << 13)))) + (0x800)) + (((sqInt)((usqInt)(baseReg) << 5)))) + sourceReg);
	return 8;
}


/*	Answer if CallFull and/or JumpFull are relative and hence need relocating
	on method
	compation. If so, they are annotated with IsRelativeCall in methods and
	relocated in
	relocateIfCallOrMethodReference:mcpc:delta: */

	/* CogARMv8Compiler>>#fullCallsAreRelative */
static sqInt NoDbgRegParms
fullCallsAreRelative(AbstractInstruction * self_in_fullCallsAreRelative)
{
	return 0;
}


/*	Divide regDividend by regDivisor storing the quotient in regQuotient and
	remainder in regRemainder.
	MSUB Multiply-subtract		MSUB	C6-1109
	SDIV Signed divide			SDIV	C6-1174 */
/*	For the MSUB to work we must preserve regDivisor and regDividend for the
	MSUB; i.e. the DivRRR must not overwrite either regDivisor or regDividend. */

	/* CogARMv8Compiler>>#genDivR:R:Quo:Rem: */
static AbstractInstruction * NoDbgRegParms
genDivRRQuoRem(AbstractInstruction * self_in_genDivRRQuoRem, sqInt regDivisor, sqInt regDividend, sqInt regQuotient, sqInt regRemainder)
{
    AbstractInstruction *instr;
    sqInt safeRegQuotient;

	safeRegQuotient = ((regQuotient == regDividend)
	 || (regQuotient == regDivisor)
		? RISCTempReg
		: regQuotient);

	/* MSUB <Xd>, <Xn>, <Xm>, <Xa>, Xd := Xa - (Xn * Xm) */
	/* MSUB regRemainder, regQuotient, regDivisor, regQuotient */
	instr = genoperandoperandoperand(DivRRR, regDividend, regDivisor, safeRegQuotient);
	genoperandoperandoperand(MSubRRR, (((sqInt)((usqInt)(safeRegQuotient) << 5))) + regDivisor, regDividend, regRemainder);
	if (safeRegQuotient != regQuotient) {
		genoperandoperand(MoveRR, safeRegQuotient, regQuotient);
	}
	return instr;
}


/*	Use the DC instruction to implement ceFlushDCache(void *start, void *end);
	see flushDCacheFrom:to:.
	If there is a dual mapped zone then clean data via DC_CVAU as address +
	codeToDataDelta, then invalidate data at address via CIVAC. */
/*	D4.4.7		About cache maintenance in AArch64 state													D4-2478
	
	Terminology for Clean, Invalidate, and Clean and Invalidate
	instructions									D4-2479 ...
	-	For instructions operating by VA, the following conceptual points are
	defined:						D4-2480 Point of Unification (PoU)							
	The PoU for a PE is the point by which the instruction and data caches and
	the translation table walks of that
	PE are guaranteed to see the same copy of a memory location. In many
	cases, the Point of Unification is the
	point in a uniprocessor memory system by which the instruction and data
	caches and the translation table
	walks have merged.
	
	The PoU for an Inner Shareable shareability domain is the point by which
	the instruction and data caches
	and the translation table walks of all the PEs in that Inner Shareable
	shareability domain are guaranteed to
	see the same copy of a memory location. Defining this point permits
	self-modifying software to ensure future
	instruction fetches are associated with the modified version of the
	software by using the standard correctness
	policy of:
	1. Clean data cache entry by address.
	2. Invalidate instruction cache entry by address.
	
	Example code for cache maintenance instructions D4-2490 - D4-2491 */

	/* CogARMv8Compiler>>#generateDCacheFlush */
static void NoDbgRegParms
generateDCacheFlush(AbstractInstruction * self_in_generateDCacheFlush)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *loop;
    sqInt mask;
    sqInt quickConstant;
    sqInt quickConstant1;


#  if !(__APPLE__ && __MACH__)
	assert((getCodeToDataDelta()) != 0);

	/* Since this is used from C code we must use only caller-saved registers.
	   C arg registers 2 & 3 are such a convenient pair of caller-saved registers. */
	mask = (1ULL << (highBit(limitAddress))) - (dataCacheLineLength(self_in_generateDCacheFlush));
	gAndCqRR(mask, CArg0Reg, CArg2Reg);
	gAddCqRR(getCodeToDataDelta(), CArg2Reg, CArg3Reg);

	/* see concretizeDataCacheControl */
	loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genoperandoperand(DC, CArg3Reg, DC_CVAU);
	genoperandoperand(DC, CArg2Reg, DC_CIVAC);
	/* begin AddCq:R: */
	quickConstant = dataCacheLineLength(self_in_generateDCacheFlush);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, CArg2Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin AddCq:R: */
	quickConstant1 = dataCacheLineLength(self_in_generateDCacheFlush);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AddCqR, quickConstant1, CArg3Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	assert(!((CArg1Reg == SPReg)));
	genoperandoperand(CmpRR, CArg1Reg, CArg2Reg);
	genConditionalBranchoperand(JumpBelow, ((sqInt)loop));
	genoperand(RetN, 0);
#  endif // !(__APPLE__ && __MACH__)
}


/*	Use DC VAUC, DSB, IC IVAU, and ISB instructions to implement
	ceFlushICache(void *start, void *end); see flushICacheFrom:to:.
	One might think that if there is a dual zone then data at address +
	codeToDataDelta must be cleaned,
	but this isn't the case. All we need to do is clean data at address via DC
	VAUC and instructions via IC IVAU. */
/*	B2.2.5		Concurrent modification and execution of
	instructions											B2-112 
	...to avoid UNPREDICTABLE or CONSTRAINED UNPREDICTABLE behavior,
	instruction modifications must be explicitly synchronized before they are
	executed. The required synchronization is as follows:
	
	1.	No PE must be executing an instruction when another PE is modifying
	that instruction.
	
	2.	To ensure that the modified instructions are observable, a PE that is
	writing the instructions must issue the following sequence of instructions
	and operations:
	
	; Coherency example for data and instruction accesses within the same
	Inner Shareable domain.
	; enter this code with <Wt> containing a new 32-bit instruction, to be
	held in Cacheable space at a location pointed to by Xn.
	
	STR Wt, [Xn]
	DC CVAU, Xn		; Clean data cache by VA to point of unification (PoU)
	DSB ISH			; Ensure visibility of the data cleaned from cache
	IC IVAU, Xn			; Invalidate instruction cache by VA to PoU
	DSB ISH
	
	Note
	-	The DC CVAU operation is not required if the area of memory is either
	Non-cacheable or Write-Through Cacheable.
	-	If the contents of physical memory differ between the mappings, changing
	the mapping of VAs to PAs can cause
	the instructions to be concurrently modified by one PE and executed by
	another PE. If the modifications affect
	instructions other than those listed as being acceptable for modification,
	synchronization must be used to avoid
	UNPREDICTABLE or CONSTRAINED UNPREDICTABLE behavior.
	
	3.	In a multiprocessor system, the IC IVAU is broadcast to all PEs within
	the Inner Shareable domain of the PE running this sequence.
	However, when the modified instructions are observable, each PE that is
	executing the modified instructions must issue the following
	instruction to ensure execution of the modified instructions:
	
	ISB					; Synchronize fetched instruction stream */
/*	D4.4.7		About cache maintenance in AArch64 state													D4-2478
	
	Terminology for Clean, Invalidate, and Clean and Invalidate
	instructions									D4-2479 ...
	-	For instructions operating by VA, the following conceptual points are
	defined:						D4-2480 Point of Unification (PoU)							
	The PoU for a PE is the point by which the instruction and data caches and
	the translation table walks of that
	PE are guaranteed to see the same copy of a memory location. In many
	cases, the Point of Unification is the
	point in a uniprocessor memory system by which the instruction and data
	caches and the translation table
	walks have merged.
	
	The PoU for an Inner Shareable shareability domain is the point by which
	the instruction and data caches
	and the translation table walks of all the PEs in that Inner Shareable
	shareability domain are guaranteed to
	see the same copy of a memory location. Defining this point permits
	self-modifying software to ensure future
	instruction fetches are associated with the modified version of the
	software by using the standard correctness
	policy of:
	1. Clean data cache entry by address.
	2. Invalidate instruction cache entry by address.
	
	Example code for cache maintenance instructions D4-2490 - D4-2491 */

	/* CogARMv8Compiler>>#generateICacheFlush */
static void NoDbgRegParms
generateICacheFlush(AbstractInstruction * self_in_generateICacheFlush)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *loop;
    sqInt mask;
    sqInt quickConstant;
    sqInt quickConstant1;


#  if !(__APPLE__ && __MACH__)

	/* See concretizeCacheControlOp1:CRm:Op2: &
	   http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.100403_0200_00_en/lau1443435580346.html */
	if (dataCacheFlushRequired(self_in_generateICacheFlush)) {

		/* CTR_EL0.IDC is zero; must clean data cache to point of unification. */
		/* Since this is used from C code we must use only caller-saved registers.
		   C arg registers 2 & 3 are as such a convenient pair of caller-saved registers. */
		/* Mask is large enough to encompass the method zone and has the correct minimum alignment. */
		mask = (1ULL << (highBit(limitAddress))) - (dataCacheLineLength(self_in_generateICacheFlush));
		gAndCqRR(mask, CArg0Reg, CArg2Reg);

		/* see concretizeDataCacheControl */
		loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genoperandoperand(DC, CArg2Reg, DC_CVAU);
		/* begin AddCq:R: */
		quickConstant = dataCacheLineLength(self_in_generateICacheFlush);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, quickConstant, CArg2Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		assert(!((CArg1Reg == SPReg)));
		genoperandoperand(CmpRR, CArg1Reg, CArg2Reg);
		genConditionalBranchoperand(JumpBelow, ((sqInt)loop));
	}
	genoperandoperand(DSB, DSB_ISH, DSB_ALL);
	if (instructionCacheFlushRequired(self_in_generateICacheFlush)) {

		/* CTR_EL0.DIC is zero; must clean instruction cache to point of unification. */
		/* Mask is large enough to encompass the method zone and has the correct minimum alignment. */
		mask = (1ULL << (highBit(limitAddress))) - (instructionCacheLineLength(self_in_generateICacheFlush));
		gAndCqRR(mask, CArg0Reg, CArg2Reg);

		/* see concretizeDataCacheControl */
		loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genoperandoperand(IC, CArg2Reg, IC_IVAU);
		/* begin AddCq:R: */
		quickConstant1 = instructionCacheLineLength(self_in_generateICacheFlush);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(AddCqR, quickConstant1, CArg2Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
		}
		assert(!((CArg1Reg == SPReg)));
		genoperandoperand(CmpRR, CArg1Reg, CArg2Reg);
		genConditionalBranchoperand(JumpBelow, ((sqInt)loop));
		genoperandoperand(DSB, DSB_ISH, DSB_ALL);
	}
	gen(ISB);
	genoperand(RetN, 0);
#  endif // !(__APPLE__ && __MACH__)
}


/*	Load the frame and stack pointer registers with those of the C stack,
	effecting a switch to the C stack. Used when machine code calls into
	the CoInterpreter run-time (e.g. to invoke interpreter primitives).
	N.B. CoInterpreter stack layout dictates that the stack pointer should be
	loaded first.
	The stack zone is allocated on the C stack before the interpreter runs and
	hence before CStackPointer and CFramePointer are captured. So when running
	in machine
	code the native stack pointer and frame pointer appear to be on a colder
	part of the
	stack to CStackPointer and CFramePointer. When CStackPointerhas been set
	and the frame pointer is still in machine code the current frame looks
	like it has lots of
	stack. If the frame pointer was set to CFramePointer before hand then it
	would be beyond the stack pointer for that one instruction. */
/*	Load the frame and stack pointer registers with those of the C stack,
	effecting a switch to the C stack. Used when machine code calls into
	the CoInterpreter run-time (e.g. to invoke interpreter primitives).
	Override to try and use MoveAwRR/ldp */

	/* CogARMv8Compiler>>#genLoadCStackPointers */
static sqInt NoDbgRegParms
genLoadCStackPointers(AbstractInstruction * self_in_genLoadCStackPointers)
{
    sqInt operandOne;
    sqInt operandOne1;

	if (((cStackPointerAddress()) + 8) == (cFramePointerAddress())) {
		genoperandoperandoperand(MoveAwRR, cStackPointerAddress(), SP, FP);
		return 0;
	}
	/* begin gen:literal:operand: */
	operandOne = cStackPointerAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, NativeSPReg));
	/* begin gen:literal:operand: */
	operandOne1 = cFramePointerAddress();
	checkLiteralforInstruction(operandOne1, genoperandoperand(MoveAwR, operandOne1, FPReg));
	return 0;
}

	/* CogARMv8Compiler>>#genLoadNativeSPRegWithAlignedSPReg */
static void NoDbgRegParms
genLoadNativeSPRegWithAlignedSPReg(AbstractInstruction * self_in_genLoadNativeSPRegWithAlignedSPReg)
{
	gAndCqRR((-1 - (((BytesPerWord * 2) - 1))), SPReg, SP);
}


/*	Switch back to the Smalltalk stack. Assign SPReg first
	because typically it is used immediately afterwards. */
/*	Switch back to the Smalltalk stack. Assign SPReg first
	because typically it is used immediately afterwards.
	Override to try and use MoveAwRR/ldp */

	/* CogARMv8Compiler>>#genLoadStackPointers */
static sqInt NoDbgRegParms
genLoadStackPointers(AbstractInstruction * self_in_genLoadStackPointers)
{
    sqInt operandOne;
    sqInt operandOne1;

	if (((stackPointerAddress()) + 8) == (framePointerAddress())) {
		genoperandoperandoperand(MoveAwRR, stackPointerAddress(), SPReg, FPReg);
		return 0;
	}
	if (((framePointerAddress()) + 8) == (stackPointerAddress())) {
		genoperandoperandoperand(MoveAwRR, framePointerAddress(), FPReg, SPReg);
		return 0;
	}
	/* begin gen:literal:operand: */
	operandOne = stackPointerAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, SPReg));
	/* begin gen:literal:operand: */
	operandOne1 = framePointerAddress();
	checkLiteralforInstruction(operandOne1, genoperandoperand(MoveAwR, operandOne1, FPReg));
	return 0;
}


/*	Generate the code to pass up to four arguments in a C run-time call. Hack:
	each argument is
	either a negative number, which encodes a constant, or a non-negative
	number, that of a register.
	The encoding for constants is defined by trampolineArgConstant: &
	trampolineArgValue:. Pass a constant as the result of
	trampolineArgConstant:. 
	Run-time calls have no more than four arguments, so chosen so that on ARM,
	where in its C ABI the
	first four integer arguments are passed in registers, all arguments can be
	passed in registers. We
	defer to the back end to generate this code not so much that the back end
	knows whether it uses
	the stack or registers to pass arguments (it does, but...). In fact we
	defer for an extremely evil reason.
	Doing so allows the x64 (where up to 6 args are passed) to assign the
	register arguments in an order
	that allows some of the argument registers to be used for specific
	abstract registers, specifically
	ReceiverResultReg and ClassReg. This is evil, evil, evil, but also it's
	really nice to keep using the old
	register assignments the original author has grown accustomed to. */

	/* CogARMv8Compiler>>#genMarshallNArgs:arg:arg:arg:arg: */
static AbstractInstruction * NoDbgRegParms
genMarshallNArgsargargargarg(AbstractInstruction * self_in_genMarshallNArgsargargargarg, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;

	if (numArgs == 0) {
		return self_in_genMarshallNArgsargargargarg;
	}
	if (numArgs > 1) {
		if ((!(regOrConst1 < NoReg))
		 && (regOrConst1 == CArg0Reg)) {
			genoperandoperand(MoveRR, regOrConst1, Extra0Reg);
			return genMarshallNArgsargargargarg(self_in_genMarshallNArgsargargargarg, numArgs, regOrConst0, Extra0Reg, regOrConst2, regOrConst3);
		}
		if (numArgs > 2) {
			if ((!(regOrConst2 < NoReg))
			 && ((regOrConst2 == CArg0Reg)
			 || (regOrConst2 == CArg1Reg))) {
				genoperandoperand(MoveRR, regOrConst2, Extra1Reg);
				return genMarshallNArgsargargargarg(self_in_genMarshallNArgsargargargarg, numArgs, regOrConst0, regOrConst1, Extra1Reg, regOrConst3);
			}
			if (numArgs > 3) {
				if ((!(regOrConst3 < NoReg))
				 && ((regOrConst3 == CArg0Reg)
				 || ((regOrConst3 == CArg1Reg)
				 || (regOrConst3 == CArg2Reg)))) {
					genoperandoperand(MoveRR, regOrConst3, Extra2Reg);
					return genMarshallNArgsargargargarg(self_in_genMarshallNArgsargargargarg, numArgs, regOrConst0, regOrConst1, regOrConst2, Extra2Reg);
				}
			}
		}
	}
	if (regOrConst0 < NoReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, -2 - regOrConst0, CArg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(-2 - regOrConst0, BytesPerOop));
		}
	}
	else {
		genoperandoperand(MoveRR, regOrConst0, CArg0Reg);
	}
	if (numArgs == 1) {
		return self_in_genMarshallNArgsargargargarg;
	}
	if (regOrConst1 < NoReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, -2 - regOrConst1, CArg1Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(-2 - regOrConst1, BytesPerOop));
		}
	}
	else {
		genoperandoperand(MoveRR, regOrConst1, CArg1Reg);
	}
	if (numArgs == 2) {
		return self_in_genMarshallNArgsargargargarg;
	}
	if (regOrConst2 < NoReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(MoveCqR, -2 - regOrConst2, CArg2Reg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(-2 - regOrConst2, BytesPerOop));
		}
	}
	else {
		genoperandoperand(MoveRR, regOrConst2, CArg2Reg);
	}
	if (numArgs == 3) {
		return self_in_genMarshallNArgsargargargarg;
	}
	if (regOrConst3 < NoReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(MoveCqR, -2 - regOrConst3, CArg3Reg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(-2 - regOrConst3, BytesPerOop));
		}
	}
	else {
		genoperandoperand(MoveRR, regOrConst3, CArg3Reg);
	}
	return 0;
}

	/* CogARMv8Compiler>>#genMulOverflowR:R: */
static AbstractInstruction * NoDbgRegParms
genMulOverflowRR(AbstractInstruction * self_in_genMulOverflowRR, sqInt regSource, sqInt regDest)
{
	return genoperandoperandoperand(MulOverflowRRR, regSource, regDest, regDest);
}

	/* CogARMv8Compiler>>#genMulR:R: */
static AbstractInstruction * NoDbgRegParms
genMulRR(AbstractInstruction * self_in_genMulRR, sqInt regSource, sqInt regDest)
{
	return genoperandoperandoperand(MulRRR, regSource, regDest, regDest);
}


/*	Ensure that the register args are pushed before the outer and
	inner retpcs at an entry miss for arity <= self numRegArgs. The
	outer retpc is that of a call at a send site. The inner is the call
	from a method or PIC abort/miss to the trampoline. */
/*	Putting the receiver and args above the return address means the
	CoInterpreter has a single machine-code frame format which saves
	us a lot of work. */
/*	Iff there are register args convert
	sp		->	outerRetpc			(send site retpc)
	linkReg = innerRetpc			(PIC abort/miss retpc)
	to
	base	->	receiver
	(arg0)
	(arg1)
	sp		->	outerRetpc			(send site retpc)
	sp		->	linkReg/innerRetpc	(PIC abort/miss retpc) */

	/* CogARMv8Compiler>>#genPushRegisterArgsForAbortMissNumArgs: */
static void NoDbgRegParms
genPushRegisterArgsForAbortMissNumArgs(AbstractInstruction * self_in_genPushRegisterArgsForAbortMissNumArgs, sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;

	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMwrR, 0, SPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveRMwr, ReceiverResultReg, 0, SPReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		if (numArgs > 0) {
			genoperand(PushR, Arg0Reg);
			if (numArgs > 1) {
				genoperand(PushR, Arg1Reg);
			}
		}
		genoperand(PushR, TempReg);
	}
	genoperand(PushR, LinkReg);
}


/*	Ensure that the register args are pushed before the retpc for arity <=
	self numRegArgs.
 */
/*	This is easy on a RISC like ARM because the return address is in the link
	register. Putting
	the receiver and args above the return address means the CoInterpreter has
	a single
	machine-code frame format which saves us a lot of work
	NOTA BENE: we do NOT push the return address here, which means it must be
	dealt with later. */

	/* CogARMv8Compiler>>#genPushRegisterArgsForNumArgs:scratchReg: */
static void NoDbgRegParms
genPushRegisterArgsForNumArgsscratchReg(AbstractInstruction * self_in_genPushRegisterArgsForNumArgsscratchReg, sqInt numArgs, sqInt ignored)
{
	if (numArgs <= (numRegArgs())) {
		assert((numRegArgs()) <= 2);
		genoperand(PushR, ReceiverResultReg);
		if (numArgs > 0) {
			genoperand(PushR, Arg0Reg);
			if (numArgs > 1) {
				genoperand(PushR, Arg1Reg);
			}
		}
	}
}


/*	This is a no-op on ARM64 since the ABI passes up to 6 args in registers
	and trampolines currently observe that limit, using only 4.
 */

	/* CogARMv8Compiler>>#genRemoveNArgsFromStack: */
static sqInt NoDbgRegParms
genRemoveNArgsFromStack(AbstractInstruction * self_in_genRemoveNArgsFromStack, sqInt n)
{
	assert(n <= 6);
	return 0;
}


/*	Restore the registers in regMask as saved by genSaveRegs:.
	See
	http://infocenter.arm.com/help/topic/com.arm.doc.den0028b/ARM_DEN0028B_SMC_Calling_Convention.pdf
	N.B. Alignment is handled by
	genAlignCStackSavingRegisters:numArgs:wordAlignment:. 
 */

	/* CogARMv8Compiler>>#genRestoreRegs: */
static sqInt NoDbgRegParms
genRestoreRegs(AbstractInstruction * self_in_genRestoreRegs, sqInt regMask)
{
    sqInt nRegs;
    sqInt pair;
    sqInt reg;

	if (regMask == 0) {
		return 0;
	}
	assert(!((registerisInMask(RISCTempReg, regMask))));
	nRegs = 0;
	for (reg = R1; reg <= R17; reg += 1) {
		if (((regMask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0)) {
			nRegs += 1;
		}
	}
	pair = ((!(nRegs & 1))
		? NoReg
		: RISCTempReg);
	for (reg = R1; reg <= R17; reg += 1) {
		if (((regMask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0)) {
			if (pair == NoReg) {
				pair = reg;
			}
			else {
				genoperandoperand(NativePopRR, reg, pair);
				pair = NoReg;
			}
		}
	}
	assert(pair == NoReg);
	return 0;
}


/*	Save the registers in regMask for a call into the C run-time from a
	trampoline. See
	http://infocenter.arm.com/help/topic/com.arm.doc.den0028b/ARM_DEN0028B_SMC_Calling_Convention.pdf
	N.B. Alignment is handled by
	genAlignCStackSavingRegisters:numArgs:wordAlignment:. 
 */

	/* CogARMv8Compiler>>#genSaveRegs: */
static sqInt NoDbgRegParms
genSaveRegs(AbstractInstruction * self_in_genSaveRegs, sqInt regMask)
{
    sqInt pair;
    sqInt reg;

	if (regMask == 0) {
		return 0;
	}
	assert(!((registerisInMask(RISCTempReg, regMask))));
	pair = NoReg;
	for (reg = R17; reg >= R0; reg += -1) {
		if (((regMask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0)) {
			if (pair == NoReg) {
				pair = reg;
			}
			else {
				genoperandoperand(NativePushRR, pair, reg);
				pair = NoReg;
			}
		}
	}
	if (pair != NoReg) {
		genoperandoperand(NativePushRR, pair, RISCTempReg);
	}
	return 0;
}


/*	Save the frame and stack pointer registers to the framePointer
	and stackPointer variables. Used to save the machine code frame
	for use by the run-time when calling into the CoInterpreter run-time. */
/*	Save the frame and stack pointer registers to the framePointer
	and stackPointer variables. Used to save the machine code frame
	for use by the run-time when calling into the CoInterpreter run-time.
	Override to try and use MoveRRAw/stp */

	/* CogARMv8Compiler>>#genSaveStackPointers */
static sqInt NoDbgRegParms
genSaveStackPointers(AbstractInstruction * self_in_genSaveStackPointers)
{
    sqInt operandTwo;
    sqInt operandTwo1;

	if (((stackPointerAddress()) + 8) == (framePointerAddress())) {
		genoperandoperandoperand(MoveRRAw, SPReg, FPReg, stackPointerAddress());
		return 0;
	}
	/* begin gen:operand:literal: */
	operandTwo = framePointerAddress();
	checkLiteralforInstruction(operandTwo, genoperandoperand(MoveRAw, FPReg, operandTwo));
	/* begin gen:operand:literal: */
	operandTwo1 = stackPointerAddress();
	checkLiteralforInstruction(operandTwo1, genoperandoperand(MoveRAw, SPReg, operandTwo1));
	return 0;
}

	/* CogARMv8Compiler>>#genSubstituteReturnAddress: */
static AbstractInstruction * NoDbgRegParms
genSubstituteReturnAddress(AbstractInstruction * self_in_genSubstituteReturnAddress, sqInt retpc)
{
	/* begin gen:literal:operand: */
	return checkLiteralforInstruction(retpc, genoperandoperand(MoveCwR, retpc, LR));
}


/*	Answer if the processor has a dedicated callee-saved register to point to
	the base of commonly-accessed variables. On ARMv8 we use R27 for this. */

	/* CogARMv8Compiler>>#hasVarBaseRegister */
static sqInt NoDbgRegParms
hasVarBaseRegister(AbstractInstruction * self_in_hasVarBaseRegister)
{
	return 1;
}

	/* CogARMv8Compiler>>#inlineCacheTagAt: */
static usqInt NoDbgRegParms
inlineCacheTagAt(AbstractInstruction * self_in_inlineCacheTagAt, sqInt callSiteReturnAddress)
{
    sqInt pc;

	/* begin instructionAt: */
	pc = pcRelativeAddressAt(self_in_inlineCacheTagAt, ((usqInt)(callSiteReturnAddress - 8)));
	return ((unsigned int) (long32At(pc)));
}


/*	Answer the instruction address immediately preceding mcpc. */

	/* CogARMv8Compiler>>#instructionAddressBefore: */
static sqInt NoDbgRegParms
instructionAddressBefore(AbstractInstruction * self_in_instructionAddressBefore, sqInt mcpc)
{
	return mcpc - 4;
}

	/* CogARMv8Compiler>>#instructionAt: */
static unsigned int NoDbgRegParms
instructionAt(AbstractInstruction * self_in_instructionAt, sqInt pc)
{
	return long32At(pc);
}


/*	Answer the instruction immediately preceding followingAddress. */

	/* CogARMv8Compiler>>#instructionBeforeAddress: */
static usqInt NoDbgRegParms
instructionBeforeAddress(AbstractInstruction * self_in_instructionBeforeAddress, sqInt followingAddress)
{
    sqInt pc;

	/* begin instructionAt: */
	pc = followingAddress - 4;
	return ((unsigned int) (long32At(pc)));
}


/*	C6.2.10 ADR	C6-773 */

	/* CogARMv8Compiler>>#instructionIsADR: */
static sqInt NoDbgRegParms
instructionIsADR(AbstractInstruction * self_in_instructionIsADR, sqInt word)
{
	return ((((usqInt)(word)) >> 24) & 159) == 16;
}


/*	C6.2.26 B		C6-799
	C6.2.33	BL		C6-812 */
/*	BL is 2r100101, B is 2r000101 */

	/* CogARMv8Compiler>>#instructionIsImm26BorBL: */
static sqInt NoDbgRegParms
instructionIsImm26BorBL(AbstractInstruction * self_in_instructionIsImm26BorBL, usqInt word)
{
	return (((word) >> 26) & 0x1F) == 5;
}


/*	C4.1	A64 instruction set encoding on page C4-252 */

	/* CogARMv8Compiler>>#instructionIsLoadStore: */
static sqInt NoDbgRegParms
instructionIsLoadStore(AbstractInstruction * self_in_instructionIsLoadStore, sqInt instr)
{
	return ((((usqInt)(instr)) >> 25) & 5) == 4;
}


/*	C6.2.131	LDR (literal)		C6-979
	C6.2.143	LDRSW (literal)	C6-1008 */

	/* CogARMv8Compiler>>#instructionIsPCRelativeLoad: */
static sqInt NoDbgRegParms
instructionIsPCRelativeLoad(AbstractInstruction * self_in_instructionIsPCRelativeLoad, sqInt instr)
{
	return ((((usqInt)(instr)) >> 24) & 0x3F) == 24;
}


/*	Answer the instruction size at pc.Simple on ARM ;-) */

	/* CogARMv8Compiler>>#instructionSizeAt: */
static sqInt NoDbgRegParms
instructionSizeAt(AbstractInstruction * self_in_instructionSizeAt, sqInt pc)
{
	return 4;
}


/*	Several of the opcodes are inverses. Answer the inverse for an opcode if
	it has one.
	See Table A3-2 in sec A3.4 Data-processing instructions of the AARM. */

	/* CogARMv8Compiler>>#inverseForArithOp: */
static sqInt NoDbgRegParms
inverseForArithOp(AbstractInstruction * self_in_inverseForArithOp, sqInt arithOp)
{
	switch (arithOp) {
	case ArithmeticAdd:
		return ArithmeticSub;

	case ArithmeticAddS:
		return ArithmeticSubS;

	case ArithmeticSub:
		return ArithmeticAdd;

	case ArithmeticSubS:
		return ArithmeticAddS;

	default:
		error("Case not found and no otherwise clause");
		return -1;
	}
}


/*	Support for addressing variables off the dedicated VarBaseReg */

	/* CogARMv8Compiler>>#isAddressRelativeToVarBase: */
static sqInt NoDbgRegParms
isAddressRelativeToVarBase(AbstractInstruction * self_in_isAddressRelativeToVarBase, usqInt varAddress)
{
	return (varAddress != null)
	 && (((varAddress >= ((varBaseAddress()) - (0x100))) && (varAddress <= (((varBaseAddress()) + (0x1000)) - 1))));
}


/*	Assuming mcpc is a send return pc answer if the instruction before it is a
	call (not a CallFull).
 */
/*	There are two types of calls: BL & BLR; BLR is used for CallFull */

	/* CogARMv8Compiler>>#isCallPrecedingReturnPC: */
static sqInt NoDbgRegParms
isCallPrecedingReturnPC(AbstractInstruction * self_in_isCallPrecedingReturnPC, sqInt mcpc)
{
    usqInt instruction;
    sqInt pc;

	/* begin instructionBeforeAddress: */
	pc = mcpc - 4;
	instruction = ((unsigned int) (long32At(pc)));
	return (((instruction) >> 26) == 37)
	 || ((instruction | (0x3E0)) == 3594453984U);
}

	/* CogARMv8Compiler>>#isFullJumpAtPC: */
static sqInt NoDbgRegParms
isFullJumpAtPC(AbstractInstruction * self_in_isFullJumpAtPC, sqInt mcpc)
{
    usqInt word;

	/* begin instructionIsBR: */
	word = long32At(mcpc);
	return (word | (0x3E0)) == 3592356832U;
}


/*	ARM64 load/store immediate offsets have an 11 bit unsigned amd a 9 bit
	signed form.
 */

	/* CogARMv8Compiler>>#isImm12orImm9offset: */
static sqInt NoDbgRegParms
isImm12orImm9offset(AbstractInstruction * self_in_isImm12orImm9offset, sqInt offset)
{
	return (offset >= -256)
	 && (offset < 0x1000);
}


/*	ARM64 calls span +/- 128 mb.
	C6.2.33 BL		C6-812 */

	/* CogARMv8Compiler>>#isInImmediateBranchAndLinkRange: */
static sqInt NoDbgRegParms
isInImmediateBranchAndLinkRange(AbstractInstruction * self_in_isInImmediateBranchAndLinkRange, sqIntptr_t offset)
{
	assert((!(offset & 3)));
	return (((((((sqLong) offset))) >> 27) >= -1) && ((((((sqLong) offset))) >> 27) <= 0));
}


/*	ARM64 calls and jumps span +/- 1 mb. */

	/* CogARMv8Compiler>>#isInImmediateBranchRange: */
static sqInt NoDbgRegParms
isInImmediateBranchRange(AbstractInstruction * self_in_isInImmediateBranchRange, usqIntptr_t offset)
{
	assert((!(offset & 3)));
	return (((((((sqLong) offset))) >> 18) >= -1) && ((((((sqLong) offset))) >> 18) <= 0));
}


/*	ARMv8 calls and jumps span +/- 128 mb, more than enough for intra-zone
	calls and jumps.
 */

	/* CogARMv8Compiler>>#isInImmediateJumpRange: */
static sqInt NoDbgRegParms
isInImmediateJumpRange(AbstractInstruction * self_in_isInImmediateJumpRange, usqIntptr_t operand)
{
	assert((!(operand & 3)));
	return (((((int) operand)) >= ((-0x7FFFFFFF-1))) && ((((int) operand)) <= (0x7FFFFFF)));
}


/*	ARM64 load/store immediates have an 11 bit unsigned amd a 9 bit signed
	form. 
 */

	/* CogARMv8Compiler>>#isInImmediateOffsetRange: */
static sqInt NoDbgRegParms
isInImmediateOffsetRange(AbstractInstruction * self_in_isInImmediateOffsetRange, sqInt offset)
{
	return (offset >= -256)
	 && (offset < 0x1000);
}

	/* CogARMv8Compiler>>#isJump */
static sqInt NoDbgRegParms
isJump(AbstractInstruction * self_in_isJump)
{
	return (((((self_in_isJump->opcode)) >= FirstJump) && (((self_in_isJump->opcode)) <= LastJump)))
	 || (((((self_in_isJump->opcode)) >= CBNZ) && (((self_in_isJump->opcode)) <= CBZ)));
}


/*	C4.1	A64 instruction set encoding on page C4-252
	C4.1.3 Branches, Exception Generating and System instructions */
/*	cogit processor disassembleInstructionAt: pc In: objectMemory memory */

	/* CogARMv8Compiler>>#isJumpAt: */
static sqInt NoDbgRegParms
isJumpAt(AbstractInstruction * self_in_isJumpAt, sqInt pc)
{
    sqInt op0_101_op1MSB;

	op0_101_op1MSB = (((usqInt)((long32At(pc)))) >> 25);
	return (op0_101_op1MSB == 42)
	 || ((op0_101_op1MSB == 107)
	 || (((op0_101_op1MSB & 0x7E) == 74)
	 || ((op0_101_op1MSB & 0x7E) == 10)));
}


/*	Answer if the receiver is a pc-dependent instruction. With out-of-line
	literals any instruction
	that refers to a literal depends on the address of the literal, so add
	them in addition to the jumps. */

	/* CogARMv8Compiler>>#isPCDependent */
static sqInt NoDbgRegParms
isPCDependent(AbstractInstruction * self_in_isPCDependent)
{
	return (isJump(self_in_isPCDependent))
	 || ((((self_in_isPCDependent->opcode)) == AlignmentNops)
	 || ((((self_in_isPCDependent->opcode)) != Literal)
	 && ((((self_in_isPCDependent->dependent)) != null)
	 && (((((self_in_isPCDependent->dependent))->opcode)) == Literal))));
}

	/* CogARMv8Compiler>>#isShiftedMask: */
static sqInt NoDbgRegParms
isShiftedMask(AbstractInstruction * self_in_isShiftedMask, sqInt anInteger)
{
    sqInt bits;

	return (anInteger != 0)
	 && (((bits = (anInteger - 1) | anInteger),
	(bits & (bits + 1)) == 0));
}

	/* CogARMv8Compiler>>#isUnsigned12BitMultipleOf8: */
static sqInt NoDbgRegParms
isUnsigned12BitMultipleOf8(AbstractInstruction * self_in_isUnsigned12BitMultipleOf8, sqInt anInteger)
{
	return ((anInteger % 8) == 0)
	 && ((((anInteger / 8) >= 0) && ((anInteger / 8) <= (0xFFF))));
}


/*	Branch/Call ranges. Jump[Cond] can be generated as short as possible.
	Call/Jump[Cond]Long must be generated
	in the same number of bytes irrespective of displacement since their
	targets may be updated, but they need only
	span 16Mb, the maximum size of the code zone. This allows e.g. ARM to use
	single-word call and jump instructions
	for most calls and jumps. CallFull/JumpFull must also be generated in the
	same number of bytes irrespective of
	displacement for the same reason, but they must be able to span the full
	(32-bit or 64-bit) address space because
	they are used to call code in the C runtime, which may be distant from the
	code zone
 */

	/* CogARMv8Compiler>>#jumpLongByteSize */
static sqInt NoDbgRegParms
jumpLongByteSize(AbstractInstruction * self_in_jumpLongByteSize)
{
	return 4;
}

	/* CogARMv8Compiler>>#jumpLongConditionalByteSize */
static sqInt NoDbgRegParms
jumpLongConditionalByteSize(AbstractInstruction * self_in_jumpLongConditionalByteSize)
{
	return 8;
}


/*	C6.2.26 B		C6-799 */

	/* CogARMv8Compiler>>#jumpLongTargetBeforeFollowingAddress: */
static sqInt NoDbgRegParms
jumpLongTargetBeforeFollowingAddress(AbstractInstruction * self_in_jumpLongTargetBeforeFollowingAddress, sqInt mcpc)
{
	return callTargetFromReturnAddress(self_in_jumpLongTargetBeforeFollowingAddress, mcpc);
}


/*	cogit processor disassembleInstructionAt: pc In: objectMemory memory */

	/* CogARMv8Compiler>>#jumpTargetPCAt: */
static usqInt NoDbgRegParms
jumpTargetPCAt(AbstractInstruction * self_in_jumpTargetPCAt, sqInt pc)
{
    sqInt operand;
    unsigned int word;

	/* begin instructionAt: */
	word = long32At(pc);
	assert((((usqInt)(word)) >> 26) == 21);
	operand = (((usqInt)(word)) >> 5) & 0x7FFFF;
	if (((operand & 0x40000) != 0)) {
		operand -= 0x80000;
	}
	return (operand * 4) + pc;
}


/*	Answer the 32-bit constant loaded by the instruction sequence just before
	this address:
	CmpC32R MoveC32R */

	/* CogARMv8Compiler>>#literal32BeforeFollowingAddress: */
static usqInt NoDbgRegParms
literal32BeforeFollowingAddress(AbstractInstruction * self_in_literal32BeforeFollowingAddress, sqInt followingAddress)
{
    sqInt pc;

	/* begin instructionAt: */
	pc = pcRelativeAddressAt(self_in_literal32BeforeFollowingAddress, instructionAddressBefore(self_in_literal32BeforeFollowingAddress, (instructionIsPCRelativeLoad(self_in_literal32BeforeFollowingAddress, instructionBeforeAddress(self_in_literal32BeforeFollowingAddress, followingAddress))
		? followingAddress
		: followingAddress - 4)));
	return ((unsigned int) (long32At(pc)));
}


/*	Answer the literal referenced by the instruction immediately preceding
	followingAddress. ArithCwR MoveCwR PushCw */

	/* CogARMv8Compiler>>#literalBeforeFollowingAddress: */
static sqInt NoDbgRegParms
literalBeforeFollowingAddress(AbstractInstruction * self_in_literalBeforeFollowingAddress, sqInt followingAddress)
{
	return longAt(pcRelativeAddressAt(self_in_literalBeforeFollowingAddress, instructionAddressBefore(self_in_literalBeforeFollowingAddress, (instructionIsPCRelativeLoad(self_in_literalBeforeFollowingAddress, instructionBeforeAddress(self_in_literalBeforeFollowingAddress, followingAddress))
		? followingAddress
		: followingAddress - 4))));
}


/*	Answer the size of a literal load instruction (which does not include the
	size of the literal).
	With out-of-line literals this is always a single LDR instruction that
	refers to the literal.
 */

	/* CogARMv8Compiler>>#literalLoadInstructionBytes */
static sqInt NoDbgRegParms
literalLoadInstructionBytes(AbstractInstruction * self_in_literalLoadInstructionBytes)
{
	return 4;
}


/*	Answer the byte size of a MoveCwR opcode's corresponding machine code.
	On ARMv8 this is a single instruction pc-relative register load */

	/* CogARMv8Compiler>>#loadLiteralByteSize */
static sqInt NoDbgRegParms
loadLiteralByteSize(AbstractInstruction * self_in_loadLiteralByteSize)
{
	return 4;
}


/*	Answer the byte size of a MoveCwR opcode's corresponding machine code
	when the argument is a PIC. This is for the self-reference at the end of a
	closed PIC. On ARM this is a single instruction pc-relative register load. */

	/* CogARMv8Compiler>>#loadPICLiteralByteSize */
static sqInt NoDbgRegParms
loadPICLiteralByteSize(AbstractInstruction * self_in_loadPICLiteralByteSize)
{
	return 4;
}


/*	Answer the maximum number of bytes of machine code generated for any
	abstract instruction.
	Likely to be quite different for AARCH64
 */

	/* CogARMv8Compiler>>#machineCodeBytes */
static sqInt NoDbgRegParms
machineCodeBytes(AbstractInstruction * self_in_machineCodeBytes)
{
	return 12;
}


/*	Answer the maximum number of words of machine code generated for any
	abstract instruction.
	Likely to be quite different for AARCH64
 */

	/* CogARMv8Compiler>>#machineCodeWords */
static sqInt NoDbgRegParms
machineCodeWords(AbstractInstruction * self_in_machineCodeWords)
{
	return (machineCodeBytes(self_in_machineCodeWords)) / 4;
}

	/* CogARMv8Compiler>>#movern:rd: */
static sqInt NoDbgRegParms
movernrd(AbstractInstruction * self_in_movernrd, sqInt srcReg, sqInt destReg)
{
	return addrnrdimmshiftBy12(self_in_movernrd, srcReg, destReg, 0, 0);
}


/*	Morph an Overflow/NoOverflow check after a MulOverflow into an efficient
	test for overflow, hence avoiding having to set and test the V flag.
	MulOverflowRRR generates
	mul		Rd,Rm,Rn
	smulh RISCTempReg,Rm,Rn
	cmp	RISCTempReg, #0x1
	so it must be followed by an unsigned conditional jump, Above for
	Overflow, BelowOrEqual for NoOverflow. */

	/* CogARMv8Compiler>>#noteFollowingConditionalBranch: */
static AbstractInstruction * NoDbgRegParms
noteFollowingConditionalBranch(AbstractInstruction * self_in_noteFollowingConditionalBranch, AbstractInstruction *branch)
{
	if ((((self_in_noteFollowingConditionalBranch->opcode)) == MulOverflowRRR)
	 && ((((branch->opcode)) == JumpOverflow)
	 || (((branch->opcode)) == JumpNoOverflow))) {
		(branch->opcode = (((branch->opcode)) == JumpOverflow
			? JumpMulOverflow
			: JumpNoMulOverflow));
	}
	return branch;
}

	/* CogARMv8Compiler>>#numDCacheFlushOpcodes */
static sqInt NoDbgRegParms
numDCacheFlushOpcodes(AbstractInstruction * self_in_numDCacheFlushOpcodes)
{

#  if DUAL_MAPPED_CODE_ZONE
	return ((getCodeToDataDelta()) != 0
		? 15
		: 0);
#  else
	return 0;
#  endif
}


/*	Apple's code is simple and works; we can't better it... */

	/* CogARMv8Compiler>>#numICacheFlushOpcodes */
static sqInt NoDbgRegParms
numICacheFlushOpcodes(AbstractInstruction * self_in_numICacheFlushOpcodes)
{

#  if __APPLE__
	return 0;
#  else
	return 24;
#  endif
}


/*	See e.g.
	http://infocenter.arm.com/help/topic/com.arm.doc.den0028b/ARM_DEN0028B_SMC_Calling_Convention.pdf
	Table 3-1 Register Usage in AArch64 SMC32, HVC32, SMC64, and HVC64 calls */

	/* CogARMv8Compiler>>#numIntRegArgs */
static sqInt NoDbgRegParms
numIntRegArgs(AbstractInstruction * self_in_numIntRegArgs)
{
	return 6;
}


/*	The maximum offset in a LDR (literal) is -2^18 to 2^18-1.
	And this is multiplied by 4 to produce the effective address.
	This is a huge range; we have no grounds for concern. */

	/* CogARMv8Compiler>>#outOfLineLiteralOpcodeLimit */
static sqInt NoDbgRegParms
outOfLineLiteralOpcodeLimit(AbstractInstruction * self_in_outOfLineLiteralOpcodeLimit)
{
	return 0x3FFFF;
}

	/* CogARMv8Compiler>>#padIfPossibleWithStopsFrom:to: */
static void NoDbgRegParms
padIfPossibleWithStopsFromto(AbstractInstruction * self_in_padIfPossibleWithStopsFromto, sqInt startAddr, sqInt endAddr)
{
    sqInt nullBytes;
    sqInt p;

	nullBytes = ((endAddr - startAddr) + 1) % 4;
	stopsFromto(self_in_padIfPossibleWithStopsFromto, startAddr, endAddr - nullBytes);
	for (p = ((endAddr - nullBytes) + 1); p <= endAddr; p += 1) {
		codeByteAtput(p, 0xFF);
	}
}


/*	Extract the address of the adr rX, offset instruction at address mcpc
	C6.2.131	LDR (literal)		C6-979 */

	/* CogARMv8Compiler>>#pcRelativeAddressAt: */
static sqInt NoDbgRegParms
pcRelativeAddressAt(AbstractInstruction * self_in_pcRelativeAddressAt, sqInt mcpc)
{
    unsigned int instr;

	/* begin instructionAt: */
	instr = long32At(mcpc);
	assert(instructionIsPCRelativeLoad(self_in_pcRelativeAddressAt, instr));
	return (((((usqInt)(instr)) >> 3) & (0x3FFFFC)) - ((((usqInt)(instr)) >> 2) & (0x200000))) + mcpc;
}


/*	C6.2.211	PRFM (immediate)	C6-1136 */
/*	Unsigned offset, C6-1136 */
/*	This is the only case we can make use of so far... */

	/* CogARMv8Compiler>>#prn:imm:shiftBy12: */
static sqInt NoDbgRegParms
prnimmshiftBy12(AbstractInstruction * self_in_prnimmshiftBy12, sqInt baseReg, sqInt offset, sqInt shiftBy12)
{
	assert(((offset % 8) == 0)
	 && ((((offset / 8) >= 0) && ((offset / 8) <= (0xFFF)))));
	return (((0xF9800000U) + (((sqInt)((usqInt)(offset) << 7)))) + (((sqInt)((usqInt)(baseReg) << 5))));
}

	/* CogARMv8Compiler>>#pushLinkRegisterByteSize */
static sqInt NoDbgRegParms
pushLinkRegisterByteSize(AbstractInstruction * self_in_pushLinkRegisterByteSize)
{
	return 4;
}


/*	C6.2.26 B		C6-799
	C6.2.33	BL		C6-812 */

	/* CogARMv8Compiler>>#relocateCallBeforeReturnPC:by: */
static void NoDbgRegParms
relocateCallBeforeReturnPCby(AbstractInstruction * self_in_relocateCallBeforeReturnPCby, sqInt retpc, sqInt delta)
{
    usqInt distanceDiv4;
    usqInt instr;
    sqInt pc;

	assert((delta % 4) == 0);
	if (delta != 0) {
		/* begin instructionBeforeAddress: */
		pc = retpc - 4;
		instr = ((unsigned int) (long32At(pc)));
		assert(instructionIsImm26BorBL(self_in_relocateCallBeforeReturnPCby, instr));
		distanceDiv4 = instr & (0x7FFFFFF);
		distanceDiv4 += delta / 4;
		codeLong32Atput(retpc - 4, (instr & 0xFC000000U) | (distanceDiv4 & 0x3FFFFFF));
	}
}


/*	We generate the method address using pc-relative addressing.
	Simply check that pc-relative addressing is being used. c.f.
	emitMoveCw:intoR:at: */

	/* CogARMv8Compiler>>#relocateMethodReferenceBeforeAddress:by: */
static void NoDbgRegParms
relocateMethodReferenceBeforeAddressby(AbstractInstruction * self_in_relocateMethodReferenceBeforeAddressby, sqInt pc, sqInt delta)
{
	assert((instructionIsADR(self_in_relocateMethodReferenceBeforeAddressby, instructionAt(self_in_relocateMethodReferenceBeforeAddressby, pc - 4)))
	 || (instructionIsADR(self_in_relocateMethodReferenceBeforeAddressby, instructionAt(self_in_relocateMethodReferenceBeforeAddressby, pc - 8))));
}


/*	Rewrite a call instruction to call a different target. This variant is
	used to link PICs
	in ceSendMiss et al, and to rewrite cached primitive calls. Answer the
	extent of
	the code change which is used to compute the range of the icache to flush. */

	/* CogARMv8Compiler>>#rewriteCallAt:target: */
static sqInt NoDbgRegParms
rewriteCallAttarget(AbstractInstruction * self_in_rewriteCallAttarget, usqInt callSiteReturnAddress, usqInt callTargetAddress)
{
	codeLong32Atput(callSiteReturnAddress - 4, (assert((!((callTargetAddress - (callSiteReturnAddress - 4)) & 3))),
	(0x94000000U) + ((((callTargetAddress - (callSiteReturnAddress - 4))) >> 2) & (0x3FFFFFF))));
	return 4;
}


/*	Rewrite a long jump/call instruction to jump/call to a different target.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush.
	C6.2.25	B.cond		C6-798 */
/*	cogit processor disassembleInstructionAt: followingAddress - 4 In:
	objectMemory memory
 */

	/* CogARMv8Compiler>>#rewriteImm19JumpBefore:target: */
static sqInt NoDbgRegParms
rewriteImm19JumpBeforetarget(AbstractInstruction * self_in_rewriteImm19JumpBeforetarget, sqInt followingAddress, sqInt targetAddress)
{
    sqInt instrOpcode;
    usqInt instruction;
    sqInt mcpc;
    sqInt offset;
    sqInt pc;

	/* begin instructionAddressBefore: */
	mcpc = followingAddress - 4;
	offset = targetAddress - mcpc;
	/* begin instructionBeforeAddress: */
	pc = followingAddress - 4;
	instruction = ((unsigned int) (long32At(pc)));
	instrOpcode = (instruction) >> 26;
	assert(instrOpcode == 21);
	codeLong32Atput(mcpc, ((((sqInt)((usqInt)(instrOpcode) << 26))) + (((sqInt)((usqInt)((((offset) >> 2) & (0x7FFFF))) << 5)))) + (instruction & 15));
	return 4;
}


/*	Rewrite a long jump/call instruction to jump/call to a different target.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush.
	C6.2.26	B	C6-799
	C6.2.33 BL	C6-812 */
/*	cogit processor disassembleInstructionAt: followingAddress - 4 In:
	objectMemory memory
 */

	/* CogARMv8Compiler>>#rewriteImm26JumpBefore:target: */
static sqInt NoDbgRegParms
rewriteImm26JumpBeforetarget(AbstractInstruction * self_in_rewriteImm26JumpBeforetarget, sqInt followingAddress, sqInt targetAddress)
{
    sqInt instrOpcode;
    sqInt mcpc;
    sqInt offset;

	/* begin instructionAddressBefore: */
	mcpc = followingAddress - 4;
	offset = targetAddress - mcpc;
	instrOpcode = ((instructionBeforeAddress(self_in_rewriteImm26JumpBeforetarget, followingAddress))) >> 26;
	assert((instrOpcode == 5)
	 || (instrOpcode == 37));
	codeLong32Atput(mcpc, (((sqInt)((usqInt)(instrOpcode) << 26))) + (((offset) >> 2) & (0x3FFFFFF)));
	return 4;
}


/*	Rewrite an inline cache to call a different target for a new tag. This
	variant is used
	to link unlinked sends in ceSend:to:numArgs: et al. Answer the extent of
	the code
	change which is used to compute the range of the icache to flush.
	N.B. On 64-bit platforms the inline cache tag is only 32-bits wide, hence
	this code
	is very similar to that for ARM32 CogOutOfLineLiteralsARMCompiler. */

	/* CogARMv8Compiler>>#rewriteInlineCacheAt:tag:target: */
static sqInt NoDbgRegParms
rewriteInlineCacheAttagtarget(AbstractInstruction * self_in_rewriteInlineCacheAttagtarget, usqInt callSiteReturnAddress, sqInt cacheTag, usqInt callTargetAddress)
{
    sqInt call;
    usqInt callDistance;


	/* return offset */
	callDistance = ((usqInt) (int)(callTargetAddress - (callSiteReturnAddress - 4)));
	assert(isInImmediateJumpRange(self_in_rewriteInlineCacheAttagtarget, callDistance));
	/* begin bl: */
	assert((!(callDistance & 3)));
	call = (0x94000000U) + ((((sqInt)(callDistance)) >> 2) & (0x3FFFFFF));
	codeLong32Atput(callSiteReturnAddress - 4, call);
	codeLong32Atput(pcRelativeAddressAt(self_in_rewriteInlineCacheAttagtarget, callSiteReturnAddress - 8), ((usqInt) (int)cacheTag));
	assert((((int) (inlineCacheTagAt(self_in_rewriteInlineCacheAttagtarget, callSiteReturnAddress)))) == cacheTag);
	return 4;
}


/*	Rewrite an inline cache with a new tag. This variant is used
	by the garbage collector. RThis cannot happen in 64-bits as cache tags are
	guaranteed to be 32-bits or less. */

	/* CogARMv8Compiler>>#rewriteInlineCacheTag:at: */
static void NoDbgRegParms
rewriteInlineCacheTagat(AbstractInstruction * self_in_rewriteInlineCacheTagat, sqInt cacheTag, sqInt callSiteReturnAddress)
{
	error("should not happen");
}


/*	Rewrite a JumpFull instruction to jump to a different target. This variant
	is used to rewrite cached primitive calls.
	Answer the extent of the code change which is used to compute the range of
	the icache to flush. */
/*	cogit processor disassembleInstructionAt: callSiteReturnAddress - 8 In:
	objectMemory memory
 */
/*	cogit processor disassembleInstructionAt: callSiteReturnAddress - 4 In:
	objectMemory memory
 */

	/* CogARMv8Compiler>>#rewriteJumpFullAt:target: */
static sqInt NoDbgRegParms
rewriteJumpFullAttarget(AbstractInstruction * self_in_rewriteJumpFullAttarget, sqInt callSiteReturnAddress, sqInt callTargetAddress)
{
	codeLongAtput(pcRelativeAddressAt(self_in_rewriteJumpFullAttarget, callSiteReturnAddress - 8), callTargetAddress);
	return 0;
}


/*	Set the size field. The only complication is that in Smalltalk (operands
	at: 1) may be nil. */

	/* CogARMv8Compiler>>#setLiteralSize: */
static void NoDbgRegParms
setLiteralSize(AbstractInstruction * self_in_setLiteralSize, sqInt sizeOfLiteral)
{
    usqInt operand;

	((self_in_setLiteralSize->operands))[1] = ((((operand = ((self_in_setLiteralSize->operands))[1]),
(operand == null
		? 0
		: ((operand | 30) - 30)))) + (((sqInt)((usqInt)(sizeOfLiteral) << 1))));
}


/*	to save Slang from having to be a real compiler (it can't inline switches
	that return)
 */
/*	Answer if the receiver's opcode sets the condition codes correctly for the
	given conditional jump opcode.
	ARMv8 appears not to set condition codes at all in its shift
	instruction(s), which are aliases for
	SBFM (signed bit field move) and C6.2.332 UBFM.
	
	C6.2.232	SBFM	C6-1170
	C6.2.332	UBFM	C6-1351 */

	/* CogARMv8Compiler>>#setsConditionCodesFor: */
static sqInt NoDbgRegParms
setsConditionCodesFor(AbstractInstruction * self_in_setsConditionCodesFor, sqInt aConditionalJumpOpcode)
{
	return 0;
}


/*	Size a jump and set its address. The target may be another instruction
	or an absolute address. On entry the address inst var holds our virtual
	address. On exit address is set to eventualAbsoluteAddress, which is
	where this instruction will be output. The span of a jump to a following
	instruction is therefore between that instruction's address and this
	instruction's address ((which are both still their virtual addresses), but
	the span of a jump to a preceding instruction or to an absolute address is
	between that instruction's address (which by now is its eventual absolute
	address) or absolute address and eventualAbsoluteAddress.
	
	ARMv8 is simple; the 26-bit call/jump range (for a signed 28 bit extent,
	+/- 128Mb) and
	19 bit conditional branch range (for a signed 21 bit extent, +/- 1Mb)
	means no short
	jumps. This routine only has to determine the targets of jumps, not
	determine sizes.
	
	This version also deals with out-of-line literals. If this is the real
	literal, update the stand-in in literalsManager with the address (because
	instructions referring to the literal are referring to the stand-in). If
	this is annotated with
	IsObjectReference transfer the annotation to the stand-in, whence it will
	be transferred to the real literal, simplifying update of literals. */

	/* CogARMv8Compiler>>#sizePCDependentInstructionAt: */
static usqInt NoDbgRegParms
sizePCDependentInstructionAt(AbstractInstruction * self_in_sizePCDependentInstructionAt, sqInt eventualAbsoluteAddress)
{
    usqInt alignment;

	if (((self_in_sizePCDependentInstructionAt->opcode)) == AlignmentNops) {
		(self_in_sizePCDependentInstructionAt->address) = eventualAbsoluteAddress;
		alignment = ((self_in_sizePCDependentInstructionAt->operands))[0];
		return ((self_in_sizePCDependentInstructionAt->machineCodeSize) = ((eventualAbsoluteAddress + (alignment - 1)) & (-alignment)) - eventualAbsoluteAddress);
	}
	assert((isJump(self_in_sizePCDependentInstructionAt))
	 || ((((self_in_sizePCDependentInstructionAt->opcode)) == Call)
	 || ((((self_in_sizePCDependentInstructionAt->opcode)) == CallFull)
	 || ((((self_in_sizePCDependentInstructionAt->dependent)) != null)
	 && (((((self_in_sizePCDependentInstructionAt->dependent))->opcode)) == Literal)))));
	if (isJump(self_in_sizePCDependentInstructionAt)) {
		resolveJumpTarget(self_in_sizePCDependentInstructionAt);
	}
	(self_in_sizePCDependentInstructionAt->address) = eventualAbsoluteAddress;
	if ((((self_in_sizePCDependentInstructionAt->dependent)) != null)
	 && (((((self_in_sizePCDependentInstructionAt->dependent))->opcode)) == Literal)) {
		if (((self_in_sizePCDependentInstructionAt->opcode)) == Literal) {
			(((self_in_sizePCDependentInstructionAt->dependent))->address = (self_in_sizePCDependentInstructionAt->address));
		}
		if (((self_in_sizePCDependentInstructionAt->annotation)) == (getIsObjectReference())) {
			(((self_in_sizePCDependentInstructionAt->dependent))->annotation = (self_in_sizePCDependentInstructionAt->annotation));
			(self_in_sizePCDependentInstructionAt->annotation) = null;
		}
	}
	return ((self_in_sizePCDependentInstructionAt->machineCodeSize) = (self_in_sizePCDependentInstructionAt->maxSize));
}

	/* CogARMv8Compiler>>#stopsFrom:to: */
static void NoDbgRegParms
stopsFromto(AbstractInstruction * self_in_stopsFromto, sqInt startAddr, sqInt endAddr)
{
    sqInt addr;

	assert((((endAddr - startAddr) + 1) % 4) == 0);
	for (addr = startAddr; addr <= endAddr; addr += 4) {
		codeLong32Atput(addr, 0xD4400000U /* stop */);
	}
}


/*	Rewrite the 32-bit literal in the instruction immediately preceding
	followingAddress. 
 */

	/* CogARMv8Compiler>>#storeLiteral32:beforeFollowingAddress: */
static void NoDbgRegParms
storeLiteral32beforeFollowingAddress(AbstractInstruction * self_in_storeLiteral32beforeFollowingAddress, sqInt literal, sqInt followingAddress)
{
	codeLong32Atput(pcRelativeAddressAt(self_in_storeLiteral32beforeFollowingAddress, instructionAddressBefore(self_in_storeLiteral32beforeFollowingAddress, (instructionIsPCRelativeLoad(self_in_storeLiteral32beforeFollowingAddress, instructionBeforeAddress(self_in_storeLiteral32beforeFollowingAddress, followingAddress))
		? followingAddress
		: followingAddress - 4))), literal);
}


/*	Rewrite the literal in the instruction immediately preceding
	followingAddress. 
 */

	/* CogARMv8Compiler>>#storeLiteral:beforeFollowingAddress: */
static void NoDbgRegParms
storeLiteralbeforeFollowingAddress(AbstractInstruction * self_in_storeLiteralbeforeFollowingAddress, sqInt literal, sqInt followingAddress)
{
	codeLongAtput(pcRelativeAddressAt(self_in_storeLiteralbeforeFollowingAddress, instructionAddressBefore(self_in_storeLiteralbeforeFollowingAddress, (instructionIsPCRelativeLoad(self_in_storeLiteralbeforeFollowingAddress, instructionBeforeAddress(self_in_storeLiteralbeforeFollowingAddress, followingAddress))
		? followingAddress
		: followingAddress - 4))), literal);
}


/*	C6.2.273	STR (immediate)	C6-1239
	C6.2.275	STRB (immediate)	C6-1244 */

	/* CogARMv8Compiler>>#st:rn:rt:imm:shiftBy12: */
static sqInt NoDbgRegParms
strnrtimmshiftBy12(AbstractInstruction * self_in_strnrtimmshiftBy12, sqInt unitSizeLog2MinusOne, sqInt baseReg, sqInt targetReg, sqInt offset, sqInt shiftBy12)
{
    sqInt unitSize;

	unitSize = 1ULL << unitSizeLog2MinusOne;
	assert(!((SP == targetReg)));
	assert(!((baseReg == targetReg)));
	if (((offset % unitSize) == 0)
	 && ((((offset / unitSize) >= 0) && ((offset / unitSize) <= (0xFFF))))) {
		return ((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x39000000)) + (((sqInt)((usqInt)(offset) << (10 - unitSizeLog2MinusOne))))) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetReg;
	}
	if (!(asserta(((offset >= -256) && (offset <= 0xFF))))) {
		error("unhandled immediate offset in store");
	}
	return ((((((sqInt)((usqInt)(unitSizeLog2MinusOne) << 30))) + (0x38000000)) + (((sqInt)((usqInt)((offset & 0x1FF)) << 12)))) + (((sqInt)((usqInt)(baseReg) << 5)))) + targetReg;
}


/*	Answer if the receiver uses an out-of-line literal. Needs only
	to work for the opcodes created with gen:literal:operand: et al. */

	/* CogARMv8Compiler>>#usesOutOfLineLiteral */
static sqInt NoDbgRegParms
usesOutOfLineLiteral(AbstractInstruction * self_in_usesOutOfLineLiteral)
{
    sqInt constant;
    sqInt constant1;
    sqInt imm;
    sqInt imm1;
    sqInt immediate;
    sqInt immediate1;
    sqInt immediate2;
    sqInt immediate3;
    sqInt immr1;
    sqInt immr2;
    usqInt mask;
    usqInt mask1;
    usqInt n1;
    usqInt n2;
    usqInt nImms;
    usqInt nImms1;
    sqInt numLeadingOnes;
    sqInt numLeadingOnes1;
    sqInt numTrailingOnes;
    sqInt numTrailingOnes1;
    sqInt rotateCount;
    sqInt rotateCount1;
    sqInt size;
    sqInt size1;

	switch ((self_in_usesOutOfLineLiteral->opcode)) {
	case CallFull:
	case JumpFull:
	case AddCwR:
	case AndCwR:
	case CmpCwR:
	case CmpC32R:
	case OrCwR:
	case SubCwR:
	case XorCwR:
	case MoveC32R:
		return 1;

	case AddCqR:
	case AddCqRR:
	case SubCqR:
	case CmpCqR:
		/* begin isPossiblyShiftableNegatableImm12:ifTrue:ifFalse: */
		immediate = ((sqLong) (((self_in_usesOutOfLineLiteral->operands))[0]));
		if (((immediate >= (-4096)) && (immediate <= (0xFFF)))) {
			return 0;
		}
		if (((!(immediate & (0xFFF))))
		 && (((((immediate) >> 12) >= (-4096)) && (((immediate) >> 12) <= (0xFFF))))) {
			return 0;
		}
		return 1;

	case LoadEffectiveAddressMwrR:
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		immediate1 = ((sqLong) (((self_in_usesOutOfLineLiteral->operands))[0]));
		if (((immediate1 >= 0) && (immediate1 <= (0xFFF)))) {
			return 0;
		}
		if (((!(immediate1 & (0xFFF))))
		 && (((((immediate1) >> 12) >= 0) && (((immediate1) >> 12) <= (0xFFF))))) {
			return 0;
		}
		return 1;

	case AndCqR:
	case AndCqRR:
	case OrCqRR:
	case OrCqR:
	case TstCqR:
	case XorCqR:
		/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
		constant = ((self_in_usesOutOfLineLiteral->operands))[0];
		if (((constant >= -1) && (constant <= 0))) {
			return 1;
		}

		/* First, determine the element size. */
		imm = constant;
		size = 32;
		while (1) {
			mask = (1ULL << size) - 1;
			if (!(((imm & mask) != ((((usqInt)(imm)) >> size) & mask)
				? ((size = size * 2),
					0)
				: size > 2))) break;
			size = size / 2;
		}
		mask = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size);
		imm = imm & mask;
		if (isShiftedMask(self_in_usesOutOfLineLiteral, imm)) {
			rotateCount = countTrailingZeros(self_in_usesOutOfLineLiteral, imm);
			numTrailingOnes = countTrailingOnes(self_in_usesOutOfLineLiteral, ((usqInt)(imm)) >> rotateCount);
		}
		else {
			imm = imm | (~(usqIntptr_t)mask);
			if (!(isShiftedMask(self_in_usesOutOfLineLiteral, imm))) {
				return 1;
			}
			numLeadingOnes = countLeadingOnes(self_in_usesOutOfLineLiteral, imm);
			rotateCount = 64 - numLeadingOnes;
			numTrailingOnes = (numLeadingOnes + (countTrailingOnes(self_in_usesOutOfLineLiteral, imm))) - (64 - size);
		}
		assert(size > rotateCount);

		/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
		immr1 = (size - rotateCount) & (size - 1);

		/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
		nImms = ((sqInt)((usqInt)((~(usqIntptr_t)(size - 1))) << 1));

		/* Extract the seventh bit and toggle it to create the N field. */
		nImms = nImms | (numTrailingOnes - 1);
		n1 = (((nImms) >> 6) & 1) ^ 1;
		nImms = nImms & 0x3F;
		assert((decodeNimmsimmr(self_in_usesOutOfLineLiteral, n1, nImms, immr1)) == (((usqInt) constant)));
		return 0;

	case MoveCqR:
		/* begin isPossiblyShiftableImm12:ifTrue:ifFalse: */
		immediate2 = ((self_in_usesOutOfLineLiteral->operands))[0];
		if (((immediate2 >= 0) && (immediate2 <= (0xFFF)))) {
			return 0;
			goto l1;
		}
		if (((!(immediate2 & (0xFFF))))
		 && (((((immediate2) >> 12) >= 0) && (((immediate2) >> 12) <= (0xFFF))))) {
			return 0;
			goto l1;
		}
	l1:	/* end isPossiblyShiftableImm12:ifTrue:ifFalse: */;
		/* begin isImmNImmSImmREncodableBitmask:ifTrue:ifFalse: */
		constant1 = ((self_in_usesOutOfLineLiteral->operands))[0];
		if (((constant1 >= -1) && (constant1 <= 0))) {
			return 1;
		}

		/* First, determine the element size. */
		imm1 = constant1;
		size1 = 32;
		while (1) {
			mask1 = (1ULL << size1) - 1;
			if (!(((imm1 & mask1) != ((((usqInt)(imm1)) >> size1) & mask1)
				? ((size1 = size1 * 2),
					0)
				: size1 > 2))) break;
			size1 = size1 / 2;
		}
		mask1 = ((usqInt)((0xFFFFFFFFFFFFFFFFULL))) >> (64 - size1);
		imm1 = imm1 & mask1;
		if (isShiftedMask(self_in_usesOutOfLineLiteral, imm1)) {
			rotateCount1 = countTrailingZeros(self_in_usesOutOfLineLiteral, imm1);
			numTrailingOnes1 = countTrailingOnes(self_in_usesOutOfLineLiteral, ((usqInt)(imm1)) >> rotateCount1);
		}
		else {
			imm1 = imm1 | (~(usqIntptr_t)mask1);
			if (!(isShiftedMask(self_in_usesOutOfLineLiteral, imm1))) {
				return 1;
			}
			numLeadingOnes1 = countLeadingOnes(self_in_usesOutOfLineLiteral, imm1);
			rotateCount1 = 64 - numLeadingOnes1;
			numTrailingOnes1 = (numLeadingOnes1 + (countTrailingOnes(self_in_usesOutOfLineLiteral, imm1))) - (64 - size1);
		}
		assert(size1 > rotateCount1);

		/* If size has a 1 in the n'th bit, create a value that has zeroes in bits [0, n] and ones above that. */
		immr2 = (size1 - rotateCount1) & (size1 - 1);

		/* Or the CTO value into the low bits, which must be below the Nth bit mentioned above. */
		nImms1 = ((sqInt)((usqInt)((~(usqIntptr_t)(size1 - 1))) << 1));

		/* Extract the seventh bit and toggle it to create the N field. */
		nImms1 = nImms1 | (numTrailingOnes1 - 1);
		n2 = (((nImms1) >> 6) & 1) ^ 1;
		nImms1 = nImms1 & 0x3F;
		assert((decodeNimmsimmr(self_in_usesOutOfLineLiteral, n2, nImms1, immr2)) == (((usqInt) constant1)));
		return 0;

	case MoveCwR:
	case PushCw:
		return !(((addressIsInInstructions(((AbstractInstruction *) (((self_in_usesOutOfLineLiteral->operands))[0]))))
		 || ((((AbstractInstruction *) (((self_in_usesOutOfLineLiteral->operands))[0]))) == (methodLabel())))
		 || (((((usqInt)(((self_in_usesOutOfLineLiteral->operands))[0]))) >= ((methodLabel->address)))
		 && ((((usqInt)(((self_in_usesOutOfLineLiteral->operands))[0]))) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))));

	case MoveAwR:
	case MoveAbR:
	case PrefetchAw:
		return !(((((self_in_usesOutOfLineLiteral->operands))[0]) != null)
		 && ((((((self_in_usesOutOfLineLiteral->operands))[0]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_usesOutOfLineLiteral->operands))[0]) <= (((varBaseAddress()) + (0x1000)) - 1)))));

	case MoveRAw:
	case MoveRAb:
		return !(((((self_in_usesOutOfLineLiteral->operands))[1]) != null)
		 && ((((((self_in_usesOutOfLineLiteral->operands))[1]) >= ((varBaseAddress()) - (0x100))) && ((((self_in_usesOutOfLineLiteral->operands))[1]) <= (((varBaseAddress()) + (0x1000)) - 1)))));

	case MoveMwrR:
	case MoveMbrR:
	case MoveM16rR:
		return !(isImm12orImm9offset(self_in_usesOutOfLineLiteral, ((self_in_usesOutOfLineLiteral->operands))[0]));

	case MoveRMwr:
	case MoveRMbr:
	case MoveRM16r:
		/* begin isPossiblyShiftableNegatableImm12:ifTrue:ifFalse: */
		immediate3 = ((self_in_usesOutOfLineLiteral->operands))[1];
		if (((immediate3 >= (-4096)) && (immediate3 <= (0xFFF)))) {
			return 0;
		}
		if (((!(immediate3 & (0xFFF))))
		 && (((((immediate3) >> 12) >= (-4096)) && (((immediate3) >> 12) <= (0xFFF))))) {
			return 0;
		}
		return 1;

	case MoveRdM64r:
		return !(isUnsigned12BitMultipleOf8(self_in_usesOutOfLineLiteral, ((self_in_usesOutOfLineLiteral->operands))[1]));

	case MoveM64rRd:
		return !(isUnsigned12BitMultipleOf8(self_in_usesOutOfLineLiteral, ((self_in_usesOutOfLineLiteral->operands))[0]));

	case PushCq:
		return !((((((self_in_usesOutOfLineLiteral->operands))[0]) >= -256) && ((((self_in_usesOutOfLineLiteral->operands))[0]) <= 0xFF)));

	default:
		assert(0);

	}
	return 0;
}


/*	Answer if Call and JumpLong are relative and hence need to take the
	caller's relocation delta into account during code compaction, rather than
	just the
	callee's delta. */

	/* CogARMv8Compiler>>#zoneCallsAreRelative */
static sqInt NoDbgRegParms
zoneCallsAreRelative(AbstractInstruction * self_in_zoneCallsAreRelative)
{
	return 1;
}

	/* CogBlockMethod>>#cmHomeMethod */
static CogMethod * NoDbgRegParms
cmHomeMethod(CogBlockMethod * self_in_cmHomeMethod)
{
	return ((self_in_cmHomeMethod->cpicHasMNUCaseOrCMIsFullBlock)
		? ((CogMethod *) self_in_cmHomeMethod)
		: ((CogMethod *) ((((usqInt)self_in_cmHomeMethod)) - ((self_in_cmHomeMethod->homeOffset)))));
}

	/* CogBlockMethod>>#isCMBlock */
static sqInt NoDbgRegParms
isCMBlock(CogBlockMethod * self_in_isCMBlock)
{
	return ((self_in_isCMBlock->cmType)) == CMBlock;
}

	/* CogBlockMethod>>#isCMClosedPIC */
static sqInt NoDbgRegParms
isCMClosedPIC(CogBlockMethod * self_in_isCMClosedPIC)
{
	return ((self_in_isCMClosedPIC->cmType)) == CMClosedPIC;
}

	/* CogBlockMethod>>#isCMFree */
static sqInt NoDbgRegParms
isCMFree(CogBlockMethod * self_in_isCMFree)
{
	return ((self_in_isCMFree->cmType)) == CMFree;
}

	/* CogBlockMethod>>#isCMMethodEtAl */
static sqInt NoDbgRegParms
isCMMethodEtAl(CogBlockMethod * self_in_isCMMethodEtAl)
{
	return ((self_in_isCMMethodEtAl->cmType)) >= CMMethod;
}

	/* CogBlockMethod>>#isCMOpenPIC */
static sqInt NoDbgRegParms
isCMOpenPIC(CogBlockMethod * self_in_isCMOpenPIC)
{
	return ((self_in_isCMOpenPIC->cmType)) == CMOpenPIC;
}

	/* CogBytecodeDescriptor>>#isBranch */
static sqInt NoDbgRegParms
isBranch(BytecodeDescriptor * self_in_isBranch)
{
	return (((self_in_isBranch->spanFunction)) != null)
	 && (!((self_in_isBranch->isBlockCreation)));
}

	/* Cogit>>#AddCq:R: */
static AbstractInstruction * NoDbgRegParms
gAddCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction;
}


/*	N.B. if the condition codes don't require setting and three address
	arithmetic is unavailable, then LoadEffectiveAddressMw:r:R: can be used
	instead. 
 */

	/* Cogit>>#AddCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gAddCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *first;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(AddCqRR, quickConstant, srcReg, destReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction2;
	if (srcReg == destReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, quickConstant, destReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		return anInstruction;
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AddCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return first;
}


/*	destReg := addendReg + badendReg */

	/* Cogit>>#AddR:R:R: */
static AbstractInstruction * NoDbgRegParms
gAddRRR(sqInt addendReg, sqInt badendReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(AddRRR, addendReg, badendReg, destReg);
	assert(badendReg != destReg);
	first = genoperandoperand(MoveRR, addendReg, destReg);
	genoperandoperand(AddRR, badendReg, destReg);
	return first;
}

	/* Cogit>>#AndCq:R: */
static AbstractInstruction * NoDbgRegParms
gAndCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction;
}

	/* Cogit>>#AndCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gAndCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *first;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(AndCqRR, quickConstant, srcReg, destReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction2;
	if (srcReg == destReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AndCqR, quickConstant, destReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		return anInstruction;
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return first;
}


/*	destReg := (signed)srcReg >> quickConstant */

	/* Cogit>>#ArithmeticShiftRightCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gArithmeticShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(ArithmeticShiftRightCqRR, quickConstant, srcReg, destReg);
	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#abortOffset */
sqInt
abortOffset(void)
{
	return missOffset;
}

	/* Cogit>>#addCleanBlockStarts */
static void
addCleanBlockStarts(void)
{
    sqInt i;
    sqInt iLimiT;
    sqInt lit;
    sqInt startPCOrNil;

	for (i = 1, iLimiT = (literalCountOf(methodObj)); i <= iLimiT; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (!(startPCOrNil == null)) {
			maxLitIndex = ((maxLitIndex < i) ? i : maxLitIndex);
			addBlockStartAtnumArgsnumCopiedspan(startPCOrNil - 1, argumentCountOfClosure(lit), copiedValueCountOfClosure(lit), spanForCleanBlockStartingAt(startPCOrNil - 1));
		}
	}
}


/*	Perform an integrity/leak check using the heapMap.
	Set a bit at each cog method's header. */

	/* Cogit>>#addCogMethodsToHeapMap */
void
addCogMethodsToHeapMap(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			heapMapAtWordPut(cogMethod, 1);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#addressIsInCurrentCompilation: */
static sqInt NoDbgRegParms
addressIsInCurrentCompilation(sqInt address)
{
	return ((((usqInt)address)) >= ((methodLabel->address)))
	 && ((((usqInt)address)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))));
}

	/* Cogit>>#addressIsInFixups: */
static sqInt NoDbgRegParms
addressIsInFixups(BytecodeFixup *address)
{
	return (BytecodeFixup *)address >= fixups && (BytecodeFixup *)address < (fixups + numAbstractOpcodes);
}


/*	calculate the end of the n'th case statement - which is complicated
	because we have case 1 right at the top of our CPIC and then build up from
	the last one. Yes I know this sounds strange, but trust me - I'm an
	Engineer, we do things backwards all the emit
 */

	/* Cogit>>#addressOfEndOfCase:inCPIC: */
static sqInt NoDbgRegParms
addressOfEndOfCaseinCPIC(sqInt n, CogMethod *cPIC)
{
	assert((n >= 1)
	 && (n <= MaxCPICCases));
	return (n == 1
		? (((sqInt)cPIC)) + firstCPICCaseOffset
		: ((((sqInt)cPIC)) + firstCPICCaseOffset) + (((MaxCPICCases + 1) - n) * cPICCaseSize));
}


/*	Align methodZoneBase to that for the start of a method. */

	/* Cogit>>#alignMethodZoneBase */
static void
alignMethodZoneBase(void)
{
    usqInt oldBase;

	oldBase = methodZoneBase;
	methodZoneBase = roundUpToMethodAlignment(backEnd(), methodZoneBase);
	stopsFromto(backEnd, oldBase, methodZoneBase - 1);
}

	/* Cogit>>#alignUptoRoutineBoundary: */
static sqInt NoDbgRegParms
alignUptoRoutineBoundary(sqInt anAddress)
{
	return (((anAddress + 7) | 7) - 7);
}


/*	Check that all methods have valid selectors, and that all linked sends are
	to valid targets and have valid cache tags
 */

	/* Cogit>>#allMachineCodeObjectReferencesValid */
static sqInt
allMachineCodeObjectReferencesValid(void)
{
    CogMethod *cogMethod;
    sqInt ok;

	ok = 1;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!(asserta(checkValidOopReference((cogMethod->selector))))) {
				ok = 0;
			}
			if (!(asserta((cogMethodDoesntLookKosher(cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if ((((cogMethod->cmType)) >= CMMethod)
		 || (((cogMethod->cmType)) == CMOpenPIC)) {
			if (!(asserta((mapForperformUntilarg(cogMethod, checkIfValidOopRefAndTargetpccogMethod, cogMethod)) == 0))) {
				ok = 0;
			}
		}
		if ((((cogMethod->cmType)) >= CMMethod)) {
			if (((cogMethod->counters)) != 0) {
				if (!(asserta(checkValidDerivedObjectReference((cogMethod->counters))))) {
					ok = 0;
				}
			}
					}
		if (((cogMethod->cmType)) == CMClosedPIC) {
			if (!(asserta(noTargetsFreeInClosedPIC(cogMethod)))) {
				ok = 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#allMethodsHaveCorrectHeader */
static sqInt
allMethodsHaveCorrectHeader(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (!(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}

	/* Cogit>>#annotateAbsolutePCRef: */
static AbstractInstruction * NoDbgRegParms
annotateAbsolutePCRef(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = IsAbsPCReference);
	return abstractInstruction;
}

	/* Cogit>>#annotateBytecode: */
static AbstractInstruction * NoDbgRegParms
annotateBytecode(AbstractInstruction *abstractInstruction)
{
	(abstractInstruction->annotation = HasBytecodePC);
	return abstractInstruction;
}

	/* Cogit>>#annotate:objRef: */
static AbstractInstruction * NoDbgRegParms
annotateobjRef(AbstractInstruction *abstractInstruction, sqInt anOop)
{
	if (shouldAnnotateObjectReference(anOop)) {
		setHasMovableLiteral(1);
		if (isYoungObject(anOop)) {
			setHasYoungReferent(1);
		}
		(abstractInstruction->annotation = IsObjectReference);
	}
	return abstractInstruction;
}

	/* Cogit>>#assertSaneJumpTarget: */
static void NoDbgRegParms
assertSaneJumpTarget(AbstractInstruction *jumpTarget)
{
	assert((closedPICSize == null)
	 || ((openPICSize == null)
	 || ((addressIsInInstructions(jumpTarget))
	 || ((((((usqInt)jumpTarget)) >= codeBase) && ((((usqInt)jumpTarget)) <= ((((sqInt)(limitZony()))) + (((closedPICSize < openPICSize) ? openPICSize : closedPICSize)))))))));
}


/*	Answer an unused abstract register in the registerMask, or NoReg if none. */

	/* Cogit>>#availableRegisterOrNoneIn: */
static sqInt NoDbgRegParms
availableRegisterOrNoneIn(sqInt liveRegsMask)
{
    sqInt reg;

	if (liveRegsMask != 0) {
		for (reg = 0; reg <= 0x1F; reg += 1) {
			if (((liveRegsMask & (1ULL << reg)) != 0)) {
				return reg;
			}
		}
	}
	return NoReg;
}


/*	Evaluate binaryFunction with the block start mcpc and supplied arg for
	each entry in the block dispatch. If the function answers non-zero answer
	the value
	it answered. Used to update back-references to the home method in
	compaction.  */

	/* Cogit>>#blockDispatchTargetsFor:perform:arg: */
static sqInt NoDbgRegParms
blockDispatchTargetsForperformarg(CogMethod *cogMethod, usqInt (*binaryFunction)(sqInt mcpc, sqInt arg), sqInt arg)
{
    sqInt blockEntry;
    usqInt end;
    sqInt pc;
    sqInt result;
    usqInt targetpc;

	if (((cogMethod->blockEntryOffset)) == 0) {
		return null;
	}
	blockEntry = ((cogMethod->blockEntryOffset)) + (((sqInt)cogMethod));
	pc = blockEntry;
	end = (mapEndFor(cogMethod)) - 1;
	while (pc < end) {
		if (isJumpAt(backEnd, pc)) {
			targetpc = jumpTargetPCAt(backEnd, pc);
			if (targetpc < blockEntry) {
				result = binaryFunction(targetpc, arg);
				if (result != 0) {
					return result;
				}
			}
		}
		pc += instructionSizeAt(backEnd, pc);
	}
	return 0;
}


/*	Answer the zero-relative bytecode pc matching the machine code pc argument
	in cogMethod, given the start of the bytecodes for cogMethod's block or
	method object. */

	/* Cogit>>#bytecodePCFor:startBcpc:in: */
sqInt
bytecodePCForstartBcpcin(sqInt mcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc1;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt prim;
    sqInt result;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	/* begin mapFor:bcpc:performUntil:arg: */
	assert(((cogMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc1 = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = findIsBackwardBranchMcpcBcpcMatchingMcpc(null, (0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))), (((char *) mcpc1)), startbcpc, (((void *)mcpc)));
	if (result != 0) {
		return result;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = (cogMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj))
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc1 += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc - 1, methodObj);
							upperByte = fetchByteofObject(bcpc - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = findIsBackwardBranchMcpcBcpcMatchingMcpc(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc1)), ((isBackwardBranch
	? bcpc - (2 * nExts)
	: bcpc)), (((void *)mcpc)));
				if (result != 0) {
					return result;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc1 += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}

	/* Cogit>>#CallRT:registersToBeSavedMask: */
static AbstractInstruction * NoDbgRegParms
CallRTregistersToBeSavedMask(sqInt callTarget, sqInt registersToBeSaved)
{
    AbstractInstruction *abstractInstruction;
    sqInt callerSavedRegsToBeSaved;
    AbstractInstruction *lastInst;
    sqInt reg;
    sqInt registersToBePushed;

	reg = 0;
	callerSavedRegsToBeSaved = CallerSavedRegisterMask & registersToBeSaved;
	registersToBePushed = callerSavedRegsToBeSaved;
	reg = 0;
	while (registersToBePushed != 0) {
		if (((registersToBePushed & 1) != 0)) {
			genoperand(PushR, reg);
		}
		reg += 1;
		registersToBePushed = (registersToBePushed) >> 1;
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, callTarget);
	(abstractInstruction->annotation = IsRelativeCall);
	lastInst = abstractInstruction;
	while (reg > 0) {
		reg -= 1;
		if (((callerSavedRegsToBeSaved & (1ULL << reg)) != 0)) {
			lastInst = genoperand(PopR, reg);
		}
	}
	return lastInst;
}

	/* Cogit>>#Call: */
static AbstractInstruction * NoDbgRegParms
gCall(sqInt callTarget)
{
	return genoperand(Call, callTarget);
}

	/* Cogit>>#CmpCq:R: */
static AbstractInstruction * NoDbgRegParms
gCmpCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction;
}


/*	This is a static version of ceCallCogCodePopReceiverReg for break-pointing
	when debugging in C. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#callCogCodePopReceiver */
static void
callCogCodePopReceiver(void)
{
	realCECallCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	This is a static version of ceCallCogCodePopReceiverAndClassRegs for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#callCogCodePopReceiverAndClassRegs */
static void
callCogCodePopReceiverAndClassRegs(void)
{
	realCECallCogCodePopReceiverAndClassRegs();
}


/*	Code entry closed PIC miss. A send has fallen
	through a closed (finite) polymorphic inline cache.
	Either extend it or patch the send site to an open PIC.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */
/*	Marked <api> so the code generator won't delete it. */

	/* Cogit>>#ceCPICMiss:receiver: */
static sqInt NoDbgRegParms
ceCPICMissreceiver(CogMethod *cPIC, sqInt receiver)
{
    sqInt cacheTag;
    sqInt errorSelectorOrNil;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    sqInt outerReturn;
    sqInt result;
    sqInt selector;

	if (isOopForwarded(receiver)) {
		return ceSendFromInLineCacheMiss(cPIC);
	}
	outerReturn = stackTop();
	assert(!(((inlineCacheTagAt(backEnd, outerReturn)) == (picAbortDiscriminatorValue()))));
	if (((cPIC->cPICNumCases)) < MaxCPICCases) {
		/* begin lookup:for:methodAndErrorSelectorInto: */
		selector = (cPIC->selector);
		methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorCannotInterpret;
				goto l1;
			}
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {

				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, selector);
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = null;
			goto l1;
		}
		if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
			methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
			if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
				assert(isOopCompiledMethod(methodOrSelectorIndex));
				if ((!(methodHasCogMethod(methodOrSelectorIndex)))
				 && (methodShouldBeCogged(methodOrSelectorIndex))) {

					/* We assume cog:selector: will *not* reclaim the method zone */
					cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
				}
				newTargetMethodOrNil = methodOrSelectorIndex;
				errorSelectorOrNil = SelectorDoesNotUnderstand;
				goto l1;
			}
			newTargetMethodOrNil = null;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = methodOrSelectorIndex;
	l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	}
	else {
		newTargetMethodOrNil = (errorSelectorOrNil = null);
	}
	assert(outerReturn == (stackTop()));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	cacheTag = inlineCacheTagForInstance(receiver);
	if ((((cPIC->cPICNumCases)) >= MaxCPICCases)
	 || (((errorSelectorOrNil != null)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((newTargetMethodOrNil == null)
	 || (isYoung(newTargetMethodOrNil))))) {
		result = patchToOpenPICFornumArgsreceiver((cPIC->selector), (cPIC->cmNumArgs), receiver);
		assert(!result);
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		return ceSendFromInLineCacheMiss(cPIC);
	}
	cogExtendPICCaseNMethodtagisMNUCase(cPIC, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	executeCogPICfromLinkedSendWithReceiverandCacheTag(cPIC, receiver, instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8)))));
	return null;
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceFree: */
static void NoDbgRegParms
ceFree(void *pointer)
{
	free(pointer);
}


/*	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it. 
 */

	/* Cogit>>#ceMalloc: */
static void* NoDbgRegParms
ceMalloc(size_t size)
{
	return malloc(size);
}


/*	An in-line cache check in a method has failed. The failing entry check has
	jumped to the ceMethodAbort abort call at the start of the method which
	has called this routine.
	If possible allocate a closed PIC for the current and existing classes.
	The stack looks like:
	receiver
	args
	sender return address
	sp=>	ceMethodAbort call return address
	So we can find the method that did the failing entry check at
	ceMethodAbort call return address - missOffset
	and we can find the send site from the outer return address.
	Invoked from a trampoline. Marked <api> so the code generator won't delete
	it.  */

	/* Cogit>>#ceSICMiss: */
static sqInt NoDbgRegParms
ceSICMiss(sqInt receiver)
{
    sqInt cacheTag;
    unsigned int codeInstruction;
    unsigned int dataInstruction;
    sqInt entryPoint;
    sqInt errorSelectorOrNil;
    sqInt extent;
    usqInt innerReturn;
    sqInt methodOrSelectorIndex;
    sqInt newTargetMethodOrNil;
    usqInt outerReturn;
    CogMethod *pic;
    sqInt result;
    sqInt selector;
    CogMethod *targetMethod;


	/* Whether we can relink to a PIC or not we need to pop off the inner return and identify the target method. */
	innerReturn = ((usqInt)(popStack()));
	targetMethod = ((CogMethod *) (innerReturn - missOffset));
	if (isOopForwarded(receiver)) {
		return ceSendFromInLineCacheMiss(targetMethod);
	}
	outerReturn = ((usqInt)(stackTop()));
	assert(((outerReturn >= methodZoneBase) && (outerReturn <= (freeStart()))));
	entryPoint = callTargetFromReturnAddress(backEnd, outerReturn);
	assert(((targetMethod->selector)) != (nilObject()));
	assert(((((sqInt)targetMethod)) + cmEntryOffset) == entryPoint);
	/* begin lookup:for:methodAndErrorSelectorInto: */
	selector = (targetMethod->selector);
	methodOrSelectorIndex = lookupOrdinaryreceiver(selector, receiver);
	if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
		if (!(isOopCompiledMethod(methodOrSelectorIndex))) {
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorCannotInterpret;
			goto l1;
		}
		if ((!(methodHasCogMethod(methodOrSelectorIndex)))
		 && (methodShouldBeCogged(methodOrSelectorIndex))) {

			/* We assume cog:selector: will *not* reclaim the method zone */
			cogselector(methodOrSelectorIndex, selector);
		}
		newTargetMethodOrNil = methodOrSelectorIndex;
		errorSelectorOrNil = null;
		goto l1;
	}
	if (methodOrSelectorIndex == SelectorDoesNotUnderstand) {
		methodOrSelectorIndex = lookupMNUreceiver(splObj(SelectorDoesNotUnderstand), receiver);
		if ((((usqInt)methodOrSelectorIndex)) > (maxLookupNoMNUErrorCode())) {
			assert(isOopCompiledMethod(methodOrSelectorIndex));
			if ((!(methodHasCogMethod(methodOrSelectorIndex)))
			 && (methodShouldBeCogged(methodOrSelectorIndex))) {

				/* We assume cog:selector: will *not* reclaim the method zone */
				cogselector(methodOrSelectorIndex, splObj(SelectorDoesNotUnderstand));
			}
			newTargetMethodOrNil = methodOrSelectorIndex;
			errorSelectorOrNil = SelectorDoesNotUnderstand;
			goto l1;
		}
		newTargetMethodOrNil = null;
		errorSelectorOrNil = SelectorDoesNotUnderstand;
		goto l1;
	}
	newTargetMethodOrNil = null;
	errorSelectorOrNil = methodOrSelectorIndex;
	l1:	/* end lookup:for:methodAndErrorSelectorInto: */;
	assert(outerReturn == (stackTop()));
	cacheTag = inlineCacheTagForInstance(receiver);
	if ((((errorSelectorOrNil != null)
	 && (errorSelectorOrNil != SelectorDoesNotUnderstand))
	 || ((newTargetMethodOrNil == null)
	 || (isYoung(newTargetMethodOrNil))))
	 || ((instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8))))) == 0 /* picAbortDiscriminatorValue */)) {
		result = patchToOpenPICFornumArgsreceiver((targetMethod->selector), (targetMethod->cmNumArgs), receiver);
		assert(!result);
		return ceSendFromInLineCacheMiss(targetMethod);
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	pic = openPICWithSelector((targetMethod->selector));
	
	/* otherwise attempt to create a closed PIC for the two cases. */
	pic = cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase((targetMethod->selector), (targetMethod->cmNumArgs), targetMethod, newTargetMethodOrNil, cacheTag, errorSelectorOrNil == SelectorDoesNotUnderstand);
	if ((((((sqInt)pic)) >= MaxNegativeErrorCode) && ((((sqInt)pic)) <= -1))) {

		/* For some reason the PIC couldn't be generated, most likely a lack of code memory.
		   Continue as if this is an unlinked send. */
		if ((((sqInt)pic)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		return ceSendFromInLineCacheMiss(targetMethod);
	}
	extent = (((pic->cmType)) == CMOpenPIC
		? rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), mframeHomeMethodExport()), (((sqInt)pic)) + cmEntryOffset)
		: rewriteCallAttarget(backEnd, outerReturn, (((sqInt)pic)) + cmEntryOffset));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At((((usqInt)pic)) + cmNoCheckEntryOffset);
	dataInstruction = long32At(((((usqInt)pic)) + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)pic))), ((((usqInt)pic)) + closedPICSize) - (((usqInt)pic)));
	sys_icache_invalidate(((void *) (((usqInt)pic))), ((((usqInt)pic)) + closedPICSize) - (((usqInt)pic)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)pic), (((usqInt)pic)) + closedPICSize);
#  endif
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) ((((usqInt)outerReturn)) - extent)), (((usqInt)outerReturn)) - ((((usqInt)outerReturn)) - extent));
	sys_icache_invalidate(((void *) ((((usqInt)outerReturn)) - extent)), (((usqInt)outerReturn)) - ((((usqInt)outerReturn)) - extent));
#  else // __APPLE__ && __MACH__
	ceFlushICache((((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
#  endif
	executeCogPICfromLinkedSendWithReceiverandCacheTag(pic, receiver, instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)(outerReturn - 8)))));
	return null;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRefAndTarget:pc:cogMethod: */
static sqInt NoDbgRegParms
checkIfValidOopRefAndTargetpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    usqInt cacheTag1;
    sqInt entryPoint;
    sqInt entryPoint1;
    sqInt literal;
    sqInt offset1;
    sqInt pc;
    sqInt *sendTable1;
    sqInt tagCouldBeObj;
    CogMethod *targetMethod1;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (!(asserta(checkValidOopReference(literal)))) {
			return 1;
		}
		if ((couldBeObject(literal))
		 && (isReallyYoungObject(literal))) {
			if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
				return 2;
			}
		}
	}
	if (annotation >= IsSendCall) {
		if (!(asserta(isCMMethodEtAl(((CogBlockMethod *) (((CogMethod *) cogMethod))))))) {
			return 3;
		}
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		pc = pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8)));
		cacheTag1 = ((unsigned int) (long32At(pc)));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj = 0;
		entryPoint = entryPoint1;
		if (tagCouldBeObj) {
			if (couldBeObject(cacheTag1)) {
				if (!(asserta(checkValidOopReference(cacheTag1)))) {
					return 4;
				}
			}
			else {
				if (!(asserta(validInlineCacheTag(cacheTag1)))) {
					return 5;
				}
			}
			if ((couldBeObject(cacheTag1))
			 && (isReallyYoungObject(cacheTag1))) {
				if (!(asserta(((((CogMethod *) cogMethod))->cmRefersToYoung)))) {
					return 6;
				}
			}
		}
		else {
			if (entryPointTagIsSelector(entryPoint)) {
				if ((((int) cacheTag1)) < 0) {
					if ((-(((int) cacheTag1))) > NumSpecialSelectors) {
						return 7;
					}
				}
				else {
					if (cacheTag1 >= (literalCountOf((enumeratingCogMethod->methodObject)))) {
						return 8;
					}
				}
			}
			else {
				if (!(asserta(validInlineCacheTag(cacheTag1)))) {
					return 9;
				}
			}
		}
		if (entryPoint > methodZoneBase) {

			/* It's a linked send; find which kind. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (!(asserta((isCMMethodEtAl(((CogBlockMethod *) targetMethod1)))
				 || ((isCMClosedPIC(((CogBlockMethod *) targetMethod1)))
				 || (isCMOpenPIC(((CogBlockMethod *) targetMethod1))))))) {
				return 10;
			}
		}
	}
	return 0;
}


/*	Check for a valid object reference, if any, at a map entry. Answer a code
	unique to each error for debugging. */

	/* Cogit>>#checkIfValidOopRef:pc:cogMethod: */
static sqInt NoDbgRegParms
checkIfValidOopRefpccogMethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    sqInt entryPoint;
    sqInt literal;
    sqInt offset;
    sqInt offset1;
    sqInt pc;
    usqInt selectorOrCacheTag;
    sqInt *sendTable;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (!(checkValidOopReference(literal))) {
			print("object ref leak in CM ");
			printHex(((sqInt)cogMethod));
			print(" @ ");
			printHex(((sqInt)mcpc));
			eekcr();
			return 1;
		}
	}
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {
			offset = entryPoint;
		}
		else {
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable = superSendTrampolines;
					}
				}
			}
			offset = offset1;
		}
		/* begin instructionAt: */
		pc = pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8)));
		selectorOrCacheTag = ((unsigned int) (long32At(pc)));
		if ((entryPoint > methodZoneBase)
		 && ((offset != cmNoCheckEntryOffset)
		 && (!((((((CogMethod *) (entryPoint - offset)))->cmType)) == CMOpenPIC)))) {

			/* linked non-super send, cacheTag is a cacheTag */
			if (!(validInlineCacheTag(selectorOrCacheTag))) {
				print("cache tag leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" @ ");
				printHex(((sqInt)mcpc));
				eekcr();
				return 1;
			}
		}
		else {

			/* unlinked send or super send; cacheTag is a selector unless 64-bit, in which case it is an index. */
					}
	}
	return 0;
}


/*	Answer if all references to objects in machine-code are valid. */

	/* Cogit>>#checkIntegrityOfObjectReferencesInCode: */
sqInt
checkIntegrityOfObjectReferencesInCode(sqInt gcModes)
{
    CogMethod *cogMethod;
    sqInt count;
    sqInt ok;

	cogMethod = ((CogMethod *) methodZoneBase);
	ok = 1;
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((cogMethod->cmRefersToYoung)) {
				if (((count = occurrencesInYoungReferrers(cogMethod))) != 1) {
					print("young referrer CM ");
					printHex(((sqInt)cogMethod));
					if (count == 0) {
						print(" is not in youngReferrers");
						eekcr();
					}
					else {
						print(" is in youngReferrers ");
						printNum(count);
						print(" times!");
						eekcr();
					}
					ok = 0;
				}
			}
			if (!(checkValidOopReference((cogMethod->selector)))) {
				print("object leak in CM ");
				printHex(((sqInt)cogMethod));
				print(" selector");
				eekcr();
				ok = 0;
			}
			if (((cogMethod->cmType)) >= CMMethod) {
				assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
				if (!(checkValidObjectReference((cogMethod->methodObject)))) {
					print("object leak in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if (!(isOopCompiledMethod((cogMethod->methodObject)))) {
					print("non-method in CM ");
					printHex(((sqInt)cogMethod));
					print(" methodObject");
					eekcr();
					ok = 0;
				}
				if ((mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) != 0) {
					ok = 0;
				}
				if (((isYoungObject((cogMethod->methodObject)))
				 || (isYoung((cogMethod->selector))))
				 && (!((cogMethod->cmRefersToYoung)))) {
					print("CM ");
					printHex(((sqInt)cogMethod));
					print(" refers to young but not marked as such");
					eekcr();
					ok = 0;
				}
			}
			else {
				if (((cogMethod->cmType)) == CMClosedPIC) {
					if (!(checkValidObjectReferencesInClosedPIC(cogMethod))) {
						ok = 0;
					}
				}
				else {
					if (((cogMethod->cmType)) == CMOpenPIC) {
						if ((mapForperformUntilarg(cogMethod, checkIfValidOopRefpccogMethod, cogMethod)) != 0) {
							ok = 0;
						}
					}
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return ok;
}

	/* Cogit>>#checkMaybeObjRefInClosedPIC: */
static sqInt NoDbgRegParms
checkMaybeObjRefInClosedPIC(sqInt maybeObject)
{
	if (maybeObject == 0) {
		return 1;
	}
	if (!(couldBeObject(maybeObject))) {
		return 1;
	}
	return checkValidObjectReference(maybeObject);
}

	/* Cogit>>#checkValidObjectReferencesInClosedPIC: */
static sqInt NoDbgRegParms
checkValidObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt ok;
    sqInt pc;

	ok = 1;

	/* first we check the obj ref at the beginning of the CPIC */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		print("object leak in CPIC ");
		printHex(((sqInt)cPIC));
		print(" @ ");
		printHex(pc - (jumpLongByteSize(backEnd)));
		cr();
		ok = 0;
	}

	/* For each case we check any object reference at the end address - sizeof(conditional instruction) and then increment the end address by case size */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (!(checkMaybeObjRefInClosedPIC(literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			print("object leak in CPIC ");
			printHex(((sqInt)cPIC));
			print(" @ ");
			printHex(pc - (jumpLongConditionalByteSize(backEnd)));
			cr();
			ok = 0;
		}
		pc += cPICCaseSize;
	}
	return ok;
}


/*	i.e. this should never be called, so keep it out of the main path. */

	/* Cogit>>#cleanUpFailingCogCodeConstituents: */
static sqInt NoDbgRegParms NeverInline
cleanUpFailingCogCodeConstituents(CogMethod *cogMethodArg)
{
    CogMethod *cogMethod;

	cogMethod = cogMethodArg;
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMClosedPIC) {
			(cogMethod->methodObject = 0);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	popRemappableOop();
	return null;
}


/*	Answer if the ClosedPIC refers to any unmarked objects or freed/freeable
	target methods,
	applying markAndTraceOrFreeCogMethod:firstVisit: to those targets to
	determine if freed/freeable.
 */

	/* Cogit>>#closedPICRefersToUnmarkedObject: */
static sqInt NoDbgRegParms
closedPICRefersToUnmarkedObject(CogMethod *cPIC)
{
    sqInt i;
    usqInt object;
    sqInt pc;

	if (!((isImmediate((cPIC->selector)))
		 || (isMarked((cPIC->selector))))) {
		return 1;
	}
	pc = addressOfEndOfCaseinCPIC(1, cPIC);
	if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)))))) {
		if (!(isMarked(object))) {
			return 1;
		}
	}
	if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
		return 1;
	}
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (couldBeObject((object = literalBeforeFollowingAddress(backEnd, (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))))) {
			if (!(isMarked(object))) {
				return 1;
			}
		}
		if (markAndTraceOrFreePICTargetin(jumpLongTargetBeforeFollowingAddress(backEnd, pc), cPIC)) {
			return 1;
		}
	}
	return 0;
}

	/* Cogit>>#codeEntryFor: */
char *
codeEntryFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i + 1];
		}
	}
	return null;
}

	/* Cogit>>#codeEntryNameFor: */
char *
codeEntryNameFor(char *address)
{
    sqInt i;

	for (i = 0; i <= (trampolineTableIndex - 3); i += 2) {
		if (((address >= (trampolineAddresses[i + 1])) && (address <= ((trampolineAddresses[i + 3]) - 1)))) {
			return trampolineAddresses[i];
		}
	}
	return null;
}


/*	used e.g. in the platform's backtrace generators. Declared api to place it
	in cogit.h
 */

	/* Cogit>>#cogCodeBase */
sqInt
cogCodeBase(void)
{
	return codeBase;
}


/*	Answer the contents of the code zone as an array of pair-wise element,
	address in ascending address order.
	Answer a string for a runtime routine or abstract label (beginning, end,
	etc), a CompiledMethod for a CMMethod,
	or a selector (presumably a Symbol) for a PIC.
	If withDetails is true
	- answer machine-code to bytecode pc mapping information for methods
	- answer class, target pair information for closed PIC
	N.B. Since the class tag for the first case of a closed PIC is stored at
	the send site, it must be collected
	by scanning methods (see
	collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method:). Since closed PICs
	are never shared they always come after the method that references them,
	so we don't need an extra pass
	to collect the first case class tags, which are (temporarily) assigned to
	each closed PIC's methodObject field.
	But we do need to reset the methodObject fields to zero. This is done in
	createPICData:, unless memory
	runs out, in which case it is done by cleanUpFailingCogCodeConstituents:. */

	/* Cogit>>#cogCodeConstituents: */
sqInt
cogCodeConstituents(sqInt withDetails)
{
    CogMethod *cogMethod;
    sqInt constituents;
    sqInt count;
    sqInt i;
    sqInt label;
    sqInt profileData;
    sqInt value;


	/* + 3 for start, freeStart and end */
	count = (trampolineTableIndex / 2) + 3;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			count += 1;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	constituents = instantiateClassindexableSize(classArray(), count * 2);
	if (!constituents) {
		return constituents;
	}
	pushRemappableOop(constituents);
	if ((((label = stringForCString("CogCode"))) == null)
	 || (((value = positive64BitIntegerFor(codeBase))) == null)) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, constituents, label);
	storePointerUncheckedofObjectwithValue(1, constituents, value);
	for (i = 0; i < trampolineTableIndex; i += 2) {
		if ((((label = stringForCString(trampolineAddresses[i]))) == null)
		 || (((value = positive64BitIntegerFor(((usqInt)(trampolineAddresses[i + 1]))))) == null)) {
			popRemappableOop();
			return null;
		}
		storePointerUncheckedofObjectwithValue(2 + i, constituents, label);
		storePointerUncheckedofObjectwithValue(3 + i, constituents, value);
	}
	count = trampolineTableIndex + 2;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			profileData = (((cogMethod->cmType)) >= CMMethod
				? (cogMethod->methodObject)
				: (withDetails
					 && (((cogMethod->cmType)) == CMClosedPIC)
						? createCPICData(cogMethod)
						: (cogMethod->selector)));
			if (!profileData) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count, constituents, profileData);
			if (withDetails) {
				value = collectCogMethodConstituent(cogMethod);
			}
			else {
				value = positive64BitIntegerFor(((usqInt)cogMethod));
			}
			if (!value) {
				return cleanUpFailingCogCodeConstituents(cogMethod);
			}
			storePointerUncheckedofObjectwithValue(count + 1, constituents, value);
			count += 2;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if ((((label = stringForCString("CCFree"))) == null)
	 || (((value = positive64BitIntegerFor(mzFreeStart))) == null)) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count, constituents, label);
	storePointerUncheckedofObjectwithValue(count + 1, constituents, value);
	if ((((label = stringForCString("CCEnd"))) == null)
	 || (((value = positive64BitIntegerFor(limitAddress))) == null)) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(count + 2, constituents, label);
	storePointerUncheckedofObjectwithValue(count + 3, constituents, value);
	constituents = popRemappableOop();
	beRootIfOld(constituents);
	return constituents;
}


/*	Extend the cPIC with the supplied case. If caseNMethod is cogged dispatch
	direct to
	its unchecked entry-point. If caseNMethod is not cogged, jump to the fast
	interpreter dispatch, and if isMNUCase then dispatch to fast MNU
	invocation and mark the cPIC as
	having the MNU case for cache flushing. */

	/* Cogit>>#cogExtendPIC:CaseNMethod:tag:isMNUCase: */
static void NoDbgRegParms
cogExtendPICCaseNMethodtagisMNUCase(CogMethod *cPIC, sqInt caseNMethod, sqInt caseNTag, sqInt isMNUCase)
{
    sqInt address;
    usqInt addressFollowingJump;
    unsigned int codeInstruction;
    unsigned int dataInstruction;
    sqInt operand;
    CogBlockMethod * self_in_cpicHasMNUCase;
    sqInt target;

	compilationBreakpointclassTagisMNUCase((cPIC->selector), caseNTag, isMNUCase);
	assert(!(inlineCacheTagIsYoung(caseNTag)));
	assert((caseNMethod != null)
	 && (!(isYoung(caseNMethod))));
	if ((!isMNUCase)
	 && (methodHasCogMethod(caseNMethod))) {

		/* this isn't an MNU and we have an already cogged method to jump to */
		operand = 0;
		target = (((sqInt)(cogMethodOf(caseNMethod)))) + cmNoCheckEntryOffset;
	}
	else {
		operand = caseNMethod;
		if (isMNUCase) {

			/* this is an MNU so tag the CPIC header and setup a jump to the MNUAbort */
			/* begin cpicHasMNUCase: */
			self_in_cpicHasMNUCase = ((CogBlockMethod *) (((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta))));
			(self_in_cpicHasMNUCase->cpicHasMNUCaseOrCMIsFullBlock) = 1;
			target = (((sqInt)cPIC)) + (sizeof(CogMethod));
		}
		else {

			/* setup a jump to the interpretAborth so we can cog the target method */
			target = (((sqInt)cPIC)) + (picInterpretAbortOffset());
		}
	}
	address = addressOfEndOfCaseinCPIC(((cPIC->cPICNumCases)) + 1, cPIC);
	rewriteCPICCaseAttagobjReftarget(address, caseNTag, operand, target);
	/* begin rewriteCPIC:caseJumpTo: */
	addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	rewriteImm19JumpBeforetarget(((AbstractInstruction *) backEnd), addressFollowingJump, address - cPICCaseSize);
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = ((cPIC->cPICNumCases)) + 1);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)cPIC))), ((((usqInt)cPIC)) + closedPICSize) - (((usqInt)cPIC)));
	sys_icache_invalidate(((void *) (((usqInt)cPIC))), ((((usqInt)cPIC)) + closedPICSize) - (((usqInt)cPIC)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)cPIC), (((usqInt)cPIC)) + closedPICSize);
#  endif
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At((((usqInt)cPIC)) + cmNoCheckEntryOffset);
	dataInstruction = long32At(((((usqInt)cPIC)) + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
}


/*	Attempt to produce a machine code method for the bytecode method
	object aMethodObj. N.B. If there is no code memory available do *NOT*
	attempt to reclaim the method zone. Certain clients (e.g. ceSICMiss:)
	depend on the zone remaining constant across method generation. */

	/* Cogit>>#cogFullBlockMethod:numCopied: */
CogMethod *
cogFullBlockMethodnumCopied(sqInt aMethodObj, sqInt numCopied)
{
    CogMethod *cogMethod;

	assert(!((methodHasCogMethod(aMethodObj))));
	if (aMethodObj == breakMethod) {
		haltmsg("Compilation of breakMethod");
	}
	ensureNoForwardedLiteralsIn(aMethodObj);
	if (methodUsesAlternateBytecodeSet(aMethodObj)) {
		if ((numElementsIn(generatorTable)) <= 0x100) {
			return null;
		}
		bytecodeSetOffset = 0x100;
	}
	else {
		bytecodeSetOffset = 0;
	}
	assert(isFullBlockMethod(aMethodObj));
	methodObj = aMethodObj;
	methodHeader = methodHeaderOf(aMethodObj);

	/* lazy initialization */
	receiverTags = -1;
	cogMethod = compileCogFullBlockMethod(numCopied);
	if ((((((sqInt)cogMethod)) >= MaxNegativeErrorCode) && ((((sqInt)cogMethod)) <= -1))) {
		if ((((sqInt)cogMethod)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		/* begin maybeFreeCounters */
		if (counters != 0) {
			/* begin freeCounters: */
			if (counters != 0) {
				freeObject(counters - BaseHeaderSize);
			}
		}
		return null;
	}
	return cogMethod;
}

	/* Cogit>>#cogitPostGCAction: */
void
cogitPostGCAction(sqInt gcMode)
{
	if (gcMode == GCModeBecome) {
		followForwardedLiteralsInOpenPICList();
	}
	assert(allMethodsHaveCorrectHeader());
	assert(((!(gcMode & (GCModeFull + GCModeNewSpace))))
	 || (kosherYoungReferrers()));
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	Check that the header fields onf a non-free method are consistent with
	the type. Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#cogMethodDoesntLookKosher: */
static sqInt NoDbgRegParms
cogMethodDoesntLookKosher(CogMethod *cogMethod)
{
	if (((((cogMethod->blockSize)) & (BytesPerWord - 1)) != 0)
	 || ((((cogMethod->blockSize)) < (sizeof(CogMethod)))
	 || (((cogMethod->blockSize)) >= 0x8000))) {
		return 1;
	}
	if (((cogMethod->cmType)) == CMFree) {
		return 2;
	}
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!((((((cogMethod->methodHeader))) & 7) == 1))) {
			return 11;
		}
		if (!(couldBeObject((cogMethod->methodObject)))) {
			return 12;
		}
		if ((((cogMethod->stackCheckOffset)) > 0)
		 && (((cogMethod->stackCheckOffset)) < cmNoCheckEntryOffset)) {
			return 13;
		}
		if (((cogMethod->counters)) != 0) {
			if (!(couldBeDerivedObject((cogMethod->counters)))) {
				return 14;
			}
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (((cogMethod->blockSize)) != openPICSize) {
			return 21;
		}
		if (((cogMethod->methodHeader)) != 0) {
			return 22;
		}
		if (((cogMethod->objectHeader)) >= 0) {
			if (!((((cogMethod->methodObject)) == 0)
				 || (compactionInProgress
				 || (((cogMethod->methodObject)) == (((usqInt)(methodFor(((void *)((cogMethod->methodObject))))))))))) {
				return 23;
			}
		}
		if (((cogMethod->stackCheckOffset)) != 0) {
			return 24;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (((cogMethod->blockSize)) != closedPICSize) {
			return 0x1F;
		}
		if (!(((((cogMethod->cPICNumCases)) >= 1) && (((cogMethod->cPICNumCases)) <= MaxCPICCases)))) {
			return 32;
		}
		if (((cogMethod->methodHeader)) != 0) {
			return 33;
		}
		if (((cogMethod->methodObject)) != 0) {
			return 34;
		}
		return 0;
	}
	return 9;
}


/*	Attempt to create a one-case PIC for an MNU.
	The tag for the case is at the send site and so doesn't need to be
	generated. 
 */

	/* Cogit>>#cogMNUPICSelector:receiver:methodOperand:numArgs: */
CogMethod *
cogMNUPICSelectorreceivermethodOperandnumArgs(sqInt selector, sqInt rcvr, sqInt methodOperand, sqInt numArgs)
{
    CogMethod *actualPIC;
    unsigned int codeInstruction;
    unsigned int dataInstruction;
    usqInt startAddress;
    CogMethod *writablePIC;

	if ((isYoung(selector))
	 || ((inlineCacheTagForInstance(rcvr)) == 0 /* picAbortDiscriminatorValue */)) {
		return 0;
	}
	compilationBreakpointclassTagisMNUCase(selector, fetchClassTagOf(rcvr), 1);
	assert(endCPICCase0 != null);
	startAddress = allocate(closedPICSize);
	if (startAddress == 0) {
		callForCogCompiledCodeCompaction();
		return 0;
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE

	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);
	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	(writablePIC->counters = 0);
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = initialClosedPICUsageCount());
	/* begin cpicHasMNUCase: */
	((((CogBlockMethod *) writablePIC))->cpicHasMNUCaseOrCMIsFullBlock) = 1;
	(writablePIC->cPICNumCases = 1);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 1);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureMNUCPICmethodOperandnumArgsdelta((actualPIC = ((CogMethod *) startAddress)), methodOperand, numArgs, startAddress - (((usqInt)cPICPrototype)));
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) startAddress), (startAddress + closedPICSize) - startAddress);
	sys_icache_invalidate(((void *) startAddress), (startAddress + closedPICSize) - startAddress);
#  else // __APPLE__ && __MACH__
	ceFlushICache(startAddress, startAddress + closedPICSize);
#  endif
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At(startAddress + cmNoCheckEntryOffset);
	dataInstruction = long32At((startAddress + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
	return actualPIC;
}


/*	Create an Open PIC. Temporarily create a direct call of
	ceSendFromOpenPIC:. Should become a probe of the first-level method lookup
	cache followed by a
	call of ceSendFromOpenPIC: if the probe fails. */

	/* Cogit>>#cogOpenPICSelector:numArgs: */
static CogMethod * NoDbgRegParms
cogOpenPICSelectornumArgs(sqInt selector, sqInt numArgs)
{
    unsigned int codeInstruction;
    sqInt codeSize;
    unsigned int dataInstruction;
    sqInt end;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;
    CogMethod *pic;
    usqInt startAddress;

	compilationBreakpointisMNUCase(selector, 0);
	startAddress = allocate(openPICSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	(methodLabel->address = startAddress);
	(methodLabel->dependent = null);
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	compileOpenPICnumArgs(selector, numArgs);
	computeMaximumSizes();
	concretizeAt(methodLabel, startAddress);
	codeSize = generateInstructionsAt(startAddress + (sizeof(CogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	mapSize = generateMapAtstart((startAddress + openPICSize) - 1, startAddress + cmNoCheckEntryOffset);
	assert((((entry->address)) - startAddress) == cmEntryOffset);
	assert(((roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpLength(mapSize))) <= openPICSize);
	end = outputInstructionsAt(startAddress + (sizeof(CogMethod)));
	/* begin fillInOPICHeader:numArgs:selector: */
	pic = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	(pic->counters = 0);
	(pic->cmType = CMOpenPIC);
	(pic->objectHeader = 0);
	(pic->blockSize = openPICSize);
	addToOpenPICList(pic);
	(pic->methodHeader = 0);
	(pic->selector = selector);
	(pic->cmNumArgs = numArgs);
	(pic->cmHasMovableLiteral = isNonImmediate(selector));
	if ((pic->cmRefersToYoung = isYoung(selector))) {
		addToYoungReferrers(pic);
	}
	(pic->cmUsageCount = initialOpenPICUsageCount());
	/* begin cpicHasMNUCase: */
	((((CogBlockMethod *) pic))->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(pic->cPICNumCases = 0);
	(pic->blockEntryOffset = 0);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) ((((usqInt)pic)) - codeToDataDelta)), (((((usqInt)pic)) - codeToDataDelta) + openPICSize) - ((((usqInt)pic)) - codeToDataDelta));
	sys_icache_invalidate(((void *) ((((usqInt)pic)) - codeToDataDelta)), (((((usqInt)pic)) - codeToDataDelta) + openPICSize) - ((((usqInt)pic)) - codeToDataDelta));
#  else // __APPLE__ && __MACH__
	ceFlushICache((((usqInt)pic)) - codeToDataDelta, ((((usqInt)pic)) - codeToDataDelta) + openPICSize);
#  endif
	assert(isCMOpenPIC(((CogBlockMethod *) pic)));
	assert(((pic->selector)) == selector);
	assert(((pic->cmNumArgs)) == numArgs);
	assert((callTargetFromReturnAddress(backEnd, ((((sqInt)pic)) - codeToDataDelta) + missOffset)) == (picAbortTrampolineFor(numArgs)));
	assert(openPICSize == (roundUpLength(openPICSize)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At(((((usqInt)pic)) - codeToDataDelta) + cmNoCheckEntryOffset);
	dataInstruction = long32At((((((usqInt)pic)) - codeToDataDelta) + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
	return ((CogMethod *) startAddress);
}


/*	Attempt to create a two-case PIC for case0CogMethod and
	case1Method,case1Tag. The tag for case0CogMethod is at the send site and
	so doesn't need to be generated.
	case1Method may be any of
	- a Cog method; link to its unchecked entry-point
	- a CompiledMethod; link to ceInterpretMethodFromPIC:
	- a CompiledMethod; link to ceMNUFromPICMNUMethod:receiver: */

	/* Cogit>>#cogPICSelector:numArgs:Case0Method:Case1Method:tag:isMNUCase: */
static CogMethod * NoDbgRegParms
cogPICSelectornumArgsCase0MethodCase1MethodtagisMNUCase(sqInt selector, sqInt numArgs, CogMethod *case0CogMethod, sqInt case1MethodOrNil, sqInt case1Tag, sqInt isMNUCase)
{
    CogMethod *actualPIC;
    usqInt startAddress;
    CogMethod *writablePIC;

	if (isYoung(selector)) {
		return ((CogMethod *) YoungSelectorInPIC);
	}
	compilationBreakpointclassTagisMNUCase(selector, case1Tag, isMNUCase);
	startAddress = allocate(closedPICSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	maybeBreakGeneratingFromto(startAddress, startAddress + closedPICSize);

	/* memcpy the prototype across to our allocated space; because anything else would be silly */
	writablePIC = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	codeMemcpy(writablePIC, cPICPrototype, closedPICSize);
	/* begin fillInCPICHeader:numArgs:numCases:hasMNUCase:selector: */
	(writablePIC->counters = 0);
	assert(!(isYoung(selector)));
	(writablePIC->cmType = CMClosedPIC);
	(writablePIC->objectHeader = 0);
	(writablePIC->blockSize = closedPICSize);
	(writablePIC->methodObject = 0);
	(writablePIC->methodHeader = 0);
	(writablePIC->selector = selector);
	(writablePIC->cmNumArgs = numArgs);
	(writablePIC->cmHasMovableLiteral = 0);
	(writablePIC->cmRefersToYoung = 0);
	(writablePIC->cmUsageCount = initialClosedPICUsageCount());
	/* begin cpicHasMNUCase: */
	((((CogBlockMethod *) writablePIC))->cpicHasMNUCaseOrCMIsFullBlock) = isMNUCase;
	(writablePIC->cPICNumCases = 2);
	(writablePIC->blockEntryOffset = 0);
	assert(isCMClosedPIC(((CogBlockMethod *) writablePIC)));
	assert(((writablePIC->selector)) == selector);
	assert(((writablePIC->cmNumArgs)) == numArgs);
	assert(((writablePIC->cPICNumCases)) == 2);
	assert(closedPICSize == (roundUpLength(closedPICSize)));
	configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta((actualPIC = ((CogMethod *) startAddress)), case0CogMethod, case1MethodOrNil, case1Tag, isMNUCase, numArgs, startAddress - (((usqInt)cPICPrototype)));
	assert((callTargetFromReturnAddress(backEnd, startAddress + missOffset)) == (picAbortTrampolineFor(numArgs)));
	return actualPIC;
}


/*	Attempt to produce a machine code method for the bytecode method
	object aMethodObj. N.B. If there is no code memory available do *NOT*
	attempt to reclaim the method zone. Certain clients (e.g. ceSICMiss:)
	depend on the zone remaining constant across method generation. */

	/* Cogit>>#cog:selector: */
CogMethod *
cogselector(sqInt aMethodObj, sqInt aSelectorOop)
{
    CogMethod *cogMethod;
    sqInt selector;

	assert(!((methodHasCogMethod(aMethodObj))));

	/* coInterpreter stringOf: selector */
	selector = (aSelectorOop == (nilObject())
		? maybeSelectorOfMethod(aMethodObj)
		: aSelectorOop);
	if (!(selector == null)) {
		compilationBreakpointisMNUCase(selector, 0);
	}
	if (aMethodObj == breakMethod) {
		haltmsg("Compilation of breakMethod");
	}
	ensureNoForwardedLiteralsIn(aMethodObj);
	if (methodUsesAlternateBytecodeSet(aMethodObj)) {
		if ((numElementsIn(generatorTable)) <= 0x100) {
			return null;
		}
		bytecodeSetOffset = 0x100;
	}
	else {
		bytecodeSetOffset = 0;
	}
	assert(!((isFullBlockMethod(aMethodObj))));
	methodObj = aMethodObj;
	methodHeader = methodHeaderOf(aMethodObj);

	/* lazy initialization */
	receiverTags = -1;
	cogMethod = compileCogMethod(aSelectorOop);
	if ((((((sqInt)cogMethod)) >= MaxNegativeErrorCode) && ((((sqInt)cogMethod)) <= -1))) {
		if ((((sqInt)cogMethod)) == InsufficientCodeSpace) {
			callForCogCompiledCodeCompaction();
		}
		/* begin maybeFreeCounters */
		if (counters != 0) {
			/* begin freeCounters: */
			if (counters != 0) {
				freeObject(counters - BaseHeaderSize);
			}
		}
		return null;
	}
	return cogMethod;
}

	/* Cogit>>#collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
static sqInt NoDbgRegParms
collectCogConstituentForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt address;
    sqInt annotation;
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;

	if (!descriptor) {
		return 0;
	}
	if (!((descriptor->isMapped))) {
		return 0;
	}
	address = positive64BitIntegerFor(((usqInt)mcpc));
	if (!address) {
		return PrimErrNoMemory;
	}
	storePointerUncheckedofObjectwithValue(cogConstituentIndex, topRemappableOop(), address);
	storePointerUncheckedofObjectwithValue(cogConstituentIndex + 1, topRemappableOop(), (((usqInt)bcpc << 3) | 1));

	/* Collect any first case classTags for closed PICs. */
	cogConstituentIndex += 2;
	if (((!(isBackwardBranchAndAnnotation & 1)))
	 && (((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= IsSendCall))) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* send is linked */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			annotation = ((usqInt)(isBackwardBranchAndAnnotation)) >> 1;
			/* begin offsetAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmType)) == CMClosedPIC) {
				(targetMethod1->methodObject = classForInlineCacheTag(instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))))));
			}
		}
	}
	return 0;
}


/*	Answer a description of the mapping between machine code pointers and
	bytecode pointers for the Cog Method.
	First value is the address of the cog method.
	Following values are pairs of machine code pc and bytecode pc */

	/* Cogit>>#collectCogMethodConstituent: */
static sqInt NoDbgRegParms
collectCogMethodConstituent(CogMethod *cogMethod)
{
    sqInt address;
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    CogBlockMethod *cogBlockMethod;
    sqInt data;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt nSlots;
    sqInt prim;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	if (!(((cogMethod->cmType)) >= CMMethod)) {
		return positive64BitIntegerFor(((usqInt)cogMethod));
	}
	cogBlockMethod = ((CogBlockMethod *) cogMethod);
	if (((cogBlockMethod->stackCheckOffset)) == 0) {

		/* isFrameless ? */
		return positive64BitIntegerFor(((usqInt)cogMethod));
	}

	/* +1 for first address */
	nSlots = ((((byteSizeOf((cogMethod->methodObject))) - (startPCOfMethodHeader((cogMethod->methodHeader)))) * 2) + (minSlotsForShortening())) + 1;
	data = instantiateClassindexableSize(splObj(ClassArray), nSlots);
	if (!data) {
		return null;
	}
	pushRemappableOop(data);
	address = positive64BitIntegerFor(((usqInt)cogMethod));
	if (!address) {
		popRemappableOop();
		return null;
	}
	storePointerUncheckedofObjectwithValue(0, topRemappableOop(), address);
	cogConstituentIndex = 1;
	/* begin mapFor:bcpc:performUntil:arg: */
	startbcpc = startPCOfMethod((cogMethod->methodObject));
	assert(((cogBlockMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset));
	result = collectCogConstituentForAnnotationMcpcBcpcMethod(null, (0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))), (((char *) mcpc)), startbcpc, (((void *)cogMethod)));
	if (result != 0) {
		errCode = result;
		goto l9;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogBlockMethod->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = (cogBlockMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogBlockMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogBlockMethod->startpc)));
		homeMethod = cmHomeMethod(cogBlockMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogBlockMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj))
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l9;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l9;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc - 1, methodObj);
							upperByte = fetchByteofObject(bcpc - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = collectCogConstituentForAnnotationMcpcBcpcMethod(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc)), ((isBackwardBranch
	? bcpc - (2 * nExts)
	: bcpc)), (((void *)cogMethod)));
				if (result != 0) {
					errCode = result;
					goto l9;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	errCode = 0;
	l9:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode != 0) {
		popRemappableOop();
		return null;
	}
	if (cogConstituentIndex < nSlots) {
		shortentoIndexableSize(topRemappableOop(), cogConstituentIndex);
	}
	return popRemappableOop();
}

	/* Cogit>>#compactCogCompiledCode */
void
compactCogCompiledCode(void)
{
    sqInt endAddress;

	assertValidDualZone();
	assert(noCogMethodsMaximallyMarked());
	moveProfileToMethods();
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	markActiveMethodsAndReferents();
	freeOlderMethodsForCompaction();
	compactPICsWithFreedTargets();
	planCompaction();
	updateStackZoneReferencesToCompiledCodePreCompaction();
	relocateMethodsPreCompaction();
	assertValidDualZone();
	compactCompiledCode();
	stopsFromto(backEnd, freeStart(), (youngReferrers()) - 1);
	/* begin flushICacheFrom:to: */
	endAddress = ((usqInt)(youngReferrers()));
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
	sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)methodZoneBase), endAddress);
#  endif
	assert(allMethodsHaveCorrectHeader());
	assert(kosherYoungReferrers());
	assertValidDualZone();
}

	/* Cogit>>#compactPICsWithFreedTargets */
static void
compactPICsWithFreedTargets(void)
{
    CogMethod *cogMethod;
    sqInt count;

	cogMethod = ((CogMethod *) methodZoneBase);
	count = 0;
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) == CMClosedPIC)
		 && (cPICCompactAndIsNowEmpty(cogMethod))) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMFree);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		count += 1;
	}
	assert(count == (numMethods()));
}


/*	The start of a CogMethod has a call to a run-time abort routine that
	either handles an in-line cache failure or a stack overflow. The routine
	selects the
	path depending on ReceiverResultReg; if zero it takes the stack overflow
	path; if nonzero the in-line cache miss path. Neither of these paths
	returns. The abort routine must be called; In the callee the method is
	located by
	adding the relevant offset to the return address of the call.
	
	N.B. This code must match that in compilePICAbort: so that the offset of
	the return address of the call is the same in methods and closed PICs. */

	/* Cogit>>#compileAbort */
static AbstractInstruction *
compileAbort(void)
{
    AbstractInstruction *anInstruction;
    sqInt callTarget;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	stackOverflowCall = anInstruction;
	
	/* If there is a link register it must be saved (pushed onto the stack) before it
	   is smashed by the abort call, and hence needs to be manually handled here */
	sendMiss = genoperand(PushR, LinkReg);
	/* begin Call: */
	callTarget = methodAbortTrampolineFor(methodOrBlockNumArgs);
	return genoperand(Call, callTarget);
}

	/* Cogit>>#compileBlockDispatchFrom:to: */
static sqInt NoDbgRegParms
compileBlockDispatchFromto(sqInt lowBlockStartIndex, sqInt highBlockStartIndex)
{
    AbstractInstruction *anInstruction;
    BlockStart *blockStart;
    sqInt halfWay;
    AbstractInstruction *jmp;
    sqInt literal;

	if (lowBlockStartIndex == highBlockStartIndex) {
		blockStart = blockStartAt(lowBlockStartIndex);
		genoperand(Jump, ((sqInt)((blockStart->entryLabel))));
		return null;
	}
	halfWay = (highBlockStartIndex + lowBlockStartIndex) / 2;
	assert(((halfWay >= lowBlockStartIndex) && (halfWay <= highBlockStartIndex)));

	/* N.B. FLAGS := TempReg - startpc */
	blockStart = blockStartAt(halfWay);
	/* begin checkQuickConstant:forInstruction: */
	literal = (((usqInt)(((blockStart->startpc)) + 1) << 3) | 1);
	anInstruction = genoperandoperand(CmpCqR, (((usqInt)(((blockStart->startpc)) + 1) << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	if (lowBlockStartIndex == halfWay) {
		genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)((blockStart->entryLabel))));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
		return null;
	}
	if ((halfWay + 1) == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);
		genConditionalBranchoperand(JumpGreater, ((sqInt)((blockStart->entryLabel))));
		return compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	}
	jmp = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	compileBlockDispatchFromto(lowBlockStartIndex, halfWay);
	if (halfWay == highBlockStartIndex) {
		blockStart = blockStartAt(highBlockStartIndex);
		jmpTarget(jmp, (blockStart->entryLabel));
	}
	else {
		jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileBlockDispatchFromto(halfWay + 1, highBlockStartIndex);
	}
	return 0;
}


/*	Compile a block's entry. This looks like a dummy CogBlockMethod header
	(for frame parsing)
	followed by either a frame build, if a frame is required, or nothing. The
	CogMethodHeader's objectHeader field is a back pointer to the method, but
	this can't be filled in until code generation. */

	/* Cogit>>#compileBlockEntry: */
static void NoDbgRegParms
compileBlockEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    sqInt alignment;

	/* begin AlignmentNops: */
	alignment = blockAlignment();
	genoperand(AlignmentNops, alignment);
	(blockStart->fakeHeader = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	switch (sizeof(CogBlockMethod)) {
	case 8:
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	case 12:
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	case 16:
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		genoperand(Fill32, 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	(blockStart->entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (needsFrame) {
		compileBlockFrameBuild(blockStart);
		if (recordBlockTrace()) {
			/* begin CallRT: */
			abstractInstruction = genoperand(Call, ceTraceBlockActivationTrampoline);
			(abstractInstruction->annotation = IsRelativeCall);
		}
	}
	else {
		compileBlockFramelessEntry(blockStart);
	}
}


/*	Generate a call to aRoutine with up to 4 arguments. If resultRegOrNone is
	not NoReg assign the C result to resultRegOrNone. If saveRegs, save all
	registers. Hack: a negative arg value indicates an abstract register, a
	non-negative value
	indicates a constant. The encoding for constants is defined by
	trampolineArgConstant: & trampolineArgValue:. Pass a constant as the
	result of trampolineArgConstant:. */

	/* Cogit>>#compileCallFor:numArgs:arg:arg:arg:arg:resultReg:regsToSave: */
static void NoDbgRegParms
compileCallFornumArgsargargargargresultRegregsToSave(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt resultRegOrNone, sqInt regMask)
{
    const int cStackAlignment = STACK_ALIGN_BYTES;
    sqInt regsToSave;

	regsToSave = (resultRegOrNone == NoReg
		? regMask
		: ((regMask | (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))) - (((resultRegOrNone < 0) ? (((usqInt)(1)) >> (-resultRegOrNone)) : (1ULL << resultRegOrNone)))));
	if (cStackAlignment > BytesPerWord) {
		/* begin genAlignCStackSavingRegisters:numArgs:wordAlignment: */
	}
	genSaveRegs(backEnd, regsToSave);
	genMarshallNArgsargargargarg(backEnd, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3);
	/* begin gen:literal: */
	checkLiteralforInstruction(((usqInt)aRoutine), genoperand(CallFull, ((usqInt)aRoutine)));
	genWriteCResultIntoReg(backEnd, resultRegOrNone);
	/* begin genRemoveNArgsFromStack: */
	assert(numArgs <= 6);
	genRestoreRegs(backEnd, regsToSave);
}


/*	Compile the cache tag computation and the first comparison. Answer the
	address of that comparison. */

	/* Cogit>>#compileCPICEntry */
static AbstractInstruction *
compileCPICEntry(void)
{
	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, TempReg);
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}


/*	Compile the abstract instructions for the entire full block method. */

	/* Cogit>>#compileEntireFullBlockMethod: */
static sqInt NoDbgRegParms
compileEntireFullBlockMethod(sqInt numCopied)
{
    sqInt result;

	/* begin preenMethodLabel */
	(((((AbstractInstruction *) methodLabel))->operands))[1] = 0;
	compileFullBlockEntry();
	compileFullBlockMethodFrameBuild(numCopied);
	if (((result = compileMethodBody())) < 0) {
		return result;
	}
	assert(blockCount == 0);
	return 0;
}


/*	The entry code to a method checks that the class of the current receiver
	matches that in the inline cache. Other non-obvious elements are that its
	alignment must be
	different from the alignment of the noCheckEntry so that the method map
	machinery can distinguish normal and super sends (super sends bind to the
	noCheckEntry).  */

	/* Cogit>>#compileEntry */
static void
compileEntry(void)
{
    AbstractInstruction *inst;

	entry = genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, TempReg, 1);
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, TempReg);
	genConditionalBranchoperand(JumpNonZero, ((sqInt)sendMiss));
	noCheckEntry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (((traceFlags & 64) == 64)) {
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);
		/* begin gen:literal: */
		checkLiteralforInstruction(ceTraceLinkedSendTrampoline, genoperand(CallFull, ceTraceLinkedSendTrampoline));
		genoperand(PopR, LinkReg);
	}
}


/*	Compile the abstract instructions for the entire method, including blocks. */
/*	Abort for stack overflow on full block activation (no inline cache miss
	possible). The flag is SendNumArgsReg. */

	/* Cogit>>#compileFullBlockEntry */
static sqInt
compileFullBlockEntry(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt callTarget;
    AbstractInstruction *jumpNoContextSwitch;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	stackOverflowCall = anInstruction;
	/* begin PushR: */
	genoperand(PushR, LinkReg);
	/* begin Call: */
	callTarget = methodAbortTrampolineFor(methodOrBlockNumArgs);
	genoperand(Call, callTarget);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	fullBlockNoContextSwitchEntry = anInstruction1;
	jumpNoContextSwitch = genoperand(Jump, ((sqInt)0));
	/* begin AlignmentNops: */
	genoperand(AlignmentNops, ((BytesPerWord < 8) ? 8 : BytesPerWord));
	fullBlockEntry = genoperandoperand(MoveRR, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jumpNoContextSwitch, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Compile the top-level method body. */

	/* Cogit>>#compileMethodBody */
static sqInt
compileMethodBody(void)
{
	if (endPC < initialPC) {
		return 0;
	}
	return compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
}


/*	The start of a PIC has a call to a run-time abort routine that either
	handles a dispatch to an
	interpreted method or a dispatch of an MNU case. The routine selects the
	path by testing
	ClassReg, which holds the inline cache tag; if equal to the
	picAbortDiscriminatorValue (zero)
	it takes the MNU path; if nonzero the dispatch to interpreter path.
	Neither of these paths
	returns. The abort routine must be called; In the callee the PIC is
	located by adding the
	relevant offset to the return address of the call.
	
	N.B. This code must match that in compileAbort so that the offset of the
	return address of
	the call is the same in methods and closed PICs. */

	/* Cogit>>#compilePICAbort: */
static sqInt NoDbgRegParms
compilePICAbort(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    sqInt callTarget;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, 0 /* picAbortDiscriminatorValue */, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0 /* picAbortDiscriminatorValue */, BytesPerOop));
	}
	picMNUAbort = anInstruction;
	
	/* If there is a link register it must be saved (pushed onto the stack) before it
	   is smashed by the abort call, and hence needs to be manually handled here */
	picInterpretAbort = genoperand(PushR, LinkReg);
	/* begin Call: */
	callTarget = picAbortTrampolineFor(numArgs);
	genoperand(Call, callTarget);
	return 0;
}


/*	Compile the compare of stackLimit against the stack pointer, jumping to
	the stackOverflowCall if
	the stack pointer is below the limit. Answer a bytecode annotated label
	that follows the sequence.
	
	The stack check functions both as a genuine stack limit check to prevent
	calls overflowing stack pages,
	and as an event/context-switch break out. To cause an event check
	(including a check for a required
	context switch), stackLimit is set to the highest possible value, and
	hence all stack limit checks will
	fail. A path in the stack overflow abort then arranges to call event
	checking if it has been requested.
	
	Certain block activations (e.g. valueNoContextSwitch:) must not context
	switch, and in that
	case, SendNumArgs is set to zero to communicate to the stack overflow
	abort that it should
	not perform event/context-switch (yet). */

	/* Cogit>>#compileStackOverflowCheck: */
static AbstractInstruction * NoDbgRegParms
compileStackOverflowCheck(sqInt canContextSwitch)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSkip;
    AbstractInstruction *label;
    sqInt operandOne;

	/* begin gen:literal:operand: */
	operandOne = stackLimitAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, TempReg));
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SPReg);
	if (canContextSwitch) {
		genConditionalBranchoperand(JumpBelow, ((sqInt)stackOverflowCall));
		label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	else {
		jumpSkip = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		genoperand(Jump, ((sqInt)stackOverflowCall));
		jmpTarget(jumpSkip, (label = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	}
	/* begin annotateBytecode: */
	(label->annotation = HasBytecodePC);
	return label;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutine
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C
	result back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#compileTrampolineFor:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg: */
static void NoDbgRegParms
compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(void *aRoutine, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone)
{
	genSmalltalkToCStackSwitch(pushLinkReg);
	compileCallFornumArgsargargargargresultRegregsToSave(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, resultRegOrNone, regMask);
	genLoadStackPointers(backEnd);
	genTrampolineReturn(pushLinkReg);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeEntryOffsets */
static void
computeEntryOffsets(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    AbstractInstruction *sendMissCall;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 24;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	methodOrBlockNumArgs = 0;
	sendMissCall = compileAbort();
	compileEntry();
	computeMaximumSizes();
	generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	cmEntryOffset = ((entry->address)) - methodZoneBase;
	cmNoCheckEntryOffset = ((noCheckEntry->address)) - methodZoneBase;
	missOffset = (((sendMissCall->address)) + ((sendMissCall->machineCodeSize))) - methodZoneBase;
	entryPointMask = BytesPerWord - 1;
	while ((cmEntryOffset & entryPointMask) == (cmNoCheckEntryOffset & entryPointMask)) {
		entryPointMask = (entryPointMask + entryPointMask) + 1;
	}
	if (entryPointMask >= (roundUpToMethodAlignment(backEnd(), 1))) {
		error("cannot differentiate checked and unchecked entry-points with current cog method alignment");
	}
	checkedEntryAlignment = cmEntryOffset & entryPointMask;
	uncheckedEntryAlignment = cmNoCheckEntryOffset & entryPointMask;
	assert(checkedEntryAlignment != uncheckedEntryAlignment);
}


/*	Generate the entry code for a method to determine cmEntryOffset and
	cmNoCheckEntryOffset. We
	need cmNoCheckEntryOffset up front to be able to generate the map starting
	from cmNoCheckEntryOffset */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#computeFullBlockEntryOffsets */
static void
computeFullBlockEntryOffsets(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 24;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	methodOrBlockNumArgs = 0;
	compileFullBlockEntry();
	computeMaximumSizes();
	generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	cbEntryOffset = ((fullBlockEntry->address)) - methodZoneBase;
	cbNoSwitchEntryOffset = ((fullBlockNoContextSwitchEntry->address)) - methodZoneBase;
}


/*	While we order variables in the CoInterpreter in order of dynamic
	frequency, and hence
	expect that stackPointer will be output first, C optimizers and linkers
	may get their own
	ideas and ``improve upon'' this ordering. So we cannot depend on
	stackPointer being
	at the lowest address of the variables we want to access through
	VarBaseReg. Here we
	choose the minimum amongst a set to try to choose a varBaseAddress that is
	just less
	than but iwht in range of all variables we want to access through it. */

	/* Cogit>>#computeGoodVarBaseAddress */
static usqInt
computeGoodVarBaseAddress(void)
{
    usqInt minAddress;


	/* stackLimit is e.g. lowest using the clang toolchain on MacOS X */
	minAddress = stackLimitAddress();
	if ((stackPointerAddress()) < minAddress) {
		minAddress = stackPointerAddress();
	}
	if ((framePointerAddress()) < minAddress) {
		minAddress = framePointerAddress();
	}
	if ((instructionPointerAddress()) < minAddress) {
		minAddress = instructionPointerAddress();
	}
	if ((argumentCountAddress()) < minAddress) {
		minAddress = argumentCountAddress();
	}
	if ((primFailCodeAddress()) < minAddress) {
		minAddress = primFailCodeAddress();
	}
	return minAddress;
}


/*	This pass assigns maximum sizes to all abstract instructions and
	eliminates jump fixups.
	It hence assigns the maximum address an instruction will occur at which
	allows the next
	pass to conservatively size jumps. */

	/* Cogit>>#computeMaximumSizes */
static void
computeMaximumSizes(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt relativeAddress;

	dumpLiterals(0);
	relativeAddress = 0;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		(abstractInstruction->address = relativeAddress);
		(abstractInstruction->maxSize = computeMaximumSize(abstractInstruction));
		relativeAddress += (abstractInstruction->maxSize);
	}
}


/*	Configure a copy of the prototype CPIC for a two-case PIC for 
	case0CogMethod and
	case1Method
	case1Tag.
	The tag for case0CogMethod is at the send site and so doesn't need to be
	generated. case1Method may be any of
	- a Cog method; jump to its unchecked entry-point
	- a CompiledMethod; jump to the ceInterpretFromPIC trampoline
	- nil; call ceMNUFromPIC
	addDelta is the address change from the prototype to the new CPIC
	location, needed
	because the loading of the CPIC label at the end may use a literal instead
	of a pc relative load. */
/*	self disassembleFrom: cPIC asInteger + (self sizeof: CogMethod) to: cPIC
	asInteger + closedPICSize
 */

	/* Cogit>>#configureCPIC:Case0:Case1Method:tag:isMNUCase:numArgs:delta: */
static sqInt NoDbgRegParms
configureCPICCase0Case1MethodtagisMNUCasenumArgsdelta(CogMethod *cPIC, CogMethod *case0CogMethod, sqInt case1Method, sqInt case1Tag, sqInt isMNUCase, sqInt numArgs, sqInt addrDelta)
{
    sqInt callTargetAddress;
    sqInt caseEndAddress;
    sqInt operand;
    sqInt pc;
    sqInt targetEntry;

	assert(case1Method != null);
	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));
	assert(!(inlineCacheTagIsYoung(case1Tag)));
	if ((!isMNUCase)
	 && (methodHasCogMethod(case1Method))) {
		operand = 0;
		targetEntry = (((sqInt)(cogMethodOf(case1Method)))) + cmNoCheckEntryOffset;
	}
	else {

		/* We do not scavenge PICs, hence we cannot cache the MNU method if it is in new space. */
		operand = ((case1Method == null)
		 || (isYoungObject(case1Method))
			? 0
			: case1Method);
		targetEntry = (case1Method == null
			? (((sqInt)cPIC)) + (sizeof(CogMethod))
			: (((sqInt)cPIC)) + (picInterpretAbortOffset()));
	}
	/* begin rewriteJumpLongAt:target: */
	rewriteImm26JumpBeforetarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + firstCPICCaseOffset, (((sqInt)case0CogMethod)) + cmNoCheckEntryOffset);

	/* update the cpic case */
	caseEndAddress = addressOfEndOfCaseinCPIC(2, cPIC);
	rewriteCPICCaseAttagobjReftarget(caseEndAddress, case1Tag, operand, ((sqInt)((isMNUCase
	? (((sqInt)cPIC)) + (sizeof(CogMethod))
	: targetEntry))));
	/* begin relocateMethodReferenceBeforeAddress:by: */
	pc = ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd));
	assert((instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 4)))
	 || (instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 8))));
	/* begin rewriteJumpLongAt:target: */
	callTargetAddress = cPICMissTrampolineFor(numArgs);
	rewriteImm26JumpBeforetarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + cPICEndOfCodeOffset, callTargetAddress);
	return 0;
}


/*	Configure a copy of the prototype CPIC for a one-case MNU CPIC that calls
	ceMNUFromPIC for
	case0Tag The tag for case0 is at the send site and so doesn't need to be
	generated. addDelta is the address change from the prototype to the new
	CPIC location, needed
	because the loading of the CPIC label at the end may be a literal instead
	of a pc-relative load. */
/*	adjust the jump at missOffset, the ceAbortXArgs */

	/* Cogit>>#configureMNUCPIC:methodOperand:numArgs:delta: */
static sqInt NoDbgRegParms
configureMNUCPICmethodOperandnumArgsdelta(CogMethod *cPIC, sqInt methodOperand, sqInt numArgs, sqInt addrDelta)
{
    usqInt addressFollowingJump;
    sqInt callTargetAddress;
    sqInt callTargetAddress1;
    sqInt operand;
    sqInt pc;
    sqInt target;

	rewriteCallAttarget(backEnd, (((sqInt)cPIC)) + missOffset, picAbortTrampolineFor(numArgs));

	/* set the jump to the case0 method */
	operand = ((methodOperand == null)
	 || (isYoungObject(methodOperand))
		? 0
		: methodOperand);
	/* begin rewriteJumpLongAt:target: */
	callTargetAddress = (((sqInt)cPIC)) + (sizeof(CogMethod));
	rewriteImm26JumpBeforetarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + firstCPICCaseOffset, callTargetAddress);
	storeLiteralbeforeFollowingAddress(backEnd, operand, ((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd)));
	/* begin rewriteJumpLongAt:target: */
	callTargetAddress1 = cPICMissTrampolineFor(numArgs);
	rewriteImm26JumpBeforetarget(((AbstractInstruction *) backEnd), (((sqInt)cPIC)) + cPICEndOfCodeOffset, callTargetAddress1);
	/* begin relocateMethodReferenceBeforeAddress:by: */
	pc = ((((sqInt)cPIC)) + cPICEndOfCodeOffset) - (jumpLongByteSize(backEnd));
	assert((instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 4)))
	 || (instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 8))));
	/* begin rewriteCPIC:caseJumpTo: */
	target = addressOfEndOfCaseinCPIC(2, cPIC);
	/* begin rewriteCPICJumpAt:target: */
	addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	rewriteImm19JumpBeforetarget(((AbstractInstruction *) backEnd), addressFollowingJump, target);
	return 0;
}


/*	Scan the CPIC for target methods that have been freed and eliminate them.
	Since the first entry cannot be eliminated, answer that the PIC should be
	freed if the first entry is to a free target. Answer if the PIC is now
	empty or should be freed. */

	/* Cogit>>#cPICCompactAndIsNowEmpty: */
static sqInt NoDbgRegParms
cPICCompactAndIsNowEmpty(CogMethod *cPIC)
{
    usqInt addressFollowingJump;
    usqInt addressFollowingJump1;
    sqInt entryPoint;
    sqInt i;
    sqInt methods[MaxCPICCases];
    sqInt pc;
    int tags[MaxCPICCases];
    CogMethod *targetMethod;
    sqInt targets[MaxCPICCases];
    sqInt used;
    sqInt valid;

	used = 0;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
			entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		}

		/* Collect all target triples except for triples whose entry-point is a freed method */
		valid = 1;
		if (!(((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				if (i == 1) {
					return 1;
				}
				valid = 0;
			}
		}
		if (valid) {
			tags[used] = ((i > 1
	? literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd)))
	: 0));
			targets[used] = entryPoint;
			methods[used] = (literalBeforeFollowingAddress(backEnd, pc - ((i == 1
	? jumpLongByteSize(backEnd)
	: (jumpLongConditionalByteSize(backEnd)) + (cmpC32RTempByteSize(backEnd))))));
			used += 1;
		}
	}
	if (used == ((cPIC->cPICNumCases))) {
		return 0;
	}
	if (used == 0) {
		return 1;
	}
	((((CogMethod *) ((((usqInt)cPIC)) + codeToDataDelta)))->cPICNumCases = used);
	if (used == 1) {
		pc = addressOfEndOfCaseinCPIC(2, cPIC);
		/* begin rewriteCPIC:caseJumpTo: */
		addressFollowingJump = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
		rewriteImm19JumpBeforetarget(((AbstractInstruction *) backEnd), addressFollowingJump, pc);
		return 0;
	}
	for (i = 1; i < used; i += 1) {
		pc = addressOfEndOfCaseinCPIC(i + 1, cPIC);
		rewriteCPICCaseAttagobjReftarget(pc, tags[i], methods[i], targets[i]);
	}
	/* begin rewriteCPIC:caseJumpTo: */
	addressFollowingJump1 = (((((sqInt)cPIC)) + firstCPICCaseOffset) - (jumpLongByteSize(backEnd))) - (loadLiteralByteSize(backEnd));
	rewriteImm19JumpBeforetarget(((AbstractInstruction *) backEnd), addressFollowingJump1, pc - cPICCaseSize);
	return 0;
}


/*	The first case in a CPIC doesn't have a class reference so we need only
	step over actually usd subsequent cases.
 */

	/* Cogit>>#cPICHasForwardedClass: */
static sqInt NoDbgRegParms
cPICHasForwardedClass(CogMethod *cPIC)
{
    usqInt classIndex;
    sqInt i;
    sqInt pc;


	/* start by finding the address of the topmost case, the cPICNumCases'th one */
	pc = (addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC)) - (jumpLongConditionalByteSize(backEnd));
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		classIndex = literal32BeforeFollowingAddress(backEnd, pc);
		if (isForwardedClassIndex(classIndex)) {
			return 1;
		}
		pc += cPICCaseSize;
	}
	return 0;
}


/*	scan the CPIC for target methods that have been freed. */

	/* Cogit>>#cPICHasFreedTargets: */
static sqInt NoDbgRegParms
cPICHasFreedTargets(CogMethod *cPIC)
{
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    CogMethod *targetMethod;

	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
			entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		}
		if (!(((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
			 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint))))) {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
			 || (isCMFree(((CogBlockMethod *) targetMethod))));
			if (((targetMethod->cmType)) == CMFree) {
				return 1;
			}
		}
	}
	return 0;
}


/*	Whimsey; we want 16rCA5E10 + cPICPrototypeCaseOffset to be somewhere in
	the middle of the zone.
 */

	/* Cogit>>#cPICPrototypeCaseOffset */
static usqInt
cPICPrototypeCaseOffset(void)
{
	return ((methodZoneBase + (youngReferrers())) / 2) - 13262352;
}


/*	Are any of the jumps from this CPIC to targetMethod? */

	/* Cogit>>#cPIC:HasTarget: */
static sqInt NoDbgRegParms
cPICHasTarget(CogMethod *cPIC, CogMethod *targetMethod)
{
    sqInt i;
    sqInt pc;
    sqInt target;

	target = (((usqInt)targetMethod)) + cmNoCheckEntryOffset;

	/* Since this is a fast test doing simple compares we don't need to care that some
	   cases have nonsense addresses in there. Just zip on through. */
	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
		return 1;
	}
	for (i = 2; i <= MaxCPICCases; i += 1) {
		pc += cPICCaseSize;
		if (target == (jumpLongTargetBeforeFollowingAddress(backEnd, pc))) {
			return 1;
		}
	}
	return 0;
}


/*	Answer an Array of the PIC's selector, followed by class and
	targetMethod/doesNotUnderstand: for each entry in the PIC.
 */

	/* Cogit>>#createCPICData: */
static sqInt NoDbgRegParms
createCPICData(CogMethod *cPIC)
{
    sqInt class;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt picData;
    sqInt target;
    CogMethod *targetMethod;

	assert((((cPIC->methodObject)) == 0)
	 || (addressCouldBeOop((cPIC->methodObject))));
	picData = instantiateClassindexableSize(classArray(), (((cPIC->cPICNumCases)) * 2) + 1);
	if (!picData) {
		return picData;
	}
	storePointerUncheckedofObjectwithValue(0, picData, (cPIC->selector));
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {

			/* first case may have been collected and stored here by collectCogConstituentFor:Annotation:Mcpc:Bcpc:Method: */
			class = (cPIC->methodObject);
			if (class == 0) {
				class = nilObject();
			}
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			class = classForInlineCacheTag(literal32BeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd))));
			/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
			entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		}
		if (((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
			target = splObj(SelectorDoesNotUnderstand);
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			target = (targetMethod->methodObject);
		}
		storePointerUncheckedofObjectwithValue((i * 2) - 1, picData, class);
		storePointerUncheckedofObjectwithValue(i * 2, picData, target);
	}
	beRootIfOld(picData);
	(cPIC->methodObject = 0);
	return picData;
}


/*	Division is a little weird on some processors. Defer to the backEnd
	to allow it to generate any special code it may need to. */

	/* Cogit>>#DivR:R:Quo:Rem: */
static AbstractInstruction * NoDbgRegParms
gDivRRQuoRem(sqInt rDivisor, sqInt rDividend, sqInt rQuotient, sqInt rRemainder)
{
	genDivRRQuoRem(backEnd, rDivisor, rDividend, rQuotient, rRemainder);
	return abstractInstructionAt(opcodeIndex - 1);
}


/*	Answer the number of bytecodes to skip to get to the first bytecode
	past the primitive call and any store of the error code. */

	/* Cogit>>#deltaToSkipPrimAndErrorStoreIn:header: */
static sqInt NoDbgRegParms
deltaToSkipPrimAndErrorStoreInheader(sqInt aMethodObj, sqInt aMethodHeader)
{
	return (((primitiveIndexOfMethodheader(aMethodObj, aMethodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(aMethodHeader)) == (fetchByteofObject((startPCOfMethodHeader(aMethodHeader)) + (sizeOfCallPrimitiveBytecode(aMethodHeader)), aMethodObj)))
		? (sizeOfCallPrimitiveBytecode(aMethodHeader)) + (sizeOfLongStoreTempBytecode(aMethodHeader))
		: 0);
}

	/* Cogit>>#endPCOf: */
static sqInt NoDbgRegParms
endPCOf(sqInt aMethod)
{
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt end;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt newContinuation;
    sqInt pc;
    sqInt prim;
    sqInt prim1;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	pc = (latestContinuation = startPCOfMethod(aMethod));
	if (((prim = primitiveIndexOf(aMethod))) > 0) {
		if (isQuickPrimitiveIndex(prim)) {
			return pc - 1;
		}
	}
	/* begin bytecodeSetOffsetFor: */
	bsOffset = (methodUsesAlternateBytecodeSet(aMethod)
				? 0x100
				: 0);
	nExts = 0;
	end = numBytesOf(aMethod);
	while (pc <= end) {
		byte = fetchByteofObject(pc, aMethod);
		descriptor = generatorAt(byte + bsOffset);
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			end = pc;
		}
		if ((isBranch(descriptor))
		 || ((descriptor->isBlockCreation))) {
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, aMethod);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
			if ((descriptor->isBlockCreation)) {
				pc += distance;
			}
		}
		else {
			/* begin maybeUnsafeJumpContinuation:at:for:in: */
			newContinuation = latestContinuation;
			if ((descriptor->hasUnsafeJump)) {
				byte01 = fetchByteofObject(pc + 1, aMethod);

				/* pushIntegerLong */
				byte02 = fetchByteofObject(pc + 2, aMethod);
				/* begin decodePushIntegerLongBefore:in: */
				distance1 = fetchByteofObject(pc - 1, methodObj);
				upperByte = fetchByteofObject(pc - 3, methodObj);
				if (upperByte > 0x7F) {
					upperByte -= 0x100;
				}
				distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
				targetPC1 = (pc + ((descriptor->numBytes))) + distance2;
				if (!((descriptor->isMapped))) {
					if ((((usqInt)(byte02)) >> 5) == 4) {

						/* inlined sista primitive */
						prim1 = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
						if (prim1 >= 7000) {

							/* branch forward */
							newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
						}
					}
				}
			}
			latestContinuation = newContinuation;
		}
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
		pc += (descriptor->numBytes);
	}
	return end;
}


/*	This is a static version of ceEnterCogCodePopReceiverReg for
	break-pointing when debugging in C. Marked <api> so the code generator
	won't delete it. */

	/* Cogit>>#enterCogCodePopReceiver */
static void
enterCogCodePopReceiver(void)
{
	realCEEnterCogCodePopReceiverReg();
	if (!Debug) {
		error("what??");
	}
}


/*	Answer if the entryPoint's tag is expected to be a selector reference, as
	opposed to a class tag.
 */

	/* Cogit>>#entryPointTagIsSelector: */
static sqInt NoDbgRegParms
entryPointTagIsSelector(sqInt entryPoint)
{
	return (entryPoint < methodZoneBase)
	 || (((entryPoint & entryPointMask) == uncheckedEntryAlignment)
	 || (((entryPoint & entryPointMask) == checkedEntryAlignment)
	 && ((((((CogMethod *) (entryPoint - cmEntryOffset)))->cmType)) == CMOpenPIC)));
}


/*	Use asserts to check if the ClosedPICPrototype is as expected from
	compileClosedPICPrototype, and can be updated as required via
	rewriteCPICCaseAt:tag:objRef:target:. If all asserts pass, answer
	0, otherwise answer a bit mask identifying all the errors. */
/*	self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to:
	methodZoneBase + closedPICSize
 */

	/* Cogit>>#expectedClosedPICPrototype: */
static sqInt NoDbgRegParms
expectedClosedPICPrototype(CogMethod *cPIC)
{
    usqInt classTag;
    sqInt classTagPC;
    sqInt entryPoint;
    sqInt errors;
    sqInt i;
    sqInt methodObjPC;
    sqInt object;
    sqInt pc;

	errors = 0;

	/* First jump is unconditional; subsequent ones are conditional */
	pc = (((usqInt)cPIC)) + firstCPICCaseOffset;
	object = literalBeforeFollowingAddress(backEnd, pc - (jumpLongByteSize(backEnd)));
	if (!(asserta(object == (firstPrototypeMethodOop())))) {
		errors = 1;
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
	if (!(asserta(entryPoint == ((cPICPrototypeCaseOffset()) + 13262352)))) {
		errors += 2;
	}
	for (i = 1; i < MaxCPICCases; i += 1) {

		/* verify information in case is as expected. */
		pc += cPICCaseSize;
		methodObjPC = (pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == ((subsequentPrototypeMethodOop()) + i)))) {
			errors = errors | 4;
		}
		classTagPC = pc - (jumpLongConditionalByteSize(backEnd));
		classTag = literal32BeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == (0xBABE1F15U + i)))) {
			errors = errors | 8;
		}
		/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
		entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		if (!(asserta(entryPoint == (((cPICPrototypeCaseOffset()) + 13262352) + (i * 16))))) {
			errors = errors | 16;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
		object = literalBeforeFollowingAddress(backEnd, methodObjPC);
		if (!(asserta(object == (((subsequentPrototypeMethodOop()) + i) ^ 0xA5A5A5A5U)))) {
			errors = errors | 32;
		}
		classTag = literal32BeforeFollowingAddress(backEnd, classTagPC);
		if (!(asserta(classTag == ((0xBABE1F15U + i) ^ 0x5A5A5A5A)))) {
			errors = errors | 64;
		}
		/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
		entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		if (!(asserta(entryPoint == ((((cPICPrototypeCaseOffset()) + 13262352) + (i * 16)) ^ 0x55AA50)))) {
			errors = errors | 128;
		}
		rewriteCPICCaseAttagobjReftarget(pc, classTag ^ 0x5A5A5A5A, object ^ 0xA5A5A5A5U, entryPoint ^ 0x55AA50);
	}
	entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, (((usqInt)cPIC)) + cPICEndOfCodeOffset);
	if (!(asserta(entryPoint == (cPICMissTrampolineFor(0))))) {
		errors += 0x100;
	}
	return errors;
}


/*	224		11100000	aaaaaaaa	Extend A (Ext A = Ext A prev * 256 + Ext A) */

	/* Cogit>>#extABytecode */
static sqInt
extABytecode(void)
{
	extA = ((((sqInt)((usqInt)(extA) << 8)))) + byte1;
	return 0;
}


/*	225		11100001	sbbbbbbb	Extend B (Ext B = Ext B prev * 256 + Ext B) */

	/* Cogit>>#extBBytecode */
static sqInt
extBBytecode(void)
{
	extB = ((numExtB == 0)
	 && (byte1 > 0x7F)
		? byte1 - 0x100
		: ((((sqInt)((usqInt)(extB) << 8)))) + byte1);
	numExtB += 1;
	return 0;
}


/*	Fill in the block headers now we know the exact layout of the code. */

	/* Cogit>>#fillInBlockHeadersAt: */
static sqInt NoDbgRegParms
fillInBlockHeadersAt(sqInt startAddress)
{
    sqInt aCogMethodOrInteger;
    CogBlockMethod *blockHeader;
    BlockStart *blockStart;
    sqInt i;

	if (!(needsFrame
		 && (blockCount > 0))) {
		return null;
	}
	if (blockNoContextSwitchOffset == null) {
		blockNoContextSwitchOffset = ((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address));
	}
	else {
		assert(blockNoContextSwitchOffset == (((blockEntryLabel->address)) - ((blockEntryNoContextSwitch->address))));
	}
	for (i = 0; i < blockCount; i += 1) {
		blockStart = blockStartAt(i);
		/* begin writableBlockMethodFor: */
		aCogMethodOrInteger = (((blockStart->fakeHeader))->address);
		blockHeader = ((CogBlockMethod *) ((((usqInt)aCogMethodOrInteger)) + codeToDataDelta));
		(blockHeader->homeOffset = ((((blockStart->fakeHeader))->address)) - startAddress);
		(blockHeader->startpc = (blockStart->startpc));
		(blockHeader->cmType = CMBlock);
		(blockHeader->cmNumArgs = (blockStart->numArgs));
		(blockHeader->cbUsesInstVars = (blockStart->hasInstVarRef));
		(blockHeader->stackCheckOffset = (((blockStart->stackCheckLabel)) == null
			? 0
			: ((((blockStart->stackCheckLabel))->address)) - ((((blockStart->fakeHeader))->address))));
	}
	return 0;
}

	/* Cogit>>#findBackwardBranch:IsBackwardBranch:Mcpc:Bcpc:MatchingBcpc: */
static sqInt NoDbgRegParms
findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetBcpc)
{
	return ((((isBackwardBranchAndAnnotation & 1) != 0))
	 && ((((sqInt)targetBcpc)) == bcpc)
		? ((sqInt)mcpc)
		: 0);
}

	/* Cogit>>#findBlockMethodWithEntry:startBcpc: */
static usqInt NoDbgRegParms
findBlockMethodWithEntrystartBcpc(sqInt blockEntryMcpc, sqInt startBcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((cogBlockMethod->startpc)) == startBcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}

	/* Cogit>>#findMapLocationForMcpc:inMethod: */
static usqInt NoDbgRegParms
findMapLocationForMcpcinMethod(usqInt targetMcpc, CogMethod *cogMethod)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;

	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	if (mcpc == targetMcpc) {
		return map;
	}
	while (((mapByte = byteAt(map))) != MapEnd) {
		annotation = ((usqInt)(mapByte)) >> AnnotationShift;
		if (annotation != IsAnnotationExtension) {
			mcpc += 4 /* codeGranularity */ * ((annotation == IsDisplacementX2N
	? ((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))
	: mapByte & DisplacementMask));
		}
		if (mcpc >= targetMcpc) {
			assert(mcpc == targetMcpc);
			if (annotation == IsDisplacementX2N) {
				map -= 1;
				mapByte = byteAt(map);
				annotation = ((usqInt)(mapByte)) >> AnnotationShift;
				assert(annotation > IsAnnotationExtension);
			}
			return map;
		}
		map -= 1;
	}
	return 0;
}


/*	Find the CMMethod or CMBlock that has zero-relative startbcpc as its first
	bytecode pc.
	As this is for cannot resume processing and/or conversion to machine-code
	on backward
	branch, it doesn't have to be fast. Enumerate block returns and map to
	bytecode pcs. */

	/* Cogit>>#findMethodForStartBcpc:inHomeMethod: */
CogBlockMethod *
findMethodForStartBcpcinHomeMethod(sqInt startbcpc, CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	if (startbcpc == (startPCOfMethodHeader((cogMethod->methodHeader)))) {
		return ((CogBlockMethod *) cogMethod);
	}
	assert(((cogMethod->blockEntryOffset)) != 0);
	return ((CogBlockMethod *) (blockDispatchTargetsForperformarg(cogMethod, findBlockMethodWithEntrystartBcpc, startbcpc)));
}


/*	Machine code addresses map to the following bytecode for all bytecodes
	except backward branches, where they map to the backward branch itself.
	This is so that loops continue, rather than terminate prematurely. */

	/* Cogit>>#find:IsBackwardBranch:Mcpc:Bcpc:MatchingMcpc: */
static sqInt NoDbgRegParms
findIsBackwardBranchMcpcBcpcMatchingMcpc(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *targetMcpc)
{
	return (targetMcpc == mcpc
		? ((descriptor == null)
			 || (((isBackwardBranchAndAnnotation & 1) != 0))
				? bcpc
				: bcpc + ((descriptor->numBytes)))
		: 0);
}

	/* Cogit>>#firstMappedPCFor: */
static sqInt NoDbgRegParms
firstMappedPCFor(CogMethod *cogMethod)
{
	return ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
}


/*	Answer a fake value for the first method oop in the PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address. */

	/* Cogit>>#firstPrototypeMethodOop */
static sqInt
firstPrototypeMethodOop(void)
{
	return (((((usqInt)0x5EAF00D)) >= ((methodLabel->address)))
	 && ((((usqInt)0x5EAF00D)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))
		? 0xCA7F00D
		: 0x5EAF00D);
}

	/* Cogit>>#fixupAt: */
static BytecodeFixup * NoDbgRegParms
fixupAt(sqInt fixupPC)
{
	return fixupAtIndex(fixupPC - initialPC);
}

	/* Cogit>>#flagCogMethodForBecome: */
void
flagCogMethodForBecome(CogMethod *cogMethod)
{
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethodFlaggedForBecome);
}

	/* Cogit>>#followForwardedLiteralsImplementationIn: */
static void NoDbgRegParms
followForwardedLiteralsImplementationIn(CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableCogMethod;

	assert((!(isCMMethodEtAl(((CogBlockMethod *) cogMethod))))
	 || (!(isForwarded((cogMethod->methodObject)))));
	writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	hasYoungObj = isYoung((cogMethod->methodObject));
	if (shouldRemapOop((cogMethod->selector))) {
		(writableCogMethod->selector = remapObj((cogMethod->selector)));
		if (isYoung((cogMethod->selector))) {
			hasYoungObj = 1;
		}
	}
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {

			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = remapIfObjectRefpchasYoung(annotation, (((char *) mcpc)), (((void *)hasYoungObjPtr)));
			if (result != 0) {
				goto l2;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	l2:	/* end mapFor:performUntil:arg: */;
	if (hasYoungObj) {
		ensureInYoungReferrers(cogMethod);
	}
	else {
		(writableCogMethod->cmRefersToYoung = 0);
	}
}

	/* Cogit>>#followForwardedLiteralsIn: */
void
followForwardedLiteralsIn(CogMethod *cogMethod)
{
    usqInt wasInYoungReferrers;

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	wasInYoungReferrers = (cogMethod->cmRefersToYoung);
	followForwardedLiteralsImplementationIn(cogMethod);
	if (wasInYoungReferrers
	 && (!((cogMethod->cmRefersToYoung)))) {
		pruneYoungReferrers();
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	To avoid runtime checks on literal variable and literal accesses in == and
	~~, 
	we follow literals in methods having movable literals in the postBecome
	action. To avoid scanning every method, we annotate cogMethods with the 
	cmHasMovableLiteral flag. */

	/* Cogit>>#followMovableLiteralsAndUpdateYoungReferrers */
void
followMovableLiteralsAndUpdateYoungReferrers(void)
{
    CogMethod *cogMethod;
    sqInt endAddress;

	assert(kosherYoungReferrers());
	codeModified = 0;
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!((((cogMethod->cmType)) == CMFree)
			 || (((cogMethod->cmType)) == CMMethodFlaggedForBecome))) {
			if ((cogMethod->cmHasMovableLiteral)) {
				followForwardedLiteralsImplementationIn(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
		sys_icache_invalidate(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)codeBase), endAddress);
#    endif
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	N.B. because becomeEffectFlags indicates whether jitted methods were
	becommed or not, if this method is called flagged methods exist, will be
	freed, and so on. So there is no need to check. Just do it. */

	/* Cogit>>#freeBecomeFlaggedMethods */
void
freeBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	unlinkSendsToFree();
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}

	/* Cogit>>#freeCogMethod: */
void
freeCogMethod(CogMethod *cogMethod)
{
	moveProfileToMethods();
	freeMethod(cogMethod);
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	Free machine-code methods whose compiled methods are unmarked
	and open PICs whose selectors are not marked, and closed PICs that
	refer to unmarked objects. */

	/* Cogit>>#freeUnmarkedMachineCode */
void
freeUnmarkedMachineCode(void)
{
    CogMethod *cogMethod;
    sqInt freedMethod;

	moveProfileToMethods();
	freedMethod = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (!(isMarked((cogMethod->methodObject))))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		if ((((cogMethod->cmType)) == CMOpenPIC)
		 && ((!(isImmediate((cogMethod->selector))))
		 && (!(isMarked((cogMethod->selector)))))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		if ((((cogMethod->cmType)) == CMClosedPIC)
		 && (closedPICRefersToUnmarkedObject(cogMethod))) {
			freedMethod = 1;
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedMethod) {
		unlinkSendsToFree();
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	Call ceSendMustBeBooleanTo: via the relevant trampoline. */

	/* Cogit>>#genCallMustBeBooleanFor: */
static AbstractInstruction * NoDbgRegParms
genCallMustBeBooleanFor(sqInt boolean)
{
    AbstractInstruction *abstractInstruction;
    sqInt callTarget;

	/* begin CallRT: */
	callTarget = (boolean == (falseObject())
		? ceSendMustBeBooleanAddFalseTrampoline
		: ceSendMustBeBooleanAddTrueTrampoline);
	/* begin annotateCall: */
	abstractInstruction = genoperand(Call, callTarget);
	(abstractInstruction->annotation = IsRelativeCall);
	return abstractInstruction;
}

	/* Cogit>>#genConditionalBranch:operand: */
static AbstractInstruction * NoDbgRegParms
genConditionalBranchoperand(sqInt opcode, sqInt operandOne)
{
	return noteFollowingConditionalBranch(previousInstruction(), genoperand(opcode, operandOne));
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. The desired arguments and entry-point are pushed on a stackPage's
	stack. The enilopmart pops off the values to be loaded into registers and
	then executes a return instruction to pop off the entry-point and jump to
	it. 
	BEFORE				AFTER			(stacks grow down)
	whatever			stackPointer ->	whatever
	target address =>	reg1 = reg1val, etc
	reg1val				pc = target address
	reg2val
	stackPointer ->	reg3val */

	/* Cogit>>#genEnilopmartFor:and:and:forCall:called: */
static void (*genEnilopmartForandandforCallcalled(sqInt regArg1, sqInt regArg2OrNone, sqInt regArg3OrNone, sqInt forCall, char *trampolineName))(void)
{
    AbstractInstruction *anInstruction;
    sqInt endAddress;
    usqInt enilopmart;
    sqInt quickConstant;
    sqInt size;

	zeroOpcodeIndex();
	/* begin MoveCq:R: */
	quickConstant = varBaseAddress();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	genLoadStackPointers(backEnd);
	if (regArg3OrNone != NoReg) {
		genoperand(PopR, regArg3OrNone);
	}
	if (regArg2OrNone != NoReg) {
		genoperand(PopR, regArg2OrNone);
	}
	genoperand(PopR, regArg1);
	genEnilopmartReturn(forCall);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineName, enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	An enilopmart (the reverse of a trampoline) is a piece of code that makes
	the system-call-like transition from the C runtime into generated machine
	code. At the point the enilopmart enters machine code via a return
	instruction, any argument registers have been loaded with their values and
	the stack, if
	for call, looks like
	ret pc
	stackPointer ->	target address
	
	and if not for call, looks like
	whatever
	stackPointer ->	target address
	
	If forCall and running on a CISC, ret pc must be left on the stack. If
	forCall and
	running on a RISC, ret pc must be popped into LinkReg. In either case,
	target address must be removed from the stack and jumped/returned to. */

	/* Cogit>>#genEnilopmartReturn: */
static void NoDbgRegParms
genEnilopmartReturn(sqInt forCall)
{
	if (forCall) {
		genoperand(PopR, RISCTempReg);
		genoperand(PopR, LinkReg);
		genoperand(JumpR, RISCTempReg);
	}
	else {
		genoperand(PopR, RISCTempReg);
		genoperand(JumpR, RISCTempReg);
	}
}


/*	Generate the routine that writes the current values of the C frame and
	stack pointers into
	variables. These are used to establish the C stack in trampolines back
	into the C run-time.
	This routine assumes the system's frame pointer is the same as that used
	in generated code. */

	/* Cogit>>#generateCaptureCStackPointers: */
static void NoDbgRegParms NeverInline
generateCaptureCStackPointers(sqInt captureFramePointer)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt callerSavedReg;
    sqInt fixupSize;
    sqInt offset;
    sqInt opcodeSize;
    sqInt operandTwo;
    sqInt operandTwo1;
    sqInt operandTwo2;
    sqInt pushedVarBaseReg;
    sqInt quickConstant;
    usqInt startAddress;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 32;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;

	/* Must happen first; value may be used in accessing any of the following addresses */
	startAddress = methodZoneBase;
	callerSavedReg = 0;
	pushedVarBaseReg = 0;
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {

		/* VarBaseReg is not caller-saved; must save and restore it, either by using an available caller-saved reg or push/pop. */

		/* TempReg used below */
		callerSavedReg = availableRegisterOrNoneIn(((ABICallerSavedRegisterMask | (1U << TempReg)) - (1U << TempReg)));
		if (callerSavedReg == NoReg) {
			gNativePushR(VarBaseReg);
			pushedVarBaseReg = 1;
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, VarBaseReg, callerSavedReg);
		}
	}
	/* begin MoveCq:R: */
	quickConstant = varBaseAddress();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	if (captureFramePointer) {
		/* begin gen:operand:literal: */
		operandTwo = cFramePointerAddress();
		checkLiteralforInstruction(operandTwo, genoperandoperand(MoveRAw, FPReg, operandTwo));
	}
	if (pushedVarBaseReg) {
		/* begin LoadEffectiveAddressMw:r:R: */
		if (pushedVarBaseReg) {
			offset = 0 /* leafCallStackPointerDelta */ + BytesPerWord;
		}
		else {
			/* begin leafCallStackPointerDelta */
			offset = 0;
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, NativeSPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
		}
		/* begin gen:operand:literal: */
		operandTwo1 = cStackPointerAddress();
		checkLiteralforInstruction(operandTwo1, genoperandoperand(MoveRAw, TempReg, operandTwo1));
	}
	else {
		/* begin gen:operand:literal: */
		operandTwo2 = cStackPointerAddress();
		checkLiteralforInstruction(operandTwo2, genoperandoperand(MoveRAw, NativeSPReg, operandTwo2));
	}
	if (!(((CallerSavedRegisterMask & ((1U << VarBaseReg))) != 0))) {
		if (pushedVarBaseReg) {
			gNativePopR(VarBaseReg);
		}
		else {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, callerSavedReg, VarBaseReg);
		}
	}
	gNativeRetN(0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)startAddress))), (((usqInt)methodZoneBase)) - (((usqInt)startAddress)));
	sys_icache_invalidate(((void *) (((usqInt)startAddress))), (((usqInt)methodZoneBase)) - (((usqInt)startAddress)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)startAddress), ((usqInt)methodZoneBase));
#  endif
	recordGeneratedRunTimeaddress("ceCaptureCStackPointers", startAddress);
	ceCaptureCStackPointers = ((void (*)(void)) startAddress);
}


/*	Generate the prototype ClosedPIC to determine how much space a full closed
	PIC takes.
	When we first allocate a closed PIC it only has one or two cases and we
	want to grow it.
	So we have to determine how big a full one is before hand. */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateClosedPICPrototype */
static void
generateClosedPICPrototype(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    CogMethod *cPIC;
    AbstractInstruction * cPICEndOfCodeLabel;
    sqInt endAddress;
    AbstractInstruction * endCPICCase1;
    sqInt fixupSize;
    sqInt h;
    AbstractInstruction *jumpNext;
    sqInt jumpTarget;
    sqInt jumpTarget1;
    sqInt jumpTarget2;
    sqInt numArgs;
    sqInt opcode;
    sqInt opcodeSize;
    sqInt operandOne;
    sqInt wordConstant;
    sqInt wordConstant1;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = MaxCPICCases * 9;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);
	/* begin compileClosedPICPrototype */
	compilePICAbort((numArgs = 0));

	/* At the end of the entry code we need to jump to the first case code, which is actually the last chunk.
	   On each entension we must update this jump to move back one case. */
	jumpNext = compileCPICEntry();
	/* begin MoveUniqueCw:R: */
	wordConstant1 = firstPrototypeMethodOop();
	/* begin uniqueLiteral:forInstruction: */
	anInstruction1 = genoperandoperand(MoveCwR, wordConstant1, SendNumArgsReg);
	assert(usesOutOfLineLiteral(anInstruction1));
	(anInstruction1->dependent = allocateLiteral(wordConstant1));
	/* begin JumpLong: */
	jumpTarget1 = (((methodZoneBase + (youngReferrers())) / 2) - 13262352) + 13262352;
	genoperand(JumpLong, jumpTarget1);
	endCPICCase0 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (h = 1; h < MaxCPICCases; h += 1) {
		if (h == (MaxCPICCases - 1)) {
			jmpTarget(jumpNext, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		/* begin MoveUniqueCw:R: */
		wordConstant = (subsequentPrototypeMethodOop()) + h;
		/* begin uniqueLiteral:forInstruction: */
		anInstruction = genoperandoperand(MoveCwR, wordConstant, SendNumArgsReg);
		assert(usesOutOfLineLiteral(anInstruction));
		(anInstruction->dependent = allocateLiteral(wordConstant));
		/* begin gen:literal32:operand: */
		opcode = CmpC32R;
		checkLiteral32forInstruction(0xBABE1F15U + h, genoperandoperand(opcode, 0xBABE1F15U + h, TempReg));
		/* begin JumpLongZero: */
		jumpTarget = ((((methodZoneBase + (youngReferrers())) / 2) - 13262352) + 13262352) + (h * 16);
		genConditionalBranchoperand(JumpLongZero, ((sqInt)jumpTarget));
		if (h == 1) {
			endCPICCase1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
	}
	/* begin gen:literal:operand: */
	operandOne = (methodLabel->address);
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveCwR, operandOne, ClassReg));
	/* begin JumpLong: */
	jumpTarget2 = cPICMissTrampolineFor(numArgs);
	genoperand(JumpLong, jumpTarget2);
	cPICEndOfCodeLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	dumpLiterals(0);
	computeMaximumSizes();
	cPIC = ((CogMethod *) methodZoneBase);
	closedPICSize = (sizeof(CogMethod)) + (generateInstructionsAt(methodZoneBase + (sizeof(CogMethod))));
	endAddress = outputInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	assert((methodZoneBase + closedPICSize) == endAddress);
	firstCPICCaseOffset = ((endCPICCase0->address)) - methodZoneBase;
	cPICEndOfCodeOffset = ((cPICEndOfCodeLabel->address)) - methodZoneBase;
	cPICCaseSize = ((endCPICCase1->address)) - ((endCPICCase0->address));
	cPICEndSize = closedPICSize - (((MaxCPICCases - 1) * cPICCaseSize) + firstCPICCaseOffset);
	closedPICSize = roundUpToMethodAlignment(backEnd(), closedPICSize);
	assert(((picInterpretAbort->address)) == (((methodLabel->address)) + (picInterpretAbortOffset())));
	assert((expectedClosedPICPrototype(cPIC)) == 0);
	storeLiteralbeforeFollowingAddress(backEnd, 0, ((endCPICCase0->address)) - (jumpLongByteSize(backEnd)));
	methodZoneBase = alignUptoRoutineBoundary(endAddress);

	/* self cCode: ''
	   inSmalltalk:
	   [self disassembleFrom: cPIC + (self sizeof: CogMethod) to: cPIC + closedPICSize - 1.
	   self halt] */
	cPICPrototype = cPIC;
}


/*	We handle jump sizing simply. First we make a pass that asks each
	instruction to compute its maximum size. Then we make a pass that
	sizes jumps based on the maxmimum sizes. Then we make a pass
	that fixes up jumps. When fixing up a jump the jump is not allowed to
	choose a smaller offset but must stick to the size set in the second pass. */

	/* Cogit>>#generateCogFullBlock */
static CogMethod *
generateCogFullBlock(void)
{
    sqInt codeSize;
    usqIntptr_t headerSize;
    sqInt mapSize;
    CogMethod *method;
    sqInt result;
    usqInt startAddress;
    sqInt totalSize;

	headerSize = sizeof(CogMethod);
	(methodLabel->address = freeStart());
	computeMaximumSizes();
	concretizeAt(methodLabel, freeStart());
	codeSize = generateInstructionsAt(((methodLabel->address)) + headerSize);
	mapSize = generateMapAtstart(null, ((methodLabel->address)) + cbNoSwitchEntryOffset);
	totalSize = roundUpToMethodAlignment(backEnd(), (headerSize + codeSize) + mapSize);
	if (totalSize > MaxMethodSize) {
		return ((CogMethod *) MethodTooBig);
	}
	startAddress = allocate(totalSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	assert((startAddress + cbEntryOffset) == ((fullBlockEntry->address)));
	assert((startAddress + cbNoSwitchEntryOffset) == ((fullBlockNoContextSwitchEntry->address)));
	result = outputInstructionsAt(startAddress + headerSize);
	assert(((startAddress + headerSize) + codeSize) == result);
	padIfPossibleWithStopsFromto(backEnd, result, ((startAddress + totalSize) - mapSize) - 1);
	generateMapAtstart((startAddress + totalSize) - 1, startAddress + cbNoSwitchEntryOffset);
	flag("TOCHECK");
	method = ((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta));
	fillInMethodHeadersizeselector(method, totalSize, nilObject());
	(method->cpicHasMNUCaseOrCMIsFullBlock = 1);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) startAddress), (startAddress + totalSize) - startAddress);
	sys_icache_invalidate(((void *) startAddress), (startAddress + totalSize) - startAddress);
#  else // __APPLE__ && __MACH__
	ceFlushICache(startAddress, startAddress + totalSize);
#  endif
	return ((CogMethod *) startAddress);
}


/*	We handle jump sizing simply. First we make a pass that asks each
	instruction to compute its maximum size. Then we make a pass that
	sizes jumps based on the maxmimum sizes. Then we make a pass
	that fixes up jumps. When fixing up a jump the jump is not allowed to
	choose a smaller offset but must stick to the size set in the second pass. */

	/* Cogit>>#generateCogMethod: */
static CogMethod * NoDbgRegParms
generateCogMethod(sqInt selector)
{
    sqInt codeSize;
    usqIntptr_t headerSize;
    sqInt mapSize;
    sqInt result;
    usqInt startAddress;
    sqInt totalSize;

	headerSize = sizeof(CogMethod);
	(methodLabel->address = freeStart());
	computeMaximumSizes();
	concretizeAt(methodLabel, freeStart());
	codeSize = generateInstructionsAt(((methodLabel->address)) + headerSize);
	mapSize = generateMapAtstart(null, ((methodLabel->address)) + cmNoCheckEntryOffset);
	totalSize = roundUpToMethodAlignment(backEnd(), (headerSize + codeSize) + mapSize);
	if (totalSize > MaxMethodSize) {
		return ((CogMethod *) MethodTooBig);
	}
	startAddress = allocate(totalSize);
	if (startAddress == 0) {
		return ((CogMethod *) InsufficientCodeSpace);
	}
	assert((startAddress + cmEntryOffset) == ((entry->address)));
	assert((startAddress + cmNoCheckEntryOffset) == ((noCheckEntry->address)));
	result = outputInstructionsAt(startAddress + headerSize);
	assert(((startAddress + headerSize) + codeSize) == result);
	padIfPossibleWithStopsFromto(backEnd, result, ((startAddress + totalSize) - mapSize) - 1);
	generateMapAtstart((startAddress + totalSize) - 1, startAddress + cmNoCheckEntryOffset);
	fillInBlockHeadersAt(startAddress);
	fillInMethodHeadersizeselector(((CogMethod *) ((((usqInt)startAddress)) + codeToDataDelta)), totalSize, selector);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) startAddress), (startAddress + totalSize) - startAddress);
	sys_icache_invalidate(((void *) startAddress), (startAddress + totalSize) - startAddress);
#  else // __APPLE__ && __MACH__
	ceFlushICache(startAddress, startAddress + totalSize);
#  endif
	return ((CogMethod *) startAddress);
}


/*	Generate the method map at addressrNull (or compute it if addressOrNull is
	null). Answer the length of the map in byes. Each entry in the map is in
	two parts. In the
	least signficant bits are a displacement of how far from the start or
	previous entry,
	unless it is an IsAnnotationExtension byte, in which case those bits are
	the extension.
	In the most signficant bits are the type of annotation at the point
	reached. A null
	byte ends the map. */

	/* Cogit>>#generateMapAt:start: */
static sqInt NoDbgRegParms
generateMapAtstart(usqInt addressOrNull, usqInt startAddress)
{
    unsigned char annotation;
    sqInt delta;
    sqInt i;
    AbstractInstruction *instruction;
    sqInt length;
    usqInt location;
    sqInt mapEntry;
    sqInt maxDelta;
    usqInt mcpc;

	length = 0;
	location = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		annotation = (instruction->annotation);
		if (!(annotation == null)) {
			/* begin assertValidAnnotation:for: */
			assert((annotation != (getIsObjectReference()))
			 || (((instruction->opcode)) == Literal));
			/* begin mapEntryAddress */
			mcpc = (((instruction->opcode)) == Literal
				? (instruction->address)
				: ((instruction->address)) + ((instruction->machineCodeSize)));
			while (((delta = (mcpc - location) / 4 /* codeGranularity */)) > DisplacementMask) {
				maxDelta = (((((delta < MaxX2NDisplacement) ? delta : MaxX2NDisplacement)) | DisplacementMask) - DisplacementMask);
				assert((((usqInt)(maxDelta)) >> AnnotationShift) <= DisplacementMask);
				if (!(addressOrNull == null)) {
					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, (((usqInt)(maxDelta)) >> AnnotationShift) + DisplacementX2N);
				}
				location += maxDelta * 4 /* codeGranularity */;
				length += 1;
			}
			if (!(addressOrNull == null)) {
				mapEntry = delta + (((sqInt)((usqInt)((((annotation < IsSendCall) ? annotation : IsSendCall))) << AnnotationShift)));
				/* begin addToMap:instruction:byte:at:for: */
				codeByteAtput(addressOrNull - length, mapEntry);
			}
			location += delta * 4 /* codeGranularity */;
			length += 1;
			if (annotation > IsSendCall) {

				/* Add the necessary IsAnnotationExtension */
				if (!(addressOrNull == null)) {
					mapEntry = (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift))) + (annotation - IsSendCall);
					/* begin addToMap:instruction:byte:at:for: */
					codeByteAtput(addressOrNull - length, mapEntry);
				}
				length += 1;
			}
		}
	}
	if (!(addressOrNull == null)) {
		/* begin addToMap:instruction:byte:at:for: */
		codeByteAtput(addressOrNull - length, MapEnd);
	}
	return length + 1;
}


/*	Generate the prototype OpenPIC to determine how much space an open PIC
	takes. 
 */
/*	stack allocate the various collections so that they
	are effectively garbage collected on return. */

	/* Cogit>>#generateOpenPICPrototype */
static void
generateOpenPICPrototype(void)
{
    sqInt codeSize;
    sqInt fixupSize;
    sqInt mapSize;
    sqInt opcodeSize;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 100;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	(methodLabel->address = methodZoneBase);
	(methodLabel->dependent = null);
	compileOpenPICnumArgs(specialSelector(0), numRegArgs());
	computeMaximumSizes();
	concretizeAt(methodLabel, methodZoneBase);
	codeSize = generateInstructionsAt(methodZoneBase + (sizeof(CogMethod)));
	mapSize = generateMapAtstart(null, methodZoneBase + cmNoCheckEntryOffset);

	/* self cCode: ''
	   inSmalltalk:
	   [| end |
	   end := self outputInstructionsAt: methodZoneBase + headerSize.
	   self disassembleFrom: methodZoneBase + (self sizeof: CogMethod) to: end - 1.
	   self halt] */
	openPICSize = (roundUpLength((sizeof(CogMethod)) + codeSize)) + (roundUpToMethodAlignment(backEnd(), mapSize));
}


/*	Generate the run-time entries at the base of the native code zone and
	update the base.
 */

	/* Cogit>>#generateRunTimeTrampolines */
static void
generateRunTimeTrampolines(void)
{
    sqInt operandTwo;
    sqInt operandTwo2;

	ceSendMustBeBooleanAddFalseTrampoline = genMustBeBooleanTrampolineForcalled(falseObject(), "ceSendMustBeBooleanAddFalseTrampoline");
	ceSendMustBeBooleanAddTrueTrampoline = genMustBeBooleanTrampolineForcalled(trueObject(), "ceSendMustBeBooleanAddTrueTrampoline");
	/* begin genNonLocalReturnTrampoline */
	zeroOpcodeIndex();
	/* begin gen:operand:literal: */
	operandTwo2 = instructionPointerAddress();
	checkLiteralforInstruction(operandTwo2, genoperandoperand(MoveRAw, LinkReg, operandTwo2));
	ceNonLocalReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNonLocalReturn, "ceNonLocalReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
	/* begin genCheckForInterruptsTrampoline */
	zeroOpcodeIndex();
	/* begin gen:operand:literal: */
	operandTwo = instructionPointerAddress();
	checkLiteralforInstruction(operandTwo, genoperandoperand(MoveRAw, LinkReg, operandTwo));
	ceCheckForInterruptTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCheckForInterrupt, "ceCheckForInterruptTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
	ceFetchContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVar, "ceFetchContextInstVarTrampoline", 2, ReceiverResultReg, SendNumArgsReg, null, null, 0 /* emptyRegisterMask */, 1, SendNumArgsReg, 0);

	/* to keep ReceiverResultReg live. */
	ceStoreContextInstVarTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceContextinstVarvalue, "ceStoreContextInstVarTrampoline", 3, ReceiverResultReg, SendNumArgsReg, ClassReg, null, 0 /* emptyRegisterMask */, 1, ReceiverResultReg, 0);

	/* ceInvokeInterpreter is an optimization and a work-around. Historically we used setjmp/longjmp to reenter the
	   interpreter at the current C stack base.  The C stack base is set at start-up and on each callback enter and
	   callback return. The interpreter must be invoked whenever a non-machine-code method must be run.  That might
	   be when invoking an interpreter method from one of the send linking routines (ceSend:...), or on continuing from
	   an evaluation primitive such as primitiveExecuteMethod.  The problem here is that such primitives could have
	   been invoked by the interpreter or by machine code.  So some form of non-local jump is required. But at least as
	   early as MSVC Community 2017, the Microshaft longjmp performs stack unwinding which gets hoplessly confused
	   (bless its little heart) by any stack switch between machine code and C stack, and raises a spurious
	   Stack cookie instrumentation code detected a stack-based buffer overrun
	   error from the bowels of gs_report.c _GSHandlerCheck.
	   Since the CoInterpreter maintains the base of the C stack in CFramePointer & CStackPointer, it is straight-forward
	   for us to simply call interpret after doing the switch to the C stack, avoiding the stack unwind issue altogether. */
	ceCannotResumeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCannotResume, "ceCannotResumeTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 0);

	/* These two are unusual; they are reached by return instructions. */
	ceInvokeInterpret = genInvokeInterpretTrampoline();
	ceReturnToInterpreterTrampoline = genReturnToInterpreterTrampoline();
	ceBaseFrameReturnTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceBaseFrameReturn, "ceBaseFrameReturnTrampoline", 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 0);
	}


/*	Generate a routine ceCaptureCStackPointers that will capture the C stack
	pointer, and, if it is in use, the C frame pointer. These are used in
	trampolines to call
	run-time routines in the interpreter from machine-code. */

	/* Cogit>>#generateStackPointerCapture */
static void
generateStackPointerCapture(void)
{
    usqInt oldMethodZoneBase;
    sqInt oldTrampolineTableIndex;


#  if defined(cFramePointerInUse)
	assertCStackWellAligned();
	generateCaptureCStackPointers(cFramePointerInUse);
#  else

	/* For the benefit of the following assert, assume the minimum at first. */
	cFramePointerInUse = 0;
	assertCStackWellAligned();
	oldMethodZoneBase = methodZoneBase;
	oldTrampolineTableIndex = trampolineTableIndex;
	generateCaptureCStackPointers(1);
	ceCaptureCStackPointers();
	if (!((cFramePointerInUse = checkIfCFramePointerInUse()))) {
		methodZoneBase = oldMethodZoneBase;
		trampolineTableIndex = oldTrampolineTableIndex;
		generateCaptureCStackPointers(0);
	}
#  endif // defined(cFramePointerInUse)
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	assertCStackWellAligned();
}


/*	Generate the run-time entries and exits at the base of the native code
	zone and update the base.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* Cogit>>#generateTrampolines */
static void
generateTrampolines(void)
{
    sqInt fixupSize;
    usqInt methodZoneStart;
    sqInt opcodeSize;

	methodZoneStart = methodZoneBase;
	(methodLabel->address = methodZoneStart);
	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 80;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	setHasYoungReferent(0);
	maybeGenerateSelectorIndexDereferenceRoutine();
	generateSendTrampolines();
	generateMissAbortTrampolines();
	generateObjectRepresentationTrampolines();
	generateRunTimeTrampolines();
	generateSistaRuntime();
	generateEnilopmarts();
	generateTracingTrampolines();
	recordGeneratedRunTimeaddress("methodZoneBase", methodZoneBase);
}

	/* Cogit>>#generatorForPC: */
static BytecodeDescriptor * NoDbgRegParms
generatorForPC(sqInt pc)
{
	return generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
}


/*	Generate a pair of routines that answer the frame pointer, and the stack
	pointer immediately
	after a leaf call, used for checking stack pointer alignment, frame
	pointer usage, etc. N.B.
	these are exported to the CoInterpreter et al via Cogit
	class>>mustBeGlobal:. 
 */

	/* Cogit>>#genGetLeafCallStackPointers */
static void
genGetLeafCallStackPointers(void)
{
    sqInt fixupSize;
    sqInt opcodeSize;
    usqInt startAddress;

	/* begin allocateOpcodes:bytecodes: */
	numAbstractOpcodes = 4;
	opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
	abstractOpcodes = alloca(opcodeSize + fixupSize);
	bzero(abstractOpcodes, opcodeSize + fixupSize);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	startAddress = methodZoneBase;
	genoperandoperand(MoveRR, FPReg, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetFP", startAddress);
	ceGetFP = ((usqIntptr_t (*)(void)) startAddress);
	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genoperandoperand(MoveRR, NativeSPReg, ABIResultReg);
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceGetSP", startAddress);
	ceGetSP = ((usqIntptr_t (*)(void)) startAddress);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged target
	or a call of ceMNUFromPICMNUMethod:receiver: to handle an MNU dispatch
	in a closed PIC. It distinguishes the two by testing ClassReg. If the
	register is zero then this is an MNU.
	
	This poses a problem in 32-bit Spur, where zero is the cache tag for
	immediate characters (tag pattern 2r10) because SmallIntegers have tag
	patterns 2r11
	and 2r01, so anding with 1 reduces these to 0 & 1. We solve the ambiguity
	by patching send sites with a 0 cache tag to open PICs instead of closed
	PICs.  */

	/* Cogit>>#genInnerPICAbortTrampoline: */
static usqInt NoDbgRegParms
genInnerPICAbortTrampoline(char *name)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpMNUCase;

	/* begin CmpCq:R: */
	anInstruction = genoperandoperand(CmpCqR, 0 /* picAbortDiscriminatorValue */, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0 /* picAbortDiscriminatorValue */, BytesPerOop));
	}
	jumpMNUCase = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceInterpretMethodFromPICreceiver, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpMNUCase, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceMNUFromPICMNUMethodreceiver, name, 2, SendNumArgsReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
}


/*	Switch to the C stack (do *not* save the Smalltalk stack pointers;
	this is the caller's responsibility), and invoke interpret PDQ. */

	/* Cogit>>#genInvokeInterpretTrampoline */
static void (*genInvokeInterpretTrampoline(void))(void)
{
    AbstractInstruction *anInstruction;
    sqInt operandOne;
    sqInt quickConstant;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	/* begin MoveCq:R: */
	quickConstant = varBaseAddress();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);
	/* begin gen:literal:operand: */
	operandOne = cReturnAddressAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, LinkReg));
	/* begin JumpFull: */
	checkLiteralforInstruction(((sqInt)(((usqInt)interpret))), genoperand(JumpFull, ((sqInt)(((usqInt)interpret)))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceInvokeInterpret", startAddress);
	return ((void (*)(void)) startAddress);
}


/*	The in-line cache for a send is implemented as a constant load into
	ClassReg. We always use a 32-bit load, even in 64-bits.
	
	In the initial (unlinked) state the in-line cache is notionally loaded
	with the selector.
	But since in 64-bits an arbitrary selector oop won't fit in a 32-bit
	constant load, we
	instead load the cache with the selector's index, either into the literal
	frame of the
	current method, or into the special selector array. Negative values are
	1-relative indices into the special selector array.
	
	When a send is linked, the load of the selector, or selector index, is
	overwritten with a
	load of the receiver's class, or class tag. Hence, the 64-bit VM is
	currently constrained
	to use class indices as cache tags. If out-of-line literals are used,
	distinct caches /must
	not/ share acche locations, for if they do, send cacheing will be confused
	by the sharing.
	Hence we use the MoveUniqueC32:R: instruction that will not share literal
	locations.  */

	/* Cogit>>#genLoadInlineCacheWithSelector: */
static void NoDbgRegParms
genLoadInlineCacheWithSelector(sqInt selectorIndex)
{
    AbstractInstruction *anInstruction;
    sqInt cacheValue;
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt iLimiT;
    sqInt initialNumLiterals;
    AbstractInstruction *literalInstruction;
    AbstractInstruction *litInst;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;
    sqInt opcode;

	assert((selectorIndex < 0
		? (((-selectorIndex) >= 1) && ((-selectorIndex) <= (numSpecialSelectors())))
		: ((selectorIndex >= 0) && (selectorIndex <= ((literalCountOf(methodObj)) - 1)))));
	cacheValue = selectorIndex;
	/* begin gen:uniqueLiteral32:operand: */
	opcode = MoveC32R;
	/* begin uniqueLiteral32:forInstruction: */
	anInstruction = genoperandoperand(opcode, cacheValue, ClassReg);
	assert(usesOutOfLineLiteral(anInstruction));
	/* begin allocateLiteral: */
	if (nextLiteralIndex >= literalsSize) {
		/* begin allocateLiterals: */
		initialNumLiterals = literalsSize + 8;
		if (initialNumLiterals > literalsSize) {

			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (!(literals == null)) {
				for (i = 0; i < nextLiteralIndex; i += 1) {
					existingInst = literalInstructionAt(i);
					newInst = (&(newLiterals[i]));
					cloneLiteralFrom(newInst, existingInst);
					assert(((existingInst->dependent)) == null);
					(existingInst->dependent = newInst);
				}
				for (i = 0, iLimiT = (opcodeIndex - 1); i <= iLimiT; i += 1) {
					existingInst = abstractInstructionAt(i);
					if ((((existingInst->dependent)) != null)
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	litInst = literalInstructionAt(nextLiteralIndex);
	initializeUniqueLiteral(litInst, cacheValue);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}
	literalInstruction = litInst;
	setLiteralSize(literalInstruction, 4);
	(anInstruction->dependent = literalInstruction);
}

	/* Cogit>>#genReturnToInterpreterTrampoline */
static usqInt
genReturnToInterpreterTrampoline(void)
{
    AbstractInstruction *anInstruction;
    sqInt operandOne;
    sqInt operandTwo;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genoperand(PushR, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, FoxIFSavedIP, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(FoxIFSavedIP, BytesPerOop));
	}
	/* begin gen:operand:literal: */
	operandTwo = instructionPointerAddress();
	checkLiteralforInstruction(operandTwo, genoperandoperand(MoveRAw, TempReg, operandTwo));
	genSmalltalkToCStackSwitch(0);
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);
	/* begin gen:literal:operand: */
	operandOne = cReturnAddressAddress();
	checkLiteralforInstruction(operandOne, genoperandoperand(MoveAwR, operandOne, LinkReg));
	/* begin JumpFull: */
	checkLiteralforInstruction(((sqInt)(((usqInt)interpret))), genoperand(JumpFull, ((sqInt)(((usqInt)interpret)))));
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceReturnToInterpreterTrampoline", startAddress);
	return startAddress;
}


/*	If the client requires, then on an ARM-like RISC processor, the return
	address needs to
	be pushed to the stack so that the interpreter sees the same stack layout
	as on CISC.
 */

	/* Cogit>>#genSmalltalkToCStackSwitch: */
static sqInt NoDbgRegParms
genSmalltalkToCStackSwitch(sqInt pushLinkReg)
{
	if (pushLinkReg) {
		genoperand(PushR, LinkReg);
	}
	genSaveStackPointers(backEnd);
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	return 0;
}


/*	Generate a trampoline with up to four arguments. Generate either a call or
	a jump to aRoutineOrNil
	as requested by callJumpBar. If generating a call and resultRegOrNone is
	not NoReg pass the C result
	back in resultRegOrNone.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* Cogit>>#genTrampolineFor:called:numArgs:arg:arg:arg:arg:regsToSave:pushLinkReg:resultReg:appendOpcodes: */
static usqInt NoDbgRegParms
genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(void *aRoutine, char *trampolineName, sqInt numArgs, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3, sqInt regMask, sqInt pushLinkReg, sqInt resultRegOrNone, sqInt appendBoolean)
{
    unsigned int codeInstruction;
    unsigned int dataInstruction;
    usqInt startAddress;

	startAddress = methodZoneBase;
	if (!appendBoolean) {
		zeroOpcodeIndex();
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(aRoutine, numArgs, regOrConst0, regOrConst1, regOrConst2, regOrConst3, regMask, pushLinkReg, resultRegOrNone);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At(codeBase + cmNoCheckEntryOffset);
	dataInstruction = long32At((codeBase + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
	return startAddress;
}


/*	To return from a trampoline call we have to take the return address off
	the stack,
	iof it has been saved */

	/* Cogit>>#genTrampolineReturn: */
static void NoDbgRegParms
genTrampolineReturn(sqInt lnkRegWasPushed)
{
	if (lnkRegWasPushed) {
		genoperand(PopR, LinkReg);
		genoperand(RetN, 0);
	}
	else {
		genoperand(RetN, 0);
	}
}


/*	<Integer> */

	/* Cogit>>#gen: */
static AbstractInstruction * NoDbgRegParms
gen(sqInt opcode)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand: */
static AbstractInstruction * NoDbgRegParms
genoperand(sqInt opcode, sqInt operand)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operand;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand: */
static AbstractInstruction * NoDbgRegParms
genoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	return abstractInstruction;
}


/*	<Integer> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */
/*	<Integer|CogAbstractInstruction> */

	/* Cogit>>#gen:operand:operand:operand: */
static AbstractInstruction * NoDbgRegParms
genoperandoperandoperand(sqInt opcode, sqInt operandOne, sqInt operandTwo, sqInt operandThree)
{
    AbstractInstruction *abstractInstruction;

	assert(opcodeIndex < numAbstractOpcodes);
	abstractInstruction = abstractInstructionAt(opcodeIndex);
	opcodeIndex += 1;
	(abstractInstruction->opcode = opcode);
	((abstractInstruction->operands))[0] = operandOne;
	((abstractInstruction->operands))[1] = operandTwo;
	((abstractInstruction->operands))[2] = operandThree;
	return abstractInstruction;
}

	/* Cogit>>#getLiteral: */
static sqInt NoDbgRegParms
getLiteral(sqInt litIndex)
{
	if (maxLitIndex < litIndex) {
		maxLitIndex = litIndex;
	}
	return literalofMethod(litIndex, methodObj);
}


/*	Access for the literal manager. */

	/* Cogit>>#getOpcodeIndex */
static sqInt
getOpcodeIndex(void)
{
	return opcodeIndex;
}

	/* Cogit>>#incrementUsageOfTargetIfLinkedSend:mcpc:ignored: */
static sqInt NoDbgRegParms
incrementUsageOfTargetIfLinkedSendmcpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;

	if (annotation >= IsSendCall) {
		assert(annotation != IsNSSendCall);
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmUsageCount)) < (CMMaxUsageCount / 2)) {
				((((CogMethod *) ((((usqInt)targetMethod1)) + codeToDataDelta)))->cmUsageCount = ((targetMethod1->cmUsageCount)) + 1);
			}
		}
	}
	return 0;
}


/*	Answer the value to put in an inline-cache that is being loaded with the
	selector. Usually this is simply the selector, but in 64-bits the cache is
	only 32-bits wide
	and so the cache is loaded with the index of the selector. */
/*	First search the special selectors; there are only 32 of them so this
	shouldn't take too long.
	We could short-circuit this by keeping a hint bit in the target method, or
	by maintaining the
	maximum range of selector oops in specialSelectors since they're likely to
	cluster. 
 */

	/* Cogit>>#indexForSelector:in: */
static sqInt NoDbgRegParms
indexForSelectorin(sqInt selector, CogMethod *cogMethod)
{
    sqInt i;
    sqInt iLimiT;
    sqInt methodOop;

	for (i = 0; i < NumSpecialSelectors; i += 1) {
		if (selector == (specialSelector(i))) {
			return -1 - i;
		}
	}

	/* Then search the method's literal frame... open code fetchPointer:ofObject: for speed... */
	methodOop = (cogMethod->methodObject);
	for (i = LiteralStart, iLimiT = (literalCountOfMethodHeader((cogMethod->methodHeader))); i <= iLimiT; i += 1) {
		if ((longAt(((i * BytesPerOop) + BaseHeaderSize) + methodOop)) == selector) {
			assert(selector == (literalofMethod(i - 1, methodOop)));
			return i - 1;
		}
	}
	error("could not find selector in method when unlinking send site");
	return 0;
}


/*	Answer a usage count that reflects likely long-term usage. */

	/* Cogit>>#initialClosedPICUsageCount */
static sqInt
initialClosedPICUsageCount(void)
{
	return CMMaxUsageCount / 2;
}

	/* Cogit>>#initializeBackend */
static void
initializeBackend(void)
{
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt iLimiT;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;

	(methodLabel->machineCodeSize = 0);
	(methodLabel->opcode = Label);
	((methodLabel->operands))[0] = 0;
	((methodLabel->operands))[1] = 0;
	assert((!((registerMaskFor(VarBaseReg)) & CallerSavedRegisterMask)));
	varBaseAddress = computeGoodVarBaseAddress();
	assert((stackLimitAddress()) >= varBaseAddress);
	assert((cStackPointerAddress()) >= varBaseAddress);
	assert((cFramePointerAddress()) >= varBaseAddress);
	assert((cReturnAddressAddress()) >= varBaseAddress);
	assert((nextProfileTickAddress()) >= varBaseAddress);
	/* begin allocateLiterals: */
	if (4 > literalsSize) {

		/* Must copy across state (not using realloc, cuz...) and
		   must also update existing instructions to refer to the new ones...
		   It's either this or modify all generation routines to be able to retry
		   with more literals after running out of literals. */
		newLiterals = calloc(4, sizeof(CogAbstractInstruction));
		if (!(literals == null)) {
			for (i = 0; i < nextLiteralIndex; i += 1) {
				existingInst = literalInstructionAt(i);
				newInst = (&(newLiterals[i]));
				cloneLiteralFrom(newInst, existingInst);
				assert(((existingInst->dependent)) == null);
				(existingInst->dependent = newInst);
			}
			for (i = 0, iLimiT = (opcodeIndex - 1); i <= iLimiT; i += 1) {
				existingInst = abstractInstructionAt(i);
				if ((((existingInst->dependent)) != null)
				 && (((((existingInst->dependent))->opcode)) == Literal)) {
					(existingInst->dependent = (((existingInst->dependent))->dependent));
				}
			}
		}
		free(literals);
		literals = newLiterals;
		literalsSize = 4;
	}
	/* begin resetLiterals */

	/* an impossibly high value */
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
}


/*	Answer a usage count that reflects likely long-term usage.
	Answer 1 for non-primitives or quick primitives (inst var accessors),
	2 for methods with interpreter primitives, and 3 for compiled primitives. */

	/* Cogit>>#initialMethodUsageCount */
static sqInt
initialMethodUsageCount(void)
{
	if ((primitiveIndex == 1)
	 || (isQuickPrimitiveIndex(primitiveIndex))) {
		return 1;
	}
	if (!(primitiveGeneratorOrNil())) {
		return 2;
	}
	return 3;
}


/*	Answer a usage count that reflects likely long-term usage. */

	/* Cogit>>#initialOpenPICUsageCount */
static sqInt
initialOpenPICUsageCount(void)
{
	return CMMaxUsageCount - 1;
}

	/* Cogit>>#inverseBranchFor: */
static sqInt NoDbgRegParms
inverseBranchFor(sqInt opcode)
{
	switch (opcode) {
	case JumpLongZero:
		return JumpLongNonZero;

	case JumpLongNonZero:
		return JumpLongZero;

	case JumpZero:
		return JumpNonZero;

	case JumpNonZero:
		return JumpZero;

	case JumpNegative:
		return JumpNonNegative;

	case JumpNonNegative:
		return JumpNegative;

	case JumpOverflow:
		return JumpNoOverflow;

	case JumpNoOverflow:
		return JumpOverflow;

	case JumpCarry:
		return JumpNoCarry;

	case JumpNoCarry:
		return JumpCarry;

	case JumpLess:
		return JumpGreaterOrEqual;

	case JumpGreaterOrEqual:
		return JumpLess;

	case JumpGreater:
		return JumpLessOrEqual;

	case JumpLessOrEqual:
		return JumpGreater;

	case JumpBelow:
		return JumpAboveOrEqual;

	case JumpAboveOrEqual:
		return JumpBelow;

	case JumpAbove:
		return JumpBelowOrEqual;

	case JumpBelowOrEqual:
		return JumpAbove;

	default:
		error("Case not found and no otherwise clause");
	}
	error("invalid opcode for inverse");
	return 0;
}


/*	See Cogit class>>initializeAnnotationConstants */

	/* Cogit>>#isPCMappedAnnotation: */
static sqInt NoDbgRegParms
isPCMappedAnnotation(sqInt annotation)
{
	return annotation >= HasBytecodePC;
}


/*	Useful for debugging. Marked <api> so the code generator won't delete it. */

	/* Cogit>>#isPCWithinMethodZone: */
static sqInt NoDbgRegParms
isPCWithinMethodZone(void *address)
{
	return (((((usqInt)address)) >= methodZoneBase) && ((((usqInt)address)) <= (freeStart())));
}


/*	Answer if the instruction preceding retpc is a call instruction. */

	/* Cogit>>#isSendReturnPC: */
sqInt
isSendReturnPC(sqInt retpc)
{
    sqInt target;

	if (!(isCallPrecedingReturnPC(backEnd, retpc))) {
		return 0;
	}
	target = callTargetFromReturnAddress(backEnd, retpc);
	return (((target >= firstSend) && (target <= lastSend)))
	 || (((target >= methodZoneBase) && (target <= (freeStart()))));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPEqual: */
static AbstractInstruction * NoDbgRegParms
gJumpFPEqual(void *jumpTarget)
{
	/* begin genJumpFPEqual: */
	return genoperand(JumpFPEqual, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreaterOrEqual: */
static AbstractInstruction * NoDbgRegParms
gJumpFPGreaterOrEqual(void *jumpTarget)
{
	/* begin genJumpFPGreaterOrEqual: */
	return genoperand(JumpFPGreaterOrEqual, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPGreater: */
static AbstractInstruction * NoDbgRegParms
gJumpFPGreater(void *jumpTarget)
{
	/* begin genJumpFPGreater: */
	return genoperand(JumpFPGreater, ((sqInt)jumpTarget));
}


/*	Floating-point jumps are a little weird on some processors. Defer to
	the backEnd to allow it to generate any special code it may need to. */

	/* Cogit>>#JumpFPNotEqual: */
static AbstractInstruction * NoDbgRegParms
gJumpFPNotEqual(void *jumpTarget)
{
	/* begin genJumpFPNotEqual: */
	return genoperand(JumpFPNotEqual, ((sqInt)jumpTarget));
}

	/* Cogit>>#LogicalShiftLeftCq:R: */
static AbstractInstruction * NoDbgRegParms
gLogicalShiftLeftCqR(sqInt quickConstant, sqInt reg)
{
	return genoperandoperand(LogicalShiftLeftCqR, quickConstant, reg);
}


/*	destReg := srcReg << quickConstant */

	/* Cogit>>#LogicalShiftLeftCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gLogicalShiftLeftCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(LogicalShiftLeftCqRR, quickConstant, srcReg, destReg);
	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, destReg);
	return first;
}


/*	destReg := (unsigned)srcReg >> quickConstant */

	/* Cogit>>#LogicalShiftRightCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gLogicalShiftRightCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(LogicalShiftRightCqRR, quickConstant, srcReg, destReg);
	first = genoperandoperand(MoveRR, srcReg, destReg);
	genoperandoperand(LogicalShiftRightCqR, quickConstant, destReg);
	return first;
}

	/* Cogit>>#lastOpcode */
static AbstractInstruction *
lastOpcode(void)
{
	assert(opcodeIndex > 0);
	return abstractInstructionAt(opcodeIndex - 1);
}

	/* Cogit>>#linkSendAt:in:to:offset:receiver: */
void
linkSendAtintooffsetreceiver(sqInt callSiteReturnAddress, CogMethod *sendingMethod, CogMethod *targetMethod, sqInt theEntryOffset, sqInt receiver)
{
    sqInt extent;
    sqInt inlineCacheTag;

	assert((theEntryOffset == cmEntryOffset)
	 || (theEntryOffset == cmNoCheckEntryOffset));
	assert(((callSiteReturnAddress >= methodZoneBase) && (callSiteReturnAddress <= (freeStart()))));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	if (theEntryOffset == cmNoCheckEntryOffset) {

		/* no need to change selector cache tag */
		extent = rewriteCallAttarget(backEnd, callSiteReturnAddress, (((sqInt)targetMethod)) + cmNoCheckEntryOffset);
	}
	else {
		inlineCacheTag = inlineCacheTagForInstance(receiver);
		extent = rewriteInlineCacheAttagtarget(backEnd, callSiteReturnAddress, inlineCacheTag, (((sqInt)targetMethod)) + theEntryOffset);
	}
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) ((((usqInt)callSiteReturnAddress)) - extent)), (((usqInt)callSiteReturnAddress)) - ((((usqInt)callSiteReturnAddress)) - extent));
	sys_icache_invalidate(((void *) ((((usqInt)callSiteReturnAddress)) - extent)), (((usqInt)callSiteReturnAddress)) - ((((usqInt)callSiteReturnAddress)) - extent));
#  else // __APPLE__ && __MACH__
	ceFlushICache((((usqInt)callSiteReturnAddress)) - extent, ((usqInt)callSiteReturnAddress));
#  endif
}

	/* Cogit>>#loadBytesAndGetDescriptor */
static BytecodeDescriptor *
loadBytesAndGetDescriptor(void)
{
    BytecodeDescriptor *descriptor;

	byte0 = (fetchByteofObject(bytecodePC, methodObj)) + bytecodeSetOffset;
	descriptor = generatorAt(byte0);
	loadSubsequentBytesForDescriptorat(descriptor, bytecodePC);
	return descriptor;
}

	/* Cogit>>#loadSubsequentBytesForDescriptor:at: */
static void NoDbgRegParms
loadSubsequentBytesForDescriptorat(BytecodeDescriptor *descriptor, sqInt pc)
{
	if (((descriptor->numBytes)) > 1) {
		byte1 = fetchByteofObject(pc + 1, methodObj);
		if (((descriptor->numBytes)) > 2) {
			byte2 = fetchByteofObject(pc + 2, methodObj);
			if (((descriptor->numBytes)) > 3) {
				byte3 = fetchByteofObject(pc + 3, methodObj);
				if (((descriptor->numBytes)) > 4) {
					notYetImplemented();
				}
			}
		}
	}
}


/*	N.B. On certain targets (including X64) this instruction may smash TempReg
	if the target reg is either FPReg or SPReg.
 */

	/* Cogit>>#MoveAw:R: */
static AbstractInstruction * NoDbgRegParms
gMoveAwR(sqInt address, sqInt reg)
{
	/* begin gen:literal:operand: */
	return checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, reg));
}

	/* Cogit>>#MoveCw:R: */
static AbstractInstruction * NoDbgRegParms
gMoveCwR(sqInt wordConstant, sqInt reg)
{
	/* begin gen:literal:operand: */
	return checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, reg));
}

	/* Cogit>>#MovePerfCnt64R:L: */
static AbstractInstruction * NoDbgRegParms
gMovePerfCnt64RL(sqInt destReg, sqInt liveRegisterMask)
{
	assert(BytesPerWord == 8);
	return genoperandoperand(MovePerfCnt64RL, destReg, liveRegisterMask);
}

	/* Cogit>>#MovePerfCnt64R:R:L: */
static AbstractInstruction * NoDbgRegParms
gMovePerfCnt64RRL(sqInt destRegLo, sqInt destRegHi, sqInt liveRegisterMask)
{
	assert(BytesPerWord == 4);
	return genoperandoperandoperand(MovePerfCnt64RRL, destRegLo, destRegHi, liveRegisterMask);
}


/*	Answer the address of the null byte at the end of the method map. */

	/* Cogit>>#mapEndFor: */
static usqInt NoDbgRegParms
mapEndFor(CogMethod *cogMethod)
{
    usqInt end;

	end = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	while ((byteAt(end)) != MapEnd) {
		end -= 1;
		assert(end > (firstMappedPCFor(cogMethod)));
	}
	return end;
}


/*	Unlinking/GC/Disassembly support */
/*	most of the time arg is a CogMethod... */

	/* Cogit>>#mapFor:performUntil:arg: */
static sqInt NoDbgRegParms
mapForperformUntilarg(CogMethod *cogMethod, sqInt (*functionSymbol)(sqInt annotation, char *mcpc, CogMethod *arg), CogMethod *arg)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	/* begin firstMappedPCFor: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {

			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = functionSymbol(annotation, (((char *) mcpc)), arg);
			if (result != 0) {
				return result;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}


/*	Remap all object references in the closed PIC. Answer if any references
	are young.
	Set codeModified if any modifications are made. */

	/* Cogit>>#mapObjectReferencesInClosedPIC: */
static sqInt NoDbgRegParms
mapObjectReferencesInClosedPIC(CogMethod *cPIC)
{
    sqInt i;
    sqInt pc;
    sqInt refersToYoung;


	/* first we check the potential method oop load at the beginning of the CPIC */
	pc = addressOfEndOfCaseinCPIC(1, cPIC);

	/* We find the end address of the cPICNumCases'th case and can then just step forward by the case size thereafter */
	refersToYoung = remapMaybeObjRefInClosedPICAt(pc - (jumpLongByteSize(backEnd)));

	/* Next we check the potential class ref in the compare instruction, and the potential method oop load for each case. */
	pc = addressOfEndOfCaseinCPIC((cPIC->cPICNumCases), cPIC);
	for (i = 2; i <= ((cPIC->cPICNumCases)); i += 1) {
		if (remapMaybeObjRefInClosedPICAt((pc - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd)))) {
			refersToYoung = 1;
		}
		pc += cPICCaseSize;
	}
	return refersToYoung;
}


/*	Update all references to objects in the generated runtime. */

	/* Cogit>>#mapObjectReferencesInGeneratedRuntime */
static void
mapObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    sqInt mappedLiteral;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = longAt(mcpc);
		mappedLiteral = remapObject(literal);
		if (mappedLiteral != literal) {
			/* begin setCodeModified */
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			if (!codeModified) {
				codeModified = 1;
				/* begin makeCodeZoneWritable */
#        if __APPLE__ && __MACH__
				pthread_jit_write_protect_np(0);
				PJWPNClear = __LINE__;
				if (PJWPNState) {
					PJWPNChange = __LINE__;
					PJWPNState = 0;
				}
#        endif // __APPLE__ && __MACH__
			}
#      endif // DUAL_MAPPED_CODE_ZONE
			/* begin storeLiteral:atAnnotatedAddress:using: */
			codeLongAtput(mcpc, mappedLiteral);
		}
	}
}


/*	Update all references to objects in machine code for a become.
	Unlike incrementalGC or fullGC a method that does not refer to young may
	refer to young as a result of the become operation. Unlike incrementalGC
	or fullGC the reference from a Cog method to its methodObject *must not*
	change since the two are two halves of the same object. */

	/* Cogit>>#mapObjectReferencesInMachineCodeForBecome */
static void
mapObjectReferencesInMachineCodeForBecome(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    sqInt freedPIC;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt remappedMethod;
    sqInt result;
    CogMethod *writableCogMethod;

	hasYoungObj = 0;
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	codeModified = (freedPIC = 0);
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		assert(!hasYoungObj);
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				if ((isYoung((cogMethod->selector)))
				 || (mapObjectReferencesInClosedPIC(cogMethod))) {
					freedPIC = 1;
					freeMethod(cogMethod);
				}
			}
			else {
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					remappedMethod = remapOop((cogMethod->methodObject));
					if (remappedMethod != ((cogMethod->methodObject))) {
						if (methodHasCogMethod(remappedMethod)) {
							error("attempt to become two cogged methods");
						}
						if (!(withoutForwardingOnandwithsendToCogit((cogMethod->methodObject), remappedMethod, (cogMethod->cmUsesPenultimateLit), methodhasSameCodeAscheckPenultimate))) {
							error("attempt to become cogged method into different method");
						}
						if ((rawHeaderOf((cogMethod->methodObject))) == (((sqInt)cogMethod))) {
							rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
							rawHeaderOfput(remappedMethod, ((sqInt)cogMethod));
						}
						else {
							assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
							(writableCogMethod->methodHeader = rawHeaderOf(remappedMethod));
							(writableCogMethod->methodObject = remappedMethod);
						}
					}
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {

						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, (((char *) mcpc)), (((CogMethod *) hasYoungObjPtr)));
						if (result != 0) {
							goto l2;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
	l2:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					ensureInYoungReferrers(cogMethod);
					hasYoungObj = 0;
				}
				else {
					(cogMethod->cmRefersToYoung = 0);
				}
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (freedPIC) {
		unlinkSendsToFree();
	}
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
		sys_icache_invalidate(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)codeBase), endAddress);
#    endif
	}
}


/*	Update all references to objects in machine code for a full gc. Since
	the current (New)ObjectMemory GC makes everything old in a full GC
	a method not referring to young will not refer to young afterwards */

	/* Cogit>>#mapObjectReferencesInMachineCodeForFullGC */
static void
mapObjectReferencesInMachineCodeForFullGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableCogMethod;

	codeModified = 0;
	mapObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
			(writableCogMethod->selector = remapOop((cogMethod->selector)));
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(!((cogMethod->cmRefersToYoung)));
				mapObjectReferencesInClosedPIC(cogMethod);
			}
			else {
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {

						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, (((char *) mcpc)), 0);
						if (result != 0) {
							goto l2;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
	l2:	/* end mapFor:performUntil:arg: */;
							}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	pruneYoungReferrers();
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
		sys_icache_invalidate(((void *) (((usqInt)codeBase))), endAddress - (((usqInt)codeBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)codeBase), endAddress);
#    endif
	}
}


/*	Update all references to objects in machine code for either a Spur
	scavenging gc
	or a Squeak V3 incremental GC. Avoid scanning all code by using the
	youngReferrers list. In a young gc a method referring to young may no
	longer refer to young, but a
	method not referring to young cannot and will not refer to young
	afterwards.  */

	/* Cogit>>#mapObjectReferencesInMachineCodeForYoungGC */
static void
mapObjectReferencesInMachineCodeForYoungGC(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    sqInt hasYoungObj;
    sqInt hasYoungObjPtr;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    usqInt pointer;
    sqInt result;
    CogMethod *writableCogMethod;
    sqInt zoneIsWritable;

	codeModified = (zoneIsWritable = (hasYoungObj = 0));
	hasYoungObjPtr = ((sqInt)((&hasYoungObj)));
	pointer = youngReferrers();
	while (pointer < limitAddress) {
		assert(!hasYoungObj);
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (((cogMethod->cmType)) == CMFree) {
			assert(!((cogMethod->cmRefersToYoung)));
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			if ((cogMethod->cmRefersToYoung)) {
				assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
				 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
				if (!zoneIsWritable) {
					/* begin ensureWritableCodeZone */
#          if !DUAL_MAPPED_CODE_ZONE
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
#          endif // !DUAL_MAPPED_CODE_ZONE
					zoneIsWritable = 1;
				}
				writableCogMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
				(writableCogMethod->selector = remapOop((cogMethod->selector)));
				if (isYoung((cogMethod->selector))) {
					hasYoungObj = 1;
				}
				if (((cogMethod->cmType)) >= CMMethod) {
					assert(((cogMethod->objectHeader)) == (nullHeaderForMachineCodeMethod()));
					(writableCogMethod->methodObject = remapOop((cogMethod->methodObject)));
					if (isYoung((cogMethod->methodObject))) {
						hasYoungObj = 1;
					}
				}
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {

						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = remapIfObjectRefpchasYoung(annotation, (((char *) mcpc)), (((void *)hasYoungObjPtr)));
						if (result != 0) {
							goto l2;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
	l2:	/* end mapFor:performUntil:arg: */;
				if (hasYoungObj) {
					hasYoungObj = 0;
				}
				else {
					(writableCogMethod->cmRefersToYoung = 0);
				}
			}
		}
		pointer += BytesPerWord;
	}
	pruneYoungReferrers();
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
		sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)methodZoneBase), endAddress);
#    endif
	}
}


/*	Update all references to objects in machine code. */

	/* Cogit>>#mapObjectReferencesInMachineCode: */
void
mapObjectReferencesInMachineCode(sqInt gcMode)
{
	switch (gcMode) {
	case GCModeNewSpace:
		
		/* N.B. do *not* ensureWritableCodeZone for every scavenge. */
		mapObjectReferencesInMachineCodeForYoungGC();
		break;
	case GCModeFull:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		mapObjectReferencesInMachineCodeForFullGC();
		break;
	case GCModeBecome:
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		mapObjectReferencesInMachineCodeForBecome();
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	mapPerMethodProfile();
	if (!(asserta((freeStart()) <= (youngReferrers())))) {
		error("youngReferrers list overflowed");
	}
}


/*	Mark objects in machine-code of marked methods (or open PICs with marked
	selectors). 
 */

	/* Cogit>>#markAndTraceMachineCodeOfMarkedMethods */
void
markAndTraceMachineCodeOfMarkedMethods(void)
{
    sqInt annotation;
    sqInt annotation1;
    CogMethod *cogMethod;
    sqInt endAddress;
    usqInt map;
    usqInt map1;
    sqInt mapByte;
    sqInt mapByte1;
    sqInt mcpc;
    sqInt mcpc1;
    sqInt result;
    sqInt result1;
    usqInt theCounters;
    usqInt theCounters1;

	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	codeModified = 0;
	markAndTraceObjectReferencesInGeneratedRuntime();
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (isMarked((cogMethod->methodObject)))) {
			/* begin markAndTraceLiteralsIn: */
			assert(((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
			 && (isMarked((cogMethod->methodObject))))
			 || ((isCMOpenPIC(((CogBlockMethod *) cogMethod)))
			 && ((isImmediate((cogMethod->selector)))
			 || (isMarked((cogMethod->selector))))));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkCountersIn: */
			theCounters = (cogMethod->counters);
			if (theCounters != 0) {
				markAndTrace(theCounters - BaseHeaderSize);
			}
			/* begin maybeMarkIRCsIn: */
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = markLiteralspcmethod(annotation, (((char *) mcpc)), cogMethod);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		if ((((cogMethod->cmType)) == CMOpenPIC)
		 && ((isImmediate((cogMethod->selector)))
		 || (isMarked((cogMethod->selector))))) {
			/* begin markAndTraceLiteralsIn: */
			assert(((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
			 && (isMarked((cogMethod->methodObject))))
			 || ((isCMOpenPIC(((CogBlockMethod *) cogMethod)))
			 && ((isImmediate((cogMethod->selector)))
			 || (isMarked((cogMethod->selector))))));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkCountersIn: */
			theCounters1 = (cogMethod->counters);
			if (theCounters1 != 0) {
				markAndTrace(theCounters1 - BaseHeaderSize);
			}
			/* begin maybeMarkIRCsIn: */
			/* begin mapFor:performUntil:arg: */
			mcpc1 = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map1 = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte1 = byteAt(map1))) != MapEnd) {
				if (mapByte1 >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc1 += (mapByte1 & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation1 = ((usqInt)(mapByte1)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte1 = byteAt(map1 - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation1 += mapByte1 & DisplacementMask;
						map1 -= 1;
					}
					result1 = markLiteralspcmethod(annotation1, (((char *) mcpc1)), cogMethod);
					if (result1 != 0) {
						goto l4;
					}
				}
				else {
					if (mapByte1 < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc1 += (((sqInt)((usqInt)((mapByte1 - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map1 -= 1;
			}
	l4:	/* end mapFor:performUntil:arg: */;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (leakCheckFullGC()) {
		asserta(allMachineCodeObjectReferencesValid());
	}
	if (codeModified) {

		/* After updating oops in inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
		sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)methodZoneBase), endAddress);
#    endif
	}
}


/*	Mark and trace any object references in the generated run-time. */

	/* Cogit>>#markAndTraceObjectReferencesInGeneratedRuntime */
static void
markAndTraceObjectReferencesInGeneratedRuntime(void)
{
    sqInt i;
    sqInt literal;
    usqInt mcpc;

	for (i = 0; i < runtimeObjectRefIndex; i += 1) {
		mcpc = objectReferencesInRuntime[i];
		literal = longAt(mcpc);
		markAndTraceLiteralinatpc(literal, ((CogMethod *) null), ((usqInt)mcpc));
	}
}


/*	Mark and trace objects in the argument and free if it is appropriate.
	Answer if the method has been freed. firstVisit is a hint used to avoid
	scanning methods we've already seen. False positives are fine.
	For a CMMethod this
	frees if the bytecode method isnt marked,
	marks and traces object literals and selectors,
	unlinks sends to targets that should be freed.
	For a CMClosedPIC this
	frees if it refers to anything that should be freed or isn't marked.
	For a CMOpenPIC this
	frees if the selector isn't marked. */
/*	this recurses at most one level down */

	/* Cogit>>#markAndTraceOrFreeCogMethod:firstVisit: */
static sqInt NoDbgRegParms
markAndTraceOrFreeCogMethodfirstVisit(CogMethod *cogMethod, sqInt firstVisit)
{
    sqInt annotation;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    usqInt theCounters;

	if (((cogMethod->cmType)) == CMFree) {
		return 1;
	}
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
	if (((cogMethod->cmType)) >= CMMethod) {
		if (!(isMarked((cogMethod->methodObject)))) {
			/* begin ensureWritableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
			/* begin makeCodeZoneWritable */
#      if __APPLE__ && __MACH__
			pthread_jit_write_protect_np(0);
			PJWPNClear = __LINE__;
			if (PJWPNState) {
				PJWPNChange = __LINE__;
				PJWPNState = 0;
			}
#      endif // __APPLE__ && __MACH__
#      endif // !DUAL_MAPPED_CODE_ZONE
			freeMethod(cogMethod);
			return 1;
		}
		if (firstVisit) {
			/* begin markLiteralsAndUnlinkUnmarkedSendsIn: */
			assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
			assert(isMarked((cogMethod->methodObject)));
			markAndTraceLiteralinat((cogMethod->selector), cogMethod, (&((cogMethod->selector))));
			/* begin maybeMarkCountersIn: */
			theCounters = (cogMethod->counters);
			if (theCounters != 0) {
				markAndTrace(theCounters - BaseHeaderSize);
			}
			/* begin maybeMarkIRCsIn: */
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = markLiteralsAndUnlinkIfUnmarkedSendpcmethod(annotation, (((char *) mcpc)), cogMethod);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		return 0;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		if (!(closedPICRefersToUnmarkedObject(cogMethod))) {
			return 0;
		}
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		freeMethod(cogMethod);
		return 1;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		if (isMarked((cogMethod->selector))) {
			return 0;
		}
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		freeMethod(cogMethod);
		return 1;
	}
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || ((isCMClosedPIC(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod)))));
	return 0;
}


/*	If entryPoint is that of some method, then mark and trace objects in it
	and free if it is appropriate.
	Answer if the method has been freed. */

	/* Cogit>>#markAndTraceOrFreePICTarget:in: */
static sqInt NoDbgRegParms
markAndTraceOrFreePICTargetin(sqInt entryPoint, CogMethod *cPIC)
{
    CogMethod *targetMethod;

	assert((entryPoint > methodZoneBase)
	 && (entryPoint < (freeStart())));
	if (((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
	 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
		return 0;
	}
	targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
	assert((isCMMethodEtAl(((CogBlockMethod *) targetMethod)))
	 || (isCMFree(((CogBlockMethod *) targetMethod))));
	return markAndTraceOrFreeCogMethodfirstVisit(targetMethod, (((usqInt)targetMethod)) > (((usqInt)cPIC)));
}


/*	Mark and trace literals. Unlink sends that have unmarked cache tags or
	targets. 
 */

	/* Cogit>>#markLiteralsAndUnlinkIfUnmarkedSend:pc:method: */
static sqInt NoDbgRegParms
markLiteralsAndUnlinkIfUnmarkedSendpcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    usqInt cacheTag1;
    sqInt cacheTagMarked;
    sqInt entryPoint1;
    sqInt literal;
    sqInt offset1;
    sqInt pc;
    sqInt *sendTable1;
    sqInt tagCouldBeObj1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	literal = 0;
	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (markAndTraceLiteralinatpc(literal, ((CogMethod *) cogMethod), ((usqInt)mcpc))) {
			codeModified = 1;
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		pc = pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8)));
		cacheTag1 = ((unsigned int) (long32At(pc)));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
		cacheTagMarked = tagCouldBeObj1;
		if (entryPoint1 > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint1 - offset1));
			if ((!cacheTagMarked)
			 || (markAndTraceOrFreeCogMethodfirstVisit(targetMethod1, (((usqInt)targetMethod1)) > (((usqInt)mcpc))))) {

				/* Either the cacheTag is unmarked (e.g. new class) or the target
				   has been freed (because it is unmarked), so unlink the send. */
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
				markAndTraceLiteralinat((targetMethod1->selector), targetMethod1, (&((targetMethod1->selector))));
			}
		}
		else {

			/* cacheTag is selector */
					}
	}
	return 0;
}


/*	Mark and trace literals.
	Additionally in Newspeak, void push implicits that have unmarked classes. */

	/* Cogit>>#markLiterals:pc:method: */
static sqInt NoDbgRegParms
markLiteralspcmethod(sqInt annotation, char *mcpc, CogMethod *cogMethod)
{
    usqInt cacheTag1;
    sqInt entryPoint1;
    sqInt literal;
    sqInt pc;
    sqInt tagCouldBeObj1;

	literal = 0;
	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (markAndTraceLiteralinatpc(literal, cogMethod, ((usqInt)mcpc))) {
			codeModified = 1;
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		pc = pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8)));
		cacheTag1 = ((unsigned int) (long32At(pc)));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
			}
	return 0;
}

	/* Cogit>>#markMethodAndReferents: */
void
markMethodAndReferents(CogBlockMethod *aCogMethod)
{
    sqInt annotation;
    CogMethod *cogMethod;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *writableMethod;

	assert((isCMMethodEtAl(aCogMethod))
	 || (isCMBlock(aCogMethod)));
	cogMethod = (((aCogMethod->cmType)) >= CMMethod
		? ((CogMethod *) aCogMethod)
		: cmHomeMethod(aCogMethod));
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmUsageCount = CMMaxUsageCount);
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {

			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = incrementUsageOfTargetIfLinkedSendmcpcignored(annotation, (((char *) mcpc)), 0);
			if (result != 0) {
				goto l2;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	l2:	/* end mapFor:performUntil:arg: */;
}

	/* Cogit>>#maxCogMethodAddress */
usqInt
maxCogMethodAddress(void)
{
	return ((usqInt)(limitZony()));
}

	/* Cogit>>#maximumDistanceFromCodeZone: */
static sqInt NoDbgRegParms
maximumDistanceFromCodeZone(sqInt anAddress)
{
	return (anAddress > codeBase
		? anAddress - codeBase
		: limitAddress - anAddress);
}


/*	If this is the Newspeak VM and the objectRepresentation supports pinning
	then allocate space for the implicit receiver caches on the heap. */

	/* Cogit>>#maybeAllocAndInitIRCs */
static sqInt
maybeAllocAndInitIRCs(void)
{
	return 1;
}


/*	Check that the header fields are consistent with the type.
	Answer 0 if it is ok, otherwise answer a code for the error. */

	/* Cogit>>#maybeFreeCogMethodDoesntLookKosher: */
static sqInt NoDbgRegParms
maybeFreeCogMethodDoesntLookKosher(CogMethod *cogMethod)
{
    sqInt result;

	result = cogMethodDoesntLookKosher(cogMethod);
	return (result == 2
		? 0
		: result);
}

	/* Cogit>>#mclassCouldBeContext */
static sqInt
mclassCouldBeContext(void)
{
	if (receiverTags < 0) {
		receiverTags = receiverTagBitsForMethod(methodObj);
	}
	return ((receiverTags & ((1U << (numTagBits())))) != 0);
}

	/* Cogit>>#mclassIsSmallInteger */
static sqInt
mclassIsSmallInteger(void)
{
	if (receiverTags < 0) {
		receiverTags = receiverTagBitsForMethod(methodObj);
	}
	return (((receiverTags) & 7) == 1);
}


/*	Answer the absolute machine code pc matching the zero-relative
	bytecode pc of a backward branch in cogMethod, given the start
	of the bytecodes for cogMethod's block or method object. */

	/* Cogit>>#mcPCForBackwardBranch:startBcpc:in: */
usqInt
mcPCForBackwardBranchstartBcpcin(sqInt bcpc, sqInt startbcpc, CogBlockMethod *cogMethod)
{
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc1;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt prim;
    sqInt result;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	/* begin mapFor:bcpc:performUntil:arg: */
	assert(((cogMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogMethod)) + ((cogMethod->stackCheckOffset));
	result = (((((0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))) & 1) != 0))
	 && ((((sqInt)(((void *)bcpc)))) == startbcpc)
		? ((sqInt)(((char *) mcpc)))
		: 0);
	if (result != 0) {
		return result;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc1 = startbcpc;
	if (((cogMethod->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = (cogMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc1 += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc1 == ((cogMethod->startpc)));
		homeMethod = cmHomeMethod(cogMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc1 = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc1, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc1 + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc1, -1, aMethodObj))
	: 0));
		bcpc1 = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc1, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc1 >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc1 >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj);
							targetPC = (bcpc1 + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc1 + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc1 + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc1 - 1, methodObj);
							upperByte = fetchByteofObject(bcpc1 - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc1 + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc1 + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc1 = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc1, nExts, aMethodObj)) < 0));
				result = findBackwardBranchIsBackwardBranchMcpcBcpcMatchingBcpc(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc)), ((isBackwardBranch
	? bcpc1 - (2 * nExts)
	: bcpc1)), (((void *)bcpc)));
				if (result != 0) {
					return result;
				}
				bcpc1 = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}


/*	For the purposes of become: see if the two methods are similar, i.e. can
	be safely becommed.
	This is pretty strict. All literals and bytecodes must be identical. Only
	trailer bytes and header
	flags can differ. */

	/* Cogit>>#method:hasSameCodeAs:checkPenultimate: */
static sqInt NoDbgRegParms
methodhasSameCodeAscheckPenultimate(sqInt methodA, sqInt methodB, sqInt comparePenultimateLiteral)
{
    sqInt bi;
    sqInt endPCA;
    sqInt headerA;
    sqInt headerB;
    sqInt li;
    sqInt numLitsA;

	headerA = methodHeaderOf(methodA);
	headerB = methodHeaderOf(methodB);
	numLitsA = literalCountOfMethodHeader(headerA);
	endPCA = endPCOf(methodA);
	if (((argumentCountOfMethodHeader(headerA)) != (argumentCountOfMethodHeader(headerB)))
	 || (((temporaryCountOfMethodHeader(headerA)) != (temporaryCountOfMethodHeader(headerB)))
	 || (((primitiveIndexOfMethodheader(methodA, headerA)) != (primitiveIndexOfMethodheader(methodB, headerB)))
	 || ((numLitsA != (literalCountOfMethodHeader(headerB)))
	 || (endPCA > (numBytesOf(methodB))))))) {
		return 0;
	}
	for (li = 1; li < numLitsA; li += 1) {
		if ((fetchPointerofObject(li, methodA)) != (fetchPointerofObject(li, methodB))) {
			if ((li < (numLitsA - 1))
			 || (comparePenultimateLiteral)) {
				return 0;
			}
		}
	}
	for (bi = (startPCOfMethodHeader(headerA)); bi <= endPCA; bi += 1) {
		if ((fetchByteofObject(bi, methodA)) != (fetchByteofObject(bi, methodB))) {
			return 0;
		}
	}
	return 1;
}

	/* Cogit>>#mnuOffset */
sqInt
mnuOffset(void)
{
	return missOffset;
}

	/* Cogit>>#NativePopR: */
static AbstractInstruction * NoDbgRegParms
gNativePopR(sqInt reg)
{
	return genoperand(NativePopR, reg);
}

	/* Cogit>>#NativePushR: */
static AbstractInstruction * NoDbgRegParms
gNativePushR(sqInt reg)
{
	return genoperand(NativePushR, reg);
}

	/* Cogit>>#NativeRetN: */
static AbstractInstruction * NoDbgRegParms
gNativeRetN(sqInt offset)
{
	return genoperand(NativeRetN, offset);
}

	/* Cogit>>#needsFrameIfImmutability: */
static sqInt NoDbgRegParms
needsFrameIfImmutability(sqInt stackDelta)
{
	return IMMUTABILITY;
}

	/* Cogit>>#needsFrameIfInBlock: */
static sqInt NoDbgRegParms
needsFrameIfInBlock(sqInt stackDelta)
{
	return inBlock > 0;
}

	/* Cogit>>#needsFrameNever: */
static sqInt NoDbgRegParms
needsFrameNever(sqInt stackDelta)
{
	return 0;
}

	/* Cogit>>#noAssertMethodClassAssociationOf: */
static sqInt NoDbgRegParms
noAssertMethodClassAssociationOf(sqInt methodPointer)
{
	return literalofMethod((literalCountOfMethodHeader(noAssertHeaderOf(methodPointer))) - 1, methodPointer);
}


/*	Check that no method is maximally marked. A maximal mark is an indication
	the method has been scanned to increase the usage count of its referent
	methods.  */

	/* Cogit>>#noCogMethodsMaximallyMarked */
static sqInt
noCogMethodsMaximallyMarked(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->cmUsageCount)) == CMMaxUsageCount)) {
			return 0;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return 1;
}


/*	Answer if all targets in the PIC are in-use methods. */

	/* Cogit>>#noTargetsFreeInClosedPIC: */
static sqInt NoDbgRegParms
noTargetsFreeInClosedPIC(CogMethod *cPIC)
{
	return !(cPICHasFreedTargets(cPIC));
}

	/* Cogit>>#OrCq:R:R: */
static AbstractInstruction * NoDbgRegParms
gOrCqRR(sqInt quickConstant, sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *first;

	/* begin gen:quickConstant:operand:operand: */
	anInstruction = genoperandoperandoperand(OrCqRR, quickConstant, srcReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction;
	if (srcReg == destReg) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(OrCqR, quickConstant, destReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		return anInstruction1;
	}
	first = genoperandoperand(MoveRR, srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(OrCqR, quickConstant, destReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return first;
}


/*	Store the generated machine code, answering the last address */

	/* Cogit>>#outputInstructionsAt: */
static sqInt NoDbgRegParms
outputInstructionsAt(sqInt startAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    sqInt i;
    sqInt j;

	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	absoluteAddress = startAddress;
	for (i = 0; i < opcodeIndex; i += 1) {
		maybeBreakGeneratingInstructionWithIndex(i);
		abstractInstruction = abstractInstructionAt(i);
		assert(((abstractInstruction->address)) == absoluteAddress);
		/* begin outputMachineCodeAt: */
		for (j = 0; j < ((abstractInstruction->machineCodeSize)); j += 4) {
			codeLong32Atput(absoluteAddress + j, ((abstractInstruction->machineCode))[j / 4]);
		}
		absoluteAddress += (abstractInstruction->machineCodeSize);
	}
	return absoluteAddress;
}


/*	Output instructions generated for one of the generated run-time routines,
	a trampoline, etc
 */

	/* Cogit>>#outputInstructionsForGeneratedRuntimeAt: */
static sqInt NoDbgRegParms
outputInstructionsForGeneratedRuntimeAt(sqInt startAddress)
{
    sqInt endAddress;
    sqInt size;

	computeMaximumSizes();
	(methodLabel->address = startAddress);
	size = generateInstructionsAt(startAddress);
	endAddress = outputInstructionsAt(startAddress);
	assert((startAddress + size) == endAddress);
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	return startAddress;
}

	/* Cogit>>#PushCw: */
static AbstractInstruction * NoDbgRegParms
gPushCw(sqInt wordConstant)
{
	/* begin gen:literal: */
	return checkLiteralforInstruction(wordConstant, genoperand(PushCw, wordConstant));
}


/*	Code entry closed PIC full or miss to an instance of a young class or to a
	young target method.
	Attempt to patch the send site to an open PIC. Answer if the attempt
	succeeded; in fact it will
	only return if the attempt failed.
	The stack looks like:
	receiver
	args
	sp=>	sender return address */

	/* Cogit>>#patchToOpenPICFor:numArgs:receiver: */
sqInt
patchToOpenPICFornumArgsreceiver(sqInt selector, sqInt numArgs, sqInt receiver)
{
    sqInt extent;
    CogMethod *oPIC;
    sqInt outerReturn;


	/* See if an Open PIC is already available. */
	outerReturn = stackTop();
	oPIC = openPICWithSelector(selector);
	if (!oPIC) {

		/* otherwise attempt to create an Open PIC. */
		oPIC = cogOpenPICSelectornumArgs(selector, numArgs);
		if ((((((sqInt)oPIC)) >= MaxNegativeErrorCode) && ((((sqInt)oPIC)) <= -1))) {

			/* For some reason the PIC couldn't be generated, most likely a lack of code memory. */
			if ((((sqInt)oPIC)) == InsufficientCodeSpace) {
				callForCogCompiledCodeCompaction();
			}
			return 0;
		}
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	extent = rewriteInlineCacheAttagtarget(backEnd, outerReturn, inlineCacheValueForSelectorin(backEnd, selector, mframeHomeMethodExport()), (((sqInt)oPIC)) + cmEntryOffset);
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) ((((usqInt)outerReturn)) - extent)), (((usqInt)outerReturn)) - ((((usqInt)outerReturn)) - extent));
	sys_icache_invalidate(((void *) ((((usqInt)outerReturn)) - extent)), (((usqInt)outerReturn)) - ((((usqInt)outerReturn)) - extent));
#  else // __APPLE__ && __MACH__
	ceFlushICache((((usqInt)outerReturn)) - extent, ((usqInt)outerReturn));
#  endif
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)oPIC))), ((((usqInt)oPIC)) + openPICSize) - (((usqInt)oPIC)));
	sys_icache_invalidate(((void *) (((usqInt)oPIC))), ((((usqInt)oPIC)) + openPICSize) - (((usqInt)oPIC)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)oPIC), (((usqInt)oPIC)) + openPICSize);
#  endif
	executeCogMethodfromLinkedSendWithReceiver(oPIC, receiver);
	return 1;
}


/*	This value is used to decide between MNU processing
	or interpretation in the closed PIC aborts. */

	/* Cogit>>#picAbortDiscriminatorValue */
static sqInt
picAbortDiscriminatorValue(void)
{
	return 0;
}


/*	Answer the start of the abort sequence for invoking the interpreter in a
	closed PIC.
 */

	/* Cogit>>#picInterpretAbortOffset */
static sqInt
picInterpretAbortOffset(void)
{
	return (interpretOffset()) - ((pushLinkRegisterByteSize(backEnd)) + (callInstructionByteSize(backEnd)));
}

	/* Cogit>>#previousInstruction */
static AbstractInstruction *
previousInstruction(void)
{
	assert(opcodeIndex > 0);
	return abstractInstructionAt(opcodeIndex - 1);
}


/*	useful for debugging */

	/* Cogit>>#printCogMethodFor: */
void
printCogMethodFor(void *address)
{
    CogMethod *cogMethod;

	cogMethod = methodFor(address);
	if (cogMethod == null) {
		if ((codeEntryFor(address)) == null) {
			print("not a method");
			cr();
		}
		else {
			print("trampoline ");
			print(codeEntryNameFor(address));
			cr();
		}
	}
	else {
		printCogMethod(cogMethod);
	}
}


/*	useful for debugging */

	/* Cogit>>#printTrampolineTable */
void
printTrampolineTable(void)
{
    sqInt i;

	for (i = 0; i < trampolineTableIndex; i += 2) {
		fprintf(getTranscript(),
				"%p: %s\n",
				((void *)(trampolineAddresses[i + 1])),
				((char *)(trampolineAddresses[i])));
	}
}

	/* Cogit>>#processorHasDivQuoRemAndMClassIsSmallInteger */
static sqInt
processorHasDivQuoRemAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#processorHasDoublePrecisionFloatingPointSupport */
static sqInt
processorHasDoublePrecisionFloatingPointSupport(void)
{
	/* begin hasDoublePrecisionFloatingPointSupport */
	return 1;
}

	/* Cogit>>#processorHasMultiplyAndMClassIsSmallInteger */
static sqInt
processorHasMultiplyAndMClassIsSmallInteger(void)
{
	return mclassIsSmallInteger();
}

	/* Cogit>>#recordGeneratedRunTime:address: */
static void NoDbgRegParms
recordGeneratedRunTimeaddress(char *aString, sqInt address)
{
	assert((trampolineTableIndex + 2) <= (NumTrampolines * 2));
	trampolineAddresses[trampolineTableIndex] = aString;
	trampolineAddresses[trampolineTableIndex + 1] = (((char *) address));

	/* self printTrampolineTable */
	trampolineTableIndex += 2;
}


/*	This one for C support code. */

	/* Cogit>>#recordPrimTraceFunc */
sqInt
recordPrimTraceFunc(void)
{
	return recordPrimTrace();
}

	/* Cogit>>#recordRunTimeObjectReferences */
static void
recordRunTimeObjectReferences(void)
{
    sqInt i;
    AbstractInstruction *instruction;

	for (i = 0; i < opcodeIndex; i += 1) {
		instruction = abstractInstructionAt(i);
		if (((instruction->annotation)) == IsObjectReference) {
			assert(runtimeObjectRefIndex < NumObjRefsInRuntime);
			assert(!hasYoungReferent);
			if (hasYoungReferent) {
				error("attempt to generate run-time routine containing young object reference.  Cannot initialize Cogit run-time.");
			}
			objectReferencesInRuntime[runtimeObjectRefIndex] = (((usqInt)((((instruction->opcode)) == Literal
	? (instruction->address)
	: ((instruction->address)) + ((instruction->machineCodeSize))))));
			runtimeObjectRefIndex += 1;
		}
	}
}


/*	N.B. (self registerMaskFor: NoReg) = 0 */

	/* Cogit>>#registerMaskFor: */
static sqInt NoDbgRegParms
registerMaskFor(sqInt reg)
{
	return ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
}

	/* Cogit>>#registerMaskFor:and: */
static sqInt NoDbgRegParms
registerMaskForand(sqInt reg1, sqInt reg2)
{
	return (1ULL << reg1) | (1ULL << reg2);
}

	/* Cogit>>#registerMaskFor:and:and:and: */
static sqInt NoDbgRegParms
registerMaskForandandand(sqInt reg1, sqInt reg2, sqInt reg3, sqInt reg4)
{
	return (((1ULL << reg1) | (1ULL << reg2)) | (1ULL << reg3)) | (1ULL << reg4);
}

	/* Cogit>>#relocateCallsAndSelfReferencesInMethod: */
static void NoDbgRegParms
relocateCallsAndSelfReferencesInMethod(CogMethod *cogMethod)
{
    sqInt annotation;
    sqInt callDelta;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqLong refDelta;
    sqInt result;

	refDelta = (cogMethod->objectHeader);
	callDelta = refDelta;
	assert((isCMMethodEtAl(((CogBlockMethod *) cogMethod)))
	 || (isCMOpenPIC(((CogBlockMethod *) cogMethod))));
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cogMethod)) + missOffset)) == ((isCMMethodEtAl(((CogBlockMethod *) cogMethod))
		? methodAbortTrampolineFor((cogMethod->cmNumArgs))
		: picAbortTrampolineFor((cogMethod->cmNumArgs)))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cogMethod)) + missOffset, -callDelta);
	/* begin mapFor:performUntil:arg: */
	mcpc = ((((cogMethod->cmType)) >= CMMethod)
	 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
		? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
		: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
	map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
	enumeratingCogMethod = cogMethod;
	while (((mapByte = byteAt(map))) != MapEnd) {
		if (mapByte >= FirstAnnotation) {

			/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
			 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
				annotation += mapByte & DisplacementMask;
				map -= 1;
			}
			result = relocateIfCallOrMethodReferencemcpcdelta(annotation, (((char *) mcpc)), (((void *)refDelta)));
			if (result != 0) {
				goto l2;
			}
		}
		else {
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	l2:	/* end mapFor:performUntil:arg: */;
}

	/* Cogit>>#relocateCallsInClosedPIC: */
static void NoDbgRegParms
relocateCallsInClosedPIC(CogMethod *cPIC)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    sqInt pc1;
    sqLong refDelta;
    CogMethod *targetMethod;

	refDelta = (cPIC->objectHeader);
	callDelta = refDelta;
	assert((callTargetFromReturnAddress(backEnd, (((sqInt)cPIC)) + missOffset)) == (picAbortTrampolineFor((cPIC->cmNumArgs))));
	relocateCallBeforeReturnPCby(backEnd, (((sqInt)cPIC)) + missOffset, -callDelta);
	pc = (((sqInt)cPIC)) + firstCPICCaseOffset;
	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		if (i == 1) {
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
			entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		}
		if (((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {

			/* Interpret/MNU */
					}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			if (i == 1) {
				relocateJumpLongBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
			else {
				relocateJumpLongConditionalBeforeFollowingAddressby(backEnd, pc, -(callDelta - ((targetMethod->objectHeader))));
			}
		}
	}
	assert(((cPIC->cPICNumCases)) > 0);
	/* begin relocateMethodReferenceBeforeAddress:by: */
	pc1 = (addressOfEndOfCaseinCPIC(2, cPIC)) + (loadPICLiteralByteSize(backEnd));
	assert((instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc1 - 4)))
	 || (instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc1 - 8))));
	relocateJumpLongBeforeFollowingAddressby(backEnd, (((sqInt)cPIC)) + cPICEndOfCodeOffset, -callDelta);
}


/*	To placate the C static type system... */

	/* Cogit>>#relocateIfCallOrMethodReference:mcpc:delta: */
static sqInt NoDbgRegParms
relocateIfCallOrMethodReferencemcpcdelta(sqInt annotation, char *mcpc, CogMethod *refDeltaArg)
{
    sqInt callDelta;
    sqInt entryPoint;
    sqInt offset1;
    sqInt pc;
    sqInt refDelta;
    sqInt *sendTable1;
    CogMethod *targetMethod;
    sqInt unlinkedRoutine;

	refDelta = ((sqInt) refDeltaArg);
	callDelta = refDelta;
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint <= methodZoneBase) {

			/* send is not linked; just relocate */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
			return 0;
		}
		/* begin offsetAndSendTableFor:annotation:into: */
		if (annotation == IsSendCall) {
			offset1 = cmEntryOffset;
			sendTable1 = ordinarySendTrampolines;
		}
		else {
			if (annotation == IsDirectedSuperSend) {
				offset1 = cmNoCheckEntryOffset;
				sendTable1 = directedSuperSendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperBindingSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperBindingSendTrampolines;
				}
				else {
					assert(annotation == IsSuperSend);
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = superSendTrampolines;
				}
			}
		}
		targetMethod = ((CogMethod *) (entryPoint - offset1));
		if (!(((targetMethod->cmType)) == CMFree)) {

			/* send target not freed; just relocate. */
			relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -(callDelta - ((targetMethod->objectHeader))));
			
			/* See comment in planCompaction */
			restorePICUsageCount(targetMethod);
			return 0;
		}
		unlinkedRoutine = sendTable1[((((targetMethod->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod->cmNumArgs)) : (NumSendTrampolines - 1))];
		unlinkedRoutine -= callDelta;
		rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod->selector), enumeratingCogMethod), unlinkedRoutine);
		return 0;
	}
	if (annotation == IsRelativeCall) {
		relocateCallBeforeReturnPCby(backEnd, ((sqInt)mcpc), -callDelta);
		return 0;
	}
	if (annotation == IsAbsPCReference) {
		/* begin relocateMethodReferenceBeforeAddress:by: */
		pc = ((sqInt)mcpc);
		assert((instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 4)))
		 || (instructionIsADR(((AbstractInstruction *) backEnd), instructionAt(((AbstractInstruction *) backEnd), pc - 8))));
	}
	return 0;
}


/*	to placate the C static type system... */

	/* Cogit>>#remapIfObjectRef:pc:hasYoung: */
static sqInt NoDbgRegParms
remapIfObjectRefpchasYoung(sqInt annotation, char *mcpc, CogMethod *hasYoungPtr)
{
    usqInt cacheTag1;
    sqInt entryPoint1;
    sqInt literal;
    sqInt mappedCacheTag;
    sqInt mappedLiteral;
    sqInt offset1;
    sqInt pc;
    sqInt *sendTable1;
    sqInt tagCouldBeObj1;
    CogMethod *targetMethod1;

	if (annotation == IsObjectReference) {
		literal = longAt(((usqInt)mcpc));
		if (couldBeObject(literal)) {
			mappedLiteral = remapObject(literal);
			if (literal != mappedLiteral) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				/* begin storeLiteral:atAnnotatedAddress:using: */
				codeLongAtput(((usqInt)mcpc), mappedLiteral);
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedLiteral))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
	}
	if (annotation >= IsSendCall) {
		/* begin entryCacheTagAndCouldBeObjectAt:annotation:into: */
		pc = pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8)));
		cacheTag1 = ((unsigned int) (long32At(pc)));

		/* in-line cache tags are the selectors of sends if sends are unlinked,
		   the selectors of super sends (entry offset = cmNoCheckEntryOffset),
		   the selectors of open PIC sends (entry offset = cmEntryOffset, target is an Open PIC)
		   or in-line cache tags (classes, class indices, immediate bit patterns, etc).
		   Note that selectors can be immediate so there is no guarantee that they
		   are markable/remappable objects. */
		entryPoint1 = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		tagCouldBeObj1 = 0;
		if (tagCouldBeObj1
		 && (couldBeObject(cacheTag1))) {
			mappedCacheTag = remapObject(cacheTag1);
			if (cacheTag1 != mappedCacheTag) {
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheTagat(backEnd, mappedCacheTag, ((usqInt)mcpc));
			}
			if ((hasYoungPtr != 0)
			 && (isYoung(mappedCacheTag))) {
				(*((sqInt *) hasYoungPtr) = 1);
			}
		}
		if (hasYoungPtr != 0) {

			/* Since the unlinking routines may rewrite the cacheTag to the send's selector, and
			   since they don't have the cogMethod to hand and can't add it to youngReferrers,
			   the method must remain in youngReferrers if the targetMethod's selector is young. */
			if (entryPoint1 > methodZoneBase) {

				/* It's a linked send. */
				/* begin targetMethodAndSendTableFor:annotation:into: */
				if (annotation == IsSendCall) {
					offset1 = cmEntryOffset;
					sendTable1 = ordinarySendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperSendTrampolines;
					}
					else {
						if (annotation == IsDirectedSuperBindingSend) {
							offset1 = cmNoCheckEntryOffset;
							sendTable1 = directedSuperBindingSendTrampolines;
						}
						else {
							assert(annotation == IsSuperSend);
							offset1 = cmNoCheckEntryOffset;
							sendTable1 = superSendTrampolines;
						}
					}
				}
				targetMethod1 = ((CogMethod *) (entryPoint1 - offset1));
				if (isYoung((targetMethod1->selector))) {
					(*((sqInt *) hasYoungPtr) = 1);
				}
			}
		}
	}
	return 0;
}


/*	Remap a potential object reference from a closed PIC.
	This may be an object reference, an inline cache tag or null.
	Answer if the updated literal is young.
	mcpc is the address of the next instruction following either
	the load of the method literal or the compare of the class tag. */

	/* Cogit>>#remapMaybeObjRefInClosedPICAt: */
static sqInt NoDbgRegParms
remapMaybeObjRefInClosedPICAt(sqInt mcpc)
{
    sqInt object;
    sqInt subject;

	object = literalBeforeFollowingAddress(backEnd, mcpc);
	if (!(couldBeObject(object))) {
		return 0;
	}
	subject = remapOop(object);
	if (object != subject) {
		/* begin setCodeModified */
#    if DUAL_MAPPED_CODE_ZONE
		codeModified = 1;
#    else
		if (!codeModified) {
			codeModified = 1;
			/* begin makeCodeZoneWritable */
#      if __APPLE__ && __MACH__
			pthread_jit_write_protect_np(0);
			PJWPNClear = __LINE__;
			if (PJWPNState) {
				PJWPNChange = __LINE__;
				PJWPNState = 0;
			}
#      endif // __APPLE__ && __MACH__
		}
#    endif // DUAL_MAPPED_CODE_ZONE
		storeLiteralbeforeFollowingAddress(backEnd, subject, mcpc);
	}
	return isYoungObject(subject);
}


/*	Rewrite the three values involved in a CPIC case. Used by the initialize &
	extend CPICs.
	c.f. expectedClosedPICPrototype: */
/*	write the obj ref/operand via the second ldr */

	/* Cogit>>#rewriteCPICCaseAt:tag:objRef:target: */
static void NoDbgRegParms
rewriteCPICCaseAttagobjReftarget(sqInt followingAddress, sqInt newTag, sqInt newObjRef, sqInt newTarget)
{
    sqInt classTagPC;
    sqInt methodObjPC;

	methodObjPC = (followingAddress - (jumpLongConditionalByteSize(backEnd))) - (cmpC32RTempByteSize(backEnd));
	storeLiteralbeforeFollowingAddress(backEnd, newObjRef, methodObjPC);

	/* rewite the tag via the first ldr */
	classTagPC = followingAddress - (jumpLongConditionalByteSize(backEnd));
	storeLiteral32beforeFollowingAddress(backEnd, newTag, classTagPC);
	rewriteConditionalJumpLongAttarget(backEnd, followingAddress, newTarget);
}

	/* Cogit>>#SubCw:R: */
static AbstractInstruction * NoDbgRegParms
gSubCwR(sqInt wordConstant, sqInt reg)
{
	/* begin gen:literal:operand: */
	return checkLiteralforInstruction(wordConstant, genoperandoperand(SubCwR, wordConstant, reg));
}


/*	destReg := fromReg - subReg */

	/* Cogit>>#SubR:R:R: */
static AbstractInstruction * NoDbgRegParms
gSubRRR(sqInt subReg, sqInt fromReg, sqInt destReg)
{
    AbstractInstruction *first;

	return genoperandoperandoperand(SubRRR, subReg, fromReg, destReg);
	assert(subReg != destReg);
	first = genoperandoperand(MoveRR, fromReg, destReg);
	genoperandoperand(SubRR, subReg, destReg);
	return first;
}


/*	Answer the number of clean blocks found in the literal frame */

	/* Cogit>>#scanForCleanBlocks */
static sqInt
scanForCleanBlocks(void)
{
    sqInt i;
    sqInt iLimiT;
    sqInt lit;
    sqInt numCleanBlocks;
    sqInt startPCOrNil;

	numCleanBlocks = 0;
	for (i = 1, iLimiT = (literalCountOf(methodObj)); i <= iLimiT; i += 1) {
		lit = fetchPointerofObject(i, methodObj);
		startPCOrNil = startPCOrNilOfLiteralin(lit, methodObj);
		if (!(startPCOrNil == null)) {
			numCleanBlocks += 1;
		}
	}
	return numCleanBlocks;
}


/*	If a method is compiled to machine code via a block entry it won't have a
	selector. A subsequent send can find the method and hence fill in the
	selector. 
 */
/*	self disassembleMethod: cogMethod */

	/* Cogit>>#setSelectorOf:to: */
void
setSelectorOfto(CogMethod *cogMethod, sqInt aSelectorOop)
{
	compilationBreakpointisMNUCase(aSelectorOop, 0);
	assert(isCMMethodEtAl(((CogBlockMethod *) cogMethod)));
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->selector = aSelectorOop);
	if (isYoung(aSelectorOop)) {
		ensureInYoungReferrers(cogMethod);
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}

	/* Cogit>>#spanForCleanBlockStartingAt: */
static sqInt NoDbgRegParms
spanForCleanBlockStartingAt(sqInt startPC)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt pc;

	pc = startPC;
	end = numBytesOf(methodObj);
	while (pc <= end) {
		/* begin generatorForPC: */
		descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
		pc += (descriptor->numBytes);
		if ((descriptor->isReturn)) {
			return pc - startPC;
		}
	}
	error("couldn't locate end of clean block");
	return 0;
}

	/* Cogit>>#stackCheckOffsetOfBlockAt:isMcpc: */
static usqInt NoDbgRegParms
stackCheckOffsetOfBlockAtisMcpc(sqInt blockEntryMcpc, sqInt mcpc)
{
    CogBlockMethod *cogBlockMethod;

	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((((sqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset))) == mcpc) {
		return ((usqInt)cogBlockMethod);
	}
	return 0;
}


/*	Answer a fake value for the method oop in other than the first case in the
	PIC prototype.
	Since we use MoveUniqueCw:R: it must not be confused with a
	method-relative address.
 */

	/* Cogit>>#subsequentPrototypeMethodOop */
static sqInt
subsequentPrototypeMethodOop(void)
{
	return (((((usqInt)0xBADA550)) >= ((methodLabel->address)))
	 && ((((usqInt)0xBADA550)) < ((((youngReferrers()) < (((methodLabel->address)) + MaxMethodSize)) ? (youngReferrers()) : (((methodLabel->address)) + MaxMethodSize))))
		? 0xDEADEAD
		: 0xBADA550);
}

	/* Cogit>>#TstCq:R: */
static AbstractInstruction * NoDbgRegParms
gTstCqR(sqInt quickConstant, sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(TstCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	return anInstruction;
}

	/* Cogit>>#traceLinkedSendOffset */
sqInt
traceLinkedSendOffset(void)
{
	return (cmNoCheckEntryOffset + (callFullInstructionByteSize(backEnd))) + (pushLinkRegisterByteSize(backEnd));
}

	/* Cogit>>#trampolineName:numArgs: */
static char * NoDbgRegParms
trampolineNamenumArgs(char *routinePrefix, sqInt numArgs)
{
    char *theString;

	/* begin trampolineName:numArgs:limit: */
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, ((((int) numArgs)) <= (NumSendTrampolines - 2)
		? '0' + (((int) numArgs))
		: 'N'));
	return theString;
}


/*	Malloc a string with the contents for the trampoline table */

	/* Cogit>>#trampolineName:numArgs:limit: */
static char * NoDbgRegParms
trampolineNamenumArgslimit(char *routinePrefix, int numArgs, sqInt argsLimit)
{
    char *theString;

	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, (numArgs <= argsLimit
		? '0' + numArgs
		: 'N'));
	return theString;
}

	/* Cogit>>#trampolineName:numRegArgs: */
static char * NoDbgRegParms
trampolineNamenumRegArgs(char *routinePrefix, sqInt numArgs)
{
    sqInt argsLimit;
    char *theString;

	/* begin trampolineName:numArgs:limit: */
	argsLimit = numRegArgs();
	theString = malloc((strlen(routinePrefix)) + 6);
	sprintf(theString, "%s%cArgs", routinePrefix, ((((int) numArgs)) <= argsLimit
		? '0' + (((int) numArgs))
		: 'N'));
	return theString;
}

	/* Cogit>>#unflagBecomeFlaggedMethods */
void
unflagBecomeFlaggedMethods(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmType = CMMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}

	/* Cogit>>#unknownBytecode */
static sqInt
unknownBytecode(void)
{
	return EncounteredUnknownBytecode;
}


/*	Unlink all sends in cog methods. */

	/* Cogit>>#unlinkAllSends */
void
unlinkAllSends(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	cogMethod = ((CogMethod *) methodZoneBase);
	voidOpenPICList();
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcignored(annotation, (((char *) mcpc)), 0);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (!(((cogMethod->cmType)) == CMFree)) {
				freeMethod(cogMethod);
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	/* begin flushICacheFrom:to: */
	endAddress = freeStart();
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
	sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#  else // __APPLE__ && __MACH__
	ceFlushICache(((usqInt)methodZoneBase), endAddress);
#  endif
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfFreeOrLinkedSend:pc:of: */
static sqInt NoDbgRegParms
unlinkIfFreeOrLinkedSendpcof(sqInt annotation, char *mcpc, CogMethod *theSelector)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if ((((targetMethod1->cmType)) == CMFree)
			 || (((targetMethod1->selector)) == (((sqInt) theSelector)))) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfInvalidClassSend:pc:ignored: */
static sqInt NoDbgRegParms
unlinkIfInvalidClassSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send, but maybe a super send or linked to an OpenPIC, in which case the cache tag will be a selector.... */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (!(((annotation == IsSuperSend)
				 || (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend))))
				 || (((targetMethod1->cmType)) == CMOpenPIC))) {
				if (!(isValidClassTag(instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)((((sqInt)mcpc)) - 8))))))) {
					/* begin unlinkSendAt:targetMethod:sendTable: */
					unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
					/* begin setCodeModified */
#          if DUAL_MAPPED_CODE_ZONE
					codeModified = 1;
#          else
					if (!codeModified) {
						codeModified = 1;
						/* begin makeCodeZoneWritable */
#            if __APPLE__ && __MACH__
						pthread_jit_write_protect_np(0);
						PJWPNClear = __LINE__;
						if (PJWPNState) {
							PJWPNChange = __LINE__;
							PJWPNState = 0;
						}
#            endif // __APPLE__ && __MACH__
					}
#          endif // DUAL_MAPPED_CODE_ZONE
					rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
				}
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSendToFree:pc:ignored: */
static sqInt NoDbgRegParms
unlinkIfLinkedSendToFreepcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (((targetMethod1->cmType)) == CMFree) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	To placate the C static type system... */

	/* Cogit>>#unlinkIfLinkedSend:pc:if: */
static sqInt NoDbgRegParms
unlinkIfLinkedSendpcif(sqInt annotation, char *mcpc, CogMethod *criterionArg)
{
    sqInt (*criterion)(CogMethod *);
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	criterion = ((void *)criterionArg);
	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (criterion(targetMethod1)) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:ignored: */
static sqInt NoDbgRegParms
unlinkIfLinkedSendpcignored(sqInt annotation, char *mcpc, sqInt superfluity)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			/* begin unlinkSendAt:targetMethod:sendTable: */
			unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
			/* begin setCodeModified */
#      if DUAL_MAPPED_CODE_ZONE
			codeModified = 1;
#      else
			if (!codeModified) {
				codeModified = 1;
				/* begin makeCodeZoneWritable */
#        if __APPLE__ && __MACH__
				pthread_jit_write_protect_np(0);
				PJWPNClear = __LINE__;
				if (PJWPNState) {
					PJWPNChange = __LINE__;
					PJWPNState = 0;
				}
#        endif // __APPLE__ && __MACH__
			}
#      endif // DUAL_MAPPED_CODE_ZONE
			rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
		}
	}
	return 0;
}

	/* Cogit>>#unlinkIfLinkedSend:pc:to: */
static sqInt NoDbgRegParms
unlinkIfLinkedSendpcto(sqInt annotation, char *mcpc, CogMethod *theCogMethod)
{
    sqInt entryPoint;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt unlinkedRoutine;

	if (annotation >= IsSendCall) {
		entryPoint = callTargetFromReturnAddress(backEnd, ((sqInt)mcpc));
		if (entryPoint > methodZoneBase) {

			/* It's a linked send. */
			/* begin targetMethodAndSendTableFor:annotation:into: */
			if (annotation == IsSendCall) {
				offset1 = cmEntryOffset;
				sendTable1 = ordinarySendTrampolines;
			}
			else {
				if (annotation == IsDirectedSuperSend) {
					offset1 = cmNoCheckEntryOffset;
					sendTable1 = directedSuperSendTrampolines;
				}
				else {
					if (annotation == IsDirectedSuperBindingSend) {
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = directedSuperBindingSendTrampolines;
					}
					else {
						assert(annotation == IsSuperSend);
						offset1 = cmNoCheckEntryOffset;
						sendTable1 = superSendTrampolines;
					}
				}
			}
			targetMethod1 = ((CogMethod *) (entryPoint - offset1));
			if (targetMethod1 == theCogMethod) {
				/* begin unlinkSendAt:targetMethod:sendTable: */
				unlinkedRoutine = sendTable1[((((targetMethod1->cmNumArgs)) < (NumSendTrampolines - 1)) ? ((targetMethod1->cmNumArgs)) : (NumSendTrampolines - 1))];
				/* begin setCodeModified */
#        if DUAL_MAPPED_CODE_ZONE
				codeModified = 1;
#        else
				if (!codeModified) {
					codeModified = 1;
					/* begin makeCodeZoneWritable */
#          if __APPLE__ && __MACH__
					pthread_jit_write_protect_np(0);
					PJWPNClear = __LINE__;
					if (PJWPNState) {
						PJWPNChange = __LINE__;
						PJWPNState = 0;
					}
#          endif // __APPLE__ && __MACH__
				}
#        endif // DUAL_MAPPED_CODE_ZONE
				rewriteInlineCacheAttagtarget(backEnd, ((sqInt)mcpc), inlineCacheValueForSelectorin(backEnd, (targetMethod1->selector), enumeratingCogMethod), unlinkedRoutine);
			}
		}
	}
	return 0;
}


/*	Unlink all sends in cog methods whose class tag is that of a forwarded
	class. 
 */

	/* Cogit>>#unlinkSendsLinkedForInvalidClasses */
void
unlinkSendsLinkedForInvalidClasses(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    sqInt freedPIC;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	cogMethod = ((CogMethod *) methodZoneBase);
	codeModified = (freedPIC = 0);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfInvalidClassSendpcignored(annotation, (((char *) mcpc)), 0);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if ((((cogMethod->cmType)) == CMClosedPIC)
			 && (cPICHasForwardedClass(cogMethod))) {
				freeMethod(cogMethod);
				freedPIC = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedPIC) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {

			/* After possibly updating inline caches we need to flush the icache. */
			/* begin flushICacheFrom:to: */
			endAddress = freeStart();
			/* begin ensureExecutableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
			/* begin makeCodeZoneExecutable */
#      if __APPLE__ && __MACH__
			pthread_jit_write_protect_np(1);
			PJWPNSet = __LINE__;
			if (!PJWPNState) {
				PJWPNChange = __LINE__;
				PJWPNState = 1;
			}
#      endif // __APPLE__ && __MACH__
#      endif // !DUAL_MAPPED_CODE_ZONE
#      if __APPLE__ && __MACH__
			sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
			sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#      else // __APPLE__ && __MACH__
			ceFlushICache(((usqInt)methodZoneBase), endAddress);
#      endif
		}
	}
}


/*	Unlink all sends in cog methods. Free all Closed PICs with the selector,
	or with an MNU case if isMNUSelector. First check if any method actually
	has the selector; if not there can't be any linked send to it. This
	routine (including descendents) is performance critical. It contributes
	perhaps 30% of entire execution time in Compiler recompileAll. */

	/* Cogit>>#unlinkSendsOf:isMNUSelector: */
void
unlinkSendsOfisMNUSelector(sqInt selector, sqInt isMNUSelector)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt mustScanAndUnlink;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	cogMethod = ((CogMethod *) methodZoneBase);
	mustScanAndUnlink = 0;
	if (isMNUSelector) {
		while (cogMethod < (limitZony())) {
			if (!(((cogMethod->cmType)) == CMFree)) {
				if (((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				 && (((cogMethod->cmType)) == CMClosedPIC)) {
					assert(isCMClosedPIC(((CogBlockMethod *) cogMethod)));
					freeMethod(cogMethod);
					mustScanAndUnlink = 1;
				}
				else {
					if (((cogMethod->selector)) == selector) {
						mustScanAndUnlink = 1;
						if (((cogMethod->cmType)) == CMClosedPIC) {
							freeMethod(cogMethod);
						}
					}
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	else {
		while (cogMethod < (limitZony())) {
			if ((!(((cogMethod->cmType)) == CMFree))
			 && (((cogMethod->selector)) == selector)) {
				mustScanAndUnlink = 1;
				if (((cogMethod->cmType)) == CMClosedPIC) {
					freeMethod(cogMethod);
				}
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	}
	if (!mustScanAndUnlink) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfFreeOrLinkedSendpcof(annotation, (((char *) mcpc)), (((CogMethod *) selector)));
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {

		/* After possibly updating inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
		sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)methodZoneBase), endAddress);
#    endif
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	Unlink all sends in cog methods to free methods and/or pics. */

	/* Cogit>>#unlinkSendsToFree */
static void
unlinkSendsToFree(void)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = 0;
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendToFreepcignored(annotation, (((char *) mcpc)), 0);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				assert(noTargetsFreeInClosedPIC(cogMethod));
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (codeModified) {

		/* After possibly updating inline caches we need to flush the icache. */
		/* begin flushICacheFrom:to: */
		endAddress = freeStart();
		/* begin ensureExecutableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneExecutable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(1);
		PJWPNSet = __LINE__;
		if (!PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 1;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
		sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#    else // __APPLE__ && __MACH__
		ceFlushICache(((usqInt)methodZoneBase), endAddress);
#    endif
	}
}


/*	Unlink all sends in cog methods to methods with a machine code
	primitive, and free machine code primitive methods if freeIfTrue.
	To avoid having to scan PICs, free any and all PICs */

	/* Cogit>>#unlinkSendsToMethodsSuchThat:AndFreeIf: */
void
unlinkSendsToMethodsSuchThatAndFreeIf(sqInt (*criterion)(CogMethod *), sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    sqInt freedSomething;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;

	if (!methodZoneBase) {
		return;
	}
	codeModified = (freedSomething = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			if (freeIfTrue
			 && (criterion(cogMethod))) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
			else {
				/* begin mapFor:performUntil:arg: */
				mcpc = ((((cogMethod->cmType)) >= CMMethod)
				 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
					? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
					: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
				map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
				enumeratingCogMethod = cogMethod;
				while (((mapByte = byteAt(map))) != MapEnd) {
					if (mapByte >= FirstAnnotation) {

						/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
						mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
						if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
						 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
							annotation += mapByte & DisplacementMask;
							map -= 1;
						}
						result = unlinkIfLinkedSendpcif(annotation, (((char *) mcpc)), (((CogMethod *) criterion)));
						if (result != 0) {
							goto l2;
						}
					}
					else {
						if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
							mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
						}
					}
					map -= 1;
				}
	l2:	/* end mapFor:performUntil:arg: */;
			}
		}
		else {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				freeMethod(cogMethod);
				freedSomething = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freedSomething) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {

			/* After possibly updating inline caches we need to flush the icache. */
			/* begin flushICacheFrom:to: */
			endAddress = freeStart();
			/* begin ensureExecutableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
			/* begin makeCodeZoneExecutable */
#      if __APPLE__ && __MACH__
			pthread_jit_write_protect_np(1);
			PJWPNSet = __LINE__;
			if (!PJWPNState) {
				PJWPNChange = __LINE__;
				PJWPNState = 1;
			}
#      endif // __APPLE__ && __MACH__
#      endif // !DUAL_MAPPED_CODE_ZONE
#      if __APPLE__ && __MACH__
			sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
			sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#      else // __APPLE__ && __MACH__
			ceFlushICache(((usqInt)methodZoneBase), endAddress);
#      endif
		}
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}


/*	Unlink all sends in cog methods to a particular target method.
	If targetMethodObject isn't actually a method (perhaps being
	used via invokeAsMethod) then there's nothing to do. */

	/* Cogit>>#unlinkSendsTo:andFreeIf: */
void
unlinkSendsToandFreeIf(sqInt targetMethodObject, sqInt freeIfTrue)
{
    sqInt annotation;
    CogMethod *cogMethod;
    sqInt endAddress;
    sqInt freedPIC;
    usqInt map;
    sqInt mapByte;
    sqInt mcpc;
    sqInt result;
    CogMethod *targetMethod;

	if (!((isOopCompiledMethod(targetMethodObject))
		 && (methodHasCogMethod(targetMethodObject)))) {
		return;
	}
	targetMethod = cogMethodOf(targetMethodObject);
	if (!methodZoneBase) {
		return;
	}
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	codeModified = (freedPIC = 0);
	cogMethod = ((CogMethod *) methodZoneBase);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			/* begin mapFor:performUntil:arg: */
			mcpc = ((((cogMethod->cmType)) >= CMMethod)
			 && ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock))
				? (((usqInt)cogMethod)) + cbNoSwitchEntryOffset
				: (((usqInt)cogMethod)) + cmNoCheckEntryOffset);
			map = ((((usqInt)cogMethod)) + ((cogMethod->blockSize))) - 1;
			enumeratingCogMethod = cogMethod;
			while (((mapByte = byteAt(map))) != MapEnd) {
				if (mapByte >= FirstAnnotation) {

					/* If this is an IsSendCall annotation, peek ahead for an IsAnnotationExtension, and consume it. */
					mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
					if ((((annotation = ((usqInt)(mapByte)) >> AnnotationShift)) == IsSendCall)
					 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
						annotation += mapByte & DisplacementMask;
						map -= 1;
					}
					result = unlinkIfLinkedSendpcto(annotation, (((char *) mcpc)), targetMethod);
					if (result != 0) {
						goto l2;
					}
				}
				else {
					if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
						mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
					}
				}
				map -= 1;
			}
	l2:	/* end mapFor:performUntil:arg: */;
		}
		else {
			if ((((cogMethod->cmType)) == CMClosedPIC)
			 && (cPICHasTarget(cogMethod, targetMethod))) {
				freeMethod(cogMethod);
				freedPIC = 1;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	if (freeIfTrue) {
		freeMethod(targetMethod);
	}
	if (freedPIC) {
		unlinkSendsToFree();
	}
	else {
		if (codeModified) {

			/* After possibly updating inline caches we need to flush the icache. */
			/* begin flushICacheFrom:to: */
			endAddress = freeStart();
			/* begin ensureExecutableCodeZone */
#      if !DUAL_MAPPED_CODE_ZONE
			/* begin makeCodeZoneExecutable */
#      if __APPLE__ && __MACH__
			pthread_jit_write_protect_np(1);
			PJWPNSet = __LINE__;
			if (!PJWPNState) {
				PJWPNChange = __LINE__;
				PJWPNState = 1;
			}
#      endif // __APPLE__ && __MACH__
#      endif // !DUAL_MAPPED_CODE_ZONE
#      if __APPLE__ && __MACH__
			sys_dcache_flush(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
			sys_icache_invalidate(((void *) (((usqInt)methodZoneBase))), endAddress - (((usqInt)methodZoneBase)));
#      else // __APPLE__ && __MACH__
			ceFlushICache(((usqInt)methodZoneBase), endAddress);
#      endif
		}
	}
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}

	/* Cogit>>#voidCogCompiledCode */
void
voidCogCompiledCode(void)
{
    CogMethod *cogMethod;

	/* begin clearCogCompiledCode */
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) >= CMMethod) {
			freeMethod(cogMethod);
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress);
	youngReferrers = (limitAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	/* begin ensureExecutableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
}

	/* Cogit>>#XorCw:R: */
static AbstractInstruction * NoDbgRegParms
gXorCwR(sqInt wordConstant, sqInt reg)
{
	/* begin gen:literal:operand: */
	return checkLiteralforInstruction(wordConstant, genoperandoperand(XorCwR, wordConstant, reg));
}


/*	Access for the object representations when they need to prepend code to
	trampolines. 
 */
/*	Eliminate stale dependent info. */

	/* Cogit>>#zeroOpcodeIndex */
static void
zeroOpcodeIndex(void)
{
    sqInt i;

	for (i = 0; i < opcodeIndex; i += 1) {
		((abstractOpcodes[i]).dependent = null);
	}
	zeroOpcodeIndexForNewOpcodes();
}


/*	Access for the object representations when they need to prepend code to
	trampolines. 
 */

	/* Cogit>>#zeroOpcodeIndexForNewOpcodes */
static void
zeroOpcodeIndexForNewOpcodes(void)
{
	opcodeIndex = 0;
	/* begin resetLiterals */

	/* an impossibly high value */
	firstOpcodeIndex = 0x10000;
	nextLiteralIndex = (lastDumpedLiteralIndex = 0);
}

	/* CogMethodZone>>#addToOpenPICList: */
static void NoDbgRegParms
addToOpenPICList(CogMethod *anOpenPIC)
{
	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	assert((openPICList == null)
	 || (isCMOpenPIC(((CogBlockMethod *) openPICList))));
	assertValidDualZoneWriteAddress(anOpenPIC);
	(anOpenPIC->nextOpenPIC = ((usqInt)openPICList));
	openPICList = ((CogMethod *) ((((usqInt)anOpenPIC)) - (getCodeToDataDelta())));
}

	/* CogMethodZone>>#addToYoungReferrers: */
static void NoDbgRegParms
addToYoungReferrers(CogMethod *writableCogMethod)
{
	assertValidDualZoneWriteAddress(writableCogMethod);
	assert((occurrencesInYoungReferrers(writableCogMethod)) == 0);
	assert((writableCogMethod->cmRefersToYoung));
	assert((youngReferrers <= limitAddress)
	 && (youngReferrers >= (limitAddress - (methodCount * BytesPerWord))));
	if (!(asserta((limitAddress - (methodCount * BytesPerWord)) >= mzFreeStart))) {
		error("no room on youngReferrers list");
	}
	youngReferrers -= BytesPerWord;
	codeLongAtput(youngReferrers, (((usqInt)writableCogMethod)) - (getCodeToDataDelta()));
}

	/* CogMethodZone>>#allocate: */
static usqInt NoDbgRegParms
allocate(sqInt numBytes)
{
    usqInt allocation;
    sqInt roundedBytes;

	roundedBytes = (numBytes + 7) & -8;
	if ((mzFreeStart + roundedBytes) >= ((((limitAddress - (methodCount * BytesPerWord)) < allocationThreshold) ? (limitAddress - (methodCount * BytesPerWord)) : allocationThreshold))) {
		return 0;
	}
	allocation = mzFreeStart;
	mzFreeStart += roundedBytes;
	methodCount += 1;
	return allocation;
}


/*	Answer the method containing mcpc for the purposes of code zone
	compaction, where mcpc is actually the value of instructionPointer at the
	time of a compaction. */

	/* CogMethodZone>>#cogMethodContaining: */
CogMethod *
cogMethodContaining(usqInt mcpc)
{
    CogMethod *cogMethod;
    CogMethod *prevMethod;

	if (mcpc > limitAddress) {
		return null;
	}
	if (mcpc < baseAddress) {
		/* begin assertMcpcIsPrimReturn: */
		assert((mcpc == cePrimReturnEnterCogCode)
		 || (mcpc == cePrimReturnEnterCogCodeProfiling));
		return null;
	}
	assert(mcpc < (freeStart()));
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mcpc) {
		prevMethod = cogMethod;
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	assert((prevMethod != null)
	 && ((mcpc == ((((usqInt)prevMethod)) + ((prevMethod->stackCheckOffset))))
	 || ((mcpcisAtStackCheckOfBlockMethodIn(mcpc, prevMethod))
	 || (((primitiveIndexOfMethodheader((prevMethod->methodObject), (prevMethod->methodHeader))) > 0)
	 || ((isCallPrecedingReturnPC(backEnd(), mcpc))
	 && ((callTargetFromReturnAddress(backEnd(), mcpc)) == (ceCheckForInterruptTrampoline())))))));
	return prevMethod;
}

	/* CogMethodZone>>#compactCompiledCode */
static void
compactCompiledCode(void)
{
    unsigned short bytes;
    CogMethod *dest;
    sqLong objectHeaderValue;
    CogMethod *source;
    CogMethod *writableVersion;

	compactionInProgress = 1;
	methodCount = 0;
	objectHeaderValue = nullHeaderForMachineCodeMethod();
	source = ((CogMethod *) baseAddress);
	voidOpenPICList();
	voidUnpairedMethodList();
	while ((source < (limitZony()))
	 && (!(((source->cmType)) == CMFree))) {
		assert((cogMethodDoesntLookKosher(source)) == 0);
		/* begin writableMethodFor: */
		writableVersion = ((CogMethod *) ((((usqInt)source)) + codeToDataDelta));
		(writableVersion->objectHeader = objectHeaderValue);
		if (((source->cmUsageCount)) > 0) {
			(writableVersion->cmUsageCount = ((source->cmUsageCount)) / 2);
		}
		/* begin maybeLinkOnUnpairedMethodList: */
		/* begin clearSavedPICUsageCount: */
		if (((writableVersion->cmType)) == CMClosedPIC) {
			(writableVersion->blockEntryOffset = 0);
		}
		if (((source->cmType)) == CMOpenPIC) {
			addToOpenPICList(writableVersion);
		}
		methodCount += 1;
		source = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)source)) + ((source->blockSize)))));
	}
	if (source >= (limitZony())) {
		haltmsg("no free methods; cannot compact.");
		return;
	}
	dest = source;
	while (source < (limitZony())) {
		assert((maybeFreeCogMethodDoesntLookKosher(source)) == 0);
		bytes = (source->blockSize);
		if (!(((source->cmType)) == CMFree)) {
			methodCount += 1;
			codeMemmove(dest, source, bytes);
			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), (((usqInt)dest)) + bytes);
			}
#      endif
			((writableVersion = ((CogMethod *) ((((usqInt)dest)) + codeToDataDelta)))->objectHeader = objectHeaderValue);
			if (((dest->cmType)) >= CMMethod) {

				/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
				   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
				/* Only update the original method's header if it is referring to this CogMethod. */
				if ((((sqInt)(rawHeaderOf((dest->methodObject))))) == (((sqInt)source))) {
					rawHeaderOfput((dest->methodObject), ((sqInt)dest));
				}
				else {
					assert((noAssertMethodClassAssociationOf((dest->methodObject))) == (nilObject()));
					/* begin linkOnUnpairedMethodList: */
									}
			}
			else {
				/* begin clearSavedPICUsageCount: */
				if (((writableVersion->cmType)) == CMClosedPIC) {
					(writableVersion->blockEntryOffset = 0);
				}
				if (((dest->cmType)) == CMOpenPIC) {
					addToOpenPICList(writableVersion);
				}
			}
			if (((dest->cmUsageCount)) > 0) {
				(writableVersion->cmUsageCount = ((dest->cmUsageCount)) / 2);
			}
			/* begin maybeFlushWritableZoneFrom:to: */
#      if DUAL_MAPPED_CODE_ZONE
			if (codeToDataDelta > 0) {
				flushDCacheFromto(backEnd, ((usqInt)dest), ((usqInt)(dest + 1)));
			}
#      endif
			dest = ((CogMethod *) ((((usqInt)dest)) + bytes));
		}
		source = ((CogMethod *) ((((usqInt)source)) + bytes));
	}
	mzFreeStart = ((usqInt)dest);
	methodBytesFreedSinceLastCompaction = 0;
	compactionInProgress = 0;
}

	/* CogMethodZone>>#ensureInYoungReferrers: */
static void NoDbgRegParms
ensureInYoungReferrers(CogMethod *cogMethod)
{
    CogMethod *writableMethod;

	assertValidDualZoneReadAddress(cogMethod);
	if (!((cogMethod->cmRefersToYoung))) {
		assert((occurrencesInYoungReferrers(cogMethod)) == 0);
		/* begin ensureWritableCodeZone */
#    if !DUAL_MAPPED_CODE_ZONE
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
#    endif // !DUAL_MAPPED_CODE_ZONE
		((writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->cmRefersToYoung = 1);
		addToYoungReferrers(writableMethod);
	}
}

	/* CogMethodZone>>#followForwardedLiteralsInOpenPICList */
static void
followForwardedLiteralsInOpenPICList(void)
{
    CogMethod *openPIC;

	openPIC = openPICList;
	while (openPIC != null) {
		followForwardedLiteralsImplementationIn(openPIC);
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	}
	pruneYoungReferrers();
}

	/* CogMethodZone>>#freeMethod: */
static void NoDbgRegParms
freeMethod(CogMethod *cogMethod)
{
    usqInt theCounters;
    CogMethod *writableMethod;

	assert(!((isCMFree(((CogBlockMethod *) cogMethod)))));
	assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
	/* begin ensureWritableCodeZone */
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneWritable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(0);
	PJWPNClear = __LINE__;
	if (PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 0;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
	if (((cogMethod->cmType)) >= CMMethod) {
		if (((cogMethod->cmType)) == CMMethodFlaggedForBecome) {
					}
		else {

			/* For non-Newspeak there should be a one-to-one mapping between bytecoded and
			   cog methods.  For Newspeak not necessarily, but only for anonymous accessors. */
			/* Only reset the original method's header if it is referring to this CogMethod. */
			if ((((sqInt)(rawHeaderOf((cogMethod->methodObject))))) == (((sqInt)cogMethod))) {
				rawHeaderOfput((cogMethod->methodObject), (cogMethod->methodHeader));
							}
			else {
				assert((noAssertMethodClassAssociationOf((cogMethod->methodObject))) == (nilObject()));
							}
		}
		/* begin maybeFreeCountersOf: */
		theCounters = (cogMethod->counters);
		if (theCounters != 0) {
			freeObject(theCounters - BaseHeaderSize);
		}
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		removeFromOpenPICList(cogMethod);
	}
	/* begin writableMethodFor: */
	writableMethod = ((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta));
	(writableMethod->cmRefersToYoung = 0);
	(writableMethod->cmType = CMFree);
	methodBytesFreedSinceLastCompaction += (cogMethod->blockSize);
}


/*	Free methods, preferring older methods for compaction, up to some
	fraction, currently a quarter.
 */

	/* CogMethodZone>>#freeOlderMethodsForCompaction */
static void
freeOlderMethodsForCompaction(void)
{
    sqInt amountToFree;
    CogMethod *cogMethod;
    sqInt freeableUsage;
    sqInt freedSoFar;
    sqInt initialFreeSpace;
    sqInt zoneSize;

	zoneSize = ((((limitAddress) < allocationThreshold) ? (limitAddress) : allocationThreshold)) - baseAddress;
	initialFreeSpace = (((((limitAddress) < allocationThreshold) ? (limitAddress) : allocationThreshold)) - mzFreeStart) + methodBytesFreedSinceLastCompaction;
	freedSoFar = initialFreeSpace;

	/* 4 needs to be e.g. a start-up parameter */
	amountToFree = zoneSize / 4;
	freeableUsage = 0;
	do {
		cogMethod = ((CogMethod *) baseAddress);
		while (((((usqInt)cogMethod)) < mzFreeStart)
		 && (freedSoFar < amountToFree)) {
			if ((((cogMethod->cmType)) >= CMMethod
				? ((cogMethod->cmUsageCount)) <= freeableUsage
				: (!(((cogMethod->cmType)) == CMFree))
					 && (((cogMethod->cmUsageCount)) == 0))) {
				freeMethod(cogMethod);
				freedSoFar += (cogMethod->blockSize);
			}
			cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		}
	} while((freedSoFar < amountToFree)
		 && (((freeableUsage += 1)) < CMMaxUsageCount));
}


/*	Answer that all entries in youngReferrers are in-use and have the
	cmRefersToYoung flag set.
	Used to check that the youngreferrers pruning routines work correctly. */

	/* CogMethodZone>>#kosherYoungReferrers */
sqInt
kosherYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;
    CogMethod *prevMethod;

	if ((youngReferrers > limitAddress)
	 || (youngReferrers < mzFreeStart)) {
		return 0;
	}
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (!((cogMethod->cmRefersToYoung))) {
				return 0;
			}
			if ((occurrencesInYoungReferrers(cogMethod)) != 1) {
				return 0;
			}
		}
		pointer += BytesPerWord;
	}
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		prevMethod = cogMethod;
		if (!(((cogMethod->cmType)) == CMFree)) {
			if ((occurrencesInYoungReferrers(cogMethod)) != (((cogMethod->cmRefersToYoung)
				? 1
				: 0))) {
				return 0;
			}
		}
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (cogMethod == prevMethod) {
			return 0;
		}
	}
	return 1;
}


/*	For assert checking... */

	/* CogMethodZone>>#mcpc:isAtStackCheckOfBlockMethodIn: */
static sqInt NoDbgRegParms
mcpcisAtStackCheckOfBlockMethodIn(sqInt mcpc, CogMethod *cogMethod)
{
	if (((cogMethod->blockEntryOffset)) == 0) {
		return 0;
	}
	return (blockDispatchTargetsForperformarg(cogMethod, stackCheckOffsetOfBlockAtisMcpc, mcpc)) != 0;
}

	/* CogMethodZone>>#methodFor: */
CogMethod *
methodFor(void *address)
{
    CogMethod *cogMethod;
    CogMethod *nextMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((cogMethod < (limitZony()))
	 && ((((usqInt)cogMethod)) <= (((usqInt)address)))) {
		/* begin methodAfter: */
		nextMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
		if (nextMethod == cogMethod) {
			return null;
		}
		if (((((usqInt)address)) >= (((usqInt)cogMethod)))
		 && ((((usqInt)address)) < (((usqInt)nextMethod)))) {
			return cogMethod;
		}
		cogMethod = nextMethod;
	}
	return null;
}

	/* CogMethodZone>>#methodsCompiledToMachineCodeInto: */
sqInt
methodsCompiledToMachineCodeInto(sqInt arrayObj)
{
    CogMethod *cogMethod;
    sqInt methodIndex;

	methodIndex = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) >= CMMethod) {
			storePointerUncheckedofObjectwithValue(methodIndex, arrayObj, (cogMethod->methodObject));
			methodIndex += 1;
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return methodIndex;
}

	/* CogMethodZone>>#numMethods */
sqInt
numMethods(void)
{
	return methodCount;
}

	/* CogMethodZone>>#numMethodsOfType: */
sqInt
numMethodsOfType(sqInt cogMethodType)
{
    CogMethod *cogMethod;
    sqInt n;

	n = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cogMethodType) {
			n += 1;
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	return n;
}

	/* CogMethodZone>>#occurrencesInYoungReferrers: */
static sqInt NoDbgRegParms
occurrencesInYoungReferrers(CogMethod *cogMethod)
{
    sqInt count;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	count = 0;
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		if ((((sqInt)cogMethod)) == (longAt(pointer))) {
			count += 1;
		}
		pointer += BytesPerWord;
	}
	return count;
}

	/* CogMethodZone>>#openPICWithSelector: */
static CogMethod * NoDbgRegParms
openPICWithSelector(sqInt aSelector)
{
    CogMethod *openPIC;

	openPIC = openPICList;
	do {
		if ((openPIC == null)
		 || (((openPIC->selector)) == aSelector)) {
			return openPIC;
		}
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	Some methods have been freed. Compute how much each survivor needs to
	move during the ensuing compaction and record it in the objectHeader
	field. 
	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#planCompaction */
static void
planCompaction(void)
{
    CogMethod *cogMethod;
    sqInt delta;

	delta = 0;
	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (((cogMethod->cmType)) == CMFree) {
			delta -= (cogMethod->blockSize);
		}
		else {
			assert((cogMethodDoesntLookKosher(cogMethod)) == 0);
			((((CogMethod *) ((((usqInt)cogMethod)) + codeToDataDelta)))->objectHeader = delta);
			savePICUsageCount(cogMethod);
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethods */
void
printCogMethods(void)
{
    CogMethod *cogMethod;
    sqInt nb;
    sqInt nc;
    sqInt nf;
    sqInt nm;
    sqInt no;
    sqInt nu;

	/* begin printCogMethodsSummarizing: */
	nm = (nb = (nc = (no = (nf = (nu = 0)))));
	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		printCogMethod(cogMethod);
		switch ((cogMethod->cmType)) {
		case CMFree:
			nf += 1;
			break;
		case CMMethod:
			nm += 1;
			break;
		case CMMethodFlaggedForBecome:
			nb += 1;
			break;
		case CMClosedPIC:
			nc += 1;
			break;
		case CMOpenPIC:
			no += 1;
			break;
		default:
			nu += 1;

		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	print("CMMethod ");
	printNum(nm);
	if (nb > 0) {
		print(" (flagged for become: ");
		printNum(nb);
		print(")");
	}
	print(" CMClosedPIC ");
	printNum(nc);
	print(" CMOpenPIC ");
	printNum(no);
	print(" CMFree ");
	printNum(nf);
	if (nu > 0) {
		print(" UNKNOWN ");
		printNum(nu);
	}
	print(" total ");
	printNum(((((nm + nc) + no) + nf) + nu) + nb);
	cr();
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsOfType: */
void
printCogMethodsOfType(sqInt cmType)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if (((cogMethod->cmType)) == cmType) {
			printCogMethod(cogMethod);
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithMethod: */
void
printCogMethodsWithMethod(sqInt methodOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->methodObject)) == methodOop)) {
			printCogMethod(cogMethod);
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithPrimitive: */
void
printCogMethodsWithPrimitive(sqInt primIdx)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((((cogMethod->cmType)) >= CMMethod)
		 && (primIdx == (primitiveIndexOfMethodheader((cogMethod->methodObject), (cogMethod->methodHeader))))) {
			printCogMethod(cogMethod);
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogMethodsWithSelector: */
void
printCogMethodsWithSelector(sqInt selectorOop)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while (cogMethod < (limitZony())) {
		if ((!(((cogMethod->cmType)) == CMFree))
		 && (((cogMethod->selector)) == selectorOop)) {
			printCogMethod(cogMethod);
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printCogYoungReferrers */
void
printCogYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!((cogMethod->cmRefersToYoung))) {
			print("*");
		}
		if (((cogMethod->cmType)) == CMFree) {
			print("!");
		}
		if (!(((cogMethod->cmRefersToYoung))
			 && (!(((cogMethod->cmType)) == CMFree)))) {
			print(" ");
		}
		printCogMethod(cogMethod);
		pointer += BytesPerWord;
	}
}


/*	useful for debugging */

	/* CogMethodZone>>#printOpenPICList */
sqInt
printOpenPICList(void)
{
    sqInt n;
    CogMethod *openPIC;

	/* begin printOpenPICListSummarizing: */
	n = 0;
	openPIC = openPICList;
	while (!(openPIC == null)) {
		n += 1;
		printCogMethod(openPIC);
		openPIC = ((CogMethod *) ((openPIC->nextOpenPIC)));
	}
	return n;
}

	/* CogMethodZone>>#pruneYoungReferrers */
static sqInt
pruneYoungReferrers(void)
{
    usqInt dest;
    usqInt next;
    usqInt source;

	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && (((((CogMethod *) (longAt(next))))->cmRefersToYoung)))) break;
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		if (((((CogMethod *) (longAt(source))))->cmRefersToYoung)) {
			assert(source < (dest - BytesPerWord));
			if (!(next == null)) {

				/* convenient first-time flag */
				next = null;
				/* begin ensureWritableCodeZone */
#        if !DUAL_MAPPED_CODE_ZONE
				/* begin makeCodeZoneWritable */
#        if __APPLE__ && __MACH__
				pthread_jit_write_protect_np(0);
				PJWPNClear = __LINE__;
				if (PJWPNState) {
					PJWPNChange = __LINE__;
					PJWPNState = 0;
				}
#        endif // __APPLE__ && __MACH__
#        endif // !DUAL_MAPPED_CODE_ZONE
			}
			codeLongAtput((dest -= BytesPerWord), longAt(source));
		}
		source -= BytesPerWord;
	}
	youngReferrers = dest;
	assert(kosherYoungReferrers());
	return 0;
}

	/* CogMethodZone>>#relocateAndPruneYoungReferrers */
static sqInt
relocateAndPruneYoungReferrers(void)
{
    CogMethod *cogMethod;
    usqInt dest;
    usqInt next;
    usqInt source;

	assert(youngReferrers <= limitAddress);
	if (youngReferrers == limitAddress) {
		return null;
	}
	dest = limitAddress;
	while (1) {
		next = dest - BytesPerWord;
		if (!((next >= youngReferrers)
		 && ((!((((cogMethod = ((CogMethod *) (longAt(next))))->cmType)) == CMFree))
		 && ((cogMethod->cmRefersToYoung))))) break;
		if (((cogMethod->objectHeader)) != 0) {
			codeLongAtput(next, (((sqInt)cogMethod)) + ((cogMethod->objectHeader)));
		}
		dest = next;
	}
	assert(dest >= youngReferrers);
	source = dest - BytesPerWord;
	while (source >= youngReferrers) {
		cogMethod = ((CogMethod *) (longAt(source)));
		if ((!(((cogMethod->cmType)) == CMFree))
		 && ((cogMethod->cmRefersToYoung))) {
			assert(source < (dest - BytesPerWord));
			if (((cogMethod->objectHeader)) != 0) {
				cogMethod = ((CogMethod *) ((((sqInt)cogMethod)) + (((sqInt)((cogMethod->objectHeader))))));
			}
			codeLongAtput((dest -= BytesPerWord), ((sqInt)cogMethod));
		}
		source -= BytesPerWord;
	}

	/* this assert must be deferred until after compaction.  See the end of compactCogCompiledCode */
	/* self assert: self kosherYoungReferrers */
	youngReferrers = dest;
	return 0;
}


/*	All surviving methods have had the amount they are going to relocate by
	stored in their objectHeader fields. Relocate all relative calls so that
	after the compaction of both the method containing each call and the call
	target the calls invoke the same target. */

	/* CogMethodZone>>#relocateMethodsPreCompaction */
static sqInt
relocateMethodsPreCompaction(void)
{
    CogMethod *cogMethod;

	cogMethod = ((CogMethod *) baseAddress);
	while ((((usqInt)cogMethod)) < mzFreeStart) {
		if (!(((cogMethod->cmType)) == CMFree)) {
			if (((cogMethod->cmType)) == CMClosedPIC) {
				relocateCallsInClosedPIC(cogMethod);
			}
			else {
				relocateCallsAndSelfReferencesInMethod(cogMethod);
			}
		}
		/* begin methodAfter: */
		cogMethod = ((CogMethod *) (roundUpToMethodAlignment(backEnd(), (((sqInt)cogMethod)) + ((cogMethod->blockSize)))));
	}
	relocateAndPruneYoungReferrers();
	return 1;
}

	/* CogMethodZone>>#removeFromOpenPICList: */
static sqInt NoDbgRegParms
removeFromOpenPICList(CogMethod *anOpenPIC)
{
    CogMethod *prevPIC;

	assert(isCMOpenPIC(((CogBlockMethod *) anOpenPIC)));
	if (!openPICList) {
		return null;
	}
	assert((isCMOpenPIC(((CogBlockMethod *) openPICList)))
	 && ((((openPICList->nextOpenPIC)) == null)
	 || (isCMOpenPIC(((CogBlockMethod *) (((CogMethod *) ((openPICList->nextOpenPIC)))))))));
	if (anOpenPIC == openPICList) {

		/* N.B. Use self rather than coInterpreter to avoid attempting to cast nil.
		   Conversion to CogMethod done in the nextOpenPIC accessor. */
		openPICList = ((CogMethod *) ((anOpenPIC->nextOpenPIC)));
		return null;
	}
	prevPIC = openPICList;
	do {
		assert((prevPIC != null)
		 && (isCMOpenPIC(((CogBlockMethod *) prevPIC))));
		if (((prevPIC->nextOpenPIC)) == (((usqInt)anOpenPIC))) {
			((((CogMethod *) ((((usqInt)prevPIC)) + codeToDataDelta)))->nextOpenPIC = (anOpenPIC->nextOpenPIC));
			return null;
		}
		prevPIC = ((CogMethod *) ((prevPIC->nextOpenPIC)));
	} while(1);
	return 0;
}


/*	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#restorePICUsageCount: */
static void NoDbgRegParms
restorePICUsageCount(CogMethod *cogMethod)
{
	if ((((cogMethod->cmType)) == CMClosedPIC)
	 && (((cogMethod->blockEntryOffset)) != 0)) {
		(cogMethod->cmUsageCount = (cogMethod->blockEntryOffset));
		(cogMethod->blockEntryOffset = 0);
	}
}


/*	Determine the default alignment for the start of a CogMethod, which in
	turn determines the size of the mask used to distinguish the checked and
	unchecked entry-points, used to distinguish normal and super sends on
	method unlinking.
	This is passed onto the backEnd to allow processors with coarse
	instructions (ARM) to increase the alignment if required. */

	/* CogMethodZone>>#roundUpLength: */
static sqInt NoDbgRegParms
roundUpLength(sqInt numBytes)
{
	return roundUpToMethodAlignment(backEnd(), numBytes);
}


/*	For Sista, where we want PICs to last so they can be observed, we need to
	keep PICs unless
	they are definitely unused. So we need to identify unused PICs. So in
	planCompact, zero the
	usage counts of all PICs, saving the actual usage count in
	blockEntryOffset. Then in
	relocateMethodsPreCompaction (actually in
	relocateIfCallOrMethodReference:mcpc:delta:) restore the usage counts of
	used PICs. Finally in compactCompiledCode, clear the blockEntryOffset
	of the unused PICs; they will then have a zero count and be reclaimed in
	the next code compaction. */

	/* CogMethodZone>>#savePICUsageCount: */
static void NoDbgRegParms
savePICUsageCount(CogMethod *cogMethod)
{
	if (((cogMethod->cmType)) == CMClosedPIC) {
		(cogMethod->blockEntryOffset = (cogMethod->cmUsageCount));
		(cogMethod->cmUsageCount = 0);
	}
}

	/* CogMethodZone>>#voidOpenPICList */
static void
voidOpenPICList(void)
{
	openPICList = null;
}

	/* CogMethodZone>>#voidUnpairedMethodList */
static void
voidUnpairedMethodList(void)
{
	}

	/* CogMethodZone>>#voidYoungReferrersPostTenureAll */
static void
voidYoungReferrersPostTenureAll(void)
{
    CogMethod *cogMethod;
    usqInt pointer;

	assert(youngReferrers <= limitAddress);
	pointer = youngReferrers;
	while (pointer < limitAddress) {
		cogMethod = ((CogMethod *) (longAt(pointer)));
		if (!(((cogMethod->cmType)) == CMFree)) {
			(cogMethod->cmRefersToYoung = 0);
		}
		pointer += BytesPerWord;
	}
	youngReferrers = limitAddress;
}

	/* CogMethodZone>>#whereIsMaybeCodeThing: */
char *
whereIsMaybeCodeThing(sqInt anOop)
{
	if (oopisGreaterThanOrEqualToandLessThan(anOop, codeBase, limitAddress)) {
		if (oopisLessThan(anOop, minCogMethodAddress())) {
			return " is in generated runtime";
		}
		if (oopisLessThan(anOop, mzFreeStart)) {
			return " is in generated methods";
		}
		if (oopisLessThan(anOop, youngReferrers)) {
			return " is in code zone";
		}
		return " is in young referrers";
	}
	return null;
}

	/* CogMethodZone>>#zoneAlignment */
static sqInt
zoneAlignment(void)
{
	return 8;
}

	/* CogObjectRepresentation>>#checkValidObjectReference: */
static sqInt NoDbgRegParms
checkValidObjectReference(sqInt anOop)
{
	return (!(isImmediate(anOop)))
	 && ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentation>>#genCmpClassFloatCompactIndexR: */
static AbstractInstruction * NoDbgRegParms
genCmpClassFloatCompactIndexR(sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, ClassFloatCompactIndex, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(ClassFloatCompactIndex, BytesPerOop));
	}
	return anInstruction;
}

	/* CogObjectRepresentation>>#genCmpClassMethodContextCompactIndexR: */
static AbstractInstruction * NoDbgRegParms
genCmpClassMethodContextCompactIndexR(sqInt reg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	return anInstruction;
}


/*	Get the method header (first word) of a CompiledMethod into headerReg.
	Deal with the method possibly being cogged. */

	/* CogObjectRepresentation>>#genGetMethodHeaderOf:into:scratch: */
static sqInt NoDbgRegParms
genGetMethodHeaderOfintoscratch(sqInt methodReg, sqInt headerReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpNotCogged;
    sqInt offset;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, methodReg, headerReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	jumpNotCogged = genJumpSmallInteger(headerReg);
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodHeader);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, offset, headerReg, headerReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	jmpTarget(jumpNotCogged, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genLoadSlot:sourceReg:destReg: */
static sqInt NoDbgRegParms
genLoadSlotsourceRegdestReg(sqInt index, sqInt sourceReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, (index * BytesPerWord) + BaseHeaderSize, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveAdd */
static sqInt
genPrimitiveAdd(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, ReceiverResultReg, ClassReg);
	/* begin JumpOverflow: */
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveAsFloat */
static sqInt
genPrimitiveAsFloat(void)
{
    AbstractInstruction *jumpFailAlloc;


	/* inline processorHasDoublePrecisionFloatingPointSupport */
	/* begin hasDoublePrecisionFloatingPointSupport */
;
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, TempReg, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveBitAnd */
static sqInt
genPrimitiveBitAnd(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	/* begin AndR:R: */
	genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitOr */
static sqInt
genPrimitiveBitOr(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	/* begin OrR:R: */
	genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	rTemp := rArg0
	rClass := tTemp
	rTemp := rTemp & 1
	jz nonInt
	rClass >>= 1
	cmp 0,rClass
	jge neg
	cmp 31,rClass // numSmallIntegerBits, jge for sign
	jge tooBig
	rTemp := rReceiver
	rTemp <<= rClass
	rTemp >>= rClass (arithmetic)
	cmp rTemp,rReceiver
	jnz ovfl
	rReceiver := rReceiver - 1
	rReceiver := rReceiver <<= rClass
	rReceiver := rReceiver + 1
	ret
	neg:
	rClass := 0 - rClass
	cmp 31,rClass // numSmallIntegerBits
	jge inRange
	rClass := 31
	inRange
	rReceiver := rReceiver >>= rClass.
	rReceiver := rReceiver | smallIntegerTags.
	ret
	ovfl
	tooBig
	nonInt:
	fail
 */

	/* CogObjectRepresentation>>#genPrimitiveBitShift */
static sqInt
genPrimitiveBitShift(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *jumpInRange;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;
    AbstractInstruction *jumpTooBig;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNegative: */
	jumpNegative = genConditionalBranchoperand(JumpNegative, ((sqInt)0));
	/* begin CmpCq:R: */
	quickConstant = numSmallIntegerBits();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpTooBig = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	/* begin LogicalShiftLeftR:R: */
	genoperandoperand(LogicalShiftLeftRR, ClassReg, TempReg);
	/* begin ArithmeticShiftRightR:R: */
	genoperandoperand(ArithmeticShiftRightRR, ClassReg, TempReg);
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, ReceiverResultReg);
	/* begin JumpNonZero: */
	jumpOvfl = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);
	/* begin LogicalShiftLeftR:R: */
	genoperandoperand(LogicalShiftLeftRR, ClassReg, ReceiverResultReg);
	genAddSmallIntegerTagsTo(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegative, genoperand(NegateR, ClassReg));
	/* begin CmpCq:R: */
	quickConstant1 = numSmallIntegerBits();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, quickConstant1, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpInRange = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin MoveCq:R: */
	quickConstant2 = numSmallIntegerBits();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(MoveCqR, quickConstant2, ClassReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
	}
	jmpTarget(jumpInRange, genoperandoperand(ArithmeticShiftRightRR, ClassReg, ReceiverResultReg));
	genClearAndSetSmallIntegerTagsIn(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, jmpTarget(jumpTooBig, jmpTarget(jumpOvfl, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveBitXor */
static sqInt
genPrimitiveBitXor(void)
{
    AbstractInstruction *jumpNotSI;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);
	/* begin XorR:R: */
	genoperandoperand(XorRR, Arg0Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveClass */
static sqInt
genPrimitiveClass(void)
{
    sqInt reg;

	reg = ReceiverResultReg;
	if (methodOrBlockNumArgs > 0) {
		if (methodOrBlockNumArgs > 1) {
			return UnimplementedPrimitive;
		}
		/* begin genLoadArgAtDepth:into: */
		reg = Arg0Reg;
		assert(0 < (numRegArgs()));
	}
	if ((genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ReceiverResultReg, TempReg, reg != ReceiverResultReg)) == BadRegisterSet) {
		genGetClassObjectOfintoscratchRegmayBeAForwarder(reg, ClassReg, TempReg, reg != ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDiv */
static sqInt
genPrimitiveDiv(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, 0, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(SubCqR, 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(1, BytesPerOop));
	}
	jmpTarget(jumpSameSign, (convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveDivide */
static sqInt
genPrimitiveDivide(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpInexact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOverflow;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	/* begin JumpZero: */
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpInexact = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	jumpOverflow = genJumpNotSmallIntegerValuescratch(TempReg, Arg1Reg);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOverflow, jmpTarget(jumpInexact, jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveEqual */
static sqInt
genPrimitiveEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpZero, gJumpFPEqual, 0)
		: genSmallIntegerComparison(JumpZero));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatAdd */
static sqInt
genPrimitiveFloatAdd(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(AddRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(AddRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatDivide */
static sqInt
genPrimitiveFloatDivide(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 1)
		: genPureFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatMultiply */
static sqInt
genPrimitiveFloatMultiply(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(MulRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(MulRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSquareRoot */
static sqInt
genPrimitiveFloatSquareRoot(void)
{
    AbstractInstruction *jumpFailAlloc;


	/* inline processorHasDoublePrecisionFloatingPointSupport */
	/* begin hasDoublePrecisionFloatingPointSupport */
;
	genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	/* begin SqrtRd: */
	genoperand(SqrtRd, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveFloatSubtract */
static sqInt
genPrimitiveFloatSubtract(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(SubRdRd, null, 1)
		: genPureFloatArithmeticpreOpCheckboxed(SubRdRd, null, 1));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterOrEqual */
static sqInt
genPrimitiveGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreaterOrEqual, gJumpFPGreaterOrEqual, 0)
		: genSmallIntegerComparison(JumpGreaterOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveGreaterThan */
static sqInt
genPrimitiveGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpGreater, gJumpFPGreater, 0)
		: genSmallIntegerComparison(JumpGreater));
}


/*	Implementation notes: there are two reasons to use TempReg
	-1) if primitive fails, ReceiverResultReg must remain unchanged (we
	CompletePrimitive) -2) CLZ/BSR only work on 64bits for registers R0-R7 on
	Intel X64. But Win64 uses R9
	Normally, this should be backEnd dependent, but for now we have a single
	64bits target...
 */

	/* CogObjectRepresentation>>#genPrimitiveHighBit */
static sqInt
genPrimitiveHighBit(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpNegativeReceiver;
    AbstractInstruction *jumpNegativeReceiver1;
    sqInt quickConstant;


	/* remove excess tag bits from the receiver oop */
	gOrCqRR((1U << (numSmallIntegerTagBits())) - 1, ReceiverResultReg, TempReg);
	/* begin ArithmeticShiftRightCq:R: */
	quickConstant = (numSmallIntegerTagBits()) - 1;
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, TempReg);
	/* begin genHighBitIn:ofSmallIntegerOopWithSingleTagBit: */
	/* begin genHighBitClzIn:ofSmallIntegerOopWithSingleTagBit: */
	genoperandoperand(ClzRR, TempReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}

	/* Note the nice bit trick below:
	   highBit_1based_of_small_int_value = (BytesPerWord * 8) - leadingZeroCout_of_oop - 1 toAccountForTagBit.
	   This is like 2 complements (- reg - 1) on (BytesPerWord * 8) log2 bits, or exactly a bit invert operation... */
	jumpNegativeReceiver1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin gen:literal:operand: */
	checkLiteralforInstruction((BytesPerWord * 8) - 1, genoperandoperand(XorCwR, (BytesPerWord * 8) - 1, TempReg));
	jumpNegativeReceiver = jumpNegativeReceiver1;
	goto l4;
	l4:	/* end genHighBitIn:ofSmallIntegerOopWithSingleTagBit: */;
	if (jumpNegativeReceiver == 0) {
		return UnimplementedPrimitive;
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegativeReceiver, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveIdentical */
static sqInt
genPrimitiveIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(0);
}

	/* CogObjectRepresentation>>#genPrimitiveLessOrEqual */
static sqInt
genPrimitiveLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLessOrEqual, gJumpFPGreaterOrEqual, 1)
		: genSmallIntegerComparison(JumpLessOrEqual));
}

	/* CogObjectRepresentation>>#genPrimitiveLessThan */
static sqInt
genPrimitiveLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpLess, gJumpFPGreater, 1)
		: genSmallIntegerComparison(JumpLess));
}

	/* CogObjectRepresentation>>#genPrimitiveMod */
static sqInt
genPrimitiveMod(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpSameSign;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genRemoveSmallIntegerTagsInScratchReg(ClassReg);
	/* begin JumpZero: */
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, Arg1Reg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genRemoveSmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin XorR:R: */
	genoperandoperand(XorRR, ClassReg, Arg1Reg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ClassReg);
	jmpTarget(jumpSameSign, jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genSetSmallIntegerTagsIn(ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveMultiply */
static sqInt
genPrimitiveMultiply(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(processorHasMultiplyAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, Arg1Reg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	genRemoveSmallIntegerTagsInScratchReg(Arg1Reg);
	/* begin MulOverflowR:R: */
	genMulOverflowRR(backEnd, Arg1Reg, ClassReg);
	/* begin JumpOverflow: */
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genSetSmallIntegerTagsIn(ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	subclasses override if they can */

	/* CogObjectRepresentation>>#genPrimitiveNewMethod */
static sqInt
genPrimitiveNewMethod(void)
{
	return UnimplementedPrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveNotEqual */
static sqInt
genPrimitiveNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genSmallIntegerComparisonorDoubleComparisoninvert(JumpNonZero, gJumpFPNotEqual, 0)
		: genSmallIntegerComparison(JumpNonZero));
}

	/* CogObjectRepresentation>>#genPrimitiveNotIdentical */
static sqInt
genPrimitiveNotIdentical(void)
{
	return genPrimitiveIdenticalOrNotIf(1);
}

	/* CogObjectRepresentation>>#genPrimitiveQuo */
static sqInt
genPrimitiveQuo(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *convert;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpIsSI;
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpZero;

	if (!(processorHasDivQuoRemAndMClassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ClassReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	genShiftAwaySmallIntegerTagsInScratchReg(ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpZero = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	genShiftAwaySmallIntegerTagsInScratchReg(TempReg);
	gDivRRQuoRem(ClassReg, TempReg, TempReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin Label */
	convert = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerInRegtoSmallIntegerInReg(TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpIsSI = genJumpIsSmallIntegerValuescratch(TempReg, Arg1Reg);
	jmpTarget(jumpIsSI, convert);
	jmpTarget(jumpZero, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatAdd */
static sqInt
genPrimitiveSmallFloatAdd(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(AddRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(AddRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatDivide */
static sqInt
genPrimitiveSmallFloatDivide(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 0)
		: genPureFloatArithmeticpreOpCheckboxed(DivRdRd, genDoubleFailIfZeroArgRcvrarg, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatEqual */
static sqInt
genPrimitiveSmallFloatEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPEqual, JumpZero, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatGreaterOrEqual */
static sqInt
genPrimitiveSmallFloatGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpGreaterOrEqual, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatGreaterThan */
static sqInt
genPrimitiveSmallFloatGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpGreater, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatLessOrEqual */
static sqInt
genPrimitiveSmallFloatLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpLessOrEqual, 1, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 1, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatLessThan */
static sqInt
genPrimitiveSmallFloatLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpLess, 1, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 1, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatMultiply */
static sqInt
genPrimitiveSmallFloatMultiply(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(MulRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(MulRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatNotEqual */
static sqInt
genPrimitiveSmallFloatNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPNotEqual, JumpNonZero, 0, 0)
		: genPureFloatComparisoninvertboxed(gJumpFPNotEqual, 0, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatSquareRoot */
static sqInt
genPrimitiveSmallFloatSquareRoot(void)
{
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpNegative;

	genGetSmallFloatValueOfscratchinto(ReceiverResultReg, SendNumArgsReg, DPFPReg0);
	/* begin XorRd:Rd: */
	genoperandoperand(XorRdRd, DPFPReg1, DPFPReg1);
	/* begin CmpRd:Rd: */
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	jumpNegative = gJumpFPGreater(0);
	/* begin SqrtRd: */
	genoperand(SqrtRd, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNegative, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}

	/* CogObjectRepresentation>>#genPrimitiveSmallFloatSubtract */
static sqInt
genPrimitiveSmallFloatSubtract(void)
{
	return (primitiveDoMixedArithmetic()
		? genFloatArithmeticpreOpCheckboxed(SubRdRd, null, 0)
		: genPureFloatArithmeticpreOpCheckboxed(SubRdRd, null, 0));
}

	/* CogObjectRepresentation>>#genPrimitiveSubtract */
static sqInt
genPrimitiveSubtract(void)
{
    AbstractInstruction *jumpNotSI;
    AbstractInstruction *jumpOvfl;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSI = genJumpNotSmallInteger(Arg0Reg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, Arg0Reg, TempReg);
	/* begin JumpOverflow: */
	jumpOvfl = genConditionalBranchoperand(JumpOverflow, ((sqInt)0));
	genAddSmallIntegerTagsTo(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOvfl, jmpTarget(jumpNotSI, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#genSmallIntegerComparison: */
static sqInt NoDbgRegParms
genSmallIntegerComparison(sqInt jumpOpcode)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt constant;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpTrue;
    sqInt literal;

	if (!(mclassIsSmallInteger())) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpFail = genJumpNotSmallInteger(Arg0Reg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, (shouldAnnotateObjectReference(trueObject())
		? annotateobjRef(gMoveCwR(trueObject(), ReceiverResultReg), trueObject())
		: (/* begin checkQuickConstant:forInstruction: */
			(literal = trueObject()),
			(anInstruction1 = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)),
			(usesOutOfLineLiteral(anInstruction1)
					? (anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop))
					: 0),
			anInstruction1)));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}

	/* CogObjectRepresentation>>#isUnannotatableConstant: */
static sqInt NoDbgRegParms
isUnannotatableConstant(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((isImmediate((simStackEntry->constant)))
	 || (!(shouldAnnotateObjectReference((simStackEntry->constant)))));
}

	/* CogObjectRepresentationFor64BitSpur>>#allImmediate:branchIf:instanceOfBehaviors:target: */
static sqInt NoDbgRegParms
allImmediatebranchIfinstanceOfBehaviorstarget(sqInt immediateMask, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt tag1;
    sqInt tag2;

	if (immediateMask == (tagMask())) {
		jmpTarget(genJumpImmediate(reg), targetFixUp);
		return 0;
	}
	tag1 = classTagForClass(fetchPointerofObject(0, arrayObj));
	tag2 = classTagForClass(fetchPointerofObject(1, arrayObj));
	if ((tag1 == (smallIntegerTag()))
	 || (tag2 == (smallIntegerTag()))) {
		jmpTarget(genJumpSmallIntegerInScratchReg(TempReg), targetFixUp);
	}
	if ((tag1 == (characterTag()))
	 || (tag2 == (characterTag()))) {
		jmpTarget(genJumpCharacterInScratchReg(TempReg), targetFixUp);
	}
	if ((tag1 == (smallFloatTag()))
	 || (tag2 == (smallFloatTag()))) {
		jmpTarget(genJumpSmallFloatInScratchReg(TempReg), targetFixUp);
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#allImmediate:branchIf:notInstanceOfBehaviors:target: */
static sqInt NoDbgRegParms
allImmediatebranchIfnotInstanceOfBehaviorstarget(sqInt immediateMask, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    AbstractInstruction *anInstruction;
    sqInt incorrectTag;
    sqInt tag1;
    sqInt tag2;

	jmpTarget(genJumpNotImmediate(reg), targetFixUp);
	if (!(immediateMask == (tagMask()))) {

		/* TempReg holds the rcvr tag */
		/* In this case one immediate tag out of the three is not present in arrayObj.
		   We look for it, and generate a jump to the fixup if the rcvr tag matches */
		tag1 = classTagForClass(fetchPointerofObject(0, arrayObj));
		tag2 = classTagForClass(fetchPointerofObject(1, arrayObj));
		/* begin fetchImmediateTagOtherThanTag1:tag2: */
		if (!((tag1 == (characterTag()))
			 || (tag2 == (characterTag())))) {
			incorrectTag = characterTag();
			goto l2;
		}
		if (!((tag1 == (smallIntegerTag()))
			 || (tag2 == (smallIntegerTag())))) {
			incorrectTag = smallIntegerTag();
			goto l2;
		}
		incorrectTag = smallFloatTag();
	l2:	/* end fetchImmediateTagOtherThanTag1:tag2: */;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, incorrectTag, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(incorrectTag, BytesPerOop));
		}
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)targetFixUp));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#classForInlineCacheTag: */
static sqInt NoDbgRegParms
classForInlineCacheTag(sqInt classIndex)
{
	return classOrNilAtIndex(classIndex);
}

	/* CogObjectRepresentationFor64BitSpur>>#genAddSmallIntegerTagsTo: */
static sqInt NoDbgRegParms
genAddSmallIntegerTagsTo(sqInt aRegister)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, 1, aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genAlloc64BitPositiveIntegerValue:into:scratchReg:scratchReg: */
static AbstractInstruction * NoDbgRegParms
genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    sqInt address;
    sqInt address1;
    sqInt allocSize;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *jumpFail;
    sqInt literal;
    usqLong newLPIHeader;

	allocSize = BaseHeaderSize + BytesPerWord;
	newLPIHeader = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, resultReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(allocSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction1 = genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, scratch1, address1));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction3 = genoperandoperand(MoveCqR, newLPIHeader, scratch1);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(newLPIHeader, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveRMwr, valueReg, BaseHeaderSize, resultReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	return jumpFail;
}

	/* CogObjectRepresentationFor64BitSpur>>#genAlloc64BitSignedIntegerValue:into:scratchReg:scratchReg: */
static AbstractInstruction * NoDbgRegParms
genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(sqInt valueReg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    sqInt address;
    sqInt address1;
    sqInt allocSize;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpNeg;
    sqInt literal;
    usqLong newLNIHeader;

	allocSize = BaseHeaderSize + BytesPerWord;
	newLNIHeader = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargeNegativeIntegerCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, resultReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(allocSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction1 = genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, scratch1, address1));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(MoveCqR, newLNIHeader, scratch1);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(newLNIHeader, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(CmpCqR, 0, valueReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpNeg = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	assert((headerForSlotsformatclassIndex(0, 0, 1)) == 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, ClassLargePositiveIntegerCompactIndex - ClassLargeNegativeIntegerCompactIndex, scratch1);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(ClassLargePositiveIntegerCompactIndex - ClassLargeNegativeIntegerCompactIndex, BytesPerOop));
	}
	jmpTarget(jumpNeg, checkQuickConstantforInstruction(0, genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg)));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperandoperand(MoveRMwr, valueReg, BaseHeaderSize, resultReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	return jumpFail;
}


/*	Override to answer a SmallFloat64 if possible. */

	/* CogObjectRepresentationFor64BitSpur>>#genAllocFloatValue:into:scratchReg:scratchReg: */
static AbstractInstruction * NoDbgRegParms
genAllocFloatValueintoscratchRegscratchReg(sqInt dpreg, sqInt resultReg, sqInt scratch1, sqInt scratch2)
{
    sqInt address;
    sqInt address1;
    usqIntptr_t allocSize;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpFail1;
    AbstractInstruction *jumpMerge;
    AbstractInstruction *jumpNotSF;
    sqInt literal;
    usqLong newFloatHeader;

	/* begin MoveRd:R: */
	assert(BytesPerWord == 8);
	genoperandoperand(MoveRdR, dpreg, resultReg);
	jumpNotSF = genJumpNotSmallFloatValueBitsscratch(resultReg, scratch1);
	genConvertBitsToSmallFloatInscratch(resultReg, scratch1);
	/* begin Jump: */
	jumpMerge = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpNotSF, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	allocSize = BaseHeaderSize + (sizeof(double));
	newFloatHeader = headerForSlotsformatclassIndex((sizeof(double)) / BytesPerWord, firstLongFormat(), ClassFloatCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, resultReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratch1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(allocSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction1 = genoperandoperand(CmpCqR, getScavengeThreshold(), scratch1);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpFail1 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, scratch1, address1));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction3 = genoperandoperand(MoveCqR, newFloatHeader, scratch1);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(newFloatHeader, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, scratch1, 0, resultReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveRdM64r, dpreg, BaseHeaderSize, resultReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	jumpFail = jumpFail1;
	jmpTarget(jumpMerge, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpFail;
}


/*	Set the SmallInteger tag bits when the tag bits may be filled with
	garbage. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genClearAndSetSmallIntegerTagsIn: */
static sqInt NoDbgRegParms
genClearAndSetSmallIntegerTagsIn(sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt quickConstant;

	/* begin AndCq:R: */
	quickConstant = -1 - (tagMask());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AndCqR, quickConstant, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(OrCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}


/*	Convert the in-SmallFloat64-range floating point value in integer register
	into a tagged SmallFloat64 oop.
	c.f. Spur64BitMemoryManager>>smallFloatObjectOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertBitsToSmallFloatIn:scratch: */
static sqInt NoDbgRegParms
genConvertBitsToSmallFloatInscratch(sqInt reg, sqInt scratch)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *jumpZero;
    sqInt literal;
    sqInt quickConstant;

	/* begin RotateLeftCq:R: */
	genoperandoperand(RotateLeftCqR, 1, reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 1, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin JumpBelowOrEqual: */
	jumpZero = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin SubCq:R: */
	quickConstant = ((sqInt)((usqInt)((smallFloatExponentOffset())) << ((smallFloatMantissaBits()) + 1)));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(SubCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	jmpTarget(jumpZero, genoperandoperand(LogicalShiftLeftCqR, numTagBits(), reg));
	/* begin checkQuickConstant:forInstruction: */
	literal = smallFloatTag();
	anInstruction2 = genoperandoperand(AddCqR, smallFloatTag(), reg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	Convert the Character in reg to a SmallInteger, assuming
	the Character's value is a valid character. */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertCharacterToSmallIntegerInReg: */
static void NoDbgRegParms
genConvertCharacterToSmallIntegerInReg(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	/* begin SubCq:R: */
	quickConstant = (characterTag()) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(SubCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertIntegerInReg:toSmallIntegerInReg: */
static sqInt NoDbgRegParms
genConvertIntegerInRegtoSmallIntegerInReg(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	gLogicalShiftLeftCqRR(numTagBits(), srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, 1, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertIntegerToSmallIntegerInReg: */
static sqInt NoDbgRegParms
genConvertIntegerToSmallIntegerInReg(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	/* begin LogicalShiftLeftCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, 1, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}


/*	Convert the SmallFloat in reg to its identityHash as a SmallInteger.
	Rotate the sign bit from bit 3 (zero-relative) to the sign bit. 
	c.f. Spur64BitMemoryManager>>rotatedFloatBitsOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallFloatToSmallFloatHashAsIntegerInReg:scratch: */
static sqInt NoDbgRegParms
genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(sqInt reg, sqInt scratch)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;
    sqInt quickConstant1;

	assert(((((usqInt)((smallFloatTag()))) >> 1) - (smallIntegerTag())) == (smallIntegerTag()));
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 1, reg);
	gAndCqRR(1U << ((numTagBits()) - 1), reg, scratch);
	/* begin SubR:R: */
	genoperandoperand(SubRR, scratch, reg);
	/* begin SubCq:R: */
	quickConstant1 = (((usqInt)((smallFloatTag()))) >> 1) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(SubCqR, quickConstant1, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant = 0x3F - ((numTagBits()) - 1);
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, scratch);
	/* begin OrR:R: */
	genoperandoperand(OrRR, scratch, reg);
	return 0;
}


/*	Convert the SmallInteger in reg to a Character, assuming
	the SmallInteger's value is a valid character. */

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallIntegerToCharacterInReg: */
static void NoDbgRegParms
genConvertSmallIntegerToCharacterInReg(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	/* begin AddCq:R: */
	quickConstant = (characterTag()) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
}

	/* CogObjectRepresentationFor64BitSpur>>#genConvertSmallIntegerToIntegerInReg: */
static sqInt NoDbgRegParms
genConvertSmallIntegerToIntegerInReg(sqInt reg)
{
    sqInt quickConstant;

	/* begin ArithmeticShiftRightCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, reg);
	return 0;
}


/*	The arguments are in an array in Arg1Reg. Its size is in sizeReg.
	Load Arg0Reg and Arg1Reg with the first two slots, as appropriate.
	Since objects always have at least one slot it is safe to load arg0
	without checking.
	But the array could be at the end of memory so we must check that it has
	two slots before it is safe to access the second slot. */

	/* CogObjectRepresentationFor64BitSpur>>#genFetchRegArgsForPerformWithArguments: */
static sqInt NoDbgRegParms
genFetchRegArgsForPerformWithArguments(sqInt sizeReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *skip;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, 2, sizeReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(2, BytesPerOop));
	}
	/* begin JumpLess: */
	skip = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize + BytesPerWord, Arg1Reg, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(BaseHeaderSize + BytesPerWord, BytesPerOop));
	}
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genFloatArithmetic:preOpCheck:boxed: */
static sqInt NoDbgRegParms
genFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed)
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpNotSmallInteger;

	jumpFailCheck = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	/* begin Label */
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (!(preOpCheckOrNil == null)) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotSmallInteger = genJumpNotSmallInteger(Arg0Reg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, Arg1Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpNotSmallInteger, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	if (!(preOpCheckOrNil == null)) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailAlloc->operands))[0])));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genFloatComparison:orIntegerComparison:invert:boxed: */
static sqInt NoDbgRegParms
genFloatComparisonorIntegerComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt jumpOpcode, sqInt invertComparison, sqInt rcvrBoxed)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *compareFloat;
    sqInt constant;
    sqInt constant1;
    AbstractInstruction *jumpAmbiguous;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpNotSmallInteger;
    AbstractInstruction *jumpTrue;
    sqInt literal;
    AbstractInstruction *returnTrue;

	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	if (invertComparison) {

		/* May need to invert for NaNs */
		/* begin CmpRd:Rd: */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		/* begin CmpRd:Rd: */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	/* begin genMoveConstant:R: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, (shouldAnnotateObjectReference(trueObject())
		? (returnTrue = annotateobjRef(gMoveCwR(trueObject(), ReceiverResultReg), trueObject()))
		: (/* begin checkQuickConstant:forInstruction: */
			(literal = trueObject()),
			(anInstruction1 = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)),
			(usesOutOfLineLiteral(anInstruction1)
					? (anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop))
					: 0),
			(returnTrue = anInstruction1))));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));

	/* Test for ambiguity, that is when floatRcvr == (double) intArg */
	jumpNotSmallInteger = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, Arg0Reg, DPFPReg1);
	/* begin CmpRd:Rd: */
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);

	/* Case of non ambiguity, use compareFloat(floatRcvr,(double) intArg) */
	jumpAmbiguous = gJumpFPEqual(0);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpAmbiguous, genoperandoperand(ConvertRdR, DPFPReg0, ReceiverResultReg));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	constant1 = falseObject();
	if (shouldAnnotateObjectReference(constant1)) {
		annotateobjRef(gMoveCwR(constant1, ReceiverResultReg), constant1);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(MoveCqR, constant1, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(constant1, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, returnTrue);
	jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Fetch the instance's identity hash into destReg, encoded as a
	SmallInteger. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genGetHashFieldNonImmOf:asSmallIntegerInto: */
static sqInt NoDbgRegParms
genGetHashFieldNonImmOfasSmallIntegerInto(sqInt instReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt literal;
    sqInt quickConstant;
    sqInt quickConstant1;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	quickConstant = (identityHashFullWordShift()) - (numTagBits());
	genoperandoperand(LogicalShiftRightCqR, quickConstant, destReg);
	/* begin AndCq:R: */
	quickConstant1 = ((sqInt)((usqInt)((identityHashHalfWordMask())) << (numTagBits())));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, quickConstant1, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = smallIntegerTag();
	anInstruction2 = genoperandoperand(AddCqR, smallIntegerTag(), destReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	Fetch the instance's identity hash into destReg, unencoded. */

	/* CogObjectRepresentationFor64BitSpur>>#genGetHashFieldNonImmOf:into: */
static sqInt NoDbgRegParms
genGetHashFieldNonImmOfinto(sqInt instReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, instReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 32, destReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = identityHashHalfWordMask();
	anInstruction1 = genoperandoperand(AndCqR, identityHashHalfWordMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	Extract the inline cache tag for the object in sourceReg into destReg. The
	inline cache tag for a given object is the value loaded in inline caches
	to distinguish
	objects of different classes. In Spur this is either the tags for
	immediates, or
	the receiver's classIndex. Answer the label for the start of the sequence. */

	/* CogObjectRepresentationFor64BitSpur>>#genGetInlineCacheClassTagFrom:into:forEntry: */
static AbstractInstruction * NoDbgRegParms
genGetInlineCacheClassTagFromintoforEntry(sqInt sourceReg, sqInt destReg, sqInt forEntry)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *entryLabel;
    AbstractInstruction *jumpImm;
    sqInt literal;

	if (forEntry) {
		/* begin AlignmentNops: */
		genoperand(AlignmentNops, BytesPerWord);
	}
	/* begin Label */
	entryLabel = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	gAndCqRR(tagMask(), sourceReg, destReg);
	/* begin JumpNonZero: */
	jumpImm = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = classIndexMask();
	anInstruction1 = genoperandoperand(AndCqR, classIndexMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	jmpTarget(jumpImm, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return entryLabel;
}


/*	Get the size in byte-sized slots of the object in srcReg into destReg.
	srcReg may equal destReg.
	destReg <- numSlots << self shiftForWord - (fmt bitAnd: 7).
	Assumes the object in srcReg has a byte format, i.e. 16 to 23 or 24 to 31 */

	/* CogObjectRepresentationFor64BitSpur>>#genGetNumBytesOf:into: */
static sqInt NoDbgRegParms
genGetNumBytesOfinto(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmp;
    sqInt literal;

	genGetRawSlotSizeOfNonImminto(srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = numSlotsMask();
	anInstruction = genoperandoperand(CmpCqR, numSlotsMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLess: */
	jmp = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genGetOverflowSlotsOfinto(srcReg, destReg);
	jmpTarget(jmp, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), destReg));
	genGetBitsofFormatByteOfinto(7, srcReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, destReg);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genGetOverflowSlotsOf:into: */
static sqInt NoDbgRegParms
genGetOverflowSlotsOfinto(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, -BaseHeaderSize, srcReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(-BaseHeaderSize, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, 8, destReg);
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, 8, destReg);
	return 0;
}


/*	Convert the SmallFloat oop in ooppReg into the corresponding float value
	in dpReg.
	c.f. Spur64BitMemoryManager>>smallFloatBitsOf: */

	/* CogObjectRepresentationFor64BitSpur>>#genGetSmallFloatValueOf:scratch:into: */
static sqInt NoDbgRegParms
genGetSmallFloatValueOfscratchinto(sqInt oopReg, sqInt scratch, sqInt dpReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpSFZero;
    sqInt quickConstant;
    sqInt quickConstant1;

	/* begin MoveR:R: */
	genoperandoperand(MoveRR, oopReg, scratch);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(LogicalShiftRightCqR, quickConstant, scratch);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 1, scratch);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpSFZero = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant1 = ((sqInt)((usqInt)((smallFloatExponentOffset())) << ((smallFloatMantissaBits()) + 1)));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AddCqR, quickConstant1, scratch);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	jmpTarget(jumpSFZero, genoperandoperand(RotateRightCqR, 1, scratch));
	/* begin MoveR:Rd: */
	assert(BytesPerWord == 8);
	genoperandoperand(MoveRRd, scratch, dpReg);
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genJumpCharacterInScratchReg: */
static AbstractInstruction * NoDbgRegParms
genJumpCharacterInScratchReg(sqInt aRegister)
{
	return genJumpCharacter(aRegister);
}


/*	Generate a compare and branch to test if aRegister contains a Character. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpCharacter: */
static AbstractInstruction * NoDbgRegParms
genJumpCharacter(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = characterTag()),
		(anInstruction = genoperandoperand(TstCqR, characterTag(), reg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpNonZero: */
		genConditionalBranchoperand(JumpNonZero, ((sqInt)0)));
}


/*	Generate a test for aRegister containing an integer value in the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. Spur64BitMemoryManager>>isIntegerValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpIsSmallIntegerValue:scratch: */
static AbstractInstruction * NoDbgRegParms
genJumpIsSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt quickConstant;

	return (gArithmeticShiftRightCqRR(0x3F - (numTagBits()), aRegister, scratchReg),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction = genoperandoperand(AddCqR, 1, scratchReg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(1, BytesPerOop))
				: 0),
		/* begin AndCq:R: */
		(quickConstant = (1U << ((numTagBits()) + 1)) - 1),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction1 = genoperandoperand(AndCqR, quickConstant, scratchReg)),
		(usesOutOfLineLiteral(anInstruction1)
				? (anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop))
				: 0),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction2 = genoperandoperand(CmpCqR, 1, scratchReg)),
		(usesOutOfLineLiteral(anInstruction2)
				? (anInstruction2->dependent = locateLiteralsize(1, BytesPerOop))
				: 0),
		/* begin JumpLessOrEqual: */
		genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0)));
}


/*	Generate a compare and branch to test if aRegister contains other than a
	Character. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotCharacter: */
static AbstractInstruction * NoDbgRegParms
genJumpNotCharacter(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = characterTag()),
		(anInstruction = genoperandoperand(TstCqR, characterTag(), reg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)0)));
}


/*	Generate a test to check that the integer register contains a floating
	point value within the SmallFloat64 range,
	and answer the jump. c.f. Spur64BitMemoryManager>>isSmallFloatValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallFloatValueBits:scratch: */
static AbstractInstruction * NoDbgRegParms
genJumpNotSmallFloatValueBitsscratch(sqInt reg, sqInt exponent)
{
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *jumpFail;
    AbstractInstruction *jumpMaxExponent;
    AbstractInstruction *jumpMinExponent;
    AbstractInstruction *jumpTest;
    AbstractInstruction *jumpZeroMantissa;
    sqInt literal;
    sqInt literal1;
    sqInt quickConstant;
    sqInt quickConstant1;

	flag("if we combine the exponent range test with the conversion to tagged representation we test for a zero exponent only once. further, if we extract tags once into a scratch on the input side we test for immediates, SmallInteger and SmallFloat using the same intermediate result.  so to do is to move fp arithmetic into the object representations");
	gLogicalShiftLeftCqRR(1, reg, exponent);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = (smallFloatMantissaBits()) + 1;
	genoperandoperand(LogicalShiftRightCqR, quickConstant, exponent);
	/* begin checkQuickConstant:forInstruction: */
	literal = smallFloatExponentOffset();
	anInstruction1 = genoperandoperand(CmpCqR, smallFloatExponentOffset(), exponent);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpMinExponent = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin CmpCq:R: */
	quickConstant1 = 0xFF + (smallFloatExponentOffset());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, quickConstant1, exponent);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpMaxExponent = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin Jump: */
	jumpFail = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpMinExponent, gTstCqR((1ULL << (smallFloatMantissaBits())) - 1, reg));
	/* begin JumpZero: */
	jumpZeroMantissa = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = smallFloatExponentOffset();
	anInstruction3 = genoperandoperand(CmpCqR, smallFloatExponentOffset(), exponent);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin Jump: */
	jumpTest = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpZeroMantissa, checkQuickConstantforInstruction(0, genoperandoperand(CmpCqR, 0, exponent)));
	jmpTarget(jumpTest, genConditionalBranchoperand(JumpNonZero, ((sqInt)jumpFail)));
	jmpTarget(jumpMaxExponent, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return jumpFail;
}


/*	Generate a compare and branch to test if aRegister contains other than a
	SmallFloat. Answer the jump. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallFloat: */
static AbstractInstruction * NoDbgRegParms
genJumpNotSmallFloat(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = smallFloatTag()),
		(anInstruction = genoperandoperand(TstCqR, smallFloatTag(), reg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)0)));
}


/*	Generate a test for aRegister containing an integer value outside the
	SmallInteger range, and a jump if so, answering the jump.
	c.f. Spur64BitMemoryManager>>isIntegerValue: */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallIntegerValue:scratch: */
static AbstractInstruction * NoDbgRegParms
genJumpNotSmallIntegerValuescratch(sqInt aRegister, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt quickConstant;

	return (gArithmeticShiftRightCqRR(0x3F - (numTagBits()), aRegister, scratchReg),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction = genoperandoperand(AddCqR, 1, scratchReg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(1, BytesPerOop))
				: 0),
		/* begin AndCq:R: */
		(quickConstant = (1U << ((numTagBits()) + 1)) - 1),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction1 = genoperandoperand(AndCqR, quickConstant, scratchReg)),
		(usesOutOfLineLiteral(anInstruction1)
				? (anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop))
				: 0),
		/* begin checkQuickConstant:forInstruction: */
		(anInstruction2 = genoperandoperand(CmpCqR, 1, scratchReg)),
		(usesOutOfLineLiteral(anInstruction2)
				? (anInstruction2->dependent = locateLiteralsize(1, BytesPerOop))
				: 0),
		/* begin JumpGreater: */
		genConditionalBranchoperand(JumpGreater, ((sqInt)0)));
}


/*	Generate a compare and branch to test if aRegister contains other than a
	SmallInteger. 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpNotSmallInteger: */
static AbstractInstruction * NoDbgRegParms
genJumpNotSmallInteger(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = smallIntegerTag()),
		(anInstruction = genoperandoperand(TstCqR, smallIntegerTag(), reg)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)0)));
}

	/* CogObjectRepresentationFor64BitSpur>>#genJumpSmallFloatInScratchReg: */
static AbstractInstruction * NoDbgRegParms
genJumpSmallFloatInScratchReg(sqInt aRegister)
{
	return genJumpSmallFloat(aRegister);
}


/*	Generate a compare and branch to test if aRegister contains a SmallFloat.
	Answer the jump, or UnimplementedOperation if this cannot be done with
	a single register. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpSmallFloat: */
static AbstractInstruction * NoDbgRegParms
genJumpSmallFloat(sqInt aRegister)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = smallFloatTag()),
		(anInstruction = genoperandoperand(TstCqR, smallFloatTag(), aRegister)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpNonZero: */
		genConditionalBranchoperand(JumpNonZero, ((sqInt)0)));
}

	/* CogObjectRepresentationFor64BitSpur>>#genJumpSmallIntegerInScratchReg: */
static AbstractInstruction * NoDbgRegParms
genJumpSmallIntegerInScratchReg(sqInt aRegister)
{
	return genJumpSmallInteger(aRegister);
}


/*	Generate a compare and branch to test if aRegister contains a
	SmallInteger. Answer the jump, or UnimplementedOperation if this cannot be
	done with
	a single register. */

	/* CogObjectRepresentationFor64BitSpur>>#genJumpSmallInteger: */
static AbstractInstruction * NoDbgRegParms
genJumpSmallInteger(sqInt aRegister)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	return (/* begin checkQuickConstant:forInstruction: */
		(literal = smallIntegerTag()),
		(anInstruction = genoperandoperand(TstCqR, smallIntegerTag(), aRegister)),
		(usesOutOfLineLiteral(anInstruction)
				? (anInstruction->dependent = locateLiteralsize(literal, BytesPerOop))
				: 0),
		/* begin JumpNonZero: */
		genConditionalBranchoperand(JumpNonZero, ((sqInt)0)));
}


/*	Generate the code for primitives 61 & 165, at:put:/basicAt:put: &
	integerAt:put:. If signedVersion is true
	then generate signed accesses to the bits classes (a la 164 & 165). If
	signedVersion is false,
	generate unsigned accesses (a la 60, 61, 63 & 64). */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveAtPutSigned: */
static sqInt NoDbgRegParms
genPrimitiveAtPutSigned(sqInt signedVersion)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction22;
    AbstractInstruction *anInstruction23;
    AbstractInstruction *anInstruction24;
    AbstractInstruction *anInstruction25;
    AbstractInstruction *anInstruction26;
    AbstractInstruction *anInstruction27;
    AbstractInstruction *anInstruction28;
    AbstractInstruction *anInstruction29;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction30;
    AbstractInstruction *anInstruction31;
    AbstractInstruction *anInstruction32;
    AbstractInstruction *anInstruction33;
    AbstractInstruction *anInstruction34;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt formatReg;
    AbstractInstruction *jump64BitArgIsImmediate;
    AbstractInstruction *jump64BitsOutOfBounds;
    AbstractInstruction *jumpArrayOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpDoubleWordsOutOfRange;
    AbstractInstruction *jumpFixedFieldsOutOfBounds;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNonSmallIntegerValue;
    AbstractInstruction *jumpNot64BitIndexable;
    AbstractInstruction *jumpNot8ByteInteger;
    AbstractInstruction *jumpNotIndexableBits;
    AbstractInstruction *jumpNotIndexablePointers;
    AbstractInstruction *jumpNotPointers;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt literal6;
    sqInt literal7;
    sqInt literal8;
    AbstractInstruction *methodInBounds;
    sqInt nSlotsOrBytesReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    sqInt quickConstant5;
    AbstractInstruction *rejoin;

	jumpDoubleWordsOutOfRange = ((AbstractInstruction *) 0);
	jumpImmutable = ((AbstractInstruction *) 0);
	jumpNegative = ((AbstractInstruction *) 0);
	nSlotsOrBytesReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(SubCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(1, BytesPerOop));
	}
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	literal8 = immutableBitMask();
	anInstruction34 = genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction34)) {
		(anInstruction34->dependent = locateLiteralsize(literal8, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = weakArrayFormat();
	anInstruction12 = genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpNotPointers = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg1Reg, TempReg, 0);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = arrayFormat();
	anInstruction13 = genoperandoperand(CmpCqR, arrayFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpBelow: */
	jumpNotIndexablePointers = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin JumpNonZero: */
	jumpHasFixedFields = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpArrayOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(AddCqR, quickConstant, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpHasFixedFields, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genGetClassIndexOfNonImminto(ReceiverResultReg, formatReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction15 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, formatReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(formatReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, formatReg);
	genConvertSmallIntegerToIntegerInReg(formatReg);
	/* begin checkQuickConstant:forInstruction: */
	literal2 = fixedFieldsOfClassFormatMask();
	anInstruction16 = genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), formatReg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction17 = genoperandoperand(AddCqR, quickConstant1, formatReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpFixedFieldsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, formatReg, Arg0Reg);
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotPointers, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNonSmallIntegerValue = genJumpNotSmallInteger(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstByteFormat();
	anInstruction18 = genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = firstShortFormat();
	anInstruction19 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = firstLongFormat();
	anInstruction20 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal6 = sixtyFourBitIndexableFormat();
	anInstruction21 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction21)) {
		(anInstruction21->dependent = locateLiteralsize(literal6, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNotIndexableBits = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	if (!signedVersion) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, 0, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpLess: */
		jumpNegative = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	}
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	rejoin = genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jump64BitsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant2 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction22 = genoperandoperand(AddCqR, quickConstant2, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction22)) {
		(anInstruction22->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, SendNumArgsReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonSmallIntegerValue, checkQuickConstantforInstruction(sixtyFourBitIndexableFormat(), genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg)));
	/* begin JumpNonZero: */
	jumpNot64BitIndexable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	jump64BitArgIsImmediate = genJumpImmediate(Arg1Reg);
	if (signedVersion) {

		/* Test top bit of 64-bit word in large integer for range check. */
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(CmpCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpLess: */
		jumpDoubleWordsOutOfRange = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction23 = genoperandoperandoperand(MoveMwrR, 0, Arg1Reg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction23)) {
		(anInstruction23->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant3 = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction24 = genoperandoperand(AndCqR, quickConstant3, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction24)) {
		(anInstruction24->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
	}
	/* begin CmpCq:R: */
	quickConstant4 = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction25 = genoperandoperand(CmpCqR, quickConstant4, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction25)) {
		(anInstruction25->dependent = locateLiteralsize(quickConstant4, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction26 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction26)) {
		(anInstruction26->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)rejoin));
	if (signedVersion) {

		/* Now check if the header is that of an 8 byte LargeNegativeInteger */
		jmpTarget(jumpNot8ByteInteger, gCmpCqR(headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargeNegativeIntegerCompactIndex), SendNumArgsReg));
		/* begin JumpNonZero: */
		jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, TempReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, SendNumArgsReg);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)rejoin));
	}
	if (signedVersion) {
		jmpTarget(jumpIsWords, gArithmeticShiftRightCqRR(0x1F + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction5 = genoperandoperand(AddCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction5)) {
			(anInstruction5->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction6 = genoperandoperand(CmpCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction6)) {
			(anInstruction6->dependent = locateLiteralsize(1, BytesPerOop));
		}
	}
	else {
		jmpTarget(jumpIsWords, checkQuickConstantforInstruction((((usqInt)0xFFFFFFFFU << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFFFFFFFFU << 3) | 1), Arg1Reg)));
	}
	/* begin JumpAbove: */
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction27 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction27)) {
		(anInstruction27->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddCq:R: */
	quickConstant5 = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction28 = genoperandoperand(AddCqR, quickConstant5, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction28)) {
		(anInstruction28->dependent = locateLiteralsize(quickConstant5, BytesPerOop));
	}
	/* begin MoveR:X32r:R: */
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	if (signedVersion) {
		jmpTarget(jumpIsBytes, gArithmeticShiftRightCqRR(7 + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction7 = genoperandoperand(AddCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction8 = genoperandoperand(CmpCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction8)) {
			(anInstruction8->dependent = locateLiteralsize(1, BytesPerOop));
		}
	}
	else {
		jmpTarget(jumpIsBytes, checkQuickConstantforInstruction((((usqInt)0xFF << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFF << 3) | 1), Arg1Reg)));
	}
	/* begin JumpAbove: */
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrBytesReg);
	gAndCqRR(BytesPerWord - 1, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal7 = firstCompiledMethodFormat();
	anInstruction29 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction29)) {
		(anInstruction29->dependent = locateLiteralsize(literal7, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	methodInBounds = genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction30 = genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction30)) {
		(anInstruction30->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:Xbr:R: */
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	if (signedVersion) {
		jmpTarget(jumpIsShorts, gArithmeticShiftRightCqRR(15 + (numTagBits()), Arg1Reg, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction9 = genoperandoperand(AddCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction9)) {
			(anInstruction9->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction10 = genoperandoperand(CmpCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction10)) {
			(anInstruction10->dependent = locateLiteralsize(1, BytesPerOop));
		}
	}
	else {
		jmpTarget(jumpIsShorts, checkQuickConstantforInstruction((((usqInt)0xFFFF << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFFFF << 3) | 1), Arg1Reg)));
	}
	/* begin JumpAbove: */
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction31 = genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction31)) {
		(anInstruction31->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction32 = genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction32)) {
		(anInstruction32->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsCompiledMethod, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	getLiteralCountOfplusOneinBytesintoscratch(ReceiverResultReg, 1, 1, nSlotsOrBytesReg, TempReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelow: */
	genConditionalBranchoperand(JumpBelow, ((sqInt)methodInBounds));
	jmpTarget(jumpNot8ByteInteger, jmpTarget(jump64BitArgIsImmediate, jmpTarget(jumpNot64BitIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpNotIndexableBits, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpArrayOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jump64BitsOutOfBounds, jmpTarget(jumpNotIndexablePointers, jmpTarget(jumpFixedFieldsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))))))))));
	if (signedVersion) {
		jmpTarget(jumpDoubleWordsOutOfRange, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
	}
	else {
		jmpTarget(jumpNegative, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
	}
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
#  endif
	/* begin checkQuickConstant:forInstruction: */
	anInstruction33 = genoperandoperand(AddCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction33)) {
		(anInstruction33->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	Generate the code for primitives 60 & 164, at:/basicAt: & integerAt:. If
	signedVersion is true
	then generate signed accesses to the bits classes (a la 164 & 165). If
	signedVersion is false,
	generate unsigned accesses (a la 60, 61, 63 & 64). */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveAtSigned: */
static sqInt NoDbgRegParms
genPrimitiveAtSigned(sqInt signedVersion)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *convertToIntAndReturn;
    sqInt formatReg;
    AbstractInstruction *jumpArrayOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFixedFieldsOutOfBounds;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsArray;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsLongs;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpLongsOutOfBounds;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotSmallInteger;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfBounds;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt literal6;
    sqInt literal7;
    AbstractInstruction *methodInBounds;
    sqInt nSlotsOrElementsReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    sqInt quickConstant5;

	nSlotsOrElementsReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(SubCqR, 1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrElementsReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstByteFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = arrayFormat();
	anInstruction5 = genoperandoperand(CmpCqR, arrayFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsArray = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin JumpBelow: */
	jumpNotIndexable = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = weakArrayFormat();
	anInstruction6 = genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpBelowOrEqual: */
	jumpHasFixedFields = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstShortFormat();
	anInstruction7 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = firstLongFormat();
	anInstruction8 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = sixtyFourBitIndexableFormat();
	anInstruction9 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsLongs = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jmpTarget(jumpNotIndexable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin Jump: */
	jumpNotIndexable = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsArray, (assert(!((Arg1Reg == SPReg))),
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg)));
	/* begin JumpBelowOrEqual: */
	jumpArrayOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant2 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AddCqR, quickConstant2, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrElementsReg));
	gAndCqRR(7, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal6 = firstCompiledMethodFormat();
	anInstruction11 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(literal6, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	methodInBounds = anInstruction12;
	/* begin MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (signedVersion) {
		/* begin SignExtend8R:R: */
		genoperandoperand(SignExtend8RR, ReceiverResultReg, ReceiverResultReg);
		goto l8;
	l8:	/* end SignExtend8R:R: */;
	}
	/* begin Label */
	convertToIntAndReturn = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperand(AndCqR, 3, formatReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(3, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	if (signedVersion) {
		/* begin SignExtend16R:R: */
		genoperandoperand(SignExtend16RR, ReceiverResultReg, ReceiverResultReg);
		goto l13;
	l13:	/* end SignExtend16R:R: */;
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpIsWords, gLogicalShiftLeftCqR((shiftForWord()) - 2, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction15 = genoperandoperand(AndCqR, 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant3 = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction16 = genoperandoperand(AddCqR, quickConstant3, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
	}
	/* begin MoveX32r:R:R: */
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (signedVersion) {
		/* begin SignExtend32R:R: */
		genoperandoperand(SignExtend32RR, ReceiverResultReg, ReceiverResultReg);
		goto l18;
	l18:	/* end SignExtend32R:R: */;
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpHasFixedFields, checkQuickConstantforInstruction(classIndexMask(), genoperandoperand(AndCqR, classIndexMask(), TempReg)));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, formatReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction17 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(formatReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, formatReg);
	genConvertSmallIntegerToIntegerInReg(formatReg);
	/* begin checkQuickConstant:forInstruction: */
	literal7 = fixedFieldsOfClassFormatMask();
	anInstruction18 = genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), formatReg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(literal7, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpFixedFieldsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, formatReg, Arg1Reg);
	/* begin AddCq:R: */
	quickConstant4 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction19 = genoperandoperand(AddCqR, quickConstant4, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(quickConstant4, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsLongs, (assert(!((Arg1Reg == SPReg))),
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg)));
	/* begin JumpBelowOrEqual: */
	jumpLongsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant5 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction20 = genoperandoperand(AddCqR, quickConstant5, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(quickConstant5, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);
	if (signedVersion) {

		/* c.f. Spur64BitMemoryManager>>#isIntegerValue: */
		/* begin ArithmeticShiftRightCq:R: */
		quickConstant = numSmallIntegerBits();
		genoperandoperand(ArithmeticShiftRightCqR, quickConstant, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AndCqR, 15, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(15, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, 1, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin JumpAbove: */
		jumpNotSmallInteger = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)convertToIntAndReturn));
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitSignedIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
	}
	else {
		/* begin LogicalShiftRightCq:R: */
		quickConstant1 = (numSmallIntegerBits()) - 1;
		genoperandoperand(LogicalShiftRightCqR, quickConstant1, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(CmpCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpNotSmallInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, ReceiverResultReg);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)convertToIntAndReturn));
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
	}
	jmpTarget(jumpIsMethod, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	getLiteralCountOfplusOneinBytesintoscratch(ReceiverResultReg, 1, 1, nSlotsOrElementsReg, TempReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelow: */
	genConditionalBranchoperand(JumpBelow, ((sqInt)methodInBounds));
	jmpTarget(jumpFailAlloc, jmpTarget(jumpLongsOutOfBounds, jmpTarget(jumpFixedFieldsOutOfBounds, jmpTarget(jumpArrayOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))))))));
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatEqual */
static sqInt
genPrimitiveFloatEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPEqual, JumpZero, 0, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPEqual, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatGreaterOrEqual */
static sqInt
genPrimitiveFloatGreaterOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpGreaterOrEqual, 0, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatGreaterThan */
static sqInt
genPrimitiveFloatGreaterThan(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpGreater, 0, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 0, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatLessOrEqual */
static sqInt
genPrimitiveFloatLessOrEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreaterOrEqual, JumpLessOrEqual, 1, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPGreaterOrEqual, 1, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatLessThan */
static sqInt
genPrimitiveFloatLessThan(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPGreater, JumpLess, 1, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPGreater, 1, 1));
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveFloatNotEqual */
static sqInt
genPrimitiveFloatNotEqual(void)
{
	return (primitiveDoMixedArithmetic()
		? (/* begin genDoubleComparison:orIntegerComparison:invert: */
			genFloatComparisonorIntegerComparisoninvertboxed(gJumpFPNotEqual, JumpNonZero, 0, 1))
		: genPureFloatComparisoninvertboxed(gJumpFPNotEqual, 0, 1));
}


/*	Arguably we should fail for immediates, but so far no one has complained,
	so... 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveIdentityHash */
static sqInt
genPrimitiveIdentityHash(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *inst;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpNotCharacter;
    AbstractInstruction *jumpNotSet;
    sqInt literal;
    AbstractInstruction *ret;


	/* uses TstCqR */
	jumpImm = genJumpImmediate(ReceiverResultReg);
	genGetHashFieldNonImmOfasSmallIntegerInto(ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, ConstZero, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(ConstZero, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpNotSet = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		ret = genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		ret = genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImm, gAndCqRR(tagMask(), ReceiverResultReg, TempReg));
	/* begin checkQuickConstant:forInstruction: */
	literal = characterTag();
	anInstruction1 = genoperandoperand(CmpCqR, characterTag(), TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNotCharacter = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertCharacterToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		ret = genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		ret = genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotCharacter, checkQuickConstantforInstruction(smallFloatTag(), genoperandoperand(CmpCqR, smallFloatTag(), TempReg)));
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)ret));
	genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(ReceiverResultReg, TempReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSet, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (!(primitiveIndex == 75)) {
		return 0;
	}
	/* begin saveAndRestoreLinkRegAround: */
	inst = genoperand(PushR, LinkReg);
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceNewHashTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	genoperand(PopR, LinkReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveImmediateAsInteger */
static sqInt
genPrimitiveImmediateAsInteger(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpNotCharacter;
    sqInt literal;
    AbstractInstruction *ret;

	gAndCqRR(tagMask(), ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = characterTag();
	anInstruction = genoperandoperand(CmpCqR, characterTag(), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNotCharacter = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertCharacterToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		ret = genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		ret = genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotCharacter, checkQuickConstantforInstruction(smallFloatTag(), genoperandoperand(CmpCqR, smallFloatTag(), TempReg)));
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)ret));
	genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(ReceiverResultReg, TempReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}


/*	Implement primitiveNew for convenient cases:
	- the receiver has a hash
	- the receiver is fixed size (excluding ephemerons to save instructions &
	miniscule time)
	- single word header/num slots < numSlotsMask
	- the result fits in eden (actually below scavengeThreshold)
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveNew */
static sqInt
genPrimitiveNew(void)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt byteSizeReg;
    AbstractInstruction *fillLoop;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpTooBig;
    AbstractInstruction *jumpUnhashed;
    AbstractInstruction *jumpVariableOrEphemeron;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 0) {
		return UnimplementedPrimitive;
	}

	/* inst spec will hold class's instance specification, then byte size and finally end of new object. */
	headerReg = (fillReg = SendNumArgsReg);

	/* get freeStart as early as possible so as not to wait later... */
	instSpecReg = (byteSizeReg = ClassReg);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, Arg1Reg));
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* begin JumpZero: */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, instSpecReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = fixedFieldsFieldWidth();
	genoperandoperand(LogicalShiftRightCqR, quickConstant, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = formatMask();
	anInstruction = genoperandoperand(AndCqR, formatMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal1 = fixedFieldsOfClassFormatMask();
	anInstruction1 = genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal2 = nonIndexablePointerFormat();
	anInstruction2 = genoperandoperand(CmpCqR, nonIndexablePointerFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpVariableOrEphemeron = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = numSlotsMask();
	anInstruction3 = genoperandoperand(CmpCqR, numSlotsMask(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpTooBig = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	quickConstant1 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant1, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	/* begin LogicalShiftLeftCq:R: */
	quickConstant2 = numSlotsFullShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant2, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(CmpCqR, 0, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(BaseHeaderSize * 2, BytesPerOop));
	}
	/* begin Jump: */
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, checkQuickConstantforInstruction(BaseHeaderSize / BytesPerWord, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg)));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = getScavengeThreshold();
	anInstruction6 = genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, byteSizeReg, address1));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperandoperand(MoveRMwr, headerReg, 0, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperandoperand(LoadEffectiveAddressMwrR, BaseHeaderSize, ReceiverResultReg, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal5 = nilObject();
	anInstruction9 = genoperandoperand(MoveCqR, nilObject(), fillReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperandoperand(MoveRMwr, fillReg, 0, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(0, BytesPerOop));
	}
	fillLoop = anInstruction10;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AddCqR, 8, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(8, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, byteSizeReg);
	/* begin JumpAbove: */
	genConditionalBranchoperand(JumpAbove, ((sqInt)fillLoop));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpUnhashed, jmpTarget(jumpVariableOrEphemeron, jmpTarget(jumpTooBig, jmpTarget(jumpNoSpace, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return 0;
}


/*	Implement primitiveNewWithArg for convenient cases:
	- the receiver has a hash
	- the receiver is variable and not compiled method
	- single word header/num slots < numSlotsMask
	- the result fits in eden
	See superclass method for dynamic frequencies of formats.
	For the moment we implement only arrayFormat, firstByteFormat &
	firstLongFormat 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveNewWithArg */
static sqInt
genPrimitiveNewWithArg(void)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt byteSizeReg;
    AbstractInstruction *fillLoop;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpArrayFormat;
    AbstractInstruction *jumpArrayTooBig;
    AbstractInstruction *jumpByteFormat;
    AbstractInstruction *jumpBytePrepDone;
    AbstractInstruction *jumpByteTooBig;
    AbstractInstruction *jumpFailCuzFixed;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpLongPrepDone;
    AbstractInstruction *jumpLongTooBig;
    AbstractInstruction *jumpNElementsNonInt;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpUnhashed;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt maxSlots;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 1) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));

	/* Assume there's an available scratch register on 64-bit machines.  This holds the saved numFixedFields and then the value to fill with */
	headerReg = SendNumArgsReg;
	fillReg = Extra0Reg;
	assert(fillReg > 0);

	/* The max slots we'll allocate here are those for a single header */
	instSpecReg = (byteSizeReg = ClassReg);

	/* get freeStart as early as possible so as not to wait later... */
	maxSlots = (numSlotsMask()) - 1;
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, Arg1Reg));
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* begin JumpZero: */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNElementsNonInt = genJumpNotSmallInteger(Arg0Reg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, instSpecReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = (fixedFieldsFieldWidth()) + (numSmallIntegerTagBits());
	genoperandoperand(LogicalShiftRightCqR, quickConstant, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = formatMask();
	anInstruction = genoperandoperand(AndCqR, formatMask(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	/* begin LogicalShiftLeftCq:R: */
	quickConstant1 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant1, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, fillReg);
	genConvertSmallIntegerToIntegerInReg(fillReg);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = arrayFormat();
	anInstruction1 = genoperandoperand(CmpCqR, arrayFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpArrayFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstByteFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstByteFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstLongFormat();
	anInstruction3 = genoperandoperand(CmpCqR, firstLongFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpFailCuzFixed = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 2) << 3) | 1), Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize((((usqInt)(maxSlots * 2) << 3) | 1), BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpLongTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(MoveCqR, BytesPerWord / 4, TempReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(BytesPerWord / 4, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant2 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant2, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(AddCqR, (BytesPerWord / 4) - 1, instSpecReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 2, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(MoveCqR, 0, fillReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin Jump: */
	jumpLongPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpByteFormat, checkQuickConstantforInstruction((((usqInt)(maxSlots * BytesPerWord) << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)(maxSlots * BytesPerWord) << 3) | 1), Arg0Reg)));
	/* begin JumpAbove: */
	jumpByteTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(MoveCqR, BytesPerWord, TempReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AndCqR, BytesPerWord - 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant3 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant3, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AddCqR, BytesPerWord - 1, instSpecReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(MoveCqR, 0, fillReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin Jump: */
	jumpBytePrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpArrayFormat, checkQuickConstantforInstruction((((usqInt)maxSlots << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)maxSlots << 3) | 1), Arg0Reg)));
	/* begin JumpAbove: */
	jumpArrayTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	literal4 = nilObject();
	anInstruction13 = genoperandoperand(MoveCqR, nilObject(), fillReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	jmpTarget(jumpBytePrepDone, jmpTarget(jumpLongPrepDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	/* begin LogicalShiftLeftCq:R: */
	quickConstant4 = numSlotsFullShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant4, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(CmpCqR, 0, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction15 = genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize(BaseHeaderSize * 2, BytesPerOop));
	}
	/* begin Jump: */
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, checkQuickConstantforInstruction(BaseHeaderSize / BytesPerWord, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg)));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = getScavengeThreshold();
	anInstruction16 = genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, byteSizeReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction17 = genoperandoperandoperand(MoveRMwr, headerReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction18 = genoperandoperandoperand(LoadEffectiveAddressMwrR, BaseHeaderSize, ReceiverResultReg, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction19 = genoperandoperandoperand(MoveRMwr, fillReg, 0, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(0, BytesPerOop));
	}
	fillLoop = anInstruction19;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction20 = genoperandoperand(AddCqR, 8, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(8, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, byteSizeReg);
	/* begin JumpAbove: */
	genConditionalBranchoperand(JumpAbove, ((sqInt)fillLoop));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNoSpace, jmpTarget(jumpUnhashed, jmpTarget(jumpFailCuzFixed, jmpTarget(jumpArrayTooBig, jmpTarget(jumpByteTooBig, jmpTarget(jumpLongTooBig, jmpTarget(jumpNElementsNonInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))));
	return 0;
}


/*	Implement primitiveShallowCopy/primitiveClone for convenient cases:
	- the receiver is not a context
	- the receiver is not a compiled method
	- the result fits in eden (actually below scavengeThreshold) */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveShallowCopy */
static sqInt
genPrimitiveShallowCopy(void)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *continuance;
    AbstractInstruction *copyLoop;
    sqInt formatReg;
    AbstractInstruction *jumpEmpty;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpTooBig;
    AbstractInstruction *jumpVariable;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt ptrReg;
    sqInt quickConstant;
    sqInt resultReg;
    sqInt slotsReg;

	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	resultReg = Arg0Reg;

	/* get freeStart as early as possible so as not to wait later... */
	slotsReg = Arg1Reg;
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, resultReg));
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (ptrReg = (formatReg = SendNumArgsReg)), NoReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstCompiledMethodFormat();
	anInstruction = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = indexablePointersFormat();
	anInstruction1 = genoperandoperand(CmpCqR, indexablePointersFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpVariable = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin Label */
	continuance = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genGetRawSlotSizeOfNonImminto(ReceiverResultReg, slotsReg);
	/* begin checkQuickConstant:forInstruction: */
	literal2 = numSlotsMask();
	anInstruction2 = genoperandoperand(CmpCqR, numSlotsMask(), slotsReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpTooBig = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(CmpCqR, 0, slotsReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpEmpty = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, slotsReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(BaseHeaderSize / BytesPerWord, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), slotsReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, resultReg, slotsReg);
	/* begin checkQuickConstant:forInstruction: */
	literal3 = getScavengeThreshold();
	anInstruction5 = genoperandoperand(CmpCqR, getScavengeThreshold(), slotsReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, resultReg, ptrReg);
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, slotsReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(SubCqR, BytesPerWord * 2, slotsReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(BytesPerWord * 2, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AndCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperandoperand(MoveRMwr, TempReg, 0, resultReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin Label */
	copyLoop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AddCqR, BytesPerWord, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AddCqR, BytesPerWord, ptrReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperandoperand(MoveMwrR, 0, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ptrReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((ptrReg == SPReg)));
	genoperandoperand(CmpRR, ptrReg, slotsReg);
	/* begin JumpAboveOrEqual: */
	genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)copyLoop));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, resultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpVariable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genGetClassIndexOfNonImminto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, ClassReg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)continuance));
	jmpTarget(jumpImmediate, jmpTarget(jumpNoSpace, jmpTarget(jumpIsMethod, jmpTarget(jumpTooBig, jmpTarget(jumpEmpty, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	return 0;
}


/*	Generate the code for primitive 173, instVarAt:. Defer to
	StackInterpreterPrimitives>>primitiveSlotAt for Contexts.
 */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveSlotAt */
static sqInt
genPrimitiveSlotAt(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *convertToIntAndReturn;
    sqInt formatReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpLongsOutOfBounds;
    AbstractInstruction *jumpNonPointers;
    AbstractInstruction *jumpPointersOutOfBounds;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpToReturnLargeInteger;
    AbstractInstruction *jumpWordsOutOfBounds;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt nSlotsOrElementsReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;

	nSlotsOrElementsReg = ClassReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(SubCqR, 1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrElementsReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = sixtyFourBitIndexableFormat();
	anInstruction2 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNonPointers = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = classIndexMask();
	anInstruction3 = genoperandoperand(AndCqR, classIndexMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpPointersOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(AddCqR, quickConstant1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonPointers, checkQuickConstantforInstruction(firstByteFormat(), genoperandoperand(CmpCqR, firstByteFormat(), formatReg)));
	/* begin JumpAboveOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstShortFormat();
	anInstruction6 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstLongFormat();
	anInstruction7 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpLongsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant2 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AddCqR, quickConstant2, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg1Reg, ReceiverResultReg, ClassReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ClassReg, TempReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = (numSmallIntegerBits()) - 1;
	genoperandoperand(LogicalShiftRightCqR, quickConstant, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpToReturnLargeInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genConvertIntegerInRegtoSmallIntegerInReg(ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpToReturnLargeInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(ClassReg, SendNumArgsReg, Extra0Reg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrElementsReg));
	gAndCqRR(7, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = firstCompiledMethodFormat();
	anInstruction9 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	/* begin Label */
	convertToIntAndReturn = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AndCqR, 3, formatReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(3, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpIsWords, gLogicalShiftLeftCqR((shiftForWord()) - 2, nSlotsOrElementsReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperand(AndCqR, 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrElementsReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, nSlotsOrElementsReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant3 = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(AddCqR, quickConstant3, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
	}
	/* begin MoveX32r:R:R: */
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)convertToIntAndReturn));
	jmpTarget(jumpFailAlloc, jmpTarget(jumpLongsOutOfBounds, jmpTarget(jumpPointersOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpIsMethod, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpIsContext, jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))));
	return 0;
}


/*	Generate the code for primitive 174, instVarAt:put:. Defer to
	StackInterpreterPrimitives>>primitiveSlotAtPut for Contexts.
 */
/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveSlotAtPut */
static sqInt
genPrimitiveSlotAtPut(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt formatReg;
    AbstractInstruction *jump64BitArgIsImmediate;
    AbstractInstruction *jump64BitsOutOfBounds;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpIsWords;
    AbstractInstruction *jumpNonSmallIntegerValue;
    AbstractInstruction *jumpNot64BitIndexable;
    AbstractInstruction *jumpNot8ByteInteger;
    AbstractInstruction *jumpNotIndexableBits;
    AbstractInstruction *jumpNotPointers;
    AbstractInstruction *jumpPointersOutOfBounds;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt literal6;
    sqInt nSlotsOrBytesReg;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    AbstractInstruction *rejoin;

	jumpImmutable = ((AbstractInstruction *) 0);
	nSlotsOrBytesReg = Extra0Reg;
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpImmediate = genJumpImmediate(ReceiverResultReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(SubCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	literal6 = immutableBitMask();
	anInstruction20 = genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(literal6, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY
	genGetNumSlotsOfinto(ReceiverResultReg, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = weakArrayFormat();
	anInstruction1 = genoperandoperand(CmpCqR, weakArrayFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpNotPointers = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genGetClassIndexOfNonImminto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsContext = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg1Reg, TempReg, 0);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpPointersOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(AddCqR, quickConstant, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, Arg1Reg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotPointers, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNonSmallIntegerValue = genJumpNotSmallInteger(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = firstByteFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstShortFormat();
	anInstruction5 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstLongFormat();
	anInstruction6 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsWords = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = sixtyFourBitIndexableFormat();
	anInstruction7 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNotIndexableBits = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	rejoin = genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jump64BitsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AddCqR, quickConstant1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, SendNumArgsReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNonSmallIntegerValue, checkQuickConstantforInstruction(sixtyFourBitIndexableFormat(), genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), formatReg)));
	/* begin JumpNonZero: */
	jumpNot64BitIndexable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));

	/* Now check if the header is that of an 8 byte LargePositiveInteger */
	jump64BitArgIsImmediate = genJumpImmediate(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperandoperand(MoveMwrR, 0, Arg1Reg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant2 = headerForSlotsformatclassIndex(numSlotsMask(), formatMask(), classIndexMask());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AndCqR, quickConstant2, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
	}
	/* begin CmpCq:R: */
	quickConstant3 = headerForSlotsformatclassIndex(1, firstByteFormat(), ClassLargePositiveIntegerCompactIndex);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(CmpCqR, quickConstant3, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNot8ByteInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, Arg1Reg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)rejoin));
	jmpTarget(jumpIsWords, checkQuickConstantforInstruction((((usqInt)0xFFFFFFFFU << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFFFFFFFFU << 3) | 1), Arg1Reg)));
	/* begin JumpAbove: */
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddCq:R: */
	quickConstant4 = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(AddCqR, quickConstant4, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(quickConstant4, BytesPerOop));
	}
	/* begin MoveR:X32r:R: */
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, checkQuickConstantforInstruction((((usqInt)0xFFFF << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFFFF << 3) | 1), Arg1Reg)));
	/* begin JumpAbove: */
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, nSlotsOrBytesReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction15 = genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction16 = genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, checkQuickConstantforInstruction((((usqInt)0xFF << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)0xFF << 3) | 1), Arg1Reg)));
	/* begin JumpAbove: */
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), nSlotsOrBytesReg);
	gAndCqRR(BytesPerWord - 1, formatReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, nSlotsOrBytesReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, nSlotsOrBytesReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = firstCompiledMethodFormat();
	anInstruction17 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction18 = genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:Xbr:R: */
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNot8ByteInteger, jmpTarget(jump64BitArgIsImmediate, jmpTarget(jumpNot64BitIndexable, jmpTarget(jumpIsContext, jmpTarget(jumpNotIndexableBits, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpPointersOutOfBounds, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jump64BitsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))))))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpIsContext->operands))[0])));
#  endif
	/* begin checkQuickConstant:forInstruction: */
	anInstruction19 = genoperandoperand(AddCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadIndex, jmpTarget(jumpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveStringAt */
static sqInt
genPrimitiveStringAt(void)
{
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *done;
    sqInt formatReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpWordsDone;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordTooBig;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt quickConstant;

	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, Arg1Reg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genConvertSmallIntegerToIntegerInReg(Arg1Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(SubCqR, 1, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstByteFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = firstShortFormat();
	anInstruction3 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstLongFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpNotIndexable = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddCq:R: */
	quickConstant = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(AddCqR, quickConstant, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin MoveX32r:R:R: */
	genoperandoperandoperand(MoveX32rRR, Arg1Reg, ReceiverResultReg, TempReg);
	jumpWordTooBig = jumpNotCharacterUnsignedValueInRegister(TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, ReceiverResultReg);
	/* begin Jump: */
	jumpWordsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(AndCqR, BytesPerWord - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AddCqR, BaseHeaderSize, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, Arg1Reg, ReceiverResultReg, ReceiverResultReg);
	jmpTarget(jumpWordsDone, (done = genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genConvertIntegerToCharacterInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg1Reg == SPReg)));
	genoperandoperand(CmpRR, Arg1Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperandoperand(MoveM16rR, BaseHeaderSize, ReceiverResultReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)done));
	jmpTarget(jumpWordTooBig, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, jmpTarget(jumpNotIndexable, jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))));
	return CompletePrimitive;
}


/*	c.f. StackInterpreter>>stSizeOf: SpurMemoryManager>>lengthOf:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveStringAtPut */
static sqInt
genPrimitiveStringAtPut(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt formatReg;
    AbstractInstruction *jumpBadArg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBytesOutOfBounds;
    AbstractInstruction *jumpBytesOutOfRange;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsCompiledMethod;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotString;
    AbstractInstruction *jumpShortsOutOfBounds;
    AbstractInstruction *jumpShortsOutOfRange;
    AbstractInstruction *jumpWordsOutOfBounds;
    AbstractInstruction *jumpWordsOutOfRange;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt quickConstant;
    sqInt quickConstant1;

	jumpImmutable = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(1 < (numRegArgs()));
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	jumpBadArg = genJumpNotCharacter(Arg1Reg);
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(SubCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
#  if IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), TempReg);
	/* begin genJumpBaseHeaderImmutable: */
	literal4 = immutableBitMask();
	anInstruction13 = genoperandoperand(TstCqR, immutableBitMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpImmutable = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
#  else // IMMUTABILITY
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, (formatReg = SendNumArgsReg), NoReg);
#  endif // IMMUTABILITY
	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstLongFormat();
	anInstruction1 = genoperandoperand(CmpCqR, firstLongFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpBelow: */
	jumpNotString = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = firstCompiledMethodFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsCompiledMethod = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstByteFormat();
	anInstruction3 = genoperandoperand(CmpCqR, firstByteFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstShortFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstShortFormat(), formatReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin CmpCq:R: */
	quickConstant = characterObjectOf((1U << (numCharacterBits())) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(CmpCqR, quickConstant, Arg1Reg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpWordsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 2, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpWordsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> ((shiftForWord()) - 1);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(AddCqR, quickConstant1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin MoveR:X32r:R: */
	genoperandoperandoperand(MoveRX32rR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsShorts, gCmpCqR(characterObjectOf(0xFFFF), Arg1Reg));
	/* begin JumpAbove: */
	jumpShortsOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - 1, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpShortsOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperandoperand(MoveRM16r, TempReg, BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIsBytes, gCmpCqR(characterObjectOf(0xFF), Arg1Reg));
	/* begin JumpAbove: */
	jumpBytesOutOfRange = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AndCqR, BytesPerWord - 1, formatReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, formatReg, ClassReg);
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ClassReg);
	/* begin JumpBelowOrEqual: */
	jumpBytesOutOfBounds = genConditionalBranchoperand(JumpBelowOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, TempReg);
	genConvertCharacterToCodeInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AddCqR, BaseHeaderSize, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveR:Xbr:R: */
	genoperandoperandoperand(MoveRXbrR, TempReg, Arg0Reg, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotString, jmpTarget(jumpBytesOutOfRange, jmpTarget(jumpShortsOutOfRange, jmpTarget(jumpWordsOutOfRange, jmpTarget(jumpIsCompiledMethod, jmpTarget(jumpBytesOutOfBounds, jmpTarget(jumpShortsOutOfBounds, jmpTarget(jumpWordsOutOfBounds, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpNotString->operands))[0])));
#  endif
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(AddCqR, 1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(1, BytesPerOop));
	}
	genConvertIntegerToSmallIntegerInReg(Arg0Reg);
	jmpTarget(jumpBadArg, jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Implement primitiveUninitializedNewWithArg for convenient cases:
	- the receiver has a hash
	- the receiver is variable and not compiled method
	- the result fits in eden
	See superclass method for dynamic frequencies of formats.
	For the moment we implement only arrayFormat, firstByteFormat &
	firstLongFormat 
 */

	/* CogObjectRepresentationFor64BitSpur>>#genPrimitiveUninitializedNewWithArg */
static sqInt
genPrimitiveUninitializedNewWithArg(void)
{
    sqInt address;
    sqInt address1;
    sqInt address2;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction22;
    AbstractInstruction *anInstruction23;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt byteSizeReg;
    sqInt fillReg;
    sqInt headerReg;
    sqInt instSpecReg;
    AbstractInstruction *jumpByteFormat;
    AbstractInstruction *jumpByteTooBig;
    AbstractInstruction *jumpDoubleByteFormat;
    AbstractInstruction *jumpDoubleBytePrepDone;
    AbstractInstruction *jumpDoubleWordFormat;
    AbstractInstruction *jumpDoubleWordPrepDone;
    AbstractInstruction *jumpDoubleWordTooBig;
    AbstractInstruction *jumpFailCuzFixed;
    AbstractInstruction *jumpHasSlots;
    AbstractInstruction *jumpLongPrepDone;
    AbstractInstruction *jumpLongTooBig;
    AbstractInstruction *jumpNElementsNonInt;
    AbstractInstruction *jumpNoSpace;
    AbstractInstruction *jumpNoSpaceBigObjects;
    AbstractInstruction *jumpOverflowHeader;
    AbstractInstruction *jumpShortTooBig;
    AbstractInstruction *jumpUnhashed;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt literal6;
    sqInt literal7;
    sqInt maxSlots;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    sqInt quickConstant5;
    sqInt quickConstant6;
    AbstractInstruction *skip;

	if (methodOrBlockNumArgs != 1) {
		return UnimplementedPrimitive;
	}
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));

	/* Assume there's an available scratch register on 64-bit machines.  This holds the saved numFixedFields and then the value to fill with */
	headerReg = SendNumArgsReg;
	fillReg = Extra0Reg;
	assert(fillReg > 0);

	/* Allow a max of 1 MB */
	instSpecReg = (byteSizeReg = ClassReg);

	/* get freeStart as early as possible so as not to wait later... */
	maxSlots = maxSlotsForNewSpaceAlloc();
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, Arg1Reg));
	genGetHashFieldNonImmOfinto(ReceiverResultReg, headerReg);
	/* begin JumpZero: */
	jumpUnhashed = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNElementsNonInt = genJumpNotSmallInteger(Arg0Reg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, ReceiverResultReg, instSpecReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = (fixedFieldsFieldWidth()) + (numSmallIntegerTagBits());
	genoperandoperand(LogicalShiftRightCqR, quickConstant, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = formatMask();
	anInstruction = genoperandoperand(AndCqR, formatMask(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	/* begin LogicalShiftLeftCq:R: */
	quickConstant1 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant1, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, fillReg);
	genConvertSmallIntegerToIntegerInReg(fillReg);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = firstByteFormat();
	anInstruction1 = genoperandoperand(CmpCqR, firstByteFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstShortFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstShortFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpDoubleByteFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = sixtyFourBitIndexableFormat();
	anInstruction3 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpDoubleWordFormat = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = firstLongFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstLongFormat(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpFailCuzFixed = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 2) << 3) | 1), Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize((((usqInt)(maxSlots * 2) << 3) | 1), BytesPerOop));
	}
	/* begin JumpAbove: */
	jumpLongTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(MoveCqR, BytesPerWord / 4, TempReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(BytesPerWord / 4, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(AndCqR, (BytesPerWord / 4) - 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant2 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant2, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AddCqR, (BytesPerWord / 4) - 1, instSpecReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize((BytesPerWord / 4) - 1, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 2, instSpecReg);
	/* begin Jump: */
	jumpLongPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpDoubleByteFormat, checkQuickConstantforInstruction((((usqInt)(maxSlots * 4) << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)(maxSlots * 4) << 3) | 1), Arg0Reg)));
	/* begin JumpAbove: */
	jumpShortTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(MoveCqR, BytesPerWord / 2, TempReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(BytesPerWord / 2, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(AndCqR, (BytesPerWord / 2) - 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant3 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant3, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(AddCqR, (BytesPerWord / 2) - 1, instSpecReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize((BytesPerWord / 2) - 1, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, (shiftForWord()) - 1, instSpecReg);
	/* begin Jump: */
	jumpDoubleBytePrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpDoubleWordFormat, checkQuickConstantforInstruction((((usqInt)maxSlots << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)maxSlots << 3) | 1), Arg0Reg)));
	/* begin JumpAbove: */
	jumpDoubleWordTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin Jump: */
	jumpDoubleWordPrepDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpByteFormat, checkQuickConstantforInstruction((((usqInt)(maxSlots * BytesPerWord) << 3) | 1), genoperandoperand(CmpCqR, (((usqInt)(maxSlots * BytesPerWord) << 3) | 1), Arg0Reg)));
	/* begin JumpAbove: */
	jumpByteTooBig = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, fillReg, instSpecReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(MoveCqR, BytesPerWord, TempReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperand(AndCqR, BytesPerWord - 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	quickConstant4 = formatShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant4, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperand(AddCqR, BytesPerWord - 1, instSpecReg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin LogicalShiftRightCq:R: */
	genoperandoperand(LogicalShiftRightCqR, shiftForWord(), instSpecReg);
	jmpTarget(jumpDoubleWordPrepDone, jmpTarget(jumpDoubleBytePrepDone, jmpTarget(jumpLongPrepDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = numSlotsMask();
	anInstruction15 = genoperandoperand(CmpCqR, numSlotsMask(), instSpecReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpOverflowHeader = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instSpecReg, TempReg);
	/* begin LogicalShiftLeftCq:R: */
	quickConstant5 = numSlotsFullShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant5, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction16 = genoperandoperand(CmpCqR, 0, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpHasSlots = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction17 = genoperandoperand(MoveCqR, BaseHeaderSize * 2, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(BaseHeaderSize * 2, BytesPerOop));
	}
	/* begin Jump: */
	skip = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasSlots, checkQuickConstantforInstruction(BaseHeaderSize / BytesPerWord, genoperandoperand(AddCqR, BaseHeaderSize / BytesPerWord, byteSizeReg)));
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	jmpTarget(skip, genoperandoperand(AddRR, Arg1Reg, byteSizeReg));
	/* begin checkQuickConstant:forInstruction: */
	literal6 = getScavengeThreshold();
	anInstruction18 = genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(literal6, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNoSpace = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, byteSizeReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction19 = genoperandoperandoperand(MoveRMwr, headerReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(0, BytesPerOop));
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOverflowHeader, checkQuickConstantforInstruction(0xFF, genoperandoperand(MoveCqR, 0xFF, TempReg)));
	/* begin LogicalShiftLeftCq:R: */
	quickConstant6 = numSlotsFullShift();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant6, TempReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, TempReg, headerReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, instSpecReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction20 = genoperandoperand(AddCqR, (BaseHeaderSize * 2) / BytesPerWord, byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize((BaseHeaderSize * 2) / BytesPerWord, BytesPerOop));
	}
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), byteSizeReg);
	/* begin AddR:R: */
	genoperandoperand(AddRR, Arg1Reg, byteSizeReg);
	/* begin checkQuickConstant:forInstruction: */
	literal7 = getScavengeThreshold();
	anInstruction21 = genoperandoperand(CmpCqR, getScavengeThreshold(), byteSizeReg);
	if (usesOutOfLineLiteral(anInstruction21)) {
		(anInstruction21->dependent = locateLiteralsize(literal7, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNoSpaceBigObjects = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg1Reg, ReceiverResultReg);
	/* begin MoveR:Aw: */
	address2 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address2, genoperandoperand(MoveRAw, byteSizeReg, address2));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction22 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction22)) {
		(anInstruction22->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction23 = genoperandoperandoperand(MoveRMwr, headerReg, BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction23)) {
		(anInstruction23->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpShortTooBig, jmpTarget(jumpDoubleWordTooBig, jmpTarget(jumpNoSpaceBigObjects, jmpTarget(jumpNoSpace, jmpTarget(jumpUnhashed, jmpTarget(jumpFailCuzFixed, jmpTarget(jumpByteTooBig, jmpTarget(jumpLongTooBig, jmpTarget(jumpNElementsNonInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))))))));
	return 0;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentationFor64BitSpur>>#genPureFloatArithmetic:preOpCheck:boxed: */
static sqInt NoDbgRegParms
genPureFloatArithmeticpreOpCheckboxed(sqInt arithmeticOperator, AbstractInstruction *(*preOpCheckOrNil)(int rcvrReg, int argReg), sqInt rcvrBoxed)
{
    AbstractInstruction *doOp;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpFailCheck;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;

	jumpFailCheck = ((AbstractInstruction *) 0);
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	/* begin Label */
	doOp = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (!(preOpCheckOrNil == null)) {
		jumpFailCheck = preOpCheckOrNil(DPFPReg0, DPFPReg1);
	}
	genoperandoperand(arithmeticOperator, DPFPReg1, DPFPReg0);
	jumpFailAlloc = genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, SendNumArgsReg, ClassReg, TempReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)doOp));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!(preOpCheckOrNil == null)) {
		jmpTarget(jumpFailCheck, ((AbstractInstruction *) (((jumpFailAlloc->operands))[0])));
	}
	return 0;
}


/*	In the Pure version, mixed arithmetic with SmallInteger is forbidden */

	/* CogObjectRepresentationFor64BitSpur>>#genPureFloatComparison:invert:boxed: */
static sqInt NoDbgRegParms
genPureFloatComparisoninvertboxed(AbstractInstruction *(*jumpFPOpcodeGenerator)(void *), sqInt invertComparison, sqInt rcvrBoxed)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *compareFloat;
    sqInt constant;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpImmediate;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotSmallFloat;
    sqInt literal;

	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	if (rcvrBoxed) {
		genGetDoubleValueOfinto(ReceiverResultReg, DPFPReg0);
	}
	else {
		genGetSmallFloatValueOfscratchinto(ReceiverResultReg, TempReg, DPFPReg0);
	}
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	if (invertComparison) {

		/* May need to invert for NaNs */
		/* begin CmpRd:Rd: */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		/* begin CmpRd:Rd: */
		compareFloat = genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	/* begin genMoveConstant:R: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, (shouldAnnotateObjectReference(trueObject())
		? annotateobjRef(gMoveCwR(trueObject(), ReceiverResultReg), trueObject())
		: (/* begin checkQuickConstant:forInstruction: */
			(literal = trueObject()),
			(anInstruction1 = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)),
			(usesOutOfLineLiteral(anInstruction1)
					? (anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop))
					: 0),
			anInstruction1)));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpImmediate = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareFloat));
	jmpTarget(jumpImmediate, jmpTarget(jumpNotBoxedFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}

	/* CogObjectRepresentationFor64BitSpur>>#genRemoveSmallIntegerTagsInScratchReg: */
static sqInt NoDbgRegParms
genRemoveSmallIntegerTagsInScratchReg(sqInt scratchReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(SubCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}

	/* CogObjectRepresentationFor64BitSpur>>#genShiftAwaySmallIntegerTagsInScratchReg: */
static sqInt NoDbgRegParms
genShiftAwaySmallIntegerTagsInScratchReg(sqInt scratchReg)
{
    sqInt quickConstant;

	/* begin ArithmeticShiftRightCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(ArithmeticShiftRightCqR, quickConstant, scratchReg);
	return 0;
}


/*	Stack looks like
	return address */

	/* CogObjectRepresentationFor64BitSpur>>#genSmallIntegerComparison:orDoubleComparison:invert: */
static sqInt NoDbgRegParms
genSmallIntegerComparisonorDoubleComparisoninvert(sqInt jumpOpcode, AbstractInstruction * NoDbgRegParms (*jumpFPOpcodeGenerator)(void *), sqInt invertComparison)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *compareIntFloat;
    sqInt constant;
    sqInt constant1;
    AbstractInstruction *jumpAmbiguous;
    AbstractInstruction *jumpCond;
    AbstractInstruction *jumpNotBoxedFloat;
    AbstractInstruction *jumpNotFloatAtAll;
    AbstractInstruction *jumpNotSmallFloat;
    AbstractInstruction *jumpTrue;
    sqInt literal;
    sqInt r;
    AbstractInstruction *returnTrue;

	r = genSmallIntegerComparison(jumpOpcode);
	if (r < 0) {
		return r;
	}
#  if defined(DPFPReg0)

	/* Fall through on non-SmallInteger argument.  Argument may be a Float : let us check or fail */
	/* check for Small Float argument */
	jumpNotSmallFloat = genJumpNotSmallFloat(Arg0Reg);
	genGetSmallFloatValueOfscratchinto(Arg0Reg, TempReg, DPFPReg1);
	/* begin Label */
	compareIntFloat = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, ReceiverResultReg, DPFPReg0);
	/* begin CmpRd:Rd: */
	genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);

	/* Case of non ambiguity, use compareFloat((double) intRcvr,floatArg) */
	jumpAmbiguous = gJumpFPEqual(0);
	if (invertComparison) {

		/* May need to invert for NaNs */
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg0, DPFPReg1);
	}
	else {
		/* begin CmpRd:Rd: */
		genoperandoperand(CmpRdRd, DPFPReg1, DPFPReg0);
	}

	/* FP jumps are a little weird */
	jumpCond = jumpFPOpcodeGenerator(0);
	/* begin genMoveConstant:R: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCond, (shouldAnnotateObjectReference(trueObject())
		? (returnTrue = annotateobjRef(gMoveCwR(trueObject(), ReceiverResultReg), trueObject()))
		: (/* begin checkQuickConstant:forInstruction: */
			(literal = trueObject()),
			(anInstruction1 = genoperandoperand(MoveCqR, trueObject(), ReceiverResultReg)),
			(usesOutOfLineLiteral(anInstruction1)
					? (anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop))
					: 0),
			(returnTrue = anInstruction1))));
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpAmbiguous, genoperandoperand(ConvertRdR, DPFPReg1, Arg0Reg));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	jumpTrue = genConditionalBranchoperand(jumpOpcode, 0);
	/* begin genMoveConstant:R: */
	constant1 = falseObject();
	if (shouldAnnotateObjectReference(constant1)) {
		annotateobjRef(gMoveCwR(constant1, ReceiverResultReg), constant1);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(MoveCqR, constant1, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(constant1, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpTrue, returnTrue);
	jmpTarget(jumpNotSmallFloat, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpNotFloatAtAll = genJumpImmediate(Arg0Reg);
	genGetCompactClassIndexNonImmOfinto(Arg0Reg, SendNumArgsReg);
	genCmpClassFloatCompactIndexR(SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpNotBoxedFloat = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genGetDoubleValueOfinto(Arg0Reg, DPFPReg1);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)compareIntFloat));
	jmpTarget(jumpNotBoxedFloat, jmpTarget(jumpNotFloatAtAll, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
#  endif // defined(DPFPReg0)
	return CompletePrimitive;
}


/*	Get the literal count of a CompiledMethod into headerReg, plus one if
	requested. If inBytes is true, scale the count by the word size. Deal with
	the possibility of
	the method being cogged. */

	/* CogObjectRepresentationFor64BitSpur>>#getLiteralCountOf:plusOne:inBytes:into:scratch: */
static sqInt NoDbgRegParms
getLiteralCountOfplusOneinBytesintoscratch(sqInt methodReg, sqInt plusOne, sqInt inBytes, sqInt litCountReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt literal;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;

	genGetMethodHeaderOfintoscratch(methodReg, litCountReg, scratchReg);
	assert((1U << (numTagBits())) == BytesPerWord);
	if (inBytes) {
		/* begin AndCq:R: */
		quickConstant1 = ((sqInt)((usqInt)((alternateHeaderNumLiteralsMask())) << (numTagBits())));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AndCqR, quickConstant1, litCountReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
		}
	}
	else {
		/* begin LogicalShiftRightCq:R: */
		quickConstant = numTagBits();
		genoperandoperand(LogicalShiftRightCqR, quickConstant, litCountReg);
		/* begin checkQuickConstant:forInstruction: */
		literal = alternateHeaderNumLiteralsMask();
		anInstruction1 = genoperandoperand(AndCqR, alternateHeaderNumLiteralsMask(), litCountReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
		}
	}
	if (plusOne) {
		/* begin AddCq:R: */
		quickConstant2 = (inBytes
			? LiteralStart * BytesPerWord
			: LiteralStart);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(AddCqR, quickConstant2, litCountReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
		}
	}
	return 0;
}


/*	Answer the relevant inline cache tag for an instance.
	c.f. getInlineCacheClassTagFrom:into: & inlineCacheTagForClass: */

	/* CogObjectRepresentationFor64BitSpur>>#inlineCacheTagForInstance: */
static sqInt NoDbgRegParms
inlineCacheTagForInstance(sqInt oop)
{
	return (isImmediate(oop)
		? oop & (tagMask())
		: classIndexOf(oop));
}

	/* CogObjectRepresentationFor64BitSpur>>#log2BytesPerWord */
static sqInt
log2BytesPerWord(void)
{
	return 3;
}


/*	Generate the routine that converts selector indices into selector objects.
	It is called from the send trampolines.
	If the selector index is negative, convert it into a positive index into
	the special selectors array and index that. Otherwise, index the current
	method. The routine uses Extra0Reg & Extra1Reg, which are available, since
	they are not live at point of send. */

	/* CogObjectRepresentationFor64BitSpur>>#maybeGenerateSelectorIndexDereferenceRoutine */
static void
maybeGenerateSelectorIndexDereferenceRoutine(void)
{
    sqInt address;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt byteOffset;
    AbstractInstruction *jumpFullBlock;
    AbstractInstruction *jumpNegative;
    AbstractInstruction *jumpNotBlock;
    sqInt literal;
    sqInt literal1;
    sqInt offset;

	jumpFullBlock = ((AbstractInstruction *) 0);
	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, 0, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpNegative = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(MoveMwrR, FoxMethod, FPReg, Extra0Reg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(FoxMethod, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, 2, ClassReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(2, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(TstCqR, MFMethodFlagIsBlockFlag, Extra0Reg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(MFMethodFlagIsBlockFlag, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpNotBlock = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal = -(zoneAlignment());
	anInstruction6 = genoperandoperand(AndCqR, -(zoneAlignment()), Extra0Reg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin bitAndByteOffsetOfIsFullBlockBitInto: */
	byteOffset = BaseHeaderSize + 1;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMbrR, byteOffset, Extra0Reg, Extra1Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(byteOffset, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(TstCqR, 16, Extra1Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(16, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpFullBlock = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperandoperand(MoveM16rR, 0, Extra0Reg, Extra1Reg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, Extra1Reg, Extra0Reg);
	jmpTarget(jumpNotBlock, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jmpTarget(jumpFullBlock, ((AbstractInstruction *) (((jumpNotBlock->operands))[0])));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = -(zoneAlignment());
	anInstruction8 = genoperandoperand(AndCqR, -(zoneAlignment()), Extra0Reg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperandoperand(MoveMwrR, offset, Extra0Reg, Extra1Reg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, ClassReg, Extra1Reg, ClassReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jumpNegative, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin NegateR: */
	genoperand(NegateR, ClassReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, 1, ClassReg);
	/* begin MoveAw:R: */
	address = specialObjectsOopAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, Extra0Reg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(SubCqR, 1, ClassReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveMwrR, (SpecialSelectors + 1) * BytesPerWord, Extra0Reg, Extra1Reg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize((SpecialSelectors + 1) * BytesPerWord, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, ClassReg, Extra1Reg, ClassReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	ceDereferenceSelectorIndex = methodZoneBase();
	outputInstructionsForGeneratedRuntimeAt(ceDereferenceSelectorIndex);
	recordGeneratedRunTimeaddress("ceDereferenceSelectorIndex", ceDereferenceSelectorIndex);
	recordRunTimeObjectReferences();
}

	/* CogObjectRepresentationFor64BitSpur>>#numCountersFor: */
static usqInt NoDbgRegParms
numCountersFor(usqInt theCounters)
{
    sqInt objOop;

	if (theCounters == 0) {
		return 0;
	}
	objOop = theCounters - BaseHeaderSize;
	return 2 * (numSlotsOf(objOop));
}

	/* CogObjectRepresentationFor64BitSpur>>#numSmallIntegerBits */
static sqInt
numSmallIntegerBits(void)
{
	return 61;
}

	/* CogObjectRepresentationFor64BitSpur>>#numSmallIntegerTagBits */
static sqInt
numSmallIntegerTagBits(void)
{
	return 3;
}


/*	The three valid tag patterns are 1 (SmallInteger), 2 (Character) and 3
	(SmallFloat64). 
 */

	/* CogObjectRepresentationFor64BitSpur>>#validInlineCacheTag: */
static sqInt NoDbgRegParms
validInlineCacheTag(sqInt classIndexOrTagPattern)
{
	return ((classIndexOrTagPattern >= 1)
	 && (classIndexOrTagPattern <= 3))
	 || ((classAtIndex(classIndexOrTagPattern)) != null);
}


/*	Generate a branch if reg is an instance of any of the classes in arrayObj,
	otherwise fall-through. reg should not be edited. */

	/* CogObjectRepresentationForSpur>>#branchIf:instanceOfBehaviors:target: */
static sqInt NoDbgRegParms
branchIfinstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt allImmediate;
    sqInt classObj;
    sqInt i;
    sqInt iLimiT;
    sqInt immediateMask;
    sqInt noneImmediate;
    sqInt numNonImmediates;


	/* let me tell you all about it, let me falsify */
	allImmediate = 1;
	noneImmediate = 1;
	immediateMask = 0;
	numNonImmediates = 0;
	for (i = 0, iLimiT = ((numSlotsOf(arrayObj)) - 1); i <= iLimiT; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		if (isImmediateClass(classObj)) {
			noneImmediate = 0;
			immediateMask += classTagForClass(classObj);
		}
		else {
			allImmediate = 0;
			numNonImmediates += 1;
		}
	}
	if (noneImmediate) {
		return noneImmediateBranchIfinstanceOfBehaviorstarget(reg, arrayObj, targetFixUp);
	}
	if (allImmediate) {
		return allImmediatebranchIfinstanceOfBehaviorstarget(immediateMask, reg, arrayObj, targetFixUp);
	}
	return mixedbranchIfinstanceOfBehaviorstarget(numNonImmediates, reg, arrayObj, targetFixUp);
}


/*	Generate a branch if reg is an instance of any of the classes in arrayObj,
	otherwise fall-through. reg should not be edited. */

	/* CogObjectRepresentationForSpur>>#branchIf:notInstanceOfBehaviors:target: */
static sqInt NoDbgRegParms
branchIfnotInstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt allImmediate;
    sqInt classObj;
    sqInt i;
    sqInt iLimiT;
    sqInt immediateMask;
    sqInt noneImmediate;
    sqInt numNonImmediates;


	/* let me tell you all about it, let me falsify */
	allImmediate = 1;
	noneImmediate = 1;
	immediateMask = 0;
	numNonImmediates = 0;
	for (i = 0, iLimiT = ((numSlotsOf(arrayObj)) - 1); i <= iLimiT; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		if (isImmediateClass(classObj)) {
			noneImmediate = 0;
			immediateMask += classTagForClass(classObj);
		}
		else {
			allImmediate = 0;
			numNonImmediates += 1;
		}
	}
	if (noneImmediate) {
		return noneImmediateBranchIfnotInstanceOfBehaviorstarget(reg, arrayObj, targetFixUp);
	}
	if (allImmediate) {
		return allImmediatebranchIfnotInstanceOfBehaviorstarget(immediateMask, reg, arrayObj, targetFixUp);
	}
	return mixedbranchIfnotInstanceOfBehaviorstarget(numNonImmediates, reg, arrayObj, targetFixUp);
}

	/* CogObjectRepresentationForSpur>>#callStoreCheckTrampoline */
static void
callStoreCheckTrampoline(void)
{
    AbstractInstruction *abstractInstruction;

	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
}

	/* CogObjectRepresentationForSpur>>#checkValidDerivedObjectReference: */
static sqInt NoDbgRegParms
checkValidDerivedObjectReference(sqInt bodyAddress)
{
	return (heapMapAtWord(pointerForOop(bodyAddress - BaseHeaderSize))) != 0;
}

	/* CogObjectRepresentationForSpur>>#checkValidOopReference: */
static sqInt NoDbgRegParms
checkValidOopReference(sqInt anOop)
{
	return (isImmediate(anOop))
	 || ((heapMapAtWord(pointerForOop(anOop))) != 0);
}

	/* CogObjectRepresentationForSpur>>#couldBeDerivedObject: */
static sqInt NoDbgRegParms
couldBeDerivedObject(sqInt bodyAddress)
{
	return oopisGreaterThanOrEqualTo(bodyAddress - BaseHeaderSize, startOfMemory());
}

	/* CogObjectRepresentationForSpur>>#couldBeObject: */
static sqInt NoDbgRegParms
couldBeObject(sqInt literal)
{
	return (isNonImmediate(literal))
	 && (oopisGreaterThanOrEqualTo(literal, startOfMemory()));
}


/*	Create a trampoline to answer the active context that will
	answer it if a frame is already married, and create it otherwise.
	Assume numArgs is in SendNumArgsReg and ClassReg is free. */

	/* CogObjectRepresentationForSpur>>#genActiveContextTrampolineLarge:inBlock:called: */
static usqInt NoDbgRegParms
genActiveContextTrampolineLargeinBlockcalled(sqInt isLarge, sqInt isInBlock, char *aString)
{
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	genGetActiveContextLargeinBlock(isLarge, isInBlock);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(aString, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Check the remembered bit of the object in objReg; answer the jump taken if
	the bit is already set.
	Only need to fetch the byte containing it, which reduces the size of the
	mask constant.
 */

	/* CogObjectRepresentationForSpur>>#genCheckRememberedBitOf:scratch: */
static AbstractInstruction * NoDbgRegParms
genCheckRememberedBitOfscratch(sqInt objReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt mask;
    sqInt rememberedBitByteOffset;

	rememberedBitByteOffset = (rememberedBitShift()) / 8;
	mask = 1ULL << ((rememberedBitShift()) % 8);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMbrR, rememberedBitByteOffset, objReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(rememberedBitByteOffset, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(TstCqR, mask, scratchReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(mask, BytesPerOop));
	}
	/* begin JumpNonZero: */
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}


/*	It is safe to use a short comparison for the known classes; these will not
	change with become, etc... But it's probably not safe to assume the hash
	of some other class won't change over time, so to be sure of generating
	the same size code over time, use a long comparison for unknown classes. */

	/* CogObjectRepresentationForSpur>>#genCmpClassIndex:R: */
static void NoDbgRegParms
genCmpClassIndexR(sqInt classIndex, sqInt reg)
{
    AbstractInstruction *anInstruction;

	if (classIndex < (classTablePageSize())) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, classIndex, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(classIndex, BytesPerOop));
		}
	}
	else {
		/* begin CmpCw:R: */
		checkLiteralforInstruction(classIndex, genoperandoperand(CmpCwR, classIndex, TempReg));
	}
}

	/* CogObjectRepresentationForSpur>>#genConvertCharacterToCodeInReg: */
static sqInt NoDbgRegParms
genConvertCharacterToCodeInReg(sqInt reg)
{
    sqInt quickConstant;

	/* begin LogicalShiftRightCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(LogicalShiftRightCqR, quickConstant, reg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genConvertIntegerToCharacterInReg: */
static sqInt NoDbgRegParms
genConvertIntegerToCharacterInReg(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt literal;
    sqInt quickConstant;

	/* begin LogicalShiftLeftCq:R: */
	quickConstant = numTagBits();
	genoperandoperand(LogicalShiftLeftCqR, quickConstant, reg);
	/* begin checkQuickConstant:forInstruction: */
	literal = characterTag();
	anInstruction = genoperandoperand(AddCqR, characterTag(), reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	Create a closure with the given startpc, numArgs and numCopied
	within a context with ctxtNumArgs, large if isLargeCtxt that is in a
	block if isInBlock. If numCopied > 0 pop those values off the stack. */

	/* CogObjectRepresentationForSpur>>#genCreateClosureAt:numArgs:numCopied:contextNumArgs:large:inBlock: */
static sqInt NoDbgRegParms
genCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock)
{
    AbstractInstruction *anInstruction;
    sqInt i;

	genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(bcpc, numArgs, numCopied, ctxtNumArgs, isLargeCtxt, isInBlock);
	for (i = 1; i <= numCopied; i += 1) {
		/* begin PopR: */
		genoperand(PopR, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveRMwr, TempReg, (((numCopied - i) + ClosureFirstCopiedValueIndex) * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize((((numCopied - i) + ClosureFirstCopiedValueIndex) * BytesPerOop) + BaseHeaderSize, BytesPerOop));
		}
	}
	return 0;
}


/*	Create a full closure with the given values. */

	/* CogObjectRepresentationForSpur>>#genCreateFullClosure:numArgs:numCopied:ignoreContext:contextNumArgs:large:inBlock: */
static sqInt NoDbgRegParms
genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(sqInt compiledBlock, sqInt numArgs, sqInt numCopied, sqInt ignoreContext, sqInt contextNumArgs, sqInt contextIsLarge, sqInt contextIsBlock)
{
    AbstractInstruction *abstractInstruction;
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    usqInt byteSize;
    sqInt constant;
    usqLong header;
    sqInt literal;
    sqInt numSlots;
    AbstractInstruction *skip;


	/* First get thisContext into ReceiverResultReg and thence in ClassReg. */
	if (ignoreContext) {
		/* begin genMoveConstant:R: */
		constant = nilObject();
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(gMoveCwR(constant, ClassReg), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction9 = genoperandoperand(MoveCqR, constant, ClassReg);
			if (usesOutOfLineLiteral(anInstruction9)) {
				(anInstruction9->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
	}
	else {
		genGetActiveContextNumArgslargeinBlock(contextNumArgs, contextIsLarge, contextIsBlock);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	}
	numSlots = FullClosureFirstCopiedValueIndex + numCopied;
	byteSize = smallObjectBytesForSlots(numSlots);
	assert(ClassFullBlockClosureCompactIndex != 0);
	header = headerForSlotsformatclassIndex(numSlots, indexablePointersFormat(), ClassFullBlockClosureCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, ReceiverResultReg));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction2 = genoperandoperand(MoveCqR, header, TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(header, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(LoadEffectiveAddressMwrR, byteSize, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(byteSize, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction4 = genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpBelow: */
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperandoperand(MoveRMwr, ClassReg, (ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize((ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	if (shouldAnnotateObjectReference(compiledBlock)) {
		annotateobjRef(gMoveCwR(compiledBlock, TempReg), compiledBlock);
	}
	else {
		/* begin MoveCq:R: */
		anInstruction = genoperandoperand(MoveCqR, compiledBlock, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(compiledBlock, BytesPerOop));
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperandoperand(MoveRMwr, TempReg, (ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize((ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(MoveCqR, (((usqInt)numArgs << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize((((usqInt)numArgs << 3) | 1), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperandoperand(MoveRMwr, TempReg, (ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize((ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	return 0;
}


/*	Make sure that the object in reg is not forwarded. This routine assumes
	the object will
	never be forwarded to an immediate, as it is used to unforward literal
	variables (associations). 
	Use the fact that isForwardedObjectClassIndexPun is a power of two to save
	an instruction. */

	/* CogObjectRepresentationForSpur>>#genEnsureObjInRegNotForwarded:scratchReg: */
static sqInt NoDbgRegParms
genEnsureObjInRegNotForwardedscratchReg(sqInt reg, sqInt scratch)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *loop;
    AbstractInstruction *ok;
    sqInt quickConstant;

	assert(reg != scratch);
	/* begin Label */
	loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, reg, scratch);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant = (classIndexMask()) - (isForwardedObjectClassIndexPun());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, quickConstant, scratch);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpNonZero: */
	ok = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, reg, reg);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)loop));
	jmpTarget(ok, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Make sure that the oop in reg is not forwarded. 
	Use the fact that isForwardedObjectClassIndexPun is a power of two to save
	an instruction. */
/*	maybe a fixup or an instruction */
/*	maybe a fixup or an instruction */

	/* CogObjectRepresentationForSpur>>#genEnsureOopInRegNotForwarded:scratchReg:ifForwarder:ifNotForwarder: */
static sqInt NoDbgRegParms
genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(sqInt reg, sqInt scratch, void *fwdJumpTarget, void *nonFwdJumpTargetOrZero)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *finished;
    AbstractInstruction *imm;
    AbstractInstruction *ok;
    sqInt quickConstant;

	assert(reg != scratch);

	/* notionally
	   self genGetClassIndexOfNonImm: reg into: scratch.
	   cogit CmpCq: objectMemory isForwardedObjectClassIndexPun R: TempReg.
	   but the following is an instruction shorter: */
	imm = genJumpImmediate(reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, reg, scratch);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant = (classIndexMask()) - (isForwardedObjectClassIndexPun());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, quickConstant, scratch);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpNonZero: */
	ok = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, reg, reg);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)fwdJumpTarget));
	if ((((usqInt)nonFwdJumpTargetOrZero)) == 0) {
		/* begin Label */
		finished = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	else {
		finished = nonFwdJumpTargetOrZero;
	}
	jmpTarget(imm, jmpTarget(ok, finished));
	return 0;
}


/*	Make sure that the oop in reg is not forwarded, updating the slot in
	objReg with the value.
 */

	/* CogObjectRepresentationForSpur>>#genEnsureOopInRegNotForwarded:scratchReg:updatingSlot:in: */
static sqInt NoDbgRegParms
genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(sqInt reg, sqInt scratch, sqInt index, sqInt objReg)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *imm;
    AbstractInstruction *inst;
    AbstractInstruction *loop;
    AbstractInstruction *ok;
    sqInt quickConstant;


	/* Open-code
	   self genEnsureOopInRegNotForwarded: reg
	   scratchReg: scratch
	   updatingMw: index * objectMemory wordSize + objectMemory baseHeaderSize
	   r: objReg.
	   to avoid calling the store check unless the receiver is forwarded. */
	assert((reg != scratch)
	 && (objReg != scratch));
	/* begin Label */
	loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);

	/* notionally
	   self genGetClassIndexOfNonImm: reg into: scratch.
	   cogit CmpCq: objectMemory isForwardedObjectClassIndexPun R: TempReg.
	   but the following is an instruction shorter: */
	imm = genJumpImmediate(reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, reg, scratch);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin AndCq:R: */
	quickConstant = (classIndexMask()) - (isForwardedObjectClassIndexPun());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, quickConstant, scratch);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpNonZero: */
	ok = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, reg, reg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveRMwr, reg, (index * BytesPerWord) + BaseHeaderSize, objReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}
	assert((reg == Arg0Reg)
	 && ((scratch == TempReg)
	 && (objReg == ReceiverResultReg)));
	if (needsFrame()) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreCheckContextReceiverTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreCheckContextReceiverTrampoline);
		(abstractInstruction1->annotation = IsRelativeCall);
		genoperand(PopR, LinkReg);
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)loop));
	jmpTarget(ok, jmpTarget(imm, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return 0;
}


/*	Do the store check. Answer the argument for the benefit of the code
	generator; ReceiverResultReg may be caller-saved and hence smashed by this
	call. Answering
	it allows the code generator to reload ReceiverResultReg cheaply.
	In Spur the only thing we leave to the run-time is adding the receiver to
	the remembered set and setting its isRemembered bit. */

	/* CogObjectRepresentationForSpur>>#generateObjectRepresentationTrampolines */
static void
generateObjectRepresentationTrampolines(void)
{
    sqInt instVarIndex;
    AbstractInstruction *jumpSC;


#  if IMMUTABILITY
	for (instVarIndex = 0; instVarIndex < NumStoreTrampolines; instVarIndex += 1) {
		ceStoreTrampolines[instVarIndex] = (genStoreTrampolineCalledinstVarIndex(trampolineNamenumArgslimit("ceStoreTrampoline", instVarIndex, NumStoreTrampolines - 2), instVarIndex));
	}
#  endif // IMMUTABILITY
	ceNewHashTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNewHashOf, "ceNewHash", 1, ReceiverResultReg, null, null, null, /* begin emptyRegisterMask */ 0, 1, ReceiverResultReg, 0);
	ceInlineNewHashTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceNewHashOf, "ceInlineNewHash", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, ReceiverResultReg, 0);
	/* begin genStoreCheckTrampoline */
	if (CheckRememberedInTrampoline) {
		zeroOpcodeIndex();
		jumpSC = genCheckRememberedBitOfscratch(ReceiverResultReg, ABIResultReg);
		assert(((jumpSC->opcode)) == JumpNonZero);
		(jumpSC->opcode = JumpZero);
		/* begin RetN: */
		genoperand(RetN, 0);
		jmpTarget(jumpSC, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	ceStoreCheckTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(remember, "ceStoreCheckTrampoline", 1, ReceiverResultReg, null, null, null, ((CallerSavedRegisterMask | ((1U << ReceiverResultReg))) - ((1U << ReceiverResultReg))), 1, (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
		? ReceiverResultReg
		: ABIResultReg), CheckRememberedInTrampoline);
	ceStoreCheckContextReceiverTrampoline = genStoreCheckContextReceiverTrampoline();
	/* begin genTrampolineFor:called:regsToSave: */
	ceScheduleScavengeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceScheduleScavenge, "ceScheduleScavengeTrampoline", 0, null, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	ceSmallActiveContextInMethodTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, 0, "ceSmallMethodContext");
	ceSmallActiveContextInBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, InVanillaBlock, "ceSmallBlockContext");
	ceSmallActiveContextInFullBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(0, InFullBlock, "ceSmallFullBlockContext");
	ceLargeActiveContextInMethodTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, 0, "ceLargeMethodContext");
	ceLargeActiveContextInBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, InVanillaBlock, "ceLargeBlockContext");
	ceLargeActiveContextInFullBlockTrampoline = genActiveContextTrampolineLargeinBlockcalled(1, InFullBlock, "ceLargeFullBlockContext");
	}


/*	Create a trampoline to answer the active context that will
	answer it if a frame is already married, and create it otherwise.
	Assume numArgs is in SendNumArgsReg and ClassReg is free. */

	/* CogObjectRepresentationForSpur>>#genGetActiveContextLarge:inBlock: */
static sqInt NoDbgRegParms
genGetActiveContextLargeinBlock(sqInt isLarge, sqInt isInBlock)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction22;
    AbstractInstruction *anInstruction23;
    AbstractInstruction *anInstruction24;
    AbstractInstruction *anInstruction25;
    AbstractInstruction *anInstruction26;
    AbstractInstruction *anInstruction27;
    AbstractInstruction *anInstruction28;
    AbstractInstruction *anInstruction29;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction30;
    AbstractInstruction *anInstruction31;
    AbstractInstruction *anInstruction32;
    AbstractInstruction *anInstruction33;
    AbstractInstruction *anInstruction34;
    AbstractInstruction *anInstruction35;
    AbstractInstruction *anInstruction36;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt constant;
    AbstractInstruction *continuation;
    AbstractInstruction *exit;
    usqLong header;
    AbstractInstruction *inst;
    AbstractInstruction *jumpNeedScavenge;
    AbstractInstruction *jumpSingle;
    sqInt literal;
    sqInt literal1;
    AbstractInstruction *loopHead;
    sqInt offset;
    sqInt offset1;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt slotSize;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperandoperand(MoveMwrR, FoxMethod, FPReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(FoxMethod, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(TstCqR, MFMethodFlagHasContextFlag, ClassReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(MFMethodFlagHasContextFlag, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpSingle = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperandoperand(MoveMwrR, FoxThisContext, FPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(FoxThisContext, BytesPerOop));
	}
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jumpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(OrCqR, MFMethodFlagHasContextFlag, ClassReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(MFMethodFlagHasContextFlag, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, ClassReg, FoxMethod, FPReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(FoxMethod, BytesPerOop));
	}
	switch (isInBlock) {
	case InFullBlock:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(SubCqR, 3, ClassReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(3, BytesPerOop));
		}
		break;
	case InVanillaBlock:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(SubCqR, 3, ClassReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(3, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperandoperand(MoveM16rR, 0, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, ClassReg);
		break;
	case 0:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(SubCqR, 1, ClassReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(1, BytesPerOop));
		}
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	slotSize = (isLarge
		? LargeContextSlots
		: SmallContextSlots);
	header = headerForSlotsformatclassIndex(slotSize, indexablePointersFormat(), ClassMethodContextCompactIndex);
	flag("endianness");
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, ReceiverResultReg));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction12 = genoperandoperand(MoveCqR, header, TempReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(header, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin LoadEffectiveAddressMw:r:R: */
	offset = smallObjectBytesForSlots(slotSize);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction14 = genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction14)) {
		(anInstruction14->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = getScavengeThreshold();
	anInstruction15 = genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	if (usesOutOfLineLiteral(anInstruction15)) {
		(anInstruction15->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpNeedScavenge = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin LoadEffectiveAddressMw:r:R: */
	anInstruction16 = genoperandoperandoperand(LoadEffectiveAddressMwrR, 1, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction16)) {
		(anInstruction16->dependent = locateLiteralsize(1, BytesPerOop));
	}
	continuation = anInstruction16;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction17 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (SenderIndex * BytesPerOop), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction17)) {
		(anInstruction17->dependent = locateLiteralsize(BaseHeaderSize + (SenderIndex * BytesPerOop), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction18 = genoperandoperandoperand(MoveMwrR, FoxSavedFP, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction18)) {
		(anInstruction18->dependent = locateLiteralsize(FoxSavedFP, BytesPerOop));
	}
	genSetSmallIntegerTagsIn(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction19 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (InstructionPointerIndex * BytesPerOop), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction19)) {
		(anInstruction19->dependent = locateLiteralsize(BaseHeaderSize + (InstructionPointerIndex * BytesPerOop), BytesPerOop));
	}
	/* begin MoveMw:r:R: */
	offset1 = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction20 = genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(offset1, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction21 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (MethodIndex * BytesPerWord), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction21)) {
		(anInstruction21->dependent = locateLiteralsize(BaseHeaderSize + (MethodIndex * BytesPerWord), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction22 = genoperandoperandoperand(MoveRMwr, ReceiverResultReg, FoxThisContext, FPReg);
	if (usesOutOfLineLiteral(anInstruction22)) {
		(anInstruction22->dependent = locateLiteralsize(FoxThisContext, BytesPerOop));
	}
	gSubRRR(SPReg, FPReg, TempReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = log2BytesPerWord();
	genoperandoperand(LogicalShiftRightCqR, quickConstant, TempReg);
	/* begin SubCq:R: */
	quickConstant1 = 3;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction23 = genoperandoperand(SubCqR, quickConstant1, TempReg);
	if (usesOutOfLineLiteral(anInstruction23)) {
		(anInstruction23->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin AddR:R: */
	genoperandoperand(AddRR, SendNumArgsReg, TempReg);
	genConvertIntegerToSmallIntegerInReg(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction24 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (StackPointerIndex * BytesPerOop), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction24)) {
		(anInstruction24->dependent = locateLiteralsize(BaseHeaderSize + (StackPointerIndex * BytesPerOop), BytesPerOop));
	}
	if (isInBlock > 0) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperandoperand(LoadEffectiveAddressMwrR, 2, SendNumArgsReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(2, BytesPerOop));
		}
		/* begin MoveXwr:R:R: */
		genoperandoperandoperand(MoveXwrRR, TempReg, FPReg, TempReg);
	}
	else {
		/* begin genMoveConstant:R: */
		constant = nilObject();
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(gMoveCwR(constant, TempReg), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction36 = genoperandoperand(MoveCqR, constant, TempReg);
			if (usesOutOfLineLiteral(anInstruction36)) {
				(anInstruction36->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction25 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (ClosureIndex * BytesPerOop), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction25)) {
		(anInstruction25->dependent = locateLiteralsize(BaseHeaderSize + (ClosureIndex * BytesPerOop), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction26 = genoperandoperandoperand(MoveMwrR, FoxMFReceiver, FPReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction26)) {
		(anInstruction26->dependent = locateLiteralsize(FoxMFReceiver, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction27 = genoperandoperandoperand(MoveRMwr, TempReg, BaseHeaderSize + (ReceiverIndex * BytesPerOop), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction27)) {
		(anInstruction27->dependent = locateLiteralsize(BaseHeaderSize + (ReceiverIndex * BytesPerOop), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction28 = genoperandoperand(MoveCqR, 1, ClassReg);
	if (usesOutOfLineLiteral(anInstruction28)) {
		(anInstruction28->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((SendNumArgsReg == SPReg)));
	loopHead = genoperandoperand(CmpRR, SendNumArgsReg, ClassReg);
	/* begin JumpGreater: */
	exit = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, ClassReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction29 = genoperandoperand(AddCqR, 2, TempReg);
	if (usesOutOfLineLiteral(anInstruction29)) {
		(anInstruction29->dependent = locateLiteralsize(2, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, TempReg, FPReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction30 = genoperandoperand(AddCqR, ReceiverIndex + (BaseHeaderSize / BytesPerWord), ClassReg);
	if (usesOutOfLineLiteral(anInstruction30)) {
		(anInstruction30->dependent = locateLiteralsize(ReceiverIndex + (BaseHeaderSize / BytesPerWord), BytesPerOop));
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, TempReg, ClassReg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction31 = genoperandoperand(SubCqR, (ReceiverIndex + (BaseHeaderSize / BytesPerWord)) - 1, ClassReg);
	if (usesOutOfLineLiteral(anInstruction31)) {
		(anInstruction31->dependent = locateLiteralsize((ReceiverIndex + (BaseHeaderSize / BytesPerWord)) - 1, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)loopHead));
	jmpTarget(exit, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	literal = nilObject();
	anInstruction5 = genoperandoperand(MoveCqR, nilObject(), TempReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction32 = genoperandoperandoperand(LoadEffectiveAddressMwrR, FoxMFReceiver, FPReg, ClassReg);
	if (usesOutOfLineLiteral(anInstruction32)) {
		(anInstruction32->dependent = locateLiteralsize(FoxMFReceiver, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction33 = genoperandoperand(AddCqR, (ReceiverIndex + 1) + (BaseHeaderSize / BytesPerWord), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction33)) {
		(anInstruction33->dependent = locateLiteralsize((ReceiverIndex + 1) + (BaseHeaderSize / BytesPerWord), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction34 = genoperandoperand(SubCqR, BytesPerWord, ClassReg);
	if (usesOutOfLineLiteral(anInstruction34)) {
		(anInstruction34->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	loopHead = anInstruction34;
	/* begin CmpR:R: */
	assert(!((ClassReg == SPReg)));
	genoperandoperand(CmpRR, ClassReg, SPReg);
	/* begin JumpAbove: */
	exit = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, TempReg, SendNumArgsReg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction35 = genoperandoperand(AddCqR, 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction35)) {
		(anInstruction35->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)loopHead));
	jmpTarget(exit, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jumpNeedScavenge, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin saveAndRestoreLinkRegAround: */
	inst = genoperand(PushR, LinkReg);
	CallRTregistersToBeSavedMask(ceScheduleScavengeTrampoline, ((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg));
	genoperand(PopR, LinkReg);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)continuation));
	return 0;
}


/*	Get the active context into ReceiverResultReg, creating it if necessary. */

	/* CogObjectRepresentationForSpur>>#genGetActiveContextNumArgs:large:inBlock: */
static sqInt NoDbgRegParms
genGetActiveContextNumArgslargeinBlock(sqInt numArgs, sqInt isLargeContext, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    sqInt routine;

	if (isLargeContext) {
		switch (isInBlock) {
		case 0:
			routine = ceLargeActiveContextInMethodTrampoline;
			break;

		case InVanillaBlock:
			routine = ceLargeActiveContextInBlockTrampoline;
			break;

		case InFullBlock:
			routine = ceLargeActiveContextInFullBlockTrampoline;
			break;

		default:
			error("Case not found and no otherwise clause");
			routine = -1;
		}
	}
	else {
		switch (isInBlock) {
		case 0:
			routine = ceSmallActiveContextInMethodTrampoline;
			break;

		case InVanillaBlock:
			routine = ceSmallActiveContextInBlockTrampoline;
			break;

		case InFullBlock:
			routine = ceSmallActiveContextInFullBlockTrampoline;
			break;

		default:
			error("Case not found and no otherwise clause");
			routine = -1;
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, numArgs, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(numArgs, BytesPerOop));
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, routine);
	(abstractInstruction->annotation = IsRelativeCall);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genGetBits:ofFormatByteOf:into: */
static sqInt NoDbgRegParms
genGetBitsofFormatByteOfinto(sqInt mask, sqInt sourceReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;

	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMbrR, 3, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(3, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(AndCqR, mask, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(mask, BytesPerOop));
	}
	return 0;
}


/*	Fetch the instance's class index into destReg. */

	/* CogObjectRepresentationForSpur>>#genGetClassIndexOfNonImm:into: */
static sqInt NoDbgRegParms
genGetClassIndexOfNonImminto(sqInt sourceReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = classIndexMask();
	anInstruction1 = genoperandoperand(AndCqR, classIndexMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	Fetch the class object whose index is in instReg into destReg.
	It is non-obvious, but the Cogit assumes loading a class does not involve
	a runtime call, so do not call classAtIndex: */

	/* CogObjectRepresentationForSpur>>#genGetClassObjectOfClassIndex:into:scratchReg: */
static sqInt NoDbgRegParms
genGetClassObjectOfClassIndexintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    sqInt literal1;
    sqInt offset;
    sqInt quickConstant;
    sqInt quickConstant1;

	assert(instReg != destReg);
	assert(instReg != scratchReg);
	assert(destReg != scratchReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instReg, scratchReg);
	/* begin LogicalShiftRightCq:R: */
	quickConstant = classTableMajorIndexShift();
	genoperandoperand(LogicalShiftRightCqR, quickConstant, scratchReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), scratchReg);
	assert(!(shouldAnnotateObjectReference(classTableRootObj())));
	/* begin MoveMw:r:R: */
	offset = (classTableRootObj()) + BaseHeaderSize;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, scratchReg, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, instReg, scratchReg);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = classTableMinorIndexMask();
	anInstruction3 = genoperandoperand(AndCqR, classTableMinorIndexMask(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, quickConstant1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, scratchReg, destReg, destReg);
	return 0;
}


/*	Fetch the instance's class into destReg. If the instance is not the
	receiver and is forwarded, follow forwarding. */

	/* CogObjectRepresentationForSpur>>#genGetClassObjectOf:into:scratchReg:mayBeAForwarder: */
static sqInt NoDbgRegParms
genGetClassObjectOfintoscratchRegmayBeAForwarder(sqInt instReg, sqInt destReg, sqInt scratchReg, sqInt mayBeForwarder)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jumpIsImm;
    AbstractInstruction *jumpNotForwarded;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    AbstractInstruction *loop;

	if ((instReg == destReg)
	 || ((instReg == scratchReg)
	 || (destReg == scratchReg))) {
		return BadRegisterSet;
	}
	/* begin MoveR:R: */
	loop = genoperandoperand(MoveRR, instReg, scratchReg);
	/* begin checkQuickConstant:forInstruction: */
	literal1 = tagMask();
	anInstruction2 = genoperandoperand(AndCqR, tagMask(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpIsImm = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(MoveMwrR, 0, instReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal2 = classIndexMask();
	anInstruction4 = genoperandoperand(AndCqR, classIndexMask(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	if (mayBeForwarder) {

		/* if it is forwarded... */
		/* begin checkQuickConstant:forInstruction: */
		literal = isForwardedObjectClassIndexPun();
		anInstruction = genoperandoperand(CmpCqR, isForwardedObjectClassIndexPun(), scratchReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpNotForwarded = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, BaseHeaderSize, instReg, instReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
		}
		/* begin Jump: */
		genoperand(Jump, ((sqInt)loop));
		jmpTarget(jumpNotForwarded, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	jmpTarget(jumpIsImm, genoperandoperand(MoveRR, scratchReg, destReg));
	if (scratchReg == TempReg) {
		/* begin PushR: */
		genoperand(PushR, instReg);
		genGetClassObjectOfClassIndexintoscratchReg(destReg, instReg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, instReg, destReg);
		/* begin PopR: */
		genoperand(PopR, instReg);
	}
	else {
		genGetClassObjectOfClassIndexintoscratchReg(destReg, scratchReg, TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, scratchReg, destReg);
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genGetClassTagOf:into:scratchReg: */
static AbstractInstruction * NoDbgRegParms
genGetClassTagOfintoscratchReg(sqInt instReg, sqInt destReg, sqInt scratchReg)
{
	return genGetInlineCacheClassTagFromintoforEntry(instReg, destReg, 1);
}


/*	Fetch the instance's class index into destReg. */

	/* CogObjectRepresentationForSpur>>#genGetCompactClassIndexNonImmOf:into: */
static sqInt NoDbgRegParms
genGetCompactClassIndexNonImmOfinto(sqInt instReg, sqInt destReg)
{
	return genGetClassIndexOfNonImminto(instReg, destReg);
}

	/* CogObjectRepresentationForSpur>>#genGetDoubleValueOf:into: */
static sqInt NoDbgRegParms
genGetDoubleValueOfinto(sqInt srcReg, sqInt destFPReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveM64rRd, BaseHeaderSize, srcReg, destFPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	return 0;
}


/*	Get the format field of the object in srcReg into destReg.
	srcReg may equal destReg. */

	/* CogObjectRepresentationForSpur>>#genGetFormatOf:into: */
static sqInt NoDbgRegParms
genGetFormatOfinto(sqInt srcReg, sqInt destReg)
{
	return genGetBitsofFormatByteOfinto(formatMask(), srcReg, destReg);
}


/*	Get the format of the object in sourceReg into destReg. If
	scratchRegOrNone is not NoReg, load at least the least significant 32-bits
	(64-bits in 64-bits) of the
	header word, which contains the format, into scratchRegOrNone. */

	/* CogObjectRepresentationForSpur>>#genGetFormatOf:into:leastSignificantHalfOfBaseHeaderIntoScratch: */
static sqInt NoDbgRegParms
genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(sqInt sourceReg, sqInt destReg, sqInt scratchRegOrNone)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt literal;

	if (scratchRegOrNone == NoReg) {
		flag("endianness");
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMbrR, 3, sourceReg, destReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(3, BytesPerOop));
		}
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchRegOrNone);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		gLogicalShiftRightCqRR(formatShift(), scratchRegOrNone, destReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = formatMask();
	anInstruction2 = genoperandoperand(AndCqR, formatMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return 0;
}


/*	ReceiverResultReg is required for the trampoline. We force the allocation,
	and we have two path to avoid conflicts in ReceiverResultReg. */

	/* CogObjectRepresentationForSpur>>#genGetIdentityHash:resultReg: */
static void NoDbgRegParms
genGetIdentityHashresultReg(sqInt rcvrReg, sqInt resultReg)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpSet;

	voidReceiverResultRegContainsSelf();
	if (resultReg == ReceiverResultReg) {
		popToReg(ssTop(), rcvrReg);
		genGetHashFieldNonImmOfasSmallIntegerInto(rcvrReg, resultReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, ConstZero, resultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(ConstZero, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpSet = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, rcvrReg, resultReg);
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceInlineNewHashTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		popToReg(ssTop(), ReceiverResultReg);
		genGetHashFieldNonImmOfasSmallIntegerInto(ReceiverResultReg, resultReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, ConstZero, resultReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(ConstZero, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpSet = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceInlineNewHashTrampoline);
		(abstractInstruction1->annotation = IsRelativeCall);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ReceiverResultReg, resultReg);
	}
	jmpTarget(jumpSet, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}


/*	Create an instance of classObj and assign it to destReg, initializing the
	instance if initializeInstance is true with 0 This is for inline
	primitives. Assume there is sufficient space in new space to complete the
	operation.  */

	/* CogObjectRepresentationForSpur>>#genGetInstanceOfByteClass:into:initializingIf:numBytes: */
static sqInt NoDbgRegParms
genGetInstanceOfByteClassintoinitializingIfnumBytes(sqInt classObj, sqInt destReg, sqInt initializeInstance, sqInt numBytes)
{
    sqInt byteFormat;
    sqInt classIndex;
    sqInt numSlots;

	classIndex = rawHashBitsOf(classObj);
	flag("duplication");
	numSlots = (numBytes + (BytesPerWord - 1)) / BytesPerWord;
	byteFormat = (firstByteFormat()) + ((8 - numBytes) & (BytesPerWord - 1));
	assert(classIndex != 0);
	genGetUninitializedInstanceWithClassIndexnumSlotsformatinto(classIndex, numSlots, byteFormat, destReg);
	if (initializeInstance
	 && (numBytes > 0)) {
		genStoreValueinstancenumSlots(0, destReg, numSlots);
	}
	return 0;
}


/*	Create an instance of classObj and assign it to destReg, initializing the
	instance if initializeInstance is true with nil This is for inline
	primitives. Assume there is sufficient space in new space to complete the
	operation.  */

	/* CogObjectRepresentationForSpur>>#genGetInstanceOfPointerClass:into:initializingIf:numVariableSlots: */
static sqInt NoDbgRegParms
genGetInstanceOfPointerClassintoinitializingIfnumVariableSlots(sqInt classObj, sqInt destReg, sqInt initializeInstance, sqInt varSlots)
{
    sqInt classFormat;
    sqInt classIndex;
    sqInt fixedSlots;
    sqInt totalNumSlots;

	classIndex = rawHashBitsOf(classObj);
	classFormat = formatOfClass(classObj);
	fixedSlots = fixedFieldsOfClassFormat(classFormat);
	totalNumSlots = varSlots + fixedSlots;
	assert(classIndex != 0);
	genGetUninitializedInstanceWithClassIndexnumSlotsformatinto(classIndex, totalNumSlots, instSpecOfClassFormat(classFormat), destReg);
	if (initializeInstance
	 && (totalNumSlots > 0)) {
		genStoreValueinstancenumSlots(nilObject(), destReg, totalNumSlots);
	}
	return 0;
}


/*	Get the size in word-sized slots of the object in srcReg into destReg.
	srcReg may equal destReg. */

	/* CogObjectRepresentationForSpur>>#genGetNumSlotsOf:into: */
static sqInt NoDbgRegParms
genGetNumSlotsOfinto(sqInt srcReg, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmp;
    sqInt literal;

	assert(srcReg != destReg);
	genGetRawSlotSizeOfNonImminto(srcReg, destReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = numSlotsMask();
	anInstruction = genoperandoperand(CmpCqR, numSlotsMask(), destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLess: */
	jmp = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genGetOverflowSlotsOfinto(srcReg, destReg);
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	The raw numSlots field is the most significant byte of the 64-bit header
	word. MoveMbrR zero-extends. */

	/* CogObjectRepresentationForSpur>>#genGetRawSlotSizeOfNonImm:into: */
static sqInt NoDbgRegParms
genGetRawSlotSizeOfNonImminto(sqInt sourceReg, sqInt destReg)
{
    AbstractInstruction *anInstruction1;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMbrR, 7, sourceReg, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(7, BytesPerOop));
	}
	return 0;
}


/*	Write in destReg the pointer to the object (adjusted based on header size)
	Can deal with large header but not with with old space allocation (max
	allocation size)
 */

	/* CogObjectRepresentationForSpur>>#genGetUninitializedInstanceWithClassIndex:numSlots:format:into: */
static sqInt NoDbgRegParms
genGetUninitializedInstanceWithClassIndexnumSlotsformatinto(sqInt classIndex, sqInt numSlots, sqInt format, sqInt destReg)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    usqLong header;
    sqInt lowNumSlots;
    sqInt offset;
    usqLong overflowHeader;

	if (numSlots >= (fixedFieldsOfClassFormatMask())) {
		return UnimplementedOperation;
	}
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, destReg));
	if (numSlots >= (numSlotsMask())) {
		overflowHeader = numSlots + (((sqInt)((usqInt)((numSlotsMask())) << (numSlotsFullShift()))));
		/* begin genStoreHeader:intoNewInstance:using: */
		anInstruction = genoperandoperand(MoveCqR, overflowHeader, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(overflowHeader, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, 0, destReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(AddCqR, BaseHeaderSize, destReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
		}
		lowNumSlots = numSlotsMask();
	}
	else {
		lowNumSlots = numSlots;
	}
	header = headerForSlotsformatclassIndex(lowNumSlots, format, classIndex);
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction3 = genoperandoperand(MoveCqR, header, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(header, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, TempReg, 0, destReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin LoadEffectiveAddressMw:r:R: */
	offset = objectBytesForSlots(numSlots);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, destReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genIfRequiredCheckRememberedBitOf:scratch: */
static AbstractInstruction * NoDbgRegParms
genIfRequiredCheckRememberedBitOfscratch(sqInt rr, sqInt scratchReg)
{
	if (!CheckRememberedInTrampoline) {
		return genCheckRememberedBitOfscratch(rr, scratchReg);
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genJumpImmediate: */
static AbstractInstruction * NoDbgRegParms
genJumpImmediate(sqInt aRegister)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	literal = tagMask();
	anInstruction = genoperandoperand(TstCqR, tagMask(), aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpNonZero: */
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}

	/* CogObjectRepresentationForSpur>>#genJumpImmutable:scratchReg: */
#if IMMUTABILITY
static AbstractInstruction * NoDbgRegParms
genJumpImmutablescratchReg(sqInt sourceReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin genJumpBaseHeaderImmutable: */
	literal = immutableBitMask();
	anInstruction1 = genoperandoperand(TstCqR, immutableBitMask(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpNonZero: */
	return genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
}
#endif /* IMMUTABILITY */

	/* CogObjectRepresentationForSpur>>#genJumpMutable:scratchReg: */
#if IMMUTABILITY
static AbstractInstruction * NoDbgRegParms
genJumpMutablescratchReg(sqInt sourceReg, sqInt scratchReg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, 0, sourceReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin genJumpBaseHeaderMutable: */
	literal = immutableBitMask();
	anInstruction1 = genoperandoperand(TstCqR, immutableBitMask(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpZero: */
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}
#endif /* IMMUTABILITY */

	/* CogObjectRepresentationForSpur>>#genJumpNotImmediate: */
static AbstractInstruction * NoDbgRegParms
genJumpNotImmediate(sqInt aRegister)
{
    AbstractInstruction *anInstruction;
    sqInt literal;

	/* begin checkQuickConstant:forInstruction: */
	literal = tagMask();
	anInstruction = genoperandoperand(TstCqR, tagMask(), aRegister);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpZero: */
	return genConditionalBranchoperand(JumpZero, ((sqInt)0));
}


/*	Generate a call to code that allocates a new Array of size.
	The Array should be initialized with nils iff initialize is true.
	The size arg is passed in SendNumArgsReg, the result
	must come back in ReceiverResultReg. */

	/* CogObjectRepresentationForSpur>>#genNewArrayOfSize:initialized: */
static sqInt NoDbgRegParms
genNewArrayOfSizeinitialized(sqInt size, sqInt initialize)
{
    AbstractInstruction *abstractInstruction;
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    usqLong header;
    sqInt i;
    sqInt literal;
    sqInt offset;
    sqInt quickConstant;
    AbstractInstruction *skip;

	assert(size < (numSlotsMask()));
	header = headerForSlotsformatclassIndex(size, arrayFormat(), ClassArrayCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, ReceiverResultReg));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction2 = genoperandoperand(MoveCqR, header, TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(header, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
	}
	if (initialize
	 && (size > 0)) {
		if (shouldAnnotateObjectReference(nilObject())) {
			annotateobjRef(gMoveCwR(nilObject(), TempReg), nilObject());
		}
		else {
			/* begin MoveCq:R: */
			quickConstant = nilObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(MoveCqR, quickConstant, TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
			}
		}
		for (i = 0; i < size; i += 1) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, (i * BytesPerWord) + BaseHeaderSize, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize((i * BytesPerWord) + BaseHeaderSize, BytesPerOop));
			}
		}
	}
	/* begin LoadEffectiveAddressMw:r:R: */
	offset = smallObjectBytesForSlots(size);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(LoadEffectiveAddressMwrR, offset, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction4 = genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpBelow: */
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Create a closure with the given startpc, numArgs and numCopied
	within a context with ctxtNumArgs, large if isLargeCtxt that is in a
	block if isInBlock. Do /not/ initialize the copied values. */

	/* CogObjectRepresentationForSpur>>#genNoPopCreateClosureAt:numArgs:numCopied:contextNumArgs:large:inBlock: */
static sqInt NoDbgRegParms
genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(sqInt bcpc, sqInt numArgs, sqInt numCopied, sqInt ctxtNumArgs, sqInt isLargeCtxt, sqInt isInBlock)
{
    AbstractInstruction *abstractInstruction;
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    usqInt byteSize;
    usqLong header;
    sqInt literal;
    sqInt numSlots;
    AbstractInstruction *skip;


	/* First get thisContext into ReceiverResultRega and thence in ClassReg. */
	genGetActiveContextNumArgslargeinBlock(ctxtNumArgs, isLargeCtxt, isInBlock);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	numSlots = ClosureFirstCopiedValueIndex + numCopied;
	byteSize = smallObjectBytesForSlots(numSlots);
	header = headerForSlotsformatclassIndex(numSlots, indexablePointersFormat(), ClassBlockClosureCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, ReceiverResultReg));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction = genoperandoperand(MoveCqR, header, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(header, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(LoadEffectiveAddressMwrR, byteSize, ReceiverResultReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(byteSize, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction3 = genoperandoperand(CmpCqR, getScavengeThreshold(), TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpBelow: */
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceScheduleScavengeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperandoperand(MoveRMwr, ClassReg, (ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize((ClosureOuterContextIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(MoveCqR, (((usqInt)bcpc << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize((((usqInt)bcpc << 3) | 1), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperandoperand(MoveRMwr, TempReg, (ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize((ClosureStartPCIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(MoveCqR, (((usqInt)numArgs << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize((((usqInt)numArgs << 3) | 1), BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperandoperand(MoveRMwr, TempReg, (ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize((ClosureNumArgsIndex * BytesPerOop) + BaseHeaderSize, BytesPerOop));
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveAsCharacter */
static sqInt
genPrimitiveAsCharacter(void)
{
    AbstractInstruction *jumpNotInt;
    AbstractInstruction *jumpOutOfRange;
    sqInt reg;

	jumpNotInt = ((AbstractInstruction *) 0);
	if (methodOrBlockNumArgs == 0) {
		reg = ReceiverResultReg;
	}
	else {
		if (methodOrBlockNumArgs > 1) {
			return UnimplementedPrimitive;
		}
		reg = Arg0Reg;
		/* begin genLoadArgAtDepth:into: */
		assert(0 < (numRegArgs()));
		/* begin genJumpNotSmallInteger:scratchReg: */
		jumpNotInt = genJumpNotSmallInteger(reg);
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, reg, TempReg);
	genConvertSmallIntegerToIntegerInReg(TempReg);
	jumpOutOfRange = jumpNotCharacterUnsignedValueInRegister(TempReg);
	genConvertSmallIntegerToCharacterInReg(reg);
	if (reg != ReceiverResultReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, reg, ReceiverResultReg);
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpOutOfRange, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (reg != ReceiverResultReg) {
		jmpTarget(jumpNotInt, ((AbstractInstruction *) (((jumpOutOfRange->operands))[0])));
	}
	return CompletePrimitive;
}


/*	Generate primitive 60, at: with unsigned access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveAt */
static sqInt
genPrimitiveAt(void)
{
	return genPrimitiveAtSigned(0);
}


/*	Generate primitive 61, at:put: with unsigned access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveAtPut */
static sqInt
genPrimitiveAtPut(void)
{
	return genPrimitiveAtPutSigned(0);
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveIdenticalOrNotIf: */
static sqInt NoDbgRegParms
genPrimitiveIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *comp;
    sqInt constant;
    sqInt constant1;
    AbstractInstruction *jumpCmp;

	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin CmpR:R: */
	assert(!((Arg0Reg == SPReg)));
	comp = genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	if (orNot) {
		/* begin JumpZero: */
		jumpCmp = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(Arg0Reg, TempReg, comp, 0);
	}
	else {
		/* begin JumpNonZero: */
		jumpCmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	}
	/* begin genMoveConstant:R: */
	constant = trueObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpCmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (!orNot) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(Arg0Reg, TempReg, comp, 0);
	}
	/* begin genMoveConstant:R: */
	constant1 = falseObject();
	if (shouldAnnotateObjectReference(constant1)) {
		annotateobjRef(gMoveCwR(constant1, ReceiverResultReg), constant1);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, constant1, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(constant1, BytesPerOop));
		}
	}
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	return UnfailingPrimitive;
}


/*	Generate primitive 164, at: with signed access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveIntegerAt */
static sqInt
genPrimitiveIntegerAt(void)
{
	return genPrimitiveAtSigned(1);
}


/*	Generate primitive 165, at:put: with signed access for pure bits classes. */

	/* CogObjectRepresentationForSpur>>#genPrimitiveIntegerAtPut */
static sqInt
genPrimitiveIntegerAtPut(void)
{
	return genPrimitiveAtPutSigned(1);
}


/*	<returnTypeC: #'AbstractInstruction *'> */

	/* CogObjectRepresentationForSpur>>#genPrimitiveMakePoint */
static sqInt
genPrimitiveMakePoint(void)
{
    sqInt address;
    sqInt address1;
    sqInt allocSize;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jumpFail;
    sqInt literal;
    usqLong newPointHeader;
    sqInt resultReg;
    sqInt scratchReg;

	resultReg = ClassReg;

	/* <var: #jumpFail type: #'AbstractInstruction *'> */
	scratchReg = SendNumArgsReg;
	allocSize = BaseHeaderSize + (BytesPerWord * 2);
	newPointHeader = headerForSlotsformatclassIndex(2, nonIndexablePointerFormat(), ClassPointCompactIndex);
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, resultReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(LoadEffectiveAddressMwrR, allocSize, resultReg, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(allocSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal = getScavengeThreshold();
	anInstruction1 = genoperandoperand(CmpCqR, getScavengeThreshold(), scratchReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpFail = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	/* begin MoveR:Aw: */
	address1 = freeStartAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, scratchReg, address1));
	/* begin genStoreHeader:intoNewInstance:using: */
	anInstruction2 = genoperandoperand(MoveCqR, newPointHeader, scratchReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(newPointHeader, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveRMwr, scratchReg, 0, resultReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(MoveRMwr, ReceiverResultReg, BaseHeaderSize, resultReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperandoperand(MoveRMwr, Arg0Reg, BaseHeaderSize + BytesPerWord, resultReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(BaseHeaderSize + BytesPerWord, BytesPerOop));
	}
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, resultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genPrimitiveObjectAt */
static sqInt
genPrimitiveObjectAt(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt headerReg;
    AbstractInstruction *jumpBadIndex;
    AbstractInstruction *jumpBounds;
    AbstractInstruction *jumpNotHeaderIndex;
    sqInt quickConstant;
    sqInt quickConstant1;

	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpBadIndex = genJumpNotSmallInteger(Arg0Reg);
	genGetMethodHeaderOfintoscratch(ReceiverResultReg, (headerReg = Arg1Reg), TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, (((usqInt)1 << 3) | 1), Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((((usqInt)1 << 3) | 1), BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpNotHeaderIndex = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, headerReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpNotHeaderIndex, gAndCqR((((usqInt)(alternateHeaderNumLiteralsMask()) << 3) | 1), headerReg));
	/* begin SubCq:R: */
	quickConstant = ((((usqInt)1 << 3) | 1)) - (smallIntegerTag());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(SubCqR, quickConstant, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((headerReg == SPReg)));
	genoperandoperand(CmpRR, headerReg, Arg0Reg);
	/* begin JumpAbove: */
	jumpBounds = genConditionalBranchoperand(JumpAbove, ((sqInt)0));
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin AddCq:R: */
	quickConstant1 = ((usqInt)(BaseHeaderSize)) >> (shiftForWord());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(AddCqR, quickConstant1, Arg0Reg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin MoveXwr:R:R: */
	genoperandoperandoperand(MoveXwrRR, Arg0Reg, ReceiverResultReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpBounds, gAddCqR(((((usqInt)1 << 3) | 1)) - (smallIntegerTag()), Arg0Reg));
	jmpTarget(jumpBadIndex, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	c.f. StackInterpreter>>stSizeOf: lengthOf:baseHeader:format:
	fixedFieldsOf:format:length: 
 */

	/* CogObjectRepresentationForSpur>>#genPrimitiveSize */
static sqInt
genPrimitiveSize(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *jump32BitLongsDone;
    AbstractInstruction *jump64BitLongsDone;
    AbstractInstruction *jumpArrayDone;
    AbstractInstruction *jumpBytesDone;
    AbstractInstruction *jumpHasFixedFields;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpIs32BitLongs;
    AbstractInstruction *jumpIsBytes;
    AbstractInstruction *jumpIsContext;
    AbstractInstruction *jumpIsContext1;
    AbstractInstruction *jumpIsShorts;
    AbstractInstruction *jumpNotIndexable;
    AbstractInstruction *jumpNotIndexable1;
    AbstractInstruction *jumpShortsDone;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt literal6;

	jumpImm = genJumpImmediate(ReceiverResultReg);
	/* begin genGetSizeOf:into:formatReg:scratchReg:abortJumpsInto: */
	genGetFormatOfintoleastSignificantHalfOfBaseHeaderIntoScratch(ReceiverResultReg, SendNumArgsReg, TempReg);
	genGetNumSlotsOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstByteFormat();
	anInstruction = genoperandoperand(CmpCqR, firstByteFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpIsBytes = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = arrayFormat();
	anInstruction1 = genoperandoperand(CmpCqR, arrayFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpArrayDone = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin JumpLess: */
	jumpNotIndexable1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal2 = weakArrayFormat();
	anInstruction2 = genoperandoperand(CmpCqR, weakArrayFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpHasFixedFields = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstShortFormat();
	anInstruction3 = genoperandoperand(CmpCqR, firstShortFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpIsShorts = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal4 = firstLongFormat();
	anInstruction4 = genoperandoperand(CmpCqR, firstLongFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(literal4, BytesPerOop));
	}
	/* begin JumpGreaterOrEqual: */
	jumpIs32BitLongs = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal5 = sixtyFourBitIndexableFormat();
	anInstruction5 = genoperandoperand(CmpCqR, sixtyFourBitIndexableFormat(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(literal5, BytesPerOop));
	}
	/* begin JumpZero: */
	jump64BitLongsDone = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	jmpTarget(jumpNotIndexable1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin Jump: */
	jumpNotIndexable1 = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsBytes, genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(AndCqR, BytesPerWord - 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(BytesPerWord - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	/* begin Jump: */
	jumpBytesDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIsShorts, gLogicalShiftLeftCqR((shiftForWord()) - 1, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(AndCqR, (((usqInt)(BytesPerWord)) >> 1) - 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize((((usqInt)(BytesPerWord)) >> 1) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	/* begin Jump: */
	jumpShortsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpIs32BitLongs, gLogicalShiftLeftCqR((shiftForWord()) - 2, ClassReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AndCqR, (((usqInt)(BytesPerWord)) >> 2) - 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize((((usqInt)(BytesPerWord)) >> 2) - 1, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	/* begin Jump: */
	jump32BitLongsDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpHasFixedFields, checkQuickConstantforInstruction(classIndexMask(), genoperandoperand(AndCqR, classIndexMask(), TempReg)));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, TempReg, SendNumArgsReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(CmpCqR, ClassMethodContextCompactIndex, TempReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(ClassMethodContextCompactIndex, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpIsContext1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genGetClassObjectOfClassIndexintoscratchReg(SendNumArgsReg, Extra0Reg, TempReg);
	genLoadSlotsourceRegdestReg(InstanceSpecificationIndex, Extra0Reg, SendNumArgsReg);
	genConvertSmallIntegerToIntegerInReg(SendNumArgsReg);
	/* begin checkQuickConstant:forInstruction: */
	literal6 = fixedFieldsOfClassFormatMask();
	anInstruction10 = genoperandoperand(AndCqR, fixedFieldsOfClassFormatMask(), SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(literal6, BytesPerOop));
	}
	/* begin SubR:R: */
	genoperandoperand(SubRR, SendNumArgsReg, ClassReg);
	jmpTarget(jumpArrayDone, jmpTarget(jump64BitLongsDone, jmpTarget(jump32BitLongsDone, jmpTarget(jumpShortsDone, jmpTarget(jumpBytesDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	jumpNotIndexable = jumpNotIndexable1;
	jumpIsContext = jumpIsContext1;
	genConvertIntegerInRegtoSmallIntegerInReg(ClassReg, ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpImm, jmpTarget(jumpNotIndexable, jmpTarget(jumpIsContext, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return CompletePrimitive;
}


/*	primitiveCompareWith: */

	/* CogObjectRepresentationForSpur>>#genPrimitiveStringCompareWith */
static sqInt
genPrimitiveStringCompareWith(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *instr;
    AbstractInstruction *jump;
    AbstractInstruction *jumpAbove;
    AbstractInstruction *jumpIncorrectFormat1;
    AbstractInstruction *jumpIncorrectFormat2;
    AbstractInstruction *jumpIncorrectFormat3;
    AbstractInstruction *jumpIncorrectFormat4;
    AbstractInstruction *jumpMidFailure;
    AbstractInstruction *jumpSuccess;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt minSizeReg;
    sqInt string1CharOrByteSizeReg;
    sqInt string1Reg;
    sqInt string2CharOrByteSizeReg;
    sqInt string2Reg;


	/* I redefine those name to ease program comprehension */
	string1Reg = ReceiverResultReg;
	string2Reg = Arg0Reg;
	string1CharOrByteSizeReg = Arg1Reg;
	string2CharOrByteSizeReg = ClassReg;

	/* Load arguments in reg */
	minSizeReg = SendNumArgsReg;
	/* begin genLoadArgAtDepth:into: */
	assert(0 < (numRegArgs()));
	genGetFormatOfinto(string1Reg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstByteFormat();
	anInstruction = genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpIncorrectFormat1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal1 = firstCompiledMethodFormat();
	anInstruction1 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIncorrectFormat2 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genGetNumSlotsOfinto(string1Reg, string1CharOrByteSizeReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), string1CharOrByteSizeReg);
	gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, string1CharOrByteSizeReg);
	genGetFormatOfinto(string2Reg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal2 = firstByteFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal2, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpIncorrectFormat3 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	literal3 = firstCompiledMethodFormat();
	anInstruction3 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(literal3, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpIncorrectFormat4 = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	genGetNumSlotsOfinto(string2Reg, string2CharOrByteSizeReg);
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), string2CharOrByteSizeReg);
	gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, TempReg, string2CharOrByteSizeReg);
	/* begin CmpR:R: */
	assert(!((string1CharOrByteSizeReg == SPReg)));
	genoperandoperand(CmpRR, string1CharOrByteSizeReg, string2CharOrByteSizeReg);
	/* begin JumpBelow: */
	jumpAbove = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, string1CharOrByteSizeReg, minSizeReg);
	/* begin Jump: */
	jump = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpAbove, genoperandoperand(MoveRR, string2CharOrByteSizeReg, minSizeReg));
	jmpTarget(jump, checkQuickConstantforInstruction(0, genoperandoperand(CmpCqR, 0, minSizeReg)));
	/* begin JumpZero: */
	jumpSuccess = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(MoveCqR, BaseHeaderSize, TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperand(AddCqR, BaseHeaderSize, minSizeReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
	}
	/* begin MoveXbr:R:R: */
	instr = genoperandoperandoperand(MoveXbrRR, TempReg, string1Reg, string1CharOrByteSizeReg);
	/* begin MoveXbr:R:R: */
	genoperandoperandoperand(MoveXbrRR, TempReg, string2Reg, string2CharOrByteSizeReg);
	/* begin SubR:R: */
	genoperandoperand(SubRR, string2CharOrByteSizeReg, string1CharOrByteSizeReg);
	/* begin JumpNonZero: */
	jumpMidFailure = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperand(AddCqR, 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, minSizeReg);
	/* begin JumpNonZero: */
	genConditionalBranchoperand(JumpNonZero, ((sqInt)instr));
	genGetNumBytesOfinto(string1Reg, string1CharOrByteSizeReg);
	genGetNumBytesOfinto(string2Reg, string2CharOrByteSizeReg);
	jmpTarget(jumpSuccess, genoperandoperand(SubRR, string2CharOrByteSizeReg, string1CharOrByteSizeReg));
	jmpTarget(jumpMidFailure, genoperandoperand(MoveRR, string1CharOrByteSizeReg, ReceiverResultReg));
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	if (methodOrBlockNumArgs <= (numRegArgs())) {
		/* begin RetN: */
		genoperand(RetN, 0);
	}
	else {
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	jmpTarget(jumpIncorrectFormat4, jmpTarget(jumpIncorrectFormat3, jmpTarget(jumpIncorrectFormat2, jmpTarget(jumpIncorrectFormat1, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	return CompletePrimitive;
}


/*	replaceFrom: start to: stop with: replacement startingAt: repStart. 
	
	The primitive in the JIT tries to deal with two pathological cases, copy
	of arrays and byteStrings,
	which often copies only a dozen of fields and where switching to the C
	runtime cost a lot.
	
	Based on heuristics on the method class, I generate a quick array path
	(typically for Array),
	a quick byteString path (typically for ByteString, ByteArray and
	LargeInteger) or no quick 
	path at all (Typically for Bitmap).
	
	The many tests to ensure that the primitive won't fail are not super
	optimised (multiple reloading
	or stack arguments in registers) but this is still good enough and worth
	it since we're avoiding 
	the Smalltalk to C stack switch. The tight copying loops are optimised. 
	
	It is possible to build a bigger version with the 2 different paths but I
	(Clement) believe this 
	is too big machine code wise to be worth it.
 */

	/* CogObjectRepresentationForSpur>>#genPrimitiveStringReplace */
static sqInt
genPrimitiveStringReplace(void)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction13;
    AbstractInstruction *anInstruction14;
    AbstractInstruction *anInstruction15;
    AbstractInstruction *anInstruction16;
    AbstractInstruction *anInstruction17;
    AbstractInstruction *anInstruction18;
    AbstractInstruction *anInstruction19;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction20;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction22;
    AbstractInstruction *anInstruction23;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt arrayReg;
    AbstractInstruction *inst;
    AbstractInstruction *instr;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jumpEmpty;
    AbstractInstruction *jumpImm;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpIncorrectFormat1;
    AbstractInstruction *jumpIncorrectFormat2;
    AbstractInstruction *jumpIncorrectFormat3;
    AbstractInstruction *jumpIncorrectFormat4;
    AbstractInstruction *jumpNotSmi1;
    AbstractInstruction *jumpNotSmi2;
    AbstractInstruction *jumpNotSmi3;
    AbstractInstruction *jumpOutOfBounds1;
    AbstractInstruction *jumpOutOfBounds2;
    AbstractInstruction *jumpOutOfBounds3;
    AbstractInstruction *jumpOutOfBounds4;
    sqInt literal;
    sqInt literal1;
    sqInt literal2;
    sqInt literal3;
    sqInt literal4;
    sqInt literal5;
    sqInt offset;
    sqInt offset1;
    sqInt offset2;
    sqInt offset3;
    sqInt offset4;
    sqInt offset5;
    sqInt offset6;
    sqInt offset7;
    sqInt offset8;
    sqInt offset9;
    sqInt replReg;
    sqInt repStartReg;
    sqInt result;
    sqInt startReg;
    sqInt stopReg;
    sqInt wordConstant;


	/* Can I generate a quick path for this method ? */
	jmpAlreadyRemembered = ((AbstractInstruction *) 0);
	jumpImmutable = ((AbstractInstruction *) 0);
	jumpOutOfBounds3 = ((AbstractInstruction *) 0);
	jumpOutOfBounds4 = ((AbstractInstruction *) 0);
	if (!((maybeMethodClassOfseemsToBeInstantiating(methodObj, arrayFormat()))
		 || (maybeMethodClassOfseemsToBeInstantiating(methodObj, firstByteFormat())))) {
		return UnimplementedPrimitive;
	}
	arrayReg = ReceiverResultReg;
	startReg = Arg0Reg;
	stopReg = Arg1Reg;
	replReg = ClassReg;

	/* Load arguments in reg */
	repStartReg = SendNumArgsReg;
	/* begin genStackArgAt:into: */
	offset6 = (0) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction20 = genoperandoperandoperand(MoveMwrR, offset6, SPReg, repStartReg);
	if (usesOutOfLineLiteral(anInstruction20)) {
		(anInstruction20->dependent = locateLiteralsize(offset6, BytesPerOop));
	}
	/* begin genStackArgAt:into: */
	offset7 = (1) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction21 = genoperandoperandoperand(MoveMwrR, offset7, SPReg, replReg);
	if (usesOutOfLineLiteral(anInstruction21)) {
		(anInstruction21->dependent = locateLiteralsize(offset7, BytesPerOop));
	}
	/* begin genStackArgAt:into: */
	offset8 = (2) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction22 = genoperandoperandoperand(MoveMwrR, offset8, SPReg, stopReg);
	if (usesOutOfLineLiteral(anInstruction22)) {
		(anInstruction22->dependent = locateLiteralsize(offset8, BytesPerOop));
	}
	/* begin genStackArgAt:into: */
	offset9 = (3) * BytesPerWord;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction23 = genoperandoperandoperand(MoveMwrR, offset9, SPReg, startReg);
	if (usesOutOfLineLiteral(anInstruction23)) {
		(anInstruction23->dependent = locateLiteralsize(offset9, BytesPerOop));
	}
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSmi1 = genJumpNotSmallInteger(repStartReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSmi2 = genJumpNotSmallInteger(stopReg);
	/* begin genJumpNotSmallInteger:scratchReg: */
	jumpNotSmi3 = genJumpNotSmallInteger(startReg);

	/* if start>stop primitive success */
	jumpImm = genJumpImmediate(replReg);
	/* begin CmpR:R: */
	assert(!((startReg == SPReg)));
	genoperandoperand(CmpRR, startReg, stopReg);
	/* begin JumpLess: */
	jumpEmpty = genConditionalBranchoperand(JumpLess, ((sqInt)0));
#  if IMMUTABILITY
	jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, TempReg);
#  endif
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(CmpCqR, (((usqInt)0 << 3) | 1), startReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize((((usqInt)0 << 3) | 1), BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpOutOfBounds1 = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction13 = genoperandoperand(CmpCqR, (((usqInt)0 << 3) | 1), repStartReg);
	if (usesOutOfLineLiteral(anInstruction13)) {
		(anInstruction13->dependent = locateLiteralsize((((usqInt)0 << 3) | 1), BytesPerOop));
	}
	/* begin JumpLessOrEqual: */
	jumpOutOfBounds2 = genConditionalBranchoperand(JumpLessOrEqual, ((sqInt)0));
	if (maybeMethodClassOfseemsToBeInstantiating(methodObj, arrayFormat())) {

		/* Are they both array format ? */
		genGetFormatOfinto(arrayReg, TempReg);
		genGetFormatOfinto(replReg, startReg);
		/* begin checkQuickConstant:forInstruction: */
		literal = arrayFormat();
		anInstruction2 = genoperandoperand(CmpCqR, arrayFormat(), startReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpIncorrectFormat1 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		literal1 = arrayFormat();
		anInstruction3 = genoperandoperand(CmpCqR, arrayFormat(), TempReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(literal1, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpIncorrectFormat2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		genGetNumSlotsOfinto(arrayReg, TempReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin CmpR:R: */
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, stopReg);
		/* begin JumpGreater: */
		jumpOutOfBounds3 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		genGetNumSlotsOfinto(replReg, TempReg);
		/* begin genStackArgAt:into: */
		offset = (3) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction14 = genoperandoperandoperand(MoveMwrR, offset, SPReg, startReg);
		if (usesOutOfLineLiteral(anInstruction14)) {
			(anInstruction14->dependent = locateLiteralsize(offset, BytesPerOop));
		}
		genConvertSmallIntegerToIntegerInReg(startReg);
		genConvertSmallIntegerToIntegerInReg(repStartReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, stopReg);
		/* begin AddR:R: */
		genoperandoperand(AddRR, repStartReg, stopReg);
		/* begin CmpR:R: */
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, stopReg);
		/* begin JumpGreater: */
		jumpOutOfBounds4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin MoveCw:R: */
		wordConstant = storeCheckBoundary();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, TempReg));
		/* begin CmpR:R: */
		assert(!((TempReg == SPReg)));
		genoperandoperand(CmpRR, TempReg, arrayReg);
		/* begin JumpBelow: */
		jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
		if (!CheckRememberedInTrampoline) {
			jmpAlreadyRemembered = genCheckRememberedBitOfscratch(arrayReg, TempReg);
		}
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);
		callStoreCheckTrampoline();
		genoperand(PopR, LinkReg);
		jmpTarget(jmpDestYoung, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		if (!CheckRememberedInTrampoline) {
			jmpTarget(jmpAlreadyRemembered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		/* begin genStackArgAt:into: */
		offset1 = (2) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction15 = genoperandoperandoperand(MoveMwrR, offset1, SPReg, stopReg);
		if (usesOutOfLineLiteral(anInstruction15)) {
			(anInstruction15->dependent = locateLiteralsize(offset1, BytesPerOop));
		}
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, repStartReg);
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), repStartReg);
		/* begin AddR:R: */
		genoperandoperand(AddRR, repStartReg, replReg);
		adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(AddCqR, adjust, startReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(AddCqR, adjust, stopReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
		}
		/* begin MoveXwr:R:R: */
		instr = genoperandoperandoperand(MoveXwrRR, startReg, replReg, TempReg);
		/* begin MoveR:Xwr:R: */
		genoperandoperandoperand(MoveRXwrR, TempReg, startReg, arrayReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(AddCqR, 1, startReg);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin CmpR:R: */
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpAboveOrEqual: */
		genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)instr));
		jmpTarget(jumpEmpty, (methodOrBlockNumArgs <= (numRegArgs())
			? (/* begin RetN: */
				genoperand(RetN, 0))
			: (/* begin RetN: */
				genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord))));
		jmpTarget(jumpIncorrectFormat1, jmpTarget(jumpIncorrectFormat2, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	}
	if (maybeMethodClassOfseemsToBeInstantiating(methodObj, firstByteFormat())) {

		/* Are they both byte array format ? CompiledMethod excluded */
		genGetFormatOfinto(arrayReg, TempReg);
		genGetFormatOfinto(replReg, repStartReg);
		/* begin checkQuickConstant:forInstruction: */
		literal2 = firstByteFormat();
		anInstruction7 = genoperandoperand(CmpCqR, firstByteFormat(), repStartReg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(literal2, BytesPerOop));
		}
		/* begin JumpLess: */
		jumpIncorrectFormat1 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		literal3 = firstCompiledMethodFormat();
		anInstruction8 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), repStartReg);
		if (usesOutOfLineLiteral(anInstruction8)) {
			(anInstruction8->dependent = locateLiteralsize(literal3, BytesPerOop));
		}
		/* begin JumpGreaterOrEqual: */
		jumpIncorrectFormat2 = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		literal4 = firstByteFormat();
		anInstruction9 = genoperandoperand(CmpCqR, firstByteFormat(), TempReg);
		if (usesOutOfLineLiteral(anInstruction9)) {
			(anInstruction9->dependent = locateLiteralsize(literal4, BytesPerOop));
		}
		/* begin JumpLess: */
		jumpIncorrectFormat3 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		literal5 = firstCompiledMethodFormat();
		anInstruction10 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
		if (usesOutOfLineLiteral(anInstruction10)) {
			(anInstruction10->dependent = locateLiteralsize(literal5, BytesPerOop));
		}
		/* begin JumpGreaterOrEqual: */
		jumpIncorrectFormat4 = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		genGetNumSlotsOfinto(arrayReg, startReg);
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), startReg);
		gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, startReg);
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin CmpR:R: */
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpGreater: */
		jumpOutOfBounds3 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, repStartReg, TempReg);
		/* begin genStackArgAt:into: */
		offset2 = (0) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction16 = genoperandoperandoperand(MoveMwrR, offset2, SPReg, repStartReg);
		if (usesOutOfLineLiteral(anInstruction16)) {
			(anInstruction16->dependent = locateLiteralsize(offset2, BytesPerOop));
		}
		/* begin genStackArgAt:into: */
		offset3 = (3) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction17 = genoperandoperandoperand(MoveMwrR, offset3, SPReg, startReg);
		if (usesOutOfLineLiteral(anInstruction17)) {
			(anInstruction17->dependent = locateLiteralsize(offset3, BytesPerOop));
		}
		genConvertSmallIntegerToIntegerInReg(startReg);
		genConvertSmallIntegerToIntegerInReg(repStartReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, stopReg);
		/* begin AddR:R: */
		genoperandoperand(AddRR, repStartReg, stopReg);
		genGetNumSlotsOfinto(replReg, startReg);
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, shiftForWord(), startReg);
		gAndCqRR(BytesPerWord - 1, TempReg, TempReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, TempReg, startReg);
		/* begin CmpR:R: */
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpGreater: */
		jumpOutOfBounds4 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
		/* begin genStackArgAt:into: */
		offset4 = (3) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction18 = genoperandoperandoperand(MoveMwrR, offset4, SPReg, startReg);
		if (usesOutOfLineLiteral(anInstruction18)) {
			(anInstruction18->dependent = locateLiteralsize(offset4, BytesPerOop));
		}
		genConvertSmallIntegerToIntegerInReg(startReg);
		/* begin genStackArgAt:into: */
		offset5 = (2) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction19 = genoperandoperandoperand(MoveMwrR, offset5, SPReg, stopReg);
		if (usesOutOfLineLiteral(anInstruction19)) {
			(anInstruction19->dependent = locateLiteralsize(offset5, BytesPerOop));
		}
		genConvertSmallIntegerToIntegerInReg(stopReg);
		/* begin SubR:R: */
		genoperandoperand(SubRR, startReg, repStartReg);
		/* begin AddR:R: */
		genoperandoperand(AddRR, repStartReg, replReg);
		adjust = BaseHeaderSize - 1;
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction5 = genoperandoperand(AddCqR, adjust, startReg);
			if (usesOutOfLineLiteral(anInstruction5)) {
				(anInstruction5->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
			/* begin checkQuickConstant:forInstruction: */
			anInstruction6 = genoperandoperand(AddCqR, adjust, stopReg);
			if (usesOutOfLineLiteral(anInstruction6)) {
				(anInstruction6->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
		}
		/* begin MoveXbr:R:R: */
		instr = genoperandoperandoperand(MoveXbrRR, startReg, replReg, TempReg);
		/* begin MoveR:Xbr:R: */
		genoperandoperandoperand(MoveRXbrR, TempReg, startReg, arrayReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction11 = genoperandoperand(AddCqR, 1, startReg);
		if (usesOutOfLineLiteral(anInstruction11)) {
			(anInstruction11->dependent = locateLiteralsize(1, BytesPerOop));
		}
		/* begin CmpR:R: */
		assert(!((startReg == SPReg)));
		genoperandoperand(CmpRR, startReg, stopReg);
		/* begin JumpAboveOrEqual: */
		genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)instr));
		jmpTarget(jumpEmpty, (methodOrBlockNumArgs <= (numRegArgs())
			? (/* begin RetN: */
				genoperand(RetN, 0))
			: (/* begin RetN: */
				genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord))));
		jmpTarget(jumpIncorrectFormat4, jmpTarget(jumpIncorrectFormat3, jmpTarget(jumpIncorrectFormat2, jmpTarget(jumpIncorrectFormat1, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	}
	if (((result = compileInterpreterPrimitive())) < 0) {
		return result;
	}
	jmpTarget(jumpImm, jmpTarget(jumpNotSmi1, jmpTarget(jumpNotSmi2, jmpTarget(jumpNotSmi3, genoperandoperand(Label, (labelCounter += 1), bytecodePC)))));
	jmpTarget(jumpOutOfBounds1, jmpTarget(jumpOutOfBounds2, jmpTarget(jumpOutOfBounds3, jmpTarget(jumpOutOfBounds4, ((AbstractInstruction *) (((jumpImm->operands))[0]))))));
#  if IMMUTABILITY
	jmpTarget(jumpImmutable, ((AbstractInstruction *) (((jumpImm->operands))[0])));
#  endif
	return CompletePrimitive;
}

	/* CogObjectRepresentationForSpur>>#genSetSmallIntegerTagsIn: */
static sqInt NoDbgRegParms
genSetSmallIntegerTagsIn(sqInt scratchReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(OrCqR, 1, scratchReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(1, BytesPerOop));
	}
	return 0;
}


/*	Create a trampoline to store-check the update of the receiver in a
	closure's outerContext in compileBlockFrameBuild:. */

	/* CogObjectRepresentationForSpur>>#genStoreCheckContextReceiverTrampoline */
static usqInt
genStoreCheckContextReceiverTrampoline(void)
{
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	genStoreCheckReceiverRegvalueRegscratchReginFrame(ReceiverResultReg, Arg0Reg, TempReg, 0);
	/* begin RetN: */
	genoperand(RetN, 0);
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress("ceStoreCheckContextReceiver", startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}


/*	Generate the code for a store check of valueReg into destReg. */

	/* CogObjectRepresentationForSpur>>#genStoreCheckReceiverReg:valueReg:scratchReg:inFrame: */
static sqInt NoDbgRegParms
genStoreCheckReceiverRegvalueRegscratchReginFrame(sqInt destReg, sqInt valueReg, sqInt scratchReg, sqInt inFrame)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *inst;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;
    sqInt wordConstant;


	/* Is value stored an immediate?  If so we're done */
	jmpAlreadyRemembered = ((AbstractInstruction *) 0);

	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(valueReg);
	/* begin MoveCw:R: */
	wordConstant = storeCheckBoundary();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, scratchReg));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, destReg);
	/* begin JumpBelow: */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, valueReg);
	/* begin JumpAboveOrEqual: */
	jmpSourceOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	if (!CheckRememberedInTrampoline) {
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(destReg, scratchReg);
	}
	assert(destReg == ReceiverResultReg);
	/* begin evaluateTrampolineCallBlock:protectLinkRegIfNot: */
	if (inFrame) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin saveAndRestoreLinkRegAround: */
		inst = genoperand(PushR, LinkReg);
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreCheckTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		genoperand(PopR, LinkReg);
	}
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jmpAlreadyRemembered, ((AbstractInstruction *) (((jmpSourceOld->operands))[0])));
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#genStoreSourceReg:slotIndex:destReg:scratchReg:inFrame:needsStoreCheck: */
static sqInt NoDbgRegParms
genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt inFrame, sqInt needsStoreCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;

	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}
	if (needsStoreCheck) {
		return genStoreCheckReceiverRegvalueRegscratchReginFrame(destReg, sourceReg, scratchReg, inFrame);
	}
	return 0;
}


/*	This method is used for unchecked stores in objects after their creation
	(typically, inlined creation of Array, closures and some temp vectors). 
	Currently there is no need to do the immutability check here
 */

	/* CogObjectRepresentationForSpur>>#genStoreSourceReg:slotIndex:intoNewObjectInDestReg: */
static sqInt NoDbgRegParms
genStoreSourceRegslotIndexintoNewObjectInDestReg(sqInt sourceReg, sqInt index, sqInt destReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}
	return 0;
}


/*	Convention:
	- RcvrResultReg holds the object mutated.
	If immutability failure:
	- TempReg holds the instance variable index mutated 
	if instVarIndex > numDedicatedStoreTrampoline
	- ClassReg holds the value to store
	Registers are not lived across this trampoline as the 
	immutability failure may need new stack frames. */

	/* CogObjectRepresentationForSpur>>#genStoreTrampolineCalled:instVarIndex: */
#if IMMUTABILITY
static usqInt NoDbgRegParms
genStoreTrampolineCalledinstVarIndex(char *trampolineName, sqInt instVarIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpImmutable1;
    AbstractInstruction *jumpRC;
    sqInt pushLinkReg;
    sqInt quickConstant;
    usqInt startAddress;

	startAddress = methodZoneBase();
	zeroOpcodeIndex();
	if (CheckRememberedInTrampoline) {
		/* begin genStoreTrampolineCheckingRememberedCalled:instVarIndex: */

		/* Store check */
		/* If on 64-bits and doing the remembered bit test here, we can combine the tests to fetch the header once. */
		jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, SendNumArgsReg);
		/* begin TstCq:R: */
		quickConstant = 1U << (rememberedBitShift());
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(TstCqR, quickConstant, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		/* begin JumpZero: */
		jumpRC = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin RetN: */
		genoperand(RetN, 0);
		jmpTarget(jumpRC, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(remember, 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 1, (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
			? ReceiverResultReg
			: ABIResultReg));
		jmpTarget(jumpImmutable, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceCannotAssignTowithIndexvalueToAssign, 3, ReceiverResultReg, (instVarIndex < (NumStoreTrampolines - 1)
			? (/* begin trampolineArgConstant: */
				assert(instVarIndex >= 0),
				-2 - instVarIndex)
			: TempReg), ClassReg, null, 0 /* emptyRegisterMask */, 1, NoReg);
	}
	else {
		/* begin genStoreTrampolineNotCheckingRememberedCalled:instVarIndex: */
		genSmalltalkToCStackSwitch((pushLinkReg = 1));

		/* Store check */
		jumpImmutable1 = genJumpImmutablescratchReg(ReceiverResultReg, SendNumArgsReg);
		compileCallFornumArgsargargargargresultRegregsToSave(remember, 1, ReceiverResultReg, null, null, null, (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)
			? ReceiverResultReg
			: ABIResultReg), 0 /* emptyRegisterMask */);
		genLoadStackPointers(backEnd());
		genTrampolineReturn(pushLinkReg);
		jmpTarget(jumpImmutable1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		compileCallFornumArgsargargargargresultRegregsToSave(ceCannotAssignTowithIndexvalueToAssign, 3, ReceiverResultReg, (instVarIndex < (NumStoreTrampolines - 1)
			? (/* begin trampolineArgConstant: */
				assert(instVarIndex >= 0),
				-2 - instVarIndex)
			: TempReg), ClassReg, null, NoReg, 0 /* emptyRegisterMask */);
		genLoadStackPointers(backEnd());
		genTrampolineReturn(pushLinkReg);
	}
	outputInstructionsForGeneratedRuntimeAt(startAddress);
	recordGeneratedRunTimeaddress(trampolineName, startAddress);
	recordRunTimeObjectReferences();
	return startAddress;
}
#endif /* IMMUTABILITY */


/*	Store the value in the instance from field 1 to numSlots
	Typically used for for inlined allocations, initializing objects with 0 or
	nil. destReg is referencing the object (oop) and is restored at the end of
	this code.
	If less than 8 fields, use a full unrolled initialization (up to 8 stores)
	If more than 8 fields, use a duff device to initialize with a 8-vectorized
	loop.  */

	/* CogObjectRepresentationForSpur>>#genStoreValue:instance:numSlots: */
static sqInt NoDbgRegParms
genStoreValueinstancenumSlots(sqInt value, sqInt destReg, sqInt numSlots)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    AbstractInstruction *branch;
    sqInt constReg;
    sqInt delta;
    sqInt i;
    AbstractInstruction *inst;
    AbstractInstruction *loop;
    sqInt loopCount;
    sqInt slotsPerIteration;

	branch = ((AbstractInstruction *) 0);
	if (numSlots <= ((slotsPerIteration = 8))) {

		/* slotsPerIteration must be even; see cogit SubCq: objectMemory bytesPerOop R: TempReg below */
		if (shouldAnnotateObjectReference(value)) {
			annotateobjRef(gMoveCwR(value, TempReg), value);
		}
		else {
			/* begin MoveCq:R: */
			anInstruction = genoperandoperand(MoveCqR, value, TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(value, BytesPerOop));
			}
		}
		for (i = 0; i < numSlots; i += 1) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperandoperand(MoveRMwr, TempReg, (i * BytesPerWord) + BaseHeaderSize, destReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize((i * BytesPerWord) + BaseHeaderSize, BytesPerOop));
			}
		}
		return 0;
	}
	constReg = allocateRegNotConflictingWith(((destReg < 0) ? (((usqInt)(1)) >> (-destReg)) : (1ULL << destReg)));
	if (shouldAnnotateObjectReference(value)) {
		annotateobjRef(gMoveCwR(value, constReg), value);
	}
	else {
		/* begin MoveCq:R: */
		anInstruction2 = genoperandoperand(MoveCqR, value, constReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(value, BytesPerOop));
		}
	}
	if ((numSlots % slotsPerIteration) != 0) {

		/* delta maps the offset at the loop entryPoint onto destReg + objectMemory baseHeaderSize */
		delta = ((slotsPerIteration - (numSlots % slotsPerIteration)) * BytesPerOop) - BaseHeaderSize;
		if (delta > 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction3 = genoperandoperand(SubCqR, delta, destReg);
			if (usesOutOfLineLiteral(anInstruction3)) {
				(anInstruction3->dependent = locateLiteralsize(delta, BytesPerOop));
			}
		}
		if (delta < 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction4 = genoperandoperand(AddCqR, -delta, destReg);
			if (usesOutOfLineLiteral(anInstruction4)) {
				(anInstruction4->dependent = locateLiteralsize(-delta, BytesPerOop));
			}
		}
		delta += BaseHeaderSize;
		/* begin Jump: */
		branch = genoperand(Jump, ((sqInt)0));
	}
	else {
		delta = 0;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction6 = genoperandoperand(AddCqR, BaseHeaderSize, destReg);
		if (usesOutOfLineLiteral(anInstruction6)) {
			(anInstruction6->dependent = locateLiteralsize(BaseHeaderSize, BytesPerOop));
		}
	}
	loopCount = ((numSlots + slotsPerIteration) - 1) / slotsPerIteration;
	assert(loopCount > 1);
	/* begin Label */
	loop = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (i = 0; i <= 7; i += 1) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction7 = genoperandoperandoperand(MoveRMwr, constReg, i * BytesPerOop, destReg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(i * BytesPerOop, BytesPerOop));
		}
		inst = anInstruction7;
		if ((slotsPerIteration - (numSlots % slotsPerIteration)) == i) {
			jmpTarget(branch, inst);
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction8 = genoperandoperand(AddCqR, slotsPerIteration * BytesPerOop, destReg);
	if (usesOutOfLineLiteral(anInstruction8)) {
		(anInstruction8->dependent = locateLiteralsize(slotsPerIteration * BytesPerOop, BytesPerOop));
	}
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, destReg);
	/* begin JumpBelow: */
	genConditionalBranchoperand(JumpBelow, ((sqInt)loop));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(SubCqR, (((loopCount * slotsPerIteration) * BytesPerOop) + BaseHeaderSize) - delta, destReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize((((loopCount * slotsPerIteration) * BytesPerOop) + BaseHeaderSize) - delta, BytesPerOop));
	}
	return 0;
}


/*	Store check code is duplicated to use a single trampoline */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityAndStoreCheckSourceReg:slotIndex:destReg:scratchReg:needRestoreRcvr: */
#if IMMUTABILITY
static sqInt NoDbgRegParms
genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *immutableJump;
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;
    sqInt wordConstant;

	jmpAlreadyRemembered = ((AbstractInstruction *) 0);
	immutableJump = genJumpImmutablescratchReg(destReg, scratchReg);
	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}

	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(sourceReg);
	/* begin MoveCw:R: */
	wordConstant = storeCheckBoundary();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, scratchReg));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, destReg);
	/* begin JumpBelow: */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, sourceReg);
	/* begin JumpAboveOrEqual: */
	jmpSourceOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	if (!CheckRememberedInTrampoline) {
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(destReg, scratchReg);
	}
	jmpTarget(immutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genStoreTrampolineCall: */
	assert(IMMUTABILITY);
	if (index >= (NumStoreTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, index, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(index, BytesPerOop));
		}
		/* begin CallRT: */
		abstractInstruction3 = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
		(abstractInstruction3->annotation = IsRelativeCall);
	}
	else {
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreTrampolines[index]);
		(abstractInstruction1->annotation = IsRelativeCall);
	}
	/* begin annotateBytecode: */
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction2->annotation = HasBytecodePC);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	if (needRestoreRcvr) {
		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
	}
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jmpAlreadyRemembered, ((AbstractInstruction *) (((jmpSourceOld->operands))[0])));
	}
	return 0;
}
#endif /* IMMUTABILITY */


/*	Gen an immutability check with no store check (e.g. assigning an immediate
	literal) 
 */
/*	imm check has its own trampoline */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityButNoStoreCheckSourceReg:slotIndex:destReg:scratchReg:needRestoreRcvr: */
#if IMMUTABILITY
static sqInt NoDbgRegParms
genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needRestoreRcvr)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *immutabilityFailure;
    AbstractInstruction *mutableJump;

	mutableJump = genJumpMutablescratchReg(destReg, scratchReg);
	/* begin genStoreTrampolineCall: */
	assert(IMMUTABILITY);
	if (index >= (NumStoreTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, index, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(index, BytesPerOop));
		}
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceStoreTrampolines[index]);
		(abstractInstruction1->annotation = IsRelativeCall);
	}
	/* begin annotateBytecode: */
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction2->annotation = HasBytecodePC);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	if (needRestoreRcvr) {
		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
	}
	/* begin Jump: */
	immutabilityFailure = genoperand(Jump, ((sqInt)0));
	jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin genTraceStores */
	if (traceStores > 0) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, ClassReg, TempReg);
		/* begin CallRT: */
		abstractInstruction3 = genoperand(Call, ceTraceStoreTrampoline);
		(abstractInstruction3->annotation = IsRelativeCall);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveRMwr, sourceReg, (index * BytesPerWord) + BaseHeaderSize, destReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize((index * BytesPerWord) + BaseHeaderSize, BytesPerOop));
	}
	jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}
#endif /* IMMUTABILITY */


/*	We know there is a frame as immutability check requires a frame */
/*	needRestoreRcvr has to be true to keep RcvrResultReg live with the
	receiver in it across the trampoline
 */
/*	Trampoline convention... */

	/* CogObjectRepresentationForSpur>>#genStoreWithImmutabilityCheckSourceReg:slotIndex:destReg:scratchReg:needsStoreCheck:needRestoreRcvr: */
#if IMMUTABILITY
static sqInt NoDbgRegParms
genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(sqInt sourceReg, sqInt index, sqInt destReg, sqInt scratchReg, sqInt needsStoreCheck, sqInt needRestoreRcvr)
{
	assert(destReg == ReceiverResultReg);
	assert(scratchReg == TempReg);
	assert(sourceReg == ClassReg);
	if (needsStoreCheck) {
		genStoreWithImmutabilityAndStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sourceReg, index, destReg, scratchReg, needRestoreRcvr);
	}
	else {
		genStoreWithImmutabilityButNoStoreCheckSourceRegslotIndexdestRegscratchRegneedRestoreRcvr(sourceReg, index, destReg, scratchReg, needRestoreRcvr);
	}
	return 0;
}
#endif /* IMMUTABILITY */

	/* CogObjectRepresentationForSpur>>#genVarIndexCallStoreTrampoline */
static void
genVarIndexCallStoreTrampoline(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;

	assert(IMMUTABILITY);
#  if IMMUTABILITY
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin annotateBytecode: */
	abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction1->annotation = HasBytecodePC);
#  endif // IMMUTABILITY
}


/*	Make sure SendNumArgsReg and ClassReg are available in addition to
	ReceiverResultReg and TempReg in
	genGetActiveContextNumArgs:large:inBlock:. 
 */

	/* CogObjectRepresentationForSpur>>#getActiveContextAllocatesInMachineCode */
static sqInt
getActiveContextAllocatesInMachineCode(void)
{
	return 1;
}


/*	Since all cache tags in Spur are class indices none of
	them are young or have to be updated in a scavenge. */

	/* CogObjectRepresentationForSpur>>#inlineCacheTagIsYoung: */
static sqInt NoDbgRegParms
inlineCacheTagIsYoung(sqInt cacheTag)
{
	return 0;
}

	/* CogObjectRepresentationForSpur>>#jumpNotCharacterUnsignedValueInRegister: */
static AbstractInstruction * NoDbgRegParms
jumpNotCharacterUnsignedValueInRegister(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt quickConstant;

	/* begin CmpCq:R: */
	quickConstant = (1U << (numCharacterBits())) - 1;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, reg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpAbove: */
	return genConditionalBranchoperand(JumpAbove, ((sqInt)0));
}


/*	Mark and trace a literal in a machine code instruction preceding address
	in cogMethodOrNil.
	Answer if code was modified. */

	/* CogObjectRepresentationForSpur>>#markAndTraceLiteral:in:atpc: */
static sqInt NoDbgRegParms
markAndTraceLiteralinatpc(sqInt literal, CogMethod *cogMethodOrNil, usqInt address)
{
    sqInt objOop;

	if (!(couldBeObject(literal))) {
		return 0;
	}
	assert(addressCouldBeObj(literal));
	if (!(isForwarded(literal))) {
		markAndTrace(literal);
		return 0;
	}
	/* begin setCodeModified */
#  if DUAL_MAPPED_CODE_ZONE
	codeModified = 1;
#  else
	if (!codeModified) {
		codeModified = 1;
		/* begin makeCodeZoneWritable */
#    if __APPLE__ && __MACH__
		pthread_jit_write_protect_np(0);
		PJWPNClear = __LINE__;
		if (PJWPNState) {
			PJWPNChange = __LINE__;
			PJWPNState = 0;
		}
#    endif // __APPLE__ && __MACH__
	}
#  endif // DUAL_MAPPED_CODE_ZONE
	objOop = followForwarded(literal);
	storeLiteralbeforeFollowingAddress(backEnd(), objOop, address);
	markAndTraceUpdatedLiteralin(objOop, cogMethodOrNil);
	return 1;
}


/*	Mark and trace a literal in a sqInt variable of cogMethod. */

	/* CogObjectRepresentationForSpur>>#markAndTraceLiteral:in:at: */
static void NoDbgRegParms
markAndTraceLiteralinat(sqInt literal, CogMethod *cogMethod, sqInt *address)
{
    sqInt objOop;

	if (!(couldBeObject(literal))) {
		return;
	}
	assert(addressCouldBeObj(literal));
	if (!(isForwarded(literal))) {
		markAndTrace(literal);
		return;
	}
	objOop = followForwarded(literal);
	address[0] = objOop;
	markAndTraceUpdatedLiteralin(objOop, cogMethod);
}


/*	Common code to mark a literal in cogMethod and add
	the cogMethod to youngReferrers if the literal is young. */

	/* CogObjectRepresentationForSpur>>#markAndTraceUpdatedLiteral:in: */
static void NoDbgRegParms
markAndTraceUpdatedLiteralin(sqInt objOop, CogMethod *cogMethodOrNil)
{
	if (isNonImmediate(objOop)) {
		if ((cogMethodOrNil != null)
		 && (isYoungObject(objOop))) {
			ensureInYoungReferrers(cogMethodOrNil);
		}
		markAndTrace(objOop);
	}
}


/*	If primIndex has an accessorDepth and fails, or it is external and fails
	with PrimErrNoMemory,
	call ceCheckAndMaybeRetryPrimitive if so If ceCheck.... answers true,
	retry the primitive. */

	/* CogObjectRepresentationForSpur>>#maybeCompileRetryOf:onPrimitiveFail:flags: */
static sqInt NoDbgRegParms
maybeCompileRetryOfonPrimitiveFailflags(void (*primitiveRoutine)(void), sqInt primIndex, sqInt flags)
{
    sqInt address;
    sqInt address1;
    sqInt address2;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jmp;

	if ((accessorDepthForPrimitiveIndex(primIndex)) >= 0) {
		/* begin MoveAw:R: */
		address = primFailCodeAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		jmp = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	}
	else {
		if (PrimNumberExternalCall != primIndex) {
			return 0;
		}
		/* begin MoveAw:R: */
		address1 = primFailCodeAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address1, genoperandoperand(MoveAwR, address1, TempReg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, PrimErrNoMemory, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(PrimErrNoMemory, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	}
	if (!(((flags & PrimCallNeedsNewMethod) != 0))) {
		genLoadNewMethod();
	}
	/* begin MoveCw:R: */
	checkLiteralforInstruction(((sqInt)primitiveRoutine), genoperandoperand(MoveCwR, ((sqInt)primitiveRoutine), TempReg));
	/* begin MoveR:Aw: */
	address2 = primitiveFunctionPointerAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address2, genoperandoperand(MoveRAw, TempReg, address2));
	compileCallFornumArgsargargargargresultRegregsToSave(ceCheckAndMaybeRetryPrimitive, 1, (assert(primIndex >= 0),
	-2 - primIndex), null, null, null, TempReg, 0 /* emptyRegisterMask */);
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Generate a shift of the register containing the class tag in a method
	cache probe.
	c.f. SpurMemoryManager>>methodCacheHashOf:with: */

	/* CogObjectRepresentationForSpur>>#maybeShiftClassTagRegisterForMethodCacheProbe: */
static sqInt NoDbgRegParms
maybeShiftClassTagRegisterForMethodCacheProbe(sqInt classTagReg)
{
	/* begin LogicalShiftLeftCq:R: */
	genoperandoperand(LogicalShiftLeftCqR, 2, classTagReg);
	return 0;
}

	/* CogObjectRepresentationForSpur>>#mixed:branchIf:instanceOfBehaviors:target: */
static sqInt NoDbgRegParms
mixedbranchIfinstanceOfBehaviorstarget(sqInt numNonImmediates, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt classIndex;
    sqInt classObj;
    sqInt classObj1;
    sqInt i;
    sqInt i1;
    sqInt iLimiT;
    sqInt index;
    sqInt j;
    AbstractInstruction *jmp;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpImmediate1;
    usqInt numCases;
    sqInt tag1;
    sqInt tag2;

	jmpImmediate1 = ((AbstractInstruction *) 0);
	tag1 = 0;
	tag2 = 0;
	numCases = numSlotsOf(arrayObj);

	/* Rcvr is non immediate */
	jmpImmediate = genJumpImmediate(reg);
	genGetClassIndexOfNonImminto(reg, TempReg);
	index = 0;
	for (i = 0; i < numCases; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		if (!(isImmediateClass(classObj))) {
			genCmpClassIndexR(classTagForClass(classObj), TempReg);
			/* begin JumpZero: */
			genConditionalBranchoperand(JumpZero, ((sqInt)targetFixUp));
			index += 1;
		}
	}
	/* begin Jump: */
	jmp = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	switch (numCases - numNonImmediates) {
	case 1:
		
		/* 1 immediate needs to jump. Find it and jump. */
		for (j = 0; j < numCases; j += 1) {
			classObj = fetchPointerofObject(j, arrayObj);
			if (isImmediateClass(classObj)) {
				/* begin branchIf:hasImmediateTag:target: */
				classIndex = classTagForClass(classObj);
				if (classIndex == (smallIntegerTag())) {
					jmpImmediate1 = genJumpSmallInteger(reg);
				}
				if (classIndex == (characterTag())) {
					jmpImmediate1 = genJumpCharacter(reg);
				}
				if (classIndex == (smallFloatTag())) {
					jmpImmediate1 = genJumpSmallFloat(reg);
				}
				jmpTarget(jmpImmediate1, targetFixUp);
			}
		}
		break;
	case 2:
		
		/* 2 immediates needs to jump */
		/* begin branch2CasesIf:instanceOfBehaviors:target: */
		for (i1 = 0, iLimiT = ((numSlotsOf(arrayObj)) - 1); i1 <= iLimiT; i1 += 1) {
			classObj1 = fetchPointerofObject(i1, arrayObj);
			if (isImmediateClass(classObj1)) {
				if (tag1 == null) {
					tag1 = classTagForClass(classObj1);
				}
				else {
					tag2 = classTagForClass(classObj1);
				}
			}
		}
		if ((tag1 == (smallIntegerTag()))
		 || (tag2 == (smallIntegerTag()))) {
			jmpTarget(genJumpSmallIntegerInScratchReg(TempReg), targetFixUp);
		}
		if ((tag1 == (characterTag()))
		 || (tag2 == (characterTag()))) {
			jmpTarget(genJumpCharacterInScratchReg(TempReg), targetFixUp);
		}
		if ((tag1 == (smallFloatTag()))
		 || (tag2 == (smallFloatTag()))) {
			jmpTarget(genJumpSmallFloatInScratchReg(TempReg), targetFixUp);
		}
		break;
	case 3:
		
		/* all 3 needs to jump */
		/* begin Jump: */
		genoperand(Jump, ((sqInt)targetFixUp));
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* CogObjectRepresentationForSpur>>#mixed:branchIf:notInstanceOfBehaviors:target: */
static sqInt NoDbgRegParms
mixedbranchIfnotInstanceOfBehaviorstarget(sqInt numNonImmediates, sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    AbstractInstruction *anInstruction;
    sqInt classIndex;
    sqInt classObj;
    sqInt classObj1;
    sqInt i;
    sqInt i1;
    sqInt iLimiT;
    sqInt incorrectTag;
    sqInt index;
    sqInt j;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpImmediate1;
    AbstractInstruction **jumps;
    AbstractInstruction *label;
    usqInt numCases;
    sqInt tag1;
    sqInt tag2;

	jmpImmediate1 = ((AbstractInstruction *) 0);
	tag1 = 0;
	tag2 = 0;
	numCases = numSlotsOf(arrayObj);

	/* Rcvr is non immediate */
	jmpImmediate = genJumpImmediate(reg);
	jumps = allocatype(numNonImmediates, AbstractInstruction *);
	genGetClassIndexOfNonImminto(reg, TempReg);
	index = 0;
	for (i = 0; i < numCases; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		if (!(isImmediateClass(classObj))) {
			genCmpClassIndexR(classTagForClass(classObj), TempReg);
			jumps[index] = (genConditionalBranchoperand(JumpZero, ((sqInt)0)));
			index += 1;
		}
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)targetFixUp));
	jmpTarget(jmpImmediate, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	switch (numCases - numNonImmediates) {
	case 1:
		
		/* 1 immediate allowed. jump to targetFixUp if the rcvr is not this immediate */
		for (j = 0; j < numCases; j += 1) {
			classObj = fetchPointerofObject(j, arrayObj);
			if (isImmediateClass(classObj)) {
				/* begin branchIf:hasNotImmediateTag:target: */
				classIndex = classTagForClass(classObj);
				if (classIndex == (smallIntegerTag())) {
					jmpImmediate1 = genJumpNotSmallInteger(reg);
				}
				if (classIndex == (characterTag())) {
					jmpImmediate1 = genJumpNotCharacter(reg);
				}
				if (classIndex == (smallFloatTag())) {
					jmpImmediate1 = genJumpNotSmallFloat(reg);
				}
				jmpTarget(jmpImmediate1, targetFixUp);
			}
		}
		break;
	case 2:
		
		/* 2 immediates allowed. On 32 bits nothing to do, all immediate are allowed, on 64 bits generates the jump to fixup for the third tag */
		/* begin branch2CasesIf:notInstanceOfBehaviors:target: */
		for (i1 = 0, iLimiT = ((numSlotsOf(arrayObj)) - 1); i1 <= iLimiT; i1 += 1) {
			classObj1 = fetchPointerofObject(i1, arrayObj);
			if (isImmediateClass(classObj1)) {
				if (tag1 == null) {
					tag1 = classTagForClass(classObj1);
				}
				else {
					tag2 = classTagForClass(classObj1);
				}
			}
		}
		/* begin fetchImmediateTagOtherThanTag1:tag2: */
		if (!((tag1 == (characterTag()))
			 || (tag2 == (characterTag())))) {
			incorrectTag = characterTag();
			goto l5;
		}
		if (!((tag1 == (smallIntegerTag()))
			 || (tag2 == (smallIntegerTag())))) {
			incorrectTag = smallIntegerTag();
			goto l5;
		}
		incorrectTag = smallFloatTag();
	l5:	/* end fetchImmediateTagOtherThanTag1:tag2: */;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, incorrectTag, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(incorrectTag, BytesPerOop));
		}
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)(((void *) targetFixUp))));
		break;
	case 3:
		
		/* nothing to do, all immediates are allowed. */
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	/* begin Label */
	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (i = 0; i < numNonImmediates; i += 1) {
		jmpTarget(jumps[i], label);
	}
	return 0;
}


/*	All classes in arrayObj are not immediate */

	/* CogObjectRepresentationForSpur>>#noneImmediateBranchIf:instanceOfBehaviors:target: */
static sqInt NoDbgRegParms
noneImmediateBranchIfinstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt classObj;
    sqInt i;
    sqInt iLimiT;
    AbstractInstruction *jmp;

	jmp = genJumpImmediate(reg);
	genGetClassIndexOfNonImminto(reg, TempReg);
	for (i = 0, iLimiT = ((numSlotsOf(arrayObj)) - 1); i <= iLimiT; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		genCmpClassIndexR(classTagForClass(classObj), TempReg);
		/* begin JumpZero: */
		genConditionalBranchoperand(JumpZero, ((sqInt)targetFixUp));
	}
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	All classes in arrayObj are not immediate */

	/* CogObjectRepresentationForSpur>>#noneImmediateBranchIf:notInstanceOfBehaviors:target: */
static sqInt NoDbgRegParms
noneImmediateBranchIfnotInstanceOfBehaviorstarget(sqInt reg, sqInt arrayObj, AbstractInstruction *targetFixUp)
{
    sqInt classObj;
    sqInt i;
    sqInt iLimiT;
    AbstractInstruction **jumps;
    AbstractInstruction *label;
    usqInt numJumps;

	jumps = allocatype(numSlotsOf(arrayObj), AbstractInstruction *);
	jmpTarget(genJumpImmediate(reg), targetFixUp);
	genGetClassIndexOfNonImminto(reg, TempReg);
	for (i = 0, iLimiT = (((numJumps = numSlotsOf(arrayObj))) - 1); i <= iLimiT; i += 1) {
		classObj = fetchPointerofObject(i, arrayObj);
		genCmpClassIndexR(classTagForClass(classObj), TempReg);
		jumps[i] = (genConditionalBranchoperand(JumpZero, ((sqInt)0)));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)targetFixUp));
	/* begin Label */
	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	for (i = 0; i < numJumps; i += 1) {
		jmpTarget(jumps[i], label);
	}
	return 0;
}

	/* CogObjectRepresentationForSpur>>#numCharacterBits */
static sqInt
numCharacterBits(void)
{
	return 30;
}

	/* CogObjectRepresentationForSpur>>#remapObject: */
static sqInt NoDbgRegParms
remapObject(sqInt objOop)
{
	assert(addressCouldBeObj(objOop));
	return (shouldRemapObj(objOop)
		? remapObj(objOop)
		: objOop);
}

	/* CogObjectRepresentationForSpur>>#remapOop: */
static sqInt NoDbgRegParms
remapOop(sqInt objOop)
{
	return (shouldRemapOop(objOop)
		? remapObj(objOop)
		: objOop);
}

	/* CogObjectRepresentationForSpur>>#resetCountersIn: */
void
resetCountersIn(CogMethod *cogMethod)
{
	fillInCountersatStartAddress(numCountersFor((cogMethod->counters)), (cogMethod->counters));
}


/*	Objects in newSpace or oldSpace except nil, true, false &
	classTableRootObj need to be annotated.
 */

	/* CogObjectRepresentationForSpur>>#shouldAnnotateObjectReference: */
static sqInt NoDbgRegParms
shouldAnnotateObjectReference(sqInt anOop)
{
	return (isNonImmediate(anOop))
	 && ((oopisGreaterThan(anOop, classTableRootObj()))
	 || (oopisLessThan(anOop, nilObject())));
}

	/* CogObjectRepresentationForSpur>>#slotOffsetOfInstVarIndex: */
static sqInt NoDbgRegParms
slotOffsetOfInstVarIndex(sqInt index)
{
	return (index * BytesPerWord) + BaseHeaderSize;
}

	/* CogObjectRepresentationForSpur>>#valueOfAssociation: */
static sqInt NoDbgRegParms
valueOfAssociation(sqInt associationOop)
{
    sqInt association;

	association = associationOop;
	if (isForwarded(association)) {
		association = followForwarded(association);
	}
	return fetchPointerofObject(ValueIndex, association);
}

	/* CogSimStackEntry>>#ensureSpilledAt:from: */
static SimStackEntry * NoDbgRegParms
ensureSpilledAtfrom(SimStackEntry * self_in_ensureSpilledAtfrom, sqInt baseOffset, sqInt baseRegister)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *inst;
    sqInt literal;
    sqInt reg;
    sqInt wordConstant;

	if ((self_in_ensureSpilledAtfrom->spilled)) {
		if (((self_in_ensureSpilledAtfrom->type)) == SSSpill) {
			assert(((((self_in_ensureSpilledAtfrom->offset)) == baseOffset)
			 && (((self_in_ensureSpilledAtfrom->registerr)) == baseRegister))
			 || (violatesEnsureSpilledSpillAssert()));
			return self_in_ensureSpilledAtfrom;
		}
	}
	assert(((self_in_ensureSpilledAtfrom->type)) != SSSpill);
	traceSpill(self_in_ensureSpilledAtfrom);
	if (((self_in_ensureSpilledAtfrom->type)) == SSConstant) {
		if (shouldAnnotateObjectReference((self_in_ensureSpilledAtfrom->constant))) {
			inst = annotateobjRef(gPushCw((self_in_ensureSpilledAtfrom->constant)), (self_in_ensureSpilledAtfrom->constant));
		}
		else {
			/* begin PushCq: */
			wordConstant = (self_in_ensureSpilledAtfrom->constant);
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperand(PushCq, wordConstant);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(wordConstant, BytesPerOop));
			}
			inst = anInstruction;
		}
	}
	else {
		if (((self_in_ensureSpilledAtfrom->type)) == SSBaseOffset) {
			/* begin checkQuickConstant:forInstruction: */
			literal = (self_in_ensureSpilledAtfrom->offset);
			anInstruction1 = genoperandoperandoperand(MoveMwrR, (self_in_ensureSpilledAtfrom->offset), (self_in_ensureSpilledAtfrom->registerr), TempReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
			}
			/* begin PushR: */
			inst = genoperand(PushR, TempReg);
		}
		else {
			assert(((self_in_ensureSpilledAtfrom->type)) == SSRegister);
			/* begin PushR: */
			reg = (self_in_ensureSpilledAtfrom->registerr);
			inst = genoperand(PushR, reg);
		}
		(self_in_ensureSpilledAtfrom->type) = SSSpill;
		(self_in_ensureSpilledAtfrom->offset) = baseOffset;
		(self_in_ensureSpilledAtfrom->registerr) = baseRegister;
	}
	(self_in_ensureSpilledAtfrom->spilled) = 1;
	return 0;
}

	/* CogSimStackEntry>>#isSameEntryAs: */
static sqInt NoDbgRegParms
isSameEntryAs(SimStackEntry * self_in_isSameEntryAs, CogSimStackEntry *ssEntry)
{
	return (((self_in_isSameEntryAs->type)) == ((ssEntry->type)))
	 && ((((((self_in_isSameEntryAs->type)) == SSBaseOffset)
	 || (((self_in_isSameEntryAs->type)) == SSSpill))
	 && ((((self_in_isSameEntryAs->offset)) == ((ssEntry->offset)))
	 && (((self_in_isSameEntryAs->registerr)) == ((ssEntry->registerr)))))
	 || (((((self_in_isSameEntryAs->type)) == SSRegister)
	 && (((self_in_isSameEntryAs->registerr)) == ((ssEntry->registerr))))
	 || ((((self_in_isSameEntryAs->type)) == SSConstant)
	 && (((self_in_isSameEntryAs->constant)) == ((ssEntry->constant))))));
}


/*	Receiver is not a forwarder, except in blocks with no inst var access.
	For now we optimize only the case where receiver is accessed in a method. */

	/* CogSimStackEntry>>#mayBeAForwarder */
static sqInt NoDbgRegParms
mayBeAForwarder(SimStackEntry * self_in_mayBeAForwarder)
{
	if ((((self_in_mayBeAForwarder->type)) == SSRegister)
	 && (isNonForwarderReceiver((self_in_mayBeAForwarder->registerr)))) {
		return 0;
	}
	return ((self_in_mayBeAForwarder->type)) != SSConstant;
}

	/* CogSimStackEntry>>#popToReg: */
static void NoDbgRegParms
popToReg(SimStackEntry * self_in_popToReg, sqInt reg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;
    sqInt quickConstant;
    sqInt reg1;

	if ((self_in_popToReg->spilled)) {
		/* begin PopR: */
		genoperand(PopR, reg);
	}
	else {
		switch ((self_in_popToReg->type)) {
		case SSBaseOffset:
			/* begin checkQuickConstant:forInstruction: */
			literal = (self_in_popToReg->offset);
			anInstruction = genoperandoperandoperand(MoveMwrR, (self_in_popToReg->offset), (self_in_popToReg->registerr), reg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
			}
			break;
		case SSConstant:
			if (shouldAnnotateObjectReference((self_in_popToReg->constant))) {
				annotateobjRef(gMoveCwR((self_in_popToReg->constant), reg), (self_in_popToReg->constant));
			}
			else {
				/* begin MoveCq:R: */
				quickConstant = (self_in_popToReg->constant);
				/* begin checkQuickConstant:forInstruction: */
				anInstruction1 = genoperandoperand(MoveCqR, quickConstant, reg);
				if (usesOutOfLineLiteral(anInstruction1)) {
					(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
				}
			}
			break;
		case SSRegister:
			if (reg != ((self_in_popToReg->registerr))) {
				/* begin MoveR:R: */
				reg1 = (self_in_popToReg->registerr);
				genoperandoperand(MoveRR, reg1, reg);
			}
			else {
				/* begin Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			break;
		default:
			error("Case not found and no otherwise clause");
		}
	}
}


/*	Answer a bit mask for the receiver's register, if any. */

	/* CogSimStackEntry>>#registerMask */
static sqInt NoDbgRegParms
registerMask(SimStackEntry * self_in_registerMask)
{
    sqInt reg;

	return ((((self_in_registerMask->type)) == SSBaseOffset)
	 || (((self_in_registerMask->type)) == SSRegister)
		? (/* begin registerMaskFor: */
			(reg = (self_in_registerMask->registerr)),
			((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))
		: 0);
}

	/* CogSimStackEntry>>#registerMaskOrNone */
static sqInt NoDbgRegParms
registerMaskOrNone(SimStackEntry * self_in_registerMaskOrNone)
{
    sqInt reg;

	return (((self_in_registerMaskOrNone->type)) == SSRegister
		? (/* begin registerMaskFor: */
			(reg = (self_in_registerMaskOrNone->registerr)),
			((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))
		: 0);
}

	/* CogSimStackEntry>>#registerOrNone */
static sqInt NoDbgRegParms
registerOrNone(SimStackEntry * self_in_registerOrNone)
{
	return (((self_in_registerOrNone->type)) == SSRegister
		? (self_in_registerOrNone->registerr)
		: NoReg);
}

	/* CogSimStackEntry>>#storeToReg: */
static void NoDbgRegParms
storeToReg(SimStackEntry * self_in_storeToReg, sqInt reg)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt literal;
    sqInt quickConstant;
    sqInt reg1;

	switch ((self_in_storeToReg->type)) {
	case SSBaseOffset:
	case SSSpill:
		/* begin checkQuickConstant:forInstruction: */
		literal = (self_in_storeToReg->offset);
		anInstruction = genoperandoperandoperand(MoveMwrR, (self_in_storeToReg->offset), (self_in_storeToReg->registerr), reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
		}
		break;
	case SSConstant:
		if (shouldAnnotateObjectReference((self_in_storeToReg->constant))) {
			annotateobjRef(gMoveCwR((self_in_storeToReg->constant), reg), (self_in_storeToReg->constant));
		}
		else {
			/* begin MoveCq:R: */
			quickConstant = (self_in_storeToReg->constant);
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(MoveCqR, quickConstant, reg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
			}
		}
		break;
	case SSRegister:
		if (reg != ((self_in_storeToReg->registerr))) {
			/* begin MoveR:R: */
			reg1 = (self_in_storeToReg->registerr);
			genoperandoperand(MoveRR, reg1, reg);
		}
		else {
			/* begin Label */
			genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
		break;
	default:
		error("Case not found and no otherwise clause");
	}
}

	/* CogSSBytecodeFixup>>#isMergeFixup */
static sqInt NoDbgRegParms
isMergeFixup(BytecodeFixup * self_in_isMergeFixup)
{
	return (((usqInt)((self_in_isMergeFixup->targetInstruction)))) == NeedsMergeFixupFlag;
}


/*	Allocate an unsharable Literal instruction for the literal and answer it. */

	/* OutOfLineLiteralsManager>>#allocateLiteral: */
static AbstractInstruction * NoDbgRegParms
allocateLiteral(sqInt aLiteral)
{
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt iLimiT;
    sqInt initialNumLiterals;
    AbstractInstruction *litInst;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;

	if (nextLiteralIndex >= literalsSize) {
		/* begin allocateLiterals: */
		initialNumLiterals = literalsSize + 8;
		if (initialNumLiterals > literalsSize) {

			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (!(literals == null)) {
				for (i = 0; i < nextLiteralIndex; i += 1) {
					existingInst = literalInstructionAt(i);
					newInst = (&(newLiterals[i]));
					cloneLiteralFrom(newInst, existingInst);
					assert(((existingInst->dependent)) == null);
					(existingInst->dependent = newInst);
				}
				for (i = 0, iLimiT = (opcodeIndex - 1); i <= iLimiT; i += 1) {
					existingInst = abstractInstructionAt(i);
					if ((((existingInst->dependent)) != null)
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	litInst = literalInstructionAt(nextLiteralIndex);
	initializeUniqueLiteral(litInst, aLiteral);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}
	return litInst;
}

	/* OutOfLineLiteralsManager>>#checkQuickConstant:forInstruction: */
static AbstractInstruction * NoDbgRegParms
checkQuickConstantforInstruction(sqInt literal, AbstractInstruction *anInstruction)
{
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return anInstruction;
}


/*	A literal is in range if its opcode index is within
	outOfLineLiteralOpcodeLimit, or if its index has yet to be assigned. */

	/* OutOfLineLiteralsManager>>#literalInstructionInRange: */
static sqInt NoDbgRegParms
literalInstructionInRange(AbstractInstruction *litInst)
{
    sqInt opcodeIdx;

	/* begin literalOpcodeIndex */
	assert(((litInst->opcode)) == Literal);
	opcodeIdx = ((sqInt)(((litInst->operands))[2]));
	return ((((sqInt)opcodeIdx)) < 0)
	 || ((assert((getOpcodeIndex()) >= opcodeIdx),
	(opcodeIndex - opcodeIdx) < (outOfLineLiteralOpcodeLimit(backEnd()))));
}

	/* OutOfLineLiteralsManager>>#mustDumpLiterals: */
static sqInt NoDbgRegParms
mustDumpLiterals(sqInt currentOpcodeIndex)
{
	return (currentOpcodeIndex >= firstOpcodeIndex)
	 && ((currentOpcodeIndex - firstOpcodeIndex) >= (outOfLineLiteralOpcodeLimit(backEnd())));
}

	/* OutOfLineLiteralsManagerFor64Bits>>#checkLiteral32:forInstruction: */
static AbstractInstruction * NoDbgRegParms
checkLiteral32forInstruction(sqInt literal, AbstractInstruction *anInstruction)
{
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, 4));
	}
	return anInstruction;
}

	/* OutOfLineLiteralsManagerFor64Bits>>#checkLiteral:forInstruction: */
static AbstractInstruction * NoDbgRegParms
checkLiteralforInstruction(sqInt literal, AbstractInstruction *anInstruction)
{
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	return anInstruction;
}


/*	Output all pending literal instructions, making the originals dependents
	on the generated ones
	so that a later pass will copy the address of each generated literal inst
	to its original in literals,
	and hence allow the instruction using the literal to compute the correct
	address. 
	Override to segregate 64-bit and 32-bit literals */

	/* OutOfLineLiteralsManagerFor64Bits>>#dumpLiterals: */
static sqInt NoDbgRegParms
dumpLiterals(sqInt generateBranchAround)
{
    AbstractInstruction * cascade0;
    AbstractInstruction * cascade1;
    sqInt i;
    sqInt index;
    sqInt index1;
    AbstractInstruction *jump;
    AbstractInstruction *litInst;

	jump = ((AbstractInstruction *) 0);
	if (generateBranchAround) {
		/* begin Jump: */
		jump = genoperand(Jump, ((sqInt)0));
	}
	genoperand(AlignmentNops, 8 /* literalAlignment */);
	for (i = lastDumpedLiteralIndex; i < nextLiteralIndex; i += 1) {
		litInst = literalInstructionAt(i);
		if (((assert(((litInst->opcode)) == Literal),
		((((litInst->operands))[1]) == null
				? BytesPerOop
				: (((((litInst->operands))[1])) >> 1) & 15))) == 8) {
			cascade0 = genoperand(Literal, ((litInst->operands))[0]);
			(cascade0->dependent = litInst);
			setLiteralSize(cascade0, 8);
			/* begin setLiteralOpcodeIndex: */
			index = opcodeIndex;
			assert(((litInst->opcode)) == Literal);
			((litInst->operands))[2] = index;
		}
	}
	for (i = lastDumpedLiteralIndex; i < nextLiteralIndex; i += 1) {
		litInst = literalInstructionAt(i);
		if (((assert(((litInst->opcode)) == Literal),
		((((litInst->operands))[1]) == null
				? BytesPerOop
				: (((((litInst->operands))[1])) >> 1) & 15))) == 4) {
			cascade1 = genoperand(Literal, ((litInst->operands))[0]);
			(cascade1->dependent = litInst);
			setLiteralSize(cascade1, 4);
			/* begin setLiteralOpcodeIndex: */
			index1 = opcodeIndex;
			assert(((litInst->opcode)) == Literal);
			((litInst->operands))[2] = index1;
		}
	}
	if (generateBranchAround) {
		jmpTarget(jump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	/* begin getOpcodeIndex */
	firstOpcodeIndex = opcodeIndex;
	lastDumpedLiteralIndex = nextLiteralIndex;
	return 0;
}


/*	Search for a Literal instruction that is in-range and answer it. Otherwise
	allocate a new sharable Literal instruction for the literal and answer it. */

	/* OutOfLineLiteralsManagerFor64Bits>>#locateLiteral:size: */
static AbstractInstruction * NoDbgRegParms
locateLiteralsize(sqInt aLiteral, sqInt litSize)
{
    AbstractInstruction *existingInst;
    sqInt i;
    sqInt i1;
    sqInt iLimiT;
    sqInt initialNumLiterals;
    AbstractInstruction *litInst;
    AbstractInstruction *newInst;
    AbstractInstruction *newLiterals;

	for (i = 0; i < nextLiteralIndex; i += 1) {
		litInst = literalInstructionAt(i);
		if ((((assert(((litInst->opcode)) == Literal),
		((((litInst->operands))[1]) == null
				? BytesPerOop
				: (((((litInst->operands))[1])) >> 1) & 15))) == litSize)
		 && (((((litInst->operands))[0]) == aLiteral)
		 && (((assert(((litInst->opcode)) == Literal),
		(((((litInst->operands))[1]) & 1) != 0)))
		 && (literalInstructionInRange(litInst))))) {
			return litInst;
		}
	}
	if (nextLiteralIndex >= literalsSize) {
		/* begin allocateLiterals: */
		initialNumLiterals = literalsSize + 8;
		if (initialNumLiterals > literalsSize) {

			/* Must copy across state (not using realloc, cuz...) and
			   must also update existing instructions to refer to the new ones...
			   It's either this or modify all generation routines to be able to retry
			   with more literals after running out of literals. */
			newLiterals = calloc(initialNumLiterals, sizeof(CogAbstractInstruction));
			if (!(literals == null)) {
				for (i1 = 0; i1 < nextLiteralIndex; i1 += 1) {
					existingInst = literalInstructionAt(i1);
					newInst = (&(newLiterals[i1]));
					cloneLiteralFrom(newInst, existingInst);
					assert(((existingInst->dependent)) == null);
					(existingInst->dependent = newInst);
				}
				for (i1 = 0, iLimiT = (opcodeIndex - 1); i1 <= iLimiT; i1 += 1) {
					existingInst = abstractInstructionAt(i1);
					if ((((existingInst->dependent)) != null)
					 && (((((existingInst->dependent))->opcode)) == Literal)) {
						(existingInst->dependent = (((existingInst->dependent))->dependent));
					}
				}
			}
			free(literals);
			literals = newLiterals;
			literalsSize = initialNumLiterals;
		}
	}
	litInst = literalInstructionAt(nextLiteralIndex);
	initializeSharableLiteral(litInst, aLiteral);
	setLiteralSize(litInst, litSize);

	/* Record the opcodeIndex of the first dependent instruction (the first instruction that references an out-of-line literal) */
	nextLiteralIndex += 1;
	if (firstOpcodeIndex > opcodeIndex) {
		firstOpcodeIndex = opcodeIndex - 1;
	}
	return litInst;
}

	/* SimpleStackBasedCogit>>#cogMethodHasExternalPrim: */
sqInt
cogMethodHasExternalPrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (primIndex == PrimNumberExternalCall)
	 || (primIndex == PrimNumberFFICall);
}

	/* SimpleStackBasedCogit>>#cogMethodHasMachineCodePrim: */
sqInt
cogMethodHasMachineCodePrim(CogMethod *aCogMethod)
{
    sqInt primIndex;

	primIndex = primitiveIndexOfMethodheader((aCogMethod->methodObject), (aCogMethod->methodHeader));
	return (((primIndex >= 1) && (primIndex <= MaxCompiledPrimitiveIndex)))
	 && ((((primitiveGeneratorTable[primIndex]).primitiveGenerator)) != null);
}


/*	Compile the jump instruction(s) at the end of the method that dispatch to
	each block body.
 */

	/* SimpleStackBasedCogit>>#compileBlockDispatch */
static sqInt
compileBlockDispatch(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpSkip;

	assert(blockCount > 0);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	blockEntryNoContextSwitch = anInstruction;
	/* begin Jump: */
	jumpSkip = genoperand(Jump, ((sqInt)0));
	/* begin MoveR:R: */
	blockEntryLabel = genoperandoperand(MoveRR, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jumpSkip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (blockCount > 1) {
		genLoadSlotsourceRegdestReg(ClosureStartPCIndex, ReceiverResultReg, TempReg);
	}
	compileBlockDispatchFromto(0, blockCount - 1);
	return 0;
}


/*	After pushing the temporaries but before the stack limit check a primitive
	method needs to fetch the error code, if any. If the primitive has failed,
	call the trampoline
	that will assign it to the last temp. */

	/* SimpleStackBasedCogit>>#compileGetErrorCode */
static void
compileGetErrorCode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt address;
    AbstractInstruction *anInstruction;
    AbstractInstruction *jmpNoError;

	/* begin MoveAw:R: */
	address = primFailCodeAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, TempReg));
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jmpNoError = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	addDependent(methodLabel, annotateAbsolutePCRef(gMoveCwR(((sqInt)methodLabel), ClassReg)));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceReapAndResetErrorCodeTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(jmpNoError, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* SimpleStackBasedCogit>>#compileInterpreterPrimitive */
static sqInt
compileInterpreterPrimitive(void)
{
    sqInt flags;
    void (*primitiveRoutine)(void);

	flags = 0;
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, (&flags));
	if (((flags & PrimCallOnSmalltalkStack) != 0)) {
		return compileOnStackExternalPrimitiveflags(primitiveRoutine, flags);
	}
	return compileInterpreterPrimitiveflags(primitiveRoutine, flags);
}


/*	Compile a call to an interpreter primitive. Call the C routine with the
	usual stack-switching dance, test the primFailCode and then either
	return on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compileInterpreterPrimitive:flags: */
static sqInt NoDbgRegParms
compileInterpreterPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags)
{
    sqInt address;
    sqInt address2;
    sqInt address4;
    sqInt address5;
    sqInt address6;
    sqInt address7;
    sqInt address8;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *continueAfterProfileSample;
    AbstractInstruction *jmp;
    AbstractInstruction *jump;
    AbstractInstruction *jumpToTakeSample;
    sqInt liveRegisterMask;
    sqInt offset;
    sqInt offset1;
    sqInt reg;
    sqInt retpc;
    AbstractInstruction *skip;

	jumpToTakeSample = ((AbstractInstruction *) 0);
	assert(!((registerisInMask(VarBaseReg, ABICallerSavedRegisterMask))));
	genExternalizePointersForPrimitiveCall();
	/* begin genLoadCStackPointersForPrimCall */
	if (cFramePointerInUse) {
		genLoadCStackPointers(backEnd);
	}
	else {
		genLoadCStackPointer(backEnd);
	}
	if (recordPrimTraceForMethod(methodObj)) {
		genFastPrimTraceUsingand(ClassReg, SendNumArgsReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(MoveCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address4 = primFailCodeAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address4, genoperandoperand(MoveRAw, TempReg, address4));
	if (methodOrBlockNumArgs != 0) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, methodOrBlockNumArgs, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(methodOrBlockNumArgs, BytesPerOop));
		}
	}
	/* begin MoveR:Aw: */
	address5 = argumentCountAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address5, genoperandoperand(MoveRAw, TempReg, address5));
	if (((flags & PrimCallNeedsNewMethod) != 0)) {
		genLoadNewMethod();
	}
	/* begin PrefetchAw: */
	address6 = primFailCodeAddress();
	/* begin gen:literal: */
	checkLiteralforInstruction(address6, genoperand(PrefetchAw, address6));
	if (((flags & PrimCallMayEndureCodeCompaction) != 0)) {

		/* The ceActivateFailingPrimitiveMethod: machinery can't handle framelessness. */
		needsFrame = 1;
		genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);
		/* begin genSubstituteReturnAddress: */
		retpc = (((flags & PrimCallCollectsProfileSamples) != 0)
			? cePrimReturnEnterCogCodeProfiling
			: cePrimReturnEnterCogCode);
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(retpc, genoperandoperand(MoveCwR, retpc, LR));
		/* begin gen:literal: */
		checkLiteralforInstruction(((sqInt)(((sqInt)primitiveRoutine))), genoperand(JumpFull, ((sqInt)(((sqInt)primitiveRoutine)))));
		return 0;
	}
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);
	/* begin gen:literal: */
	checkLiteralforInstruction(((sqInt)primitiveRoutine), genoperand(CallFull, ((sqInt)primitiveRoutine)));
	/* begin genRemoveNArgsFromStack: */
	assert(0 <= 6);
	maybeCompileRetryOfonPrimitiveFailflags(primitiveRoutine, primitiveIndex, flags);
	genLoadStackPointersForPrimCall(backEnd, ClassReg);
	/* begin MoveAw:R: */
	address = instructionPointerAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, LinkReg));
	/* begin MoveAw:R: */
	address7 = primFailCodeAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address7, genoperandoperand(MoveAwR, address7, TempReg));
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		/* begin genCheckForProfileTimerTick: */
		liveRegisterMask = (0);
		reg = Arg0Reg;
		/* begin MoveAw:R: */
		address8 = nextProfileTickAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address8, genoperandoperand(MoveAwR, address8, Arg1Reg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction7 = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		/* begin JumpAboveOrEqual: */
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpToTakeSample = jump;
		goto l31;
	l31:	/* end genCheckForProfileTimerTick: */;
	}
	/* begin MoveMw:r:R: */
	offset = 0;
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperandoperand(MoveMwrR, offset, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	continueAfterProfileSample = anInstruction5;
	/* begin RetN: */
	genoperand(RetN, BytesPerWord);
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		jmpTarget(jumpToTakeSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genTakeProfileSample();
		genLoadStackPointerForPrimCall(backEnd, ClassReg);
		/* begin MoveAw:R: */
		address2 = instructionPointerAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address2, genoperandoperand(MoveAwR, address2, LinkReg));
		/* begin Jump: */
		genoperand(Jump, ((sqInt)continueAfterProfileSample));
	}
	jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin MoveMw:r:R: */
	offset1 = BytesPerWord * (methodOrBlockNumArgs + (0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction6 = genoperandoperandoperand(MoveMwrR, offset1, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction6)) {
		(anInstruction6->dependent = locateLiteralsize(offset1, BytesPerOop));
	}
	return 0;
}


/*	Compile a fast call of a C primitive using the current stack page,
	avoiding the stack switch except on failure.
	This convention still uses stackPointer and argumentCount to access
	operands. Push all operands to the stack,
	assign stackPointer, argumentCount, and zero primFailCode. Make the call
	(saving a LinkReg if required).
	Test for failure and return. On failure on Spur, if there is an accessor
	depth, assign framePointer and newMethod,
	do the stack switch, call checkForAndFollowForwardedPrimitiveState, and
	loop back if forwarders are found.
	Fall through to frame build. */

	/* SimpleStackBasedCogit>>#compileOnStackExternalPrimitive:flags: */
static sqInt NoDbgRegParms
compileOnStackExternalPrimitiveflags(void (*primitiveRoutine)(void), sqInt flags)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt address;
    sqInt address1;
    sqInt address2;
    sqInt address3;
    sqInt address5;
    sqInt address6;
    sqInt address7;
    sqInt address8;
    sqInt address9;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt calleeSavedRegisterMask;
    AbstractInstruction *continueAfterProfileSample;
    AbstractInstruction *jmpFail;
    AbstractInstruction *jump;
    AbstractInstruction *jumpToTakeSample;
    sqInt linkRegSaveRegister;
    sqInt liveRegisterMask;
    sqInt offset;
    sqInt reg;
    AbstractInstruction *retry;
    AbstractInstruction *skip;
    AbstractInstruction *skip1;
    sqInt spRegSaveRegister;

	jumpToTakeSample = ((AbstractInstruction *) 0);
	linkRegSaveRegister = 0;
	assert(((flags & PrimCallOnSmalltalkStack) != 0));
	assert(!((registerisInMask(VarBaseReg, ABICallerSavedRegisterMask))));
	if (recordFastCCallPrimTraceForMethod(methodObj)) {
		genFastPrimTraceUsingand(ClassReg, SendNumArgsReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(MoveCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address5 = primFailCodeAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address5, genoperandoperand(MoveRAw, TempReg, address5));
	if (methodOrBlockNumArgs != 0) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, methodOrBlockNumArgs, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(methodOrBlockNumArgs, BytesPerOop));
		}
	}
	/* begin MoveR:Aw: */
	address6 = argumentCountAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address6, genoperandoperand(MoveRAw, TempReg, address6));
	genExternalizeStackPointerForFastPrimitiveCall();
	calleeSavedRegisterMask = ((ABICalleeSavedRegisterMask | ((1U << ClassReg))) - ((1U << ClassReg)));
	linkRegSaveRegister = availableRegisterOrNoneIn(calleeSavedRegisterMask);
	assert(!((linkRegSaveRegister == NoReg)));
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, LinkReg, linkRegSaveRegister);
	calleeSavedRegisterMask = ((calleeSavedRegisterMask | (((linkRegSaveRegister < 0) ? (((usqInt)(1)) >> (-linkRegSaveRegister)) : (1ULL << linkRegSaveRegister)))) - (((linkRegSaveRegister < 0) ? (((usqInt)(1)) >> (-linkRegSaveRegister)) : (1ULL << linkRegSaveRegister))));
	spRegSaveRegister = NoReg;
	if (!(((ABICalleeSavedRegisterMask & ((1U << SPReg))) != 0))) {
		spRegSaveRegister = availableRegisterOrNoneIn(calleeSavedRegisterMask);
		assert(!((spRegSaveRegister == NoReg)));
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SPReg, spRegSaveRegister);
	}
	/* begin Label */
	retry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	if (((flags & PrimCallOnSmalltalkStackAlign2x) != 0)) {
		gAndCqRR((-1 - (((BytesPerWord * 2) - 1))), SPReg, NativeSPReg);
	}
	else {
		genLoadNativeSPRegWithAlignedSPReg(backEnd);
	}
	genMarshallNArgsargargargarg(backEnd, 0, null, null, null, null);
	if (((!(flags & PrimCallIsExternalCall)))
	 && (isInImmediateBranchAndLinkRange(backEnd, maximumDistanceFromCodeZone(((sqInt)primitiveRoutine))))) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ((sqInt)primitiveRoutine));
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin gen:literal: */
		checkLiteralforInstruction(((sqInt)primitiveRoutine), genoperand(CallFull, ((sqInt)primitiveRoutine)));
	}
	/* begin genRemoveNArgsFromStack: */
	assert(0 <= 6);
	/* begin MoveAw:R: */
	address7 = primFailCodeAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address7, genoperandoperand(MoveAwR, address7, TempReg));
	if (spRegSaveRegister != NoReg) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, spRegSaveRegister, SPReg);
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction10 = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jmpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadCStackPointer(backEnd);
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		/* begin genCheckForProfileTimerTick: */
		liveRegisterMask = (0);
		reg = Arg0Reg;
		/* begin MoveAw:R: */
		address9 = nextProfileTickAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address9, genoperandoperand(MoveAwR, address9, Arg1Reg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction11 = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction11)) {
			(anInstruction11->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		skip1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		/* begin JumpAboveOrEqual: */
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip1, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpToTakeSample = jump;
		goto l41;
	l41:	/* end genCheckForProfileTimerTick: */;
	}
	/* begin MoveAw:R: */
	address8 = stackPointerAddress();
	/* begin gen:literal:operand: */
	continueAfterProfileSample = checkLiteralforInstruction(address8, genoperandoperand(MoveAwR, address8, TempReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, 0, TempReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	gAddCqRR(BytesPerWord, TempReg, SPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, linkRegSaveRegister, LinkReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	if (((flags & PrimCallCollectsProfileSamples) != 0)) {
		jmpTarget(jumpToTakeSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genTakeProfileSample();
		/* begin Jump: */
		genoperand(Jump, ((sqInt)continueAfterProfileSample));
	}
	jmpTarget(jmpFail, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if ((accessorDepthForPrimitiveMethod(methodObj)) >= 0) {

		/* Given that following primitive state to the accessor depth is recursive, we're asking for
		   trouble if we run the fixup on the Smalltalk stack page.  Run it on the full C stack instead.
		   This won't be a performance issue since primitive failure should be very rare. */
		/* begin MoveR:Aw: */
		address = framePointerAddress();
		/* begin gen:operand:literal: */
		checkLiteralforInstruction(address, genoperandoperand(MoveRAw, FPReg, address));
		/* begin MoveCw:R: */
		checkLiteralforInstruction(((sqInt)primitiveRoutine), genoperandoperand(MoveCwR, ((sqInt)primitiveRoutine), TempReg));
		/* begin MoveR:Aw: */
		address1 = primitiveFunctionPointerAddress();
		/* begin gen:operand:literal: */
		checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
		genLoadNewMethod();
		/* begin genLoadCStackPointersForPrimCall */
		if (cFramePointerInUse) {
			genLoadCStackPointers(backEnd);
		}
		else {
			genLoadCStackPointer(backEnd);
		}
		genMarshallNArgsargargargarg(backEnd, 0, 0, 0, 0, 0);
		if (isInImmediateBranchAndLinkRange(backEnd, maximumDistanceFromCodeZone(((usqIntptr_t)checkForAndFollowForwardedPrimitiveState)))) {
			/* begin CallRT: */
			abstractInstruction1 = genoperand(Call, ((usqIntptr_t)checkForAndFollowForwardedPrimitiveState));
			(abstractInstruction1->annotation = IsRelativeCall);
		}
		else {
			/* begin gen:literal: */
			checkLiteralforInstruction(((usqIntptr_t)checkForAndFollowForwardedPrimitiveState), genoperand(CallFull, ((usqIntptr_t)checkForAndFollowForwardedPrimitiveState)));
		}
		genLoadStackPointersForPrimCall(backEnd, ClassReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction5 = genoperandoperand(CmpCqR, 0, ABIResultReg);
		if (usesOutOfLineLiteral(anInstruction5)) {
			(anInstruction5->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction6 = genoperandoperand(MoveCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction6)) {
			(anInstruction6->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin MoveR:Aw: */
		address2 = primFailCodeAddress();
		/* begin gen:operand:literal: */
		checkLiteralforInstruction(address2, genoperandoperand(MoveRAw, TempReg, address2));
		/* begin Jump: */
		genoperand(Jump, ((sqInt)retry));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	else {

		/* must reload SPReg to undo any alignment change, */
		if (((flags & PrimCallOnSmalltalkStackAlign2x) != 0)) {
			/* begin MoveAw:R: */
			address3 = stackPointerAddress();
			/* begin gen:literal:operand: */
			checkLiteralforInstruction(address3, genoperandoperand(MoveAwR, address3, SPReg));
		}
	}
	genLoadCStackPointer(backEnd);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, linkRegSaveRegister, LinkReg);
	if (((ABICallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {
		/* begin MoveMw:r:R: */
		offset = (methodOrBlockNumArgs + (0)) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction8 = genoperandoperandoperand(MoveMwrR, offset, SPReg, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction8)) {
			(anInstruction8->dependent = locateLiteralsize(offset, BytesPerOop));
		}
	}
	return 0;
}


/*	Compile one method cache probe in an OpenPIC's lookup of selector.
	Answer the jump taken if the selector probe fails.
	The class tag of the receiver must be in SendNumArgsReg. ClassReg and
	TempReg are used as scratch registers.
	On a hit, the offset of the entry is in ClassReg. */

	/* SimpleStackBasedCogit>>#compileOpenPICMethodCacheProbeFor:withShift:baseRegOrNone: */
static AbstractInstruction * NoDbgRegParms
compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selector, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt offset1;

	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);
	annotateobjRef(gXorCwR(selector, ClassReg), selector);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	/* begin AndCq:R: */
	anInstruction4 = genoperandoperand(AndCqR, ((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), BytesPerOop));
	}
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
		}
	}
	else {
		/* begin AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);
		/* begin MoveMw:r:R: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), BytesPerOop));
		}
	}
	annotateobjRef(checkLiteralforInstruction(selector, genoperandoperand(CmpCwR, selector, TempReg)), selector);
	/* begin JumpNonZero: */
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset1 = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(offset1, BytesPerOop));
		}
	}
	else {
		/* begin MoveMw:r:R: */
		anInstruction3 = genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), BytesPerOop));
		}
	}
	/* begin CmpR:R: */
	assert(!((SendNumArgsReg == SPReg)));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile the code for an open PIC. Perform a probe of the first-level
	method lookup cache followed by a call of ceSendFromInLineCacheMiss: if
	the probe fails. */

	/* SimpleStackBasedCogit>>#compileOpenPIC:numArgs: */
static void NoDbgRegParms
compileOpenPICnumArgs(sqInt selector, sqInt numArgs)
{
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	/* begin preenMethodLabel */
	(((((AbstractInstruction *) methodLabel))->operands))[1] = 0;
	compilePICAbort(numArgs);
	entry = genGetClassTagOfintoscratchReg(ReceiverResultReg, SendNumArgsReg, TempReg);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 0, cacheBaseReg);
	/* begin JumpNonZero: */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveMw:r:R: */
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	itsAHit = anInstruction1;
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	jmpTarget(jumpBCMethod, picInterpretAbort);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(cmNoCheckEntryOffset, BytesPerOop));
	}
	/* begin JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compileOpenPICMethodCacheProbeForwithShiftbaseRegOrNone(selector, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genSmalltalkToCStackSwitch(1);
	addDependent(methodLabel, annotateAbsolutePCRef(gMoveCwR(((sqInt)methodLabel), SendNumArgsReg)));
	compileCallFornumArgsargargargargresultRegregsToSave(ceSendFromInLineCacheMiss, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* emptyRegisterMask */);
}


/*	Compile one method cache probe in a perform: primitive's lookup of
	selector. Answer the jump taken if the selector probe fails. */

	/* SimpleStackBasedCogit>>#compilePerformMethodCacheProbeFor:withShift:baseRegOrNone: */
static AbstractInstruction * NoDbgRegParms
compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(sqInt selectorReg, sqInt shift, sqInt baseRegOrNone)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt offset1;

	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SendNumArgsReg, ClassReg);
	maybeShiftClassTagRegisterForMethodCacheProbe(ClassReg);
	/* begin XorR:R: */
	genoperandoperand(XorRR, selectorReg, ClassReg);
	assert(shift <= (shiftForWord()));
	if (shift < (shiftForWord())) {
		/* begin LogicalShiftLeftCq:R: */
		genoperandoperand(LogicalShiftLeftCqR, (shiftForWord()) - shift, ClassReg);
	}
	/* begin AndCq:R: */
	anInstruction4 = genoperandoperand(AndCqR, ((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), ClassReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheMask) << (shiftForWord()))), BytesPerOop));
	}
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
		}
	}
	else {
		/* begin AddR:R: */
		genoperandoperand(AddRR, baseRegOrNone, ClassReg);
		/* begin MoveMw:r:R: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheSelector) << (shiftForWord()))), BytesPerOop));
		}
	}
	/* begin CmpR:R: */
	assert(!((selectorReg == SPReg)));
	genoperandoperand(CmpRR, selectorReg, TempReg);
	/* begin JumpNonZero: */
	jumpSelectorMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (baseRegOrNone == NoReg) {
		/* begin MoveMw:r:R: */
		offset1 = (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperandoperand(MoveMwrR, offset1, ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(offset1, BytesPerOop));
		}
	}
	else {
		/* begin MoveMw:r:R: */
		anInstruction3 = genoperandoperandoperand(MoveMwrR, ((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), ClassReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(((sqInt)((usqInt)(MethodCacheClass) << (shiftForWord()))), BytesPerOop));
		}
	}
	/* begin CmpR:R: */
	assert(!((SendNumArgsReg == SPReg)));
	genoperandoperand(CmpRR, SendNumArgsReg, TempReg);
	return jumpSelectorMiss;
}


/*	Compile a primitive. If possible, performance-critical primitives will
	be generated by their own routines (primitiveGenerator). Otherwise,
	if there is a primitive at all, we call the C routine with the usual
	stack-switching dance, test the primFailCode and then either return
	on success or continue to the method body. */

	/* SimpleStackBasedCogit>>#compilePrimitive */
static sqInt
compilePrimitive(void)
{
    AbstractInstruction *anInstruction;
    sqInt code;
    sqInt flags;
    AbstractInstruction *jumpFailAlloc;
    AbstractInstruction *jumpNotSmallInteger;
    sqInt opcodeIndexAtPrimitive;
    PrimitiveDescriptor *primitiveDescriptor;
    void (*primitiveRoutine)(void);
    sqInt reg;

	flags = 0;
	if (primitiveIndex == 0) {
		return 0;
	}
	if ((((primitiveDescriptor = primitiveGeneratorOrNil())) != null)
	 && ((((primitiveDescriptor->primitiveGenerator)) != null)
	 && ((((primitiveDescriptor->primNumArgs)) < 0)
	 || (((primitiveDescriptor->primNumArgs)) == methodOrBlockNumArgs)))) {

		/* Note opcodeIndex so that any arg load instructions
		   for unimplemented primitives can be discarded. */
		opcodeIndexAtPrimitive = opcodeIndex;
		code = ((primitiveDescriptor->primitiveGenerator))();
		if ((code < 0)
		 && (code != UnimplementedPrimitive)) {

			/* Generator failed, so no point continuing... */
			return code;
		}
		if (code == UnfailingPrimitive) {
			return 0;
		}
		if ((code == CompletePrimitive)
		 && (!(((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
		 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))))) {
			return 0;
		}
		if (code == UnimplementedPrimitive) {
			opcodeIndex = opcodeIndexAtPrimitive;
		}
	}
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, (&flags));
	if ((primitiveRoutine == 0)
	 || (primitiveRoutine == (((void (*)(void)) primitiveFail)))) {
		return genFastPrimFail();
	}
	if ((primitiveRoutine == (((void (*)(void)) primitiveHighResClock)))
	 && (methodOrBlockNumArgs == 0)) {
		/* begin genPrimitiveHighResClock64 */
		reg = Arg0Reg;
		assert(registerisNotInMask(reg, registerMaskForandandand(ReceiverResultReg, Arg1Reg, Extra0Reg, Extra1Reg)));
		gMovePerfCnt64RL(reg, (0));
		gLogicalShiftRightCqRR((numSmallIntegerBits()) - 1, reg, Arg1Reg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpNonZero: */
		jumpNotSmallInteger = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		genConvertIntegerInRegtoSmallIntegerInReg(reg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
		jmpTarget(jumpNotSmallInteger, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jumpFailAlloc = genAlloc64BitPositiveIntegerValueintoscratchRegscratchReg(reg, ReceiverResultReg, Extra0Reg, Extra1Reg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, SendNumArgsReg, ReceiverResultReg);
		if (methodOrBlockNumArgs <= (numRegArgs())) {
			/* begin RetN: */
			genoperand(RetN, 0);
		}
		else {
			/* begin RetN: */
			genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
		}
		jmpTarget(jumpFailAlloc, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		code = 0;
	l10:	/* end genPrimitiveHighResClock64 */;
		if (code != UnimplementedPrimitive) {
			return code;
		}
	}
	if (((flags & PrimCallOnSmalltalkStack) != 0)) {
		return compileOnStackExternalPrimitiveflags(primitiveRoutine, flags);
	}
	return compileInterpreterPrimitiveflags(primitiveRoutine, flags);
}

	/* SimpleStackBasedCogit>>#extendedPushBytecode */
static sqInt
extendedPushBytecode(void)
{
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genPushReceiverVariable(variableIndex);
	}
	if (variableType == 1) {
		return genPushTemporaryVariable(variableIndex);
	}
	if (variableType == 2) {
		return genPushLiteralIndex(variableIndex);
	}
	return genPushLiteralVariable(variableIndex);
}

	/* SimpleStackBasedCogit>>#extendedStoreAndPopBytecode */
static sqInt
extendedStoreAndPopBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(1, variableIndex);
#    if IMMUTABILITY
		/* begin annotateBytecode: */
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY
		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(1, variableIndex, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#extendedStoreBytecode */
static sqInt
extendedStoreBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt variableIndex;
    sqInt variableType;

	variableType = (((usqInt)(byte1)) >> 6) & 3;
	variableIndex = byte1 & 0x3F;
	if (variableType == 0) {
		return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
	}
	if (variableType == 1) {
		genStorePopTemporaryVariable(0, variableIndex);
#    if IMMUTABILITY
		/* begin annotateBytecode: */
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		(abstractInstruction->annotation = HasBytecodePC);
#    endif // IMMUTABILITY
		return 0;
	}
	if (variableType == 3) {
		return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, variableIndex, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
	}
	return EncounteredUnknownBytecode;
}

	/* SimpleStackBasedCogit>>#frameOffsetOfTemporary: */
static sqInt NoDbgRegParms
frameOffsetOfTemporary(sqInt index)
{
	return (index < methodOrBlockNumArgs
		? FoxCallerSavedIP + ((methodOrBlockNumArgs - index) * BytesPerWord)
		: (FoxMFReceiver - BytesPerWord) + ((methodOrBlockNumArgs - index) * BytesPerWord));
}

	/* SimpleStackBasedCogit>>#genDoubleFailIfZeroArgRcvr:arg: */
static AbstractInstruction * NoDbgRegParms
genDoubleFailIfZeroArgRcvrarg(int rcvrReg, int argReg)
{
    AbstractInstruction *anInstruction;

	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin ConvertR:Rd: */
	genoperandoperand(ConvertRRd, TempReg, DPFPReg2);
	/* begin CmpRd:Rd: */
	genoperandoperand(CmpRdRd, DPFPReg2, argReg);
	return gJumpFPEqual(0);
}


/*	Can use any of the first 32 literals for the selector and pass up to 7
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genExtendedSendBytecode */
static sqInt
genExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}

	/* SimpleStackBasedCogit>>#genExtendedSuperBytecode */
static sqInt
genExtendedSuperBytecode(void)
{
	return genSendSupernumArgs(byte1 & 0x1F, ((usqInt)(byte1)) >> 5);
}


/*	244		11110100	i i i i i i i i	Pop and Jump 0n False i i i i i i i i (+
	Extend B * 256, where Extend B >= 0)
 */

	/* SimpleStackBasedCogit>>#genExtJumpIfFalse */
static sqInt
genExtJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongForwardBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}


/*	243		11110011	i i i i i i i i	Pop and Jump 0n True i i i i i i i i (+
	Extend B * 256, where Extend B >= 0)
 */

	/* SimpleStackBasedCogit>>#genExtJumpIfTrue */
static sqInt
genExtJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongForwardBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}


/*	NewspeakV4: 221		11011101		Nop */
/*	SistaV1:		 91		01011011'		Nop */

	/* SimpleStackBasedCogit>>#genExtNopBytecode */
static sqInt
genExtNopBytecode(void)
{
	extA = (numExtB = (extB = 0));
	return 0;
}


/*	SistaV1:		233		11101001	iiiiiiii		Push Character #iiiiiiii (+ Extend A *
	256) 
 */

	/* SimpleStackBasedCogit>>#genExtPushCharacterBytecode */
static sqInt
genExtPushCharacterBytecode(void)
{
    sqInt literal;
    sqInt value;

	value = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	/* begin genPushLiteral: */
	literal = characterObjectOf(value);
	return ssPushConstant(literal);
}


/*	NewsqueakV4:	229		11100101	iiiiiiii	Push Integer #iiiiiiii (+ Extend B *
	256, where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
	SistaV1:		232		11101000	iiiiiiii	Push Integer #iiiiiiii (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#genExtPushIntegerBytecode */
static sqInt
genExtPushIntegerBytecode(void)
{
    sqInt value;

	value = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	extB = 0;
	numExtB = 0;
	return ssPushConstant((((usqInt)value << 3) | 1));
}


/*	228		11100100	i i i i i i i i	Push Literal #iiiiiiii (+ Extend A * 256) */

	/* SimpleStackBasedCogit>>#genExtPushLiteralBytecode */
static sqInt
genExtPushLiteralBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genPushLiteralIndex(index);
}


/*	227		11100011	i i i i i i i i	Push Literal Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtPushLiteralVariableBytecode */
static sqInt
genExtPushLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genPushLiteralVariable(index);
}


/*	SistaV1: *	82			01010010			Push thisContext, (then Extend B = 1 => push
	thisProcess) 
 */

	/* SimpleStackBasedCogit>>#genExtPushPseudoVariable */
static sqInt
genExtPushPseudoVariable(void)
{
    sqInt ext;

	ext = extB;
	extB = 0;
	numExtB = 0;
	switch (ext) {
	case 0:
		return genPushActiveContextBytecode();

	default:
		/* begin unknownBytecode */
		return EncounteredUnknownBytecode;

	}
	return 0;
}


/*	226		11100010	i i i i i i i i	Push Receiver Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtPushReceiverVariableBytecode */
static sqInt
genExtPushReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isReadMediatedContextInstVarIndex(index))
		? genPushMaybeContextReceiverVariable(index)
		: genPushReceiverVariable(index));
}


/*	238		11101110	i i i i i j j j	Send Literal Selector #iiiii (+ Extend A *
	32) with jjj (+ Extend B * 8) Arguments
 */

	/* SimpleStackBasedCogit>>#genExtSendBytecode */
static sqInt
genExtSendBytecode(void)
{
    sqInt litIndex;
    sqInt nArgs;

	litIndex = (((usqInt)(byte1)) >> 3) + (((sqInt)((usqInt)(extA) << 5)));
	extA = 0;
	nArgs = (byte1 & 7) + (((sqInt)((usqInt)(extB) << 3)));
	extB = 0;
	numExtB = 0;
	return genSendnumArgs(litIndex, nArgs);
}


/*	239		11101111	i i i i i j j j	Send To Superclass Literal Selector #iiiii
	(+ Extend A * 32) with jjj (+ Extend B * 8) Arguments
 */

	/* SimpleStackBasedCogit>>#genExtSendSuperBytecode */
static sqInt
genExtSendSuperBytecode(void)
{
    int isDirected;
    sqInt litIndex;
    sqInt nArgs;

	if ((isDirected = extB >= 64)) {
		extB = extB & 0x3F;
	}
	litIndex = (((usqInt)(byte1)) >> 3) + (((sqInt)((usqInt)(extA) << 5)));
	extA = 0;
	nArgs = (byte1 & 7) + (((sqInt)((usqInt)(extB) << 3)));
	extB = 0;
	numExtB = 0;
	return (isDirected
		? genSendDirectedSupernumArgs(litIndex, nArgs)
		: genSendSupernumArgs(litIndex, nArgs));
}


/*	236		11101100	i i i i i i i i	Pop and Store Literal Variable #iiiiiiii (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreAndPopLiteralVariableBytecode */
static sqInt
genExtStoreAndPopLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(1, index, ((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
}


/*	235		11101011	i i i i i i i i	Pop and Store Receiver Variable #iiiiiii (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreAndPopReceiverVariableBytecode */
static sqInt
genExtStoreAndPopReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isWriteMediatedContextInstVarIndex(index))
		? genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, index, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1)
		: genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, index, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1));
}


/*	233		11101001	i i i i i i i i	Store Literal Variable #iiiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreLiteralVariableBytecode */
static sqInt
genExtStoreLiteralVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, index, ((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
}


/*	232		11101000	i i i i i i i i	Store Receiver Variable #iiiiiii (+ Extend A
	* 256)
 */

	/* SimpleStackBasedCogit>>#genExtStoreReceiverVariableBytecode */
static sqInt
genExtStoreReceiverVariableBytecode(void)
{
    sqInt index;

	index = byte1 + (((sqInt)((usqInt)(extA) << 8)));
	extA = 0;
	return ((mclassCouldBeContext())
	 && (isWriteMediatedContextInstVarIndex(index))
		? genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, index, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1)
		: genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(0, index, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#genExtUnconditionalJump */
static sqInt
genExtUnconditionalJump(void)
{
    AbstractInstruction *abstractInstruction;
    sqInt distance;
    sqInt target;

	distance = byte1 + (((sqInt)((usqInt)(extB) << 8)));
	assert(distance == (v4LongBranchDistance(generatorAt(byte0), bytecodePC, ((extA != 0
	? 1
	: 0)) + ((extB != 0
	? 1
	: 0)), methodObj)));
	extB = 0;
	numExtB = 0;
	target = (distance + 2) + bytecodePC;
	if (distance < 0) {
		return genJumpBackTo(target);
	}
	genJumpTo(target);
	/* begin annotateBytecode: */
	abstractInstruction = lastOpcode();
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* SimpleStackBasedCogit>>#genFastPrimFail */
static sqInt
genFastPrimFail(void)
{
	primitiveIndex = 0;
	return UnfailingPrimitive;
}


/*	Support for compileInterpreterPrimitive. Generate inline code
	so as to record the primitive trace as fast as possible. */

	/* SimpleStackBasedCogit>>#genFastPrimTraceUsing:and: */
static void NoDbgRegParms
genFastPrimTraceUsingand(sqInt r1, sqInt r2)
{
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt offset;
    sqInt wordConstant;

	/* begin MoveAb:R: */
	address = primTraceLogIndexAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAbR, address, r2));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(LoadEffectiveAddressMwrR, 1, r2, r1);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin MoveR:Ab: */
	address1 = primTraceLogIndexAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAb, r1, address1));
	addDependent(methodLabel, annotateAbsolutePCRef(gMoveCwR(((sqInt)methodLabel), r1)));
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveMwrR, offset, r1, TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveCw:R: */
	wordConstant = ((sqInt)(primTraceLogAddress()));
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, r1));
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, TempReg, r2, r1);
}

	/* SimpleStackBasedCogit>>#genLoadNewMethod */
static void
genLoadNewMethod(void)
{
    sqInt address;
    sqInt address3;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt offset;

	addDependent(methodLabel, annotateAbsolutePCRef(gMoveCwR(((sqInt)methodLabel), ClassReg)));
	/* begin MoveMw:r:R: */
	offset = offsetof(CogMethod, methodObject);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, offset, ClassReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address3 = newMethodAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address3, genoperandoperand(MoveRAw, TempReg, address3));
#  if LRPCheck
	if (checkingLongRunningPrimitives()) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin MoveR:Aw: */
		address = longRunningPrimitiveStopUsecsAddress();
		/* begin gen:operand:literal: */
		checkLiteralforInstruction(address, genoperandoperand(MoveRAw, TempReg, address));
	}
#  endif // LRPCheck
}

	/* SimpleStackBasedCogit>>#genLongJumpIfFalse */
static sqInt
genLongJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genLongJumpIfTrue */
static sqInt
genLongJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = v3LongForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 2) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}


/*	230		11100110	i i i i i i i i	Push Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongPushTemporaryVariableBytecode */
static sqInt
genLongPushTemporaryVariableBytecode(void)
{
	return genPushTemporaryVariable(byte1);
}


/*	237		11101101	i i i i i i i i	Pop and Store Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongStoreAndPopTemporaryVariableBytecode */
static sqInt
genLongStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte1);
}


/*	234		11101010	i i i i i i i i	Store Temporary Variable #iiiiiiii */

	/* SimpleStackBasedCogit>>#genLongStoreTemporaryVariableBytecode */
static sqInt
genLongStoreTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(0, byte1);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalBackwardJump */
static sqInt
genLongUnconditionalBackwardJump(void)
{
    sqInt distance;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance < 0);
	return genJumpBackTo((distance + 2) + bytecodePC);
}

	/* SimpleStackBasedCogit>>#genLongUnconditionalForwardJump */
static sqInt
genLongUnconditionalForwardJump(void)
{
    sqInt distance;
    sqInt targetpc;

	distance = v3LongBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	assert(distance >= 0);
	targetpc = (distance + 2) + bytecodePC;
	return genJumpTo(targetpc);
}


/*	Compile the code for a probe of the first-level method cache for a perform
	primitive. The selector is assumed to be in Arg0Reg. Defer to
	adjustArgumentsForPerform: to
	adjust the arguments before the jump to the method. */
/*	N.B. Can't assume TempReg already contains the tag because a method can
	of course be invoked via the unchecked entry-point, e.g. as does perform:. */

	/* SimpleStackBasedCogit>>#genLookupForPerformNumArgs: */
static sqInt NoDbgRegParms
genLookupForPerformNumArgs(sqInt numArgs)
{
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBadNumArgs;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpInterpret;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;

	genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, SendNumArgsReg, 0);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 0, cacheBaseReg);
	/* begin JumpNonZero: */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveMw:r:R: */
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	itsAHit = anInstruction1;
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);

	/* check the argument count; if it's wrong fall back on the interpreter primitive. */
	jumpInterpret = genJumpImmediate(ClassReg);
	/* begin genLoadcmNumArgsOf:into: */
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveMbrR, BytesPerWord, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(CmpCqR, numArgs, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(numArgs, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpBadNumArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(cmNoCheckEntryOffset, BytesPerOop));
	}
	adjustArgumentsForPerform(numArgs);
	/* begin JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpInterpret, jmpTarget(jumpBadNumArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	return 0;
}

	/* SimpleStackBasedCogit>>#genMoveFalseR: */
static AbstractInstruction * NoDbgRegParms
genMoveFalseR(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	/* begin genMoveConstant:R: */
	constant = falseObject();
	return (shouldAnnotateObjectReference(constant)
		? annotateobjRef(gMoveCwR(constant, reg), constant)
		: (/* begin checkQuickConstant:forInstruction: */
			(anInstruction = genoperandoperand(MoveCqR, constant, reg)),
			(usesOutOfLineLiteral(anInstruction)
					? (anInstruction->dependent = locateLiteralsize(constant, BytesPerOop))
					: 0),
			anInstruction));
}

	/* SimpleStackBasedCogit>>#genMoveTrueR: */
static AbstractInstruction * NoDbgRegParms
genMoveTrueR(sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	/* begin genMoveConstant:R: */
	constant = trueObject();
	return (shouldAnnotateObjectReference(constant)
		? annotateobjRef(gMoveCwR(constant, reg), constant)
		: (/* begin checkQuickConstant:forInstruction: */
			(anInstruction = genoperandoperand(MoveCqR, constant, reg)),
			(usesOutOfLineLiteral(anInstruction)
					? (anInstruction->dependent = locateLiteralsize(constant, BytesPerOop))
					: 0),
			anInstruction));
}


/*	Implement 28-bit hashMultiply for SmallInteger and LargePositiveInteger
	receivers. 
 */

	/* SimpleStackBasedCogit>>#genPrimitiveHashMultiply */
static sqInt
genPrimitiveHashMultiply(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jmpFailImm;
    AbstractInstruction *jmpFailNotPositiveLargeInt;

	if (mclassIsSmallInteger()) {
		genConvertSmallIntegerToIntegerInReg(ReceiverResultReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(HashMultiplyConstant, BytesPerOop));
		}
		/* begin MulR:R: */
		genMulRR(backEnd, TempReg, ReceiverResultReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(HashMultiplyMask, BytesPerOop));
		}
		genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
		/* begin RetN: */
		genoperand(RetN, 0);
		return CompletePrimitive;
	}
	jmpFailImm = genJumpImmediate(ReceiverResultReg);
	genGetCompactClassIndexNonImmOfinto(ReceiverResultReg, ClassReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, ClassLargePositiveIntegerCompactIndex, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(ClassLargePositiveIntegerCompactIndex, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jmpFailNotPositiveLargeInt = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(0, ReceiverResultReg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(MoveCqR, HashMultiplyConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(HashMultiplyConstant, BytesPerOop));
	}
	/* begin MulR:R: */
	genMulRR(backEnd, TempReg, ReceiverResultReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AndCqR, HashMultiplyMask, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(HashMultiplyMask, BytesPerOop));
	}
	genConvertIntegerToSmallIntegerInReg(ReceiverResultReg);
	/* begin RetN: */
	genoperand(RetN, 0);
	jmpTarget(jmpFailImm, jmpTarget(jmpFailNotPositiveLargeInt, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	return CompletePrimitive;
}


/*	Generate the substitute return code for an external or FFI primitive call.
	On success simply return, extracting numArgs from newMethod.
	On primitive failure call ceActivateFailingPrimitiveMethod: newMethod. */

	/* SimpleStackBasedCogit>>#genPrimReturnEnterCogCodeEnilopmart: */
static void NoDbgRegParms
genPrimReturnEnterCogCodeEnilopmart(sqInt profiling)
{
    sqInt address;
    sqInt address4;
    sqInt address6;
    sqInt address7;
    sqInt address8;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *continuePostSample;
    AbstractInstruction *jmpFail;
    AbstractInstruction *jmpSample;
    AbstractInstruction *jump;
    sqInt liveRegisterMask;
    sqInt quickConstant;
    sqInt reg;
    sqInt reg1;
    AbstractInstruction *skip;

	continuePostSample = ((AbstractInstruction *) 0);
	jmpSample = ((AbstractInstruction *) 0);
	zeroOpcodeIndex();
	/* begin MoveCq:R: */
	quickConstant = varBaseAddress();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin MoveAw:R: */
	address6 = primFailCodeAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address6, genoperandoperand(MoveAwR, address6, TempReg));
	flag("ask concrete code gen if move sets condition codes?");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction7 = genoperandoperand(CmpCqR, 0, TempReg);
	if (usesOutOfLineLiteral(anInstruction7)) {
		(anInstruction7->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jmpFail = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	if (profiling) {
		/* begin genCheckForProfileTimerTick: */
		liveRegisterMask = (0);
		reg = Arg0Reg;
		/* begin MoveAw:R: */
		address = nextProfileTickAddress();
		/* begin gen:literal:operand: */
		checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, Arg1Reg));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, 0, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		skip = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		gMovePerfCnt64RL(reg, liveRegisterMask);
		/* begin CmpR:R: */
		assert(!((Arg1Reg == SPReg)));
		genoperandoperand(CmpRR, Arg1Reg, reg);
		/* begin JumpAboveOrEqual: */
		jump = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
		jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		jmpSample = jump;
		goto l12;
	l12:	/* end genCheckForProfileTimerTick: */;
		/* begin Label */
		continuePostSample = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	genLoadStackPointers(backEnd);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperandoperand(MoveMwrR, 0, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin MoveAw:R: */
	address4 = instructionPointerAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address4, genoperandoperand(MoveAwR, address4, LinkReg));
	/* begin RetN: */
	genoperand(RetN, BytesPerWord);
	jmpTarget(jmpFail, gMoveAwR(newMethodAddress(), SendNumArgsReg));
	/* begin MoveAw:R: */
	address7 = cStackPointerAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address7, genoperandoperand(MoveAwR, address7, SPReg));
	compileCallFornumArgsargargargargresultRegregsToSave(ceActivateFailingPrimitiveMethod, 1, SendNumArgsReg, null, null, null, NoReg, 0 /* emptyRegisterMask */);
	/* begin MoveAw:R: */
	address8 = instructionPointerAddress();
	reg1 = LinkReg;
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address8, genoperandoperand(MoveAwR, address8, reg1));
	genLoadStackPointers(backEnd);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction5 = genoperandoperandoperand(MoveMwrR, 0, SPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction5)) {
		(anInstruction5->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin RetN: */
	genoperand(RetN, BytesPerWord);
	if (profiling) {

		/* Call ceTakeProfileSample: to record sample and then continue.  newMethod
		   should be up-to-date.  Need to save and restore the link reg around this call. */
		jmpTarget(jmpSample, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genMarshallNArgsargargargarg(backEnd, 1, (assert(null >= 0),
		-2 - null), null, null, null);
		/* begin gen:literal: */
		checkLiteralforInstruction(((usqInt)ceTakeProfileSample), genoperand(CallFull, ((usqInt)ceTakeProfileSample)));
		/* begin genRemoveNArgsFromStack: */
		assert(1 <= 6);
		/* begin Jump: */
		genoperand(Jump, ((sqInt)continuePostSample));
	}
}

	/* SimpleStackBasedCogit>>#genPushConstantFalseBytecode */
static sqInt
genPushConstantFalseBytecode(void)
{
	return ssPushConstant(falseObject());
}

	/* SimpleStackBasedCogit>>#genPushConstantNilBytecode */
static sqInt
genPushConstantNilBytecode(void)
{
	return ssPushConstant(nilObject());
}


/*	79			01001111		Push 1 */

	/* SimpleStackBasedCogit>>#genPushConstantOneBytecode */
static sqInt
genPushConstantOneBytecode(void)
{
	return ssPushConstant((((usqInt)1 << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushConstantTrueBytecode */
static sqInt
genPushConstantTrueBytecode(void)
{
	return ssPushConstant(trueObject());
}


/*	78			01001110		Push 0 */

	/* SimpleStackBasedCogit>>#genPushConstantZeroBytecode */
static sqInt
genPushConstantZeroBytecode(void)
{
	return ssPushConstant((((usqInt)0 << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushLiteralConstantBytecode */
static sqInt
genPushLiteralConstantBytecode(void)
{
	return genPushLiteralIndex(byte0 & 0x1F);
}


/*	16-31		0001 i i i i		Push Literal Variable #iiii */

	/* SimpleStackBasedCogit>>#genPushLiteralVariable16CasesBytecode */
static sqInt
genPushLiteralVariable16CasesBytecode(void)
{
	return genPushLiteralVariable(byte0 & 15);
}

	/* SimpleStackBasedCogit>>#genPushLiteralVariableBytecode */
static sqInt
genPushLiteralVariableBytecode(void)
{
	return genPushLiteralVariable(byte0 & 0x1F);
}

	/* SimpleStackBasedCogit>>#genPushQuickIntegerConstantBytecode */
static sqInt
genPushQuickIntegerConstantBytecode(void)
{
	return ssPushConstant((((usqInt)(byte0 - 117) << 3) | 1));
}

	/* SimpleStackBasedCogit>>#genPushReceiverVariableBytecode */
static sqInt
genPushReceiverVariableBytecode(void)
{
	return genPushReceiverVariable(byte0 & 15);
}

	/* SimpleStackBasedCogit>>#genPushTemporaryVariableBytecode */
static sqInt
genPushTemporaryVariableBytecode(void)
{
	return genPushTemporaryVariable(byte0 & 15);
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnConst */
sqInt
genQuickReturnConst(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	constant = quickPrimitiveConstantFor(primitiveIndex);
	/* begin genMoveConstant:R: */
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnInstVar */
sqInt
genQuickReturnInstVar(void)
{
    sqInt index;

	index = quickPrimitiveInstVarIndexFor(primitiveIndex);
	genLoadSlotsourceRegdestReg(index, ReceiverResultReg, ReceiverResultReg);
	genUpArrowReturn();
	return UnfailingPrimitive;
}


/*	because selected by CoInterpreter>>quickPrimitiveGeneratorFor: */

	/* SimpleStackBasedCogit>>#genQuickReturnSelf */
sqInt
genQuickReturnSelf(void)
{
	genUpArrowReturn();
	return UnfailingPrimitive;
}

	/* SimpleStackBasedCogit>>#genReturnFalse */
static sqInt
genReturnFalse(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	/* begin genMoveFalseR: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnNil */
static sqInt
genReturnNil(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	/* begin genMoveNilR: */
	constant = nilObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	return genUpArrowReturn();
}

	/* SimpleStackBasedCogit>>#genReturnNilFromBlock */
static sqInt
genReturnNilFromBlock(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	assert(inBlock > 0);
	/* begin genMoveNilR: */
	constant = nilObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	return genBlockReturn();
}

	/* SimpleStackBasedCogit>>#genReturnTrue */
static sqInt
genReturnTrue(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;

	/* begin genMoveTrueR: */
	constant = trueObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, ReceiverResultReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	return genUpArrowReturn();
}


/*	Can use any of the first 64 literals for the selector and pass up to 3
	arguments. 
 */

	/* SimpleStackBasedCogit>>#genSecondExtendedSendBytecode */
static sqInt
genSecondExtendedSendBytecode(void)
{
	return genSendnumArgs(byte1 & 0x3F, ((usqInt)(byte1)) >> 6);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector0ArgsBytecode */
static sqInt
genSendLiteralSelector0ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 0);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector1ArgBytecode */
static sqInt
genSendLiteralSelector1ArgBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 1);
}

	/* SimpleStackBasedCogit>>#genSendLiteralSelector2ArgsBytecode */
static sqInt
genSendLiteralSelector2ArgsBytecode(void)
{
	return genSendnumArgs(byte0 & 15, 2);
}

	/* SimpleStackBasedCogit>>#genShortJumpIfFalse */
static sqInt
genShortJumpIfFalse(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpIfto(falseObject(), target);
}

	/* SimpleStackBasedCogit>>#genShortJumpIfTrue */
static sqInt
genShortJumpIfTrue(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpIfto(trueObject(), target);
}

	/* SimpleStackBasedCogit>>#genShortUnconditionalJump */
static sqInt
genShortUnconditionalJump(void)
{
    sqInt distance;
    sqInt target;

	distance = v3ShortForwardBranchDistance(generatorAt(byte0), bytecodePC, 0, methodObj);
	target = (distance + 1) + bytecodePC;
	return genJumpTo(target);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorEqualsEquals */
static sqInt
genSpecialSelectorEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(0);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorNotEqualsEquals */
static sqInt
genSpecialSelectorNotEqualsEquals(void)
{
	return genInlinedIdenticalOrNotIf(1);
}

	/* SimpleStackBasedCogit>>#genSpecialSelectorSend */
static sqInt
genSpecialSelectorSend(void)
{
    sqInt index;
    sqInt numArgs;

	index = byte0 - ((bytecodeSetOffset == 0x100
		? AltFirstSpecialSelector + 0x100
		: FirstSpecialSelector));
	numArgs = specialSelectorNumArgs(index);
	return genSendnumArgs((-index) - 1, numArgs);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopReceiverVariableBytecode */
static sqInt
genStoreAndPopReceiverVariableBytecode(void)
{
	return genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(1, byte0 & 7, ((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
}

	/* SimpleStackBasedCogit>>#genStoreAndPopRemoteTempLongBytecode */
static sqInt
genStoreAndPopRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(1, byte1, byte2, ((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (shouldAnnotateObjectReference(((ssTop())->constant)))));
}

	/* SimpleStackBasedCogit>>#genStoreAndPopTemporaryVariableBytecode */
static sqInt
genStoreAndPopTemporaryVariableBytecode(void)
{
	return genStorePopTemporaryVariable(1, byte0 & 7);
}

	/* SimpleStackBasedCogit>>#genStoreRemoteTempLongBytecode */
static sqInt
genStoreRemoteTempLongBytecode(void)
{
	return genStorePopRemoteTempAtneedsStoreCheck(0, byte1, byte2, ((((ssTop())->type)) != SSConstant)
	 || ((isNonImmediate(((ssTop())->constant)))
	 && (shouldAnnotateObjectReference(((ssTop())->constant)))));
}

	/* SimpleStackBasedCogit>>#genTakeProfileSample */
static void
genTakeProfileSample(void)
{
    AbstractInstruction *abstractInstruction;

	addDependent(methodLabel, annotateAbsolutePCRef(gMoveCwR(((sqInt)methodLabel), ClassReg)));
	genMarshallNArgsargargargarg(backEnd, 1, ClassReg, null, null, null);
	genLoadNativeSPRegWithAlignedSPReg(backEnd);
	if (isInImmediateBranchAndLinkRange(backEnd, maximumDistanceFromCodeZone(((sqInt)ceTakeProfileSample)))) {
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ((usqIntptr_t)ceTakeProfileSample));
		(abstractInstruction->annotation = IsRelativeCall);
	}
	else {
		/* begin gen:literal: */
		checkLiteralforInstruction(((usqIntptr_t)ceTakeProfileSample), genoperand(CallFull, ((usqIntptr_t)ceTakeProfileSample)));
	}
	/* begin genRemoveNArgsFromStack: */
	assert(1 <= 6);
	genLoadCStackPointer(backEnd);
}


/*	Collect the branch and send data for cogMethod, storing it into arrayObj. */

	/* SimpleStackBasedCogit>>#mapPCDataFor:into: */
sqInt
mapPCDataForinto(CogMethod *cogMethod, sqInt arrayObj)
{
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt prim;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	introspectionDataIndex = 0;
	introspectionData = arrayObj;
	if (((cogMethod->stackCheckOffset)) == 0) {
		assert(introspectionDataIndex == 0);
		if ((cogMethod->cpicHasMNUCaseOrCMIsFullBlock)) {
			storePointerUncheckedofObjectwithValue(0, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(1, introspectionData, (((usqInt)cbNoSwitchEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(3, introspectionData, (((usqInt)cbEntryOffset << 3) | 1));
		}
		else {
			storePointerUncheckedofObjectwithValue(0, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(1, introspectionData, (((usqInt)cmEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 3) | 1));
		}
		return 4;
	}
	/* begin mapFor:bcpc:performUntil:arg: */
	startbcpc = startPCOfMethod((cogMethod->methodObject));
	assert((((((CogBlockMethod *) cogMethod))->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)(((CogBlockMethod *) cogMethod)))) + (((((CogBlockMethod *) cogMethod))->stackCheckOffset));
	result = pcDataForAnnotationMcpcBcpcMethod(null, (0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))), (((char *) mcpc)), startbcpc, (((void *)cogMethod)));
	if (result != 0) {
		errCode = result;
		goto l9;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if ((((((CogBlockMethod *) cogMethod))->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = ((((CogBlockMethod *) cogMethod))->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) (((CogBlockMethod *) cogMethod)));
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == (((((CogBlockMethod *) cogMethod))->startpc)));
		homeMethod = cmHomeMethod(((CogBlockMethod *) cogMethod));
		map = findMapLocationForMcpcinMethod((((usqInt)(((CogBlockMethod *) cogMethod)))) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj))
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l9;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l9;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc - 1, methodObj);
							upperByte = fetchByteofObject(bcpc - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = pcDataForAnnotationMcpcBcpcMethod(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc)), ((isBackwardBranch
	? bcpc - (2 * nExts)
	: bcpc)), (((void *)cogMethod)));
				if (result != 0) {
					errCode = result;
					goto l9;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	errCode = 0;
	l9:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode != 0) {
		assert(errCode == PrimErrNoMemory);
		return -1;
	}
	if (((cogMethod->blockEntryOffset)) != 0) {
		errCode = blockDispatchTargetsForperformarg(cogMethod, pcDataForBlockEntryMethod, ((sqInt)cogMethod));
		if (errCode != 0) {
			assert(errCode == PrimErrNoMemory);
			return -1;
		}
	}
	return introspectionDataIndex;
}

	/* SimpleStackBasedCogit>>#numSpecialSelectors */
static sqInt
numSpecialSelectors(void)
{
	return (bytecodeSetOffset == 0x100
				? AltNumSpecialSelectors
				: NumSpecialSelectors);
}


/*	Collect the branch and send data for the block method starting at
	blockEntryMcpc, storing it into picData.
 */

	/* SimpleStackBasedCogit>>#pcDataForBlockEntry:Method: */
static usqInt NoDbgRegParms
pcDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod)
{
	storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)(blockEntryMcpc - blockNoContextSwitchOffset) << 3) | 1));
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
	storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)blockEntryMcpc << 3) | 1));
	introspectionDataIndex += 4;
	return 0;
}

	/* SimpleStackBasedCogit>>#pcDataFor:Annotation:Mcpc:Bcpc:Method: */
static sqInt NoDbgRegParms
pcDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt actualBcpc;
    sqInt actualMcpc;

	if (!descriptor) {

		/* this is the stackCheck offset */
		assert(introspectionDataIndex == 0);
		if (((((CogMethod *) cogMethodArg))->cpicHasMNUCaseOrCMIsFullBlock)) {
			storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)cbNoSwitchEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)cbEntryOffset << 3) | 1));
		}
		else {
			storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)cmEntryOffset << 3) | 1));
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 2, introspectionData, nilObject());
			storePointerUncheckedofObjectwithValue(introspectionDataIndex + 3, introspectionData, (((usqInt)cmNoCheckEntryOffset << 3) | 1));
		}
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 4, introspectionData, (((usqInt)(bcpc + 1) << 3) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 5, introspectionData, (((((((CogMethod *) cogMethodArg))->stackCheckOffset)) << 3) | 1));
		introspectionDataIndex += 6;
		return 0;
	}
	if ((((usqInt)(isBackwardBranchAndAnnotation)) >> 1) >= HasBytecodePC) {
		actualBcpc = (((isBackwardBranchAndAnnotation & 1) != 0)
			? bcpc + 1
			: (bcpc + ((descriptor->numBytes))) + 1);
		actualMcpc = (((usqInt)mcpc)) - (((usqInt)cogMethodArg));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex, introspectionData, (((usqInt)actualBcpc << 3) | 1));
		storePointerUncheckedofObjectwithValue(introspectionDataIndex + 1, introspectionData, (((usqInt)actualMcpc << 3) | 1));
		introspectionDataIndex += 2;
	}
	return 0;
}


/*	If there is a generator for the current primitive then answer it;
	otherwise answer nil. */

	/* SimpleStackBasedCogit>>#primitiveGeneratorOrNil */
static PrimitiveDescriptor *
primitiveGeneratorOrNil(void)
{
    PrimitiveDescriptor *primitiveDescriptor;

	if (isQuickPrimitiveIndex(primitiveIndex)) {

		/* an unused one */
		primitiveDescriptor = (&(primitiveGeneratorTable[0]));
		(primitiveDescriptor->primitiveGenerator = quickPrimitiveGeneratorFor(primitiveIndex));
		return primitiveDescriptor;
	}
	if (((primitiveIndex >= 1) && (primitiveIndex <= MaxCompiledPrimitiveIndex))) {
		return (&(primitiveGeneratorTable[primitiveIndex]));
	}
	return null;
}

	/* SimpleStackBasedCogit>>#register:isInMask: */
static sqInt NoDbgRegParms
registerisInMask(sqInt reg, sqInt mask)
{
	return ((mask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))) != 0);
}

	/* SimpleStackBasedCogit>>#register:isNotInMask: */
static sqInt NoDbgRegParms
registerisNotInMask(sqInt reg, sqInt mask)
{
	return (!(mask & (((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)))));
}

	/* SimpleStackBasedCogit>>#v3:Block:Code:Size: */
static sqInt NoDbgRegParms
v3BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts <= 0);
	return (((sqInt)((usqInt)((fetchByteofObject(pc + 2, aMethodObj))) << 8))) + (fetchByteofObject(pc + 3, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:LongForward:Branch:Distance: */
static sqInt NoDbgRegParms
v3LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return (((sqInt)((usqInt)(((fetchByteofObject(pc, aMethodObj)) & 3)) << 8))) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	Answer the distance of a two byte forward long jump. */

	/* SimpleStackBasedCogit>>#v3:Long:Branch:Distance: */
static sqInt NoDbgRegParms
v3LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return (((sqInt)((usqInt)((((fetchByteofObject(pc, aMethodObj)) & 7) - 4)) << 8))) + (fetchByteofObject(pc + 1, aMethodObj));
}


/*	N.B. This serves for both BlueBook/V3 and V4 short jumps. */

	/* SimpleStackBasedCogit>>#v3:ShortForward:Branch:Distance: */
static sqInt NoDbgRegParms
v3ShortForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	assert(nExts == 0);
	return ((fetchByteofObject(pc, aMethodObj)) & 7) + 1;
}


/*	253		11111101 eei i i kkk	jjjjjjjj		Push Closure Num Copied iii (+ Ext A
	// 16 * 8) Num Args kkk (+ Ext A \\ 16 * 8) BlockSize jjjjjjjj (+ Ext B *
	256). ee = num extensions
 */

	/* SimpleStackBasedCogit>>#v4:Block:Code:Size: */
static sqInt NoDbgRegParms
v4BlockCodeSize(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt byteOne;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;


	/* If nExts < 0 it isn't known and we rely on the number of extensions encoded in the eeiiikkk byte. */
	byteOne = fetchByteofObject(pc + 1, aMethodObj);
	assert((nExts < 0)
	 || (nExts == (((usqInt)(byteOne)) >> 6)));
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - (((usqInt)(byteOne)) >> 6)) - (((usqInt)(byteOne)) >> 6);
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 2, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */
/*	243		11110011	i i i i i i i i	Pop and Jump 0n True i i i i i i i i (+
	Extend A * 256)
 */
/*	244		11110100	i i i i i i i i	Pop and Jump 0n False i i i i i i i i (+
	Extend A * 256)
 */

	/* SimpleStackBasedCogit>>#v4:LongForward:Branch:Distance: */
static sqInt NoDbgRegParms
v4LongForwardBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;

	assert(nExts >= 0);
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - nExts) - nExts;
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 1, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}


/*	242		11110010	i i i i i i i i	Jump i i i i i i i i (+ Extend B * 256,
	where bbbbbbbb = sddddddd, e.g. -32768 = i=0, a=0, s=1)
 */

	/* SimpleStackBasedCogit>>#v4:Long:Branch:Distance: */
static sqInt NoDbgRegParms
v4LongBranchDistance(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
    sqInt byte;
    sqInt extAValue;
    sqInt extBValue;
    sqInt extBValue1;
    sqInt extByte;
    sqInt pc1;

	assert(nExts >= 0);
	/* begin parseV4Exts:priorTo:in:into: */
	extAValue = (extBValue1 = 0);
	pc1 = (pc - nExts) - nExts;
	while (pc1 < pc) {
		byte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		extByte = fetchByteofObject(pc1, aMethodObj);
		pc1 += 1;
		assert((byte == 224)
		 || (byte == 225));
		if (byte == 224) {
			extAValue = ((((sqInt)((usqInt)(extAValue) << 8)))) + extByte;
		}
		else {
			extBValue1 = ((extBValue1 == 0)
			 && (extByte > 0x7F)
				? extByte - 0x100
				: ((((sqInt)((usqInt)(extBValue1) << 8)))) + extByte);
		}
	}
	extBValue = extBValue1;
	return (fetchByteofObject(pc + 1, aMethodObj)) + (((sqInt)((usqInt)(extBValue) << 8)));
}

	/* SistaCogit>>#compileCogFullBlockMethod: */
static CogMethod * NoDbgRegParms
compileCogFullBlockMethod(sqInt numCopied)
{
    sqInt allocBytes;
    sqInt fixupBytes;
    sqInt numBlocks;
    sqInt numBytecodes;
    sqInt numberOfAbstractOpcodes;
    sqInt numCleanBlocks;
    sqInt opcodeBytes;
    sqInt result;

	counters = 0;
	methodOrBlockNumTemps = tempCountOf(methodObj);
	setHasMovableLiteral(0);
	setHasYoungReferent(isYoungObject(methodObj));
	methodOrBlockNumArgs = argumentCountOf(methodObj);
	inBlock = InFullBlock;
	maxLitIndex = -1;
	assert((primitiveIndexOf(methodObj)) == 0);

	/* initial estimate.  Actual endPC is determined in scanMethod. */
	initialPC = startPCOfMethod(methodObj);
	endPC = numBytesOf(methodObj);
	numBytecodes = (endPC - initialPC) + 1;
	primitiveIndex = 0;
	/* begin allocateOpcodes:bytecodes:ifFail: */
	numberOfAbstractOpcodes = (numBytecodes + 10) * 11 /* estimateOfAbstractOpcodesPerBytecodes */;
	numAbstractOpcodes = numberOfAbstractOpcodes;
	opcodeBytes = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupBytes = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;

	/* Document the fact that the MaxStackAllocSize ensures that the number of abstract
	   opcodes fits in a 16 bit integer (e.g. CogBytecodeFixup's instructionIndex). */
	allocBytes = opcodeBytes + fixupBytes;
	assert((((sizeof(CogAbstractInstruction)) + (sizeof(CogBytecodeFixup))) * 0xC000) > MaxStackAllocSize);
	if (allocBytes > MaxStackAllocSize) {
		return ((CogMethod *) MethodTooBig);
		goto l1;
	}
	abstractOpcodes = alloca(allocBytes);
	bzero(abstractOpcodes, allocBytes);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeBytes));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	l1:	/* end allocateOpcodes:bytecodes:ifFail: */;
	flag("TODO");
	if (((numBlocks = scanMethod())) < 0) {
		return ((CogMethod *) numBlocks);
	}
	assert(numBlocks == 0);
	numCleanBlocks = scanForCleanBlocks();
	assert(numCleanBlocks == 0);
	allocateBlockStarts(numBlocks + numCleanBlocks);
	blockCount = 0;
	if (numCleanBlocks > 0) {
		addCleanBlockStarts();
	}
	if (!((maybeAllocAndInitCounters())
		 && (maybeAllocAndInitIRCs()))) {

		/* Inaccurate error code, but it'll do.  This will likely never fail. */
		return ((CogMethod *) InsufficientCodeSpace);
	}
	blockEntryLabel = null;
	(methodLabel->dependent = null);
	if (((result = compileEntireFullBlockMethod(numCopied))) < 0) {
		return ((CogMethod *) result);
	}
	return generateCogFullBlock();
}

	/* SistaCogit>>#compileCogMethod: */
static CogMethod * NoDbgRegParms
compileCogMethod(sqInt selector)
{
    sqInt allocBytes;
    int extra;
    sqInt fixupBytes;
    sqInt numBlocks;
    sqInt numBytecodes;
    sqInt numberOfAbstractOpcodes;
    sqInt numCleanBlocks;
    sqInt opcodeBytes;
    sqInt result;

	counters = 0;
	methodOrBlockNumTemps = tempCountOf(methodObj);
	setHasMovableLiteral(0);
	setHasYoungReferent((isYoungObject(methodObj))
	 || (isYoung(selector)));
	methodOrBlockNumArgs = argumentCountOf(methodObj);
	inBlock = 0;
	maxLitIndex = -1;
	extra = ((((primitiveIndex = primitiveIndexOf(methodObj))) > 0)
	 && (!(isQuickPrimitiveIndex(primitiveIndex)))
		? 30
		: 10);

	/* initial estimate.  Actual endPC is determined in scanMethod. */
	initialPC = startPCOfMethod(methodObj);
	endPC = (isQuickPrimitiveIndex(primitiveIndex)
		? initialPC - 1
		: numBytesOf(methodObj));
	numBytecodes = (endPC - initialPC) + 1;
	/* begin allocateOpcodes:bytecodes:ifFail: */
	numberOfAbstractOpcodes = (numBytecodes + extra) * 11 /* estimateOfAbstractOpcodesPerBytecodes */;
	numAbstractOpcodes = numberOfAbstractOpcodes;
	opcodeBytes = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
	fixupBytes = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;

	/* Document the fact that the MaxStackAllocSize ensures that the number of abstract
	   opcodes fits in a 16 bit integer (e.g. CogBytecodeFixup's instructionIndex). */
	allocBytes = opcodeBytes + fixupBytes;
	assert((((sizeof(CogAbstractInstruction)) + (sizeof(CogBytecodeFixup))) * 0xC000) > MaxStackAllocSize);
	if (allocBytes > MaxStackAllocSize) {
		return ((CogMethod *) MethodTooBig);
		goto l1;
	}
	abstractOpcodes = alloca(allocBytes);
	bzero(abstractOpcodes, allocBytes);
	fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeBytes));
	zeroOpcodeIndexForNewOpcodes();
	labelCounter = 0;
	l1:	/* end allocateOpcodes:bytecodes:ifFail: */;
	if (((numBlocks = scanMethod())) < 0) {
		return ((CogMethod *) numBlocks);
	}
	numCleanBlocks = scanForCleanBlocks();
	if (methodFoundInvalidPostScan()) {
		return ((CogMethod *) ShouldNotJIT);
	}
	allocateBlockStarts(numBlocks + numCleanBlocks);
	blockCount = 0;
	if (numCleanBlocks > 0) {
		addCleanBlockStarts();
	}
	if (!((maybeAllocAndInitCounters())
		 && (maybeAllocAndInitIRCs()))) {

		/* Inaccurate error code, but it'll do.  This will likely never fail. */
		return ((CogMethod *) InsufficientCodeSpace);
	}
	blockEntryLabel = null;
	(methodLabel->dependent = null);
	if (((result = compileEntireMethod())) < 0) {
		return ((CogMethod *) result);
	}
	return generateCogMethod(selector);
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any. */
/*	Override to prefetch counters, if any. */

	/* SistaCogit>>#compileFrameBuild */
static void
compileFrameBuild(void)
{
    AbstractInstruction *anInstruction;
    sqInt constant;
    sqInt i;
    sqInt iLimiT;


#  if IMMUTABILITY
	if (useTwoPaths) {
		compileTwoPathFrameBuild();
		return;
	}
#  endif
	if (!needsFrame) {
		if (useTwoPaths) {
			compileTwoPathFramelessInit();
		}
		initSimStackForFramelessMethod(initialPC);
		return;
	}
	assert(!(useTwoPaths));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, LinkReg);
	/* begin PushR: */
	genoperand(PushR, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(gPushCw(((sqInt)methodLabel))));
	/* begin genMoveNilR: */
	constant = nilObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, SendNumArgsReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = (methodOrBlockNumArgs + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
	if (counters != 0) {
		/* begin PrefetchAw: */
		checkLiteralforInstruction(counters, genoperand(PrefetchAw, counters));
	}
}


/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. closure (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	Avoid use of SendNumArgsReg which is the flag determining whether
	context switch is allowed on stack-overflow. */
/*	Override to prefetch counters if any */

	/* SistaCogit>>#compileFullBlockMethodFrameBuild: */
static void NoDbgRegParms
compileFullBlockMethodFrameBuild(sqInt numCopied)
{
    AbstractInstruction *anInstruction;
    sqInt constant;
    sqInt i;
    sqInt iLimiT;

	if (useTwoPaths) {

		/* method with only inst var store, we compile only slow path for now */
		useTwoPaths = 0;
#    if IMMUTABILITY
		needsFrame = 1;
#    endif
	}
	if (!needsFrame) {

		/* it is OK for numCopied to be non-zero provided that the block does not actually use the copied values.
		   There are some blocks like this, e.g. that simply reference copied values to mark them as used for Slang.
		   See e.g. CroquetPlugin>>#primitiveGatherEntropy which contains the block [bufPtr. bufSize. false],
		   which the bytecode compiler optimizes to [false]. */
		compileFullBlockFramelessEntry(numCopied);
		initSimStackForFramelessBlock(initialPC);
		return;
	}
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, LinkReg);
	/* begin PushR: */
	genoperand(PushR, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	addDependent(methodLabel, annotateAbsolutePCRef(gPushCw(((sqInt)methodLabel))));
	/* begin setLabelOffset: */
	(((((AbstractInstruction *) methodLabel))->operands))[1] = MFMethodFlagIsBlockFlag;
	/* begin genMoveNilR: */
	constant = nilObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, SendNumArgsReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	flag("TODO");
	genLoadSlotsourceRegdestReg(FullClosureReceiverIndex, ClassReg, Arg0Reg);
	genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, FullClosureReceiverIndex, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = 0; i < numCopied; i += 1) {
		genLoadSlotsourceRegdestReg(i + FullClosureFirstCopiedValueIndex, ClassReg, TempReg);
		/* begin PushR: */
		genoperand(PushR, TempReg);
	}
	for (i = ((methodOrBlockNumArgs + numCopied) + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	stackCheckLabel = compileStackOverflowCheck(1);
	initSimStackForFramefulMethod(initialPC);
	if (counters != 0) {
		/* begin PrefetchAw: */
		checkLiteralforInstruction(counters, genoperand(PrefetchAw, counters));
	}
}


/*	Return the default number of bytes to allocate for native code at startup.
	The actual value can be set via vmParameterAt: and/or a preference in the
	ini file. */

	/* SistaCogit>>#defaultCogCodeSize */
sqInt
defaultCogCodeSize(void)
{
	return 0x340000;
}

	/* SistaCogit>>#fillInCounters:atStartAddress: */
static void NoDbgRegParms
fillInCountersatStartAddress(sqInt nCounters, sqInt startAddress)
{
    sqInt address;

	for (address = startAddress; address <= (startAddress + ((nCounters - 1) * CounterBytes)); address += CounterBytes) {
		long32Atput(address, (((sqInt)((usqInt)(initialCounterValue) << 16))) + initialCounterValue);
	}
}


/*	Fill in the header for theCogMethod method. This may be located at the
	writable mapping. */

	/* SistaCogit>>#fillInMethodHeader:size:selector: */
static void NoDbgRegParms
fillInMethodHeadersizeselector(CogMethod *method, sqInt size, sqInt selector)
{
    sqInt actualMethodLocation;
    unsigned int codeInstruction;
    unsigned int dataInstruction;
    CogMethod *originalMethod;
    sqInt rawHeader;

	actualMethodLocation = (((usqInt)method)) - codeToDataDelta;
	(method->cmType = CMMethod);
	(method->objectHeader = nullHeaderForMachineCodeMethod());
	(method->blockSize = size);
	(method->methodObject = methodObj);

	/* If the method has already been cogged (e.g. Newspeak accessors) then
	   leave the original method attached to its cog method, but get the right header. */
	rawHeader = rawHeaderOf(methodObj);
	if (isCogMethodReference(rawHeader)) {
		originalMethod = ((CogMethod *) rawHeader);
		assert(((originalMethod->blockSize)) == size);
		assert(methodHeader == ((originalMethod->methodHeader)));
			}
	else {
		rawHeaderOfput(methodObj, actualMethodLocation);
			}
	(method->methodHeader = methodHeader);
	(method->selector = selector);
	(method->cmNumArgs = argumentCountOfMethodHeader(methodHeader));
	(method->cmHasMovableLiteral = hasMovableLiteral);
	if ((method->cmRefersToYoung = hasYoungReferent)) {
		addToYoungReferrers(method);
	}
	(method->cmUsageCount = initialMethodUsageCount());
	/* begin cpicHasMNUCase: */
	((((CogBlockMethod *) method))->cpicHasMNUCaseOrCMIsFullBlock) = 0;
	(method->cmUsesPenultimateLit = maxLitIndex >= ((literalCountOfMethodHeader(methodHeader)) - 2));
	(method->blockEntryOffset = (blockEntryLabel != null
		? ((blockEntryLabel->address)) - actualMethodLocation
		: 0));
	if (needsFrame) {
		if (!((((stackCheckLabel->address)) - actualMethodLocation) <= MaxStackCheckOffset)) {
			error("too much code for stack check offset");
		}
	}
	(method->stackCheckOffset = (needsFrame
		? ((stackCheckLabel->address)) - actualMethodLocation
		: 0));
	assert((callTargetFromReturnAddress(backEnd, actualMethodLocation + missOffset)) == (methodAbortTrampolineFor((method->cmNumArgs))));
	assert(size == (roundUpLength(size)));
	/* begin assertValidDualZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	/* begin assertCoherentCodeAt:delta: */
	codeInstruction = long32At(actualMethodLocation + cmNoCheckEntryOffset);
	dataInstruction = long32At((actualMethodLocation + cmNoCheckEntryOffset) + codeToDataDelta);
	assert(codeInstruction == dataInstruction);
#  endif // DUAL_MAPPED_CODE_ZONE
	fillInCountersatStartAddress(numCounters, counters);
	(method->counters = counters);
}


/*	The store check requires rr to be ReceiverResultReg */

	/* SistaCogit>>#genAtPutInlinePrimitive: */
static sqInt NoDbgRegParms
genAtPutInlinePrimitive(sqInt prim)
{
	switch (prim) {
	case 0:
		genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(0, 0, 0);
		break;
	case 1:
		genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(0, 1, 0);
		break;
	case 2:
		genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(1, 0, 0);
		break;
	case 3:
		genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(1, 1, 0);
		break;
	case 4:
		genByteAtPut();
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	return 0;
}


/*	2064	pointerAt:
	Pointer object (Fixed sized or not) and not a context, Smi => (1-based,
	optimised if arg1 is a constant)
	2065	maybeContextPointerAt:
	Pointer object (Fixed sized or not), Smi => (1-based, optimised if arg1 is
	a constant)
	2066	byteAt:
	byte object, Smi => 8 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2067	shortAt:
	short object, Smi => 16 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2068	wordAt:
	word object, Smi => 32 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2069	doubleWordAt:
	double word object, Smi => 64 bits unsigned Smi or LargePositiveInteger
	(1-based, optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genBinaryAtConstInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryAtConstInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *jmpDone;
    AbstractInstruction *jmpSingle;
    sqInt rr;
    sqInt val;
    sqInt zeroBasedIndex;

	val = ((ssTop())->constant);
	if (primIndex == 65) {
		/* begin ssAllocateRequiredReg: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
		voidReceiverResultRegContainsSelf();
		rr = ReceiverResultReg;
	}
	else {
		rr = allocateRegForStackEntryAtnotConflictingWith(1, 0);
	}
	popToReg(ssValue(1), rr);
	ssPop(2);
	zeroBasedIndex = ((val >> 3)) - 1;
	switch (primIndex) {
	case 64:
		genLoadSlotsourceRegdestReg(zeroBasedIndex, rr, rr);
		break;
	case 65:
		/* begin ssAllocateRequiredReg: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << SendNumArgsReg), simStackPtr, simNativeStackPtr);
		/* begin genPushMaybeContextSlotIndex: */
		assert(needsFrame);
		if (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {

			/* We have no way of reloading ReceiverResultReg since we need the inst var value as the result. */
			voidReceiverResultRegContainsSelf();
		}
		if (zeroBasedIndex == InstructionPointerIndex) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction2 = genoperandoperand(MoveCqR, zeroBasedIndex, SendNumArgsReg);
			if (usesOutOfLineLiteral(anInstruction2)) {
				(anInstruction2->dependent = locateLiteralsize(zeroBasedIndex, BytesPerOop));
			}
			/* begin CallRT: */
			abstractInstruction = genoperand(Call, ceFetchContextInstVarTrampoline);
			(abstractInstruction->annotation = IsRelativeCall);
			return ssPushRegister(SendNumArgsReg);
		}
		genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
		/* begin genJumpNotSmallIntegerInScratchReg: */
		jmpSingle = genJumpNotSmallInteger(TempReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, zeroBasedIndex, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(zeroBasedIndex, BytesPerOop));
		}
		/* begin CallRT: */
		abstractInstruction1 = genoperand(Call, ceFetchContextInstVarTrampoline);
		(abstractInstruction1->annotation = IsRelativeCall);
		/* begin Jump: */
		jmpDone = genoperand(Jump, ((sqInt)0));
		jmpTarget(jmpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		genLoadSlotsourceRegdestReg(zeroBasedIndex, ReceiverResultReg, SendNumArgsReg);
		jmpTarget(jmpDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		return ssPushRegister(SendNumArgsReg);

	case 66:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMbrR, zeroBasedIndex + BaseHeaderSize, rr, rr);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(zeroBasedIndex + BaseHeaderSize, BytesPerOop));
		}
		genConvertIntegerToSmallIntegerInReg(rr);
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	return ssPushRegister(rr);
}


/*	2064	pointerAt:
	Pointer object (Fixed sized or not) and not a context, Smi => (1-based,
	optimised if arg1 is a constant)
	2065	maybeContextPointerAt:
	Pointer object (Fixed sized or not), Smi => (1-based, optimised if arg1 is
	a constant)
	2066	byteAt:
	byte object, Smi => 8 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2067	shortAt:
	short object, Smi => 16 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2068	wordAt:
	word object, Smi => 32 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2069	doubleWordAt:
	double word object, Smi => 64 bits unsigned Smi or LargePositiveInteger
	(1-based, optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genBinaryAtInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryAtInlinePrimitive(sqInt primIndex)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt ra;
    sqInt reg;
    sqInt rNext1;
    sqInt rr;
    sqInt rTop1;
    sqInt topRegistersMask;

	/* begin allocateRegForStackTopTwoEntriesInto: */
	topRegistersMask = 0;
	rTop1 = (rNext1 = NoReg);
	if ((registerOrNone(ssTop())) != NoReg) {
		rTop1 = registerOrNone(ssTop());
	}
	if ((registerOrNone(ssValue(1))) != NoReg) {
		/* begin registerMaskFor: */
		reg = (rNext1 = registerOrNone(ssValue(1)));
		topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	}
	if (rTop1 == NoReg) {
		rTop1 = allocateRegNotConflictingWith(topRegistersMask);
	}
	if (rNext1 == NoReg) {
		rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
	}
	assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
	ra = rTop1;
	rr = rNext1;
	popToReg(ssTop(), ra);
	ssPop(1);
	popToReg(ssTop(), rr);
	ssPop(1);
	switch (primIndex) {
	case 64:
		genConvertSmallIntegerToIntegerInReg(ra);
		adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(AddCqR, adjust, ra);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
		}
		/* begin MoveXwr:R:R: */
		genoperandoperandoperand(MoveXwrRR, ra, rr, rr);
		break;
	case 66:
		genConvertSmallIntegerToIntegerInReg(ra);
		adjust = BaseHeaderSize - 1;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(AddCqR, adjust, ra);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(adjust, BytesPerOop));
		}
		/* begin MoveXbr:R:R: */
		genoperandoperandoperand(MoveXbrRR, ra, rr, rr);
		genConvertIntegerToSmallIntegerInReg(rr);
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	return ssPushRegister(rr);
}


/*	2032	>
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2033	<
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2034	>=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2035	<=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2036	=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2037	~=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2038	rawEqualsEquals:
	not a forwarder, not a forwarder => Boolean (optimised if one operand is a
	constant, Pipelined with ifTrue:ifFalse:)
	2039	rawNotEqualsEquals:
	not a forwarder, not a forwarder => Boolean (optimised if one operand is a
	constant, Pipelined with ifTrue:ifFalse:)
 */

	/* SistaCogit>>#genBinaryCompInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryCompInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt constant;
    sqInt constant1;
    sqInt invertedOpFalse;
    sqInt invertedOpTrue;
    sqInt opFalse;
    sqInt opTrue;
    sqInt otherReg;
    sqInt resultReg;

	assert(((primIndex >= 32) && (primIndex <= 39)));
	switch (primIndex) {
	case 32:
		opTrue = JumpGreater;
		opFalse = JumpLessOrEqual;
		invertedOpTrue = JumpLess;
		invertedOpFalse = JumpGreaterOrEqual;
		break;
	case 33:
		opTrue = JumpLess;
		opFalse = JumpGreaterOrEqual;
		invertedOpTrue = JumpGreater;
		invertedOpFalse = JumpLessOrEqual;
		break;
	case 34:
		opTrue = JumpGreaterOrEqual;
		opFalse = JumpLess;
		invertedOpTrue = JumpLessOrEqual;
		invertedOpFalse = JumpGreater;
		break;
	case 35:
		opTrue = JumpLessOrEqual;
		opFalse = JumpGreater;
		invertedOpTrue = JumpGreaterOrEqual;
		invertedOpFalse = JumpLess;
		break;
	case 36:
	case 38:
		opTrue = JumpZero;
		opFalse = JumpNonZero;
		invertedOpTrue = JumpZero;
		invertedOpFalse = JumpNonZero;
		break;
	case 37:
	case 39:
		opTrue = JumpNonZero;
		opFalse = JumpZero;
		invertedOpTrue = JumpNonZero;
		invertedOpFalse = JumpZero;
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	if ((((ssTop())->type)) == SSConstant) {
		resultReg = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), resultReg);
		/* begin genCmpConstant:R: */
		constant = ((ssTop())->constant);
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, resultReg)), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(CmpCqR, constant, resultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
		ssPop(2);
		genBinaryInlineComparisonopFalsedestReg(opTrue, opFalse, resultReg);
		return ssPushRegister(resultReg);
	}
	if ((((ssValue(1))->type)) == SSConstant) {
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		popToReg(ssTop(), resultReg);
		/* begin genCmpConstant:R: */
		constant1 = ((ssValue(1))->constant);
		if (shouldAnnotateObjectReference(constant1)) {
			annotateobjRef(checkLiteralforInstruction(constant1, genoperandoperand(CmpCwR, constant1, resultReg)), constant1);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(CmpCqR, constant1, resultReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(constant1, BytesPerOop));
			}
		}
		ssPop(2);
		genBinaryInlineComparisonopFalsedestReg(invertedOpTrue, invertedOpFalse, resultReg);
		return ssPushRegister(resultReg);
	}
	otherReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), otherReg);
	resultReg = allocateRegForStackEntryAtnotConflictingWith(1, ((otherReg < 0) ? (((usqInt)(1)) >> (-otherReg)) : (1ULL << otherReg)));
	popToReg(ssValue(1), resultReg);
	/* begin CmpR:R: */
	assert(!((otherReg == SPReg)));
	genoperandoperand(CmpRR, otherReg, resultReg);
	ssPop(2);
	genBinaryInlineComparisonopFalsedestReg(opTrue, opFalse, resultReg);
	return ssPushRegister(resultReg);
}

	/* SistaCogit>>#genBinaryConstOpVarSmiInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryConstOpVarSmiInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    sqInt ra;
    sqInt untaggedVal;
    sqInt val;

	ra = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), ra);
	ssPop(1);
	val = ((ssTop())->constant);
	ssPop(1);
	untaggedVal = val - (smallIntegerTag());
	switch (primIndex) {
	case 0:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, untaggedVal, ra);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(untaggedVal, BytesPerOop));
		}
		break;
	case 1:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, val, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(val, BytesPerOop));
		}
		/* begin SubR:R: */
		genoperandoperand(SubRR, ra, TempReg);
		genAddSmallIntegerTagsTo(TempReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, TempReg, ra);
		break;
	case 2:
		genShiftAwaySmallIntegerTagsInScratchReg(ra);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(MoveCqR, untaggedVal, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(untaggedVal, BytesPerOop));
		}
		/* begin MulR:R: */
		genMulRR(backEnd, TempReg, ra);
		genSetSmallIntegerTagsIn(ra);
		break;
	case 16:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(AndCqR, val, ra);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(val, BytesPerOop));
		}
		break;
	case 17:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(OrCqR, val, ra);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(val, BytesPerOop));
		}
		break;
	case 18:
		/* begin XorCw:R: */
		checkLiteralforInstruction(untaggedVal, genoperandoperand(XorCwR, untaggedVal, ra));
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	ssPushRegister(ra);
	return 0;
}


/*	Inlined comparison. opTrue = jump for true and opFalse = jump for false */

	/* SistaCogit>>#genBinaryInlineComparison:opFalse:destReg: */
static sqInt NoDbgRegParms
genBinaryInlineComparisonopFalsedestReg(sqInt opTrue, sqInt opFalse, sqInt destReg)
{
    AbstractInstruction *anInstruction;
    sqInt bcpc;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    AbstractInstruction *condJump;
    sqInt constant;
    BytecodeDescriptor *descriptor;
    sqInt eA;
    sqInt eB;
    AbstractInstruction *jump;
    void *jumpTarget;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    sqInt savedB0;
    sqInt savedB1;
    sqInt savedB2;
    sqInt savedB3;
    sqInt savedEA;
    sqInt savedEB;
    sqInt savedNEB;
    sqInt targetBytecodePC;
    sqInt targetBytecodePC1;

	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC1 = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetBytecodePC = targetBytecodePC1;
	if (((branchDescriptor->isBranchTrue))
	 || ((branchDescriptor->isBranchFalse))) {

		/* This is the path where the inlined comparison is followed immediately by a branch */
		if ((((fixupAt(nextPC))->targetInstruction)) == 0) {

			/* The next instruction is dead.  we can skip it. */
			deadCode = 1;
			ensureFixupAt(targetBytecodePC);
			ensureFixupAt(postBranchPC);
		}
		else {
			ssPushConstant(trueObject());
		}
		genConditionalBranchoperand(((branchDescriptor->isBranchTrue)
			? opTrue
			: opFalse), ((usqInt)(ensureNonMergeFixupAt(targetBytecodePC))));
		/* begin nextDescriptorExtensionsAndNextPCInto: */
		descriptor = generatorAt(byte0);
		savedB0 = byte0;
		savedB1 = byte1;
		savedB2 = byte2;
		savedB3 = byte3;
		savedEA = extA;
		savedEB = extB;
		savedNEB = numExtB;
		bcpc = bytecodePC + ((descriptor->numBytes));
		do {
			if (bcpc > endPC) {
				nextPC = 0;
				goto l2;
			}
			byte0 = (fetchByteofObject(bcpc, methodObj)) + bytecodeSetOffset;
			descriptor = generatorAt(byte0);
			loadSubsequentBytesForDescriptorat(descriptor, bcpc);
			if (!((descriptor->isExtension))) {
				eA = extA;
				eB = extB;
				extA = savedEA;
				extB = savedEB;
				numExtB = savedNEB;
				byte0 = savedB0;
				byte1 = savedB1;
				byte2 = savedB2;
				byte3 = savedB3;
				nextPC = bcpc;
				goto l2;
			}
			((descriptor->generator))();
			bcpc += (descriptor->numBytes);
		} while(1);
	l2:	/* end nextDescriptorExtensionsAndNextPCInto: */;
		if (!(deadCode
			 && (nextPC == postBranchPC))) {
			/* begin Jump: */
			jumpTarget = ensureNonMergeFixupAt(postBranchPC);
			genoperand(Jump, ((sqInt)jumpTarget));
		}
	}
	else {

		/* This is the path where the inlined comparison is *not* followed immediately by a branch */
		condJump = genConditionalBranchoperand(opTrue, 0);
		/* begin genMoveFalseR: */
		constant = falseObject();
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(gMoveCwR(constant, destReg), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(MoveCqR, constant, destReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
		/* begin Jump: */
		jump = genoperand(Jump, ((sqInt)0));
		jmpTarget(condJump, genMoveTrueR(destReg));
		jmpTarget(jump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	return 0;
}


/*	Bulk comments: each sub-method has its own comment with the specific case.
	2000	+
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2001	-
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2002	*
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2003	/
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2004	//
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2005	\
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2006	quo:
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant)
	2016	bitAnd:
	Smi, Smi => Smi (optimised if one operand is a constant)
	2017	bitOr:
	Smi, Smi => Smi (optimised if one operand is a constant)
	2018	bitXor:
	Smi, Smi => Smi (optimised if one operand is a constant)
	2019	bitShiftLeft:
	Smi greater or equal to 0, Smi greater or equal to 0 => Smi (no overflow,
	optimised if arg1 is a constant)
	2020	bitShiftRight:
	Smi, Smi greater or equal to 0 => Smi (optimised if arg1 is a constant)
	2032	>
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2033	<
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2034	>=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2035	<=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2036	=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2037	~=
	Smi, Smi => Boolean (optimised if one operand is a constant, Pipelined
	with ifTrue:ifFalse:)
	2038	rawEqualsEquals:
	not a forwarder, not a forwarder => Boolean (optimised if one operand is a
	constant, Pipelined with ifTrue:ifFalse:)
	2039	rawNotEqualsEquals:
	not a forwarder, not a forwarder => Boolean (optimised if one operand is a
	constant, Pipelined with ifTrue:ifFalse:)
	2048	rawNew:
	literal which is a fixed-sized behavior, Smi => instance of receiver,
	fields nilled out (optimised if arg1 is a constant)
	2049	rawNewNoInit:
	literal which is a fixed-sized behavior, Smi => instance of receiver
	(Fields of returned value contain undefined data, optimised if arg1 is a
	constant) 2064	pointerAt:
	Pointer object (Fixed sized or not) and not a context, Smi => (1-based,
	optimised if arg1 is a constant)
	2065	maybeContextPointerAt:
	Pointer object (Fixed sized or not), Smi => (1-based, optimised if arg1 is
	a constant)
	2066	byteAt:
	byte object, Smi => 8 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2067	shortAt:
	short object, Smi => 16 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2068	wordAt:
	word object, Smi => 32 bits unsigned Smi (1-based, optimised if arg1 is a
	constant) 2069	doubleWordAt:
	double word object, Smi => 64 bits unsigned Smi or LargePositiveInteger
	(1-based, optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genBinaryInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryInlinePrimitive(sqInt primIndex)
{
	if ((primIndex <= 18)
	 && ((primIndex <= 2)
	 || (primIndex > 6))) {
		if ((((ssTop())->type)) == SSConstant) {
			return genBinaryVarOpConstSmiInlinePrimitive(primIndex);
		}
		if ((((ssValue(1))->type)) == SSConstant) {
			return genBinaryConstOpVarSmiInlinePrimitive(primIndex);
		}
		return genBinaryVarOpVarSmiInlinePrimitive(primIndex);
	}
	if (primIndex <= 6) {
		return genDivInlinePrimitive(primIndex);
	}
	if (primIndex == 19) {
		return genBinarySmiBitShiftLeftInlinePrimitive();
	}
	if (primIndex == 20) {
		return genBinarySmiBitShiftRightInlinePrimitive();
	}
	if (primIndex < 32) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 39) {
		return genBinaryCompInlinePrimitive(primIndex);
	}
	if (primIndex < 48) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 49) {
		return genBinaryNewInlinePrimitive(primIndex);
	}
	if (primIndex < 64) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 69) {
		if ((((ssTop())->type)) == SSConstant) {
			return genBinaryAtConstInlinePrimitive(primIndex);
		}
		return genBinaryAtInlinePrimitive(primIndex);
	}
	return EncounteredUnknownBytecode;
}


/*	2048	rawNew:
	literal which is a variable-sized behavior, Smi => instance of receiver,
	fields nilled/zeroed out (optimised if arg1 is a constant)
	2049	rawNewNoInit:
	literal which is a variable-sized behavior, Smi => instance of receiver
	(Fields of returned value contain undefined data, optimised if arg1 is a
	constant) 
 */
/*	Assertion */

	/* SistaCogit>>#genBinaryNewInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryNewInlinePrimitive(sqInt primIndex)
{
    sqInt argInt;
    sqInt classFormat;
    sqInt classObj;
    sqInt resultReg;

	if (!((((ssValue(1))->type)) == SSConstant)) {
		return UnimplementedOperation;
	}
	if (!((((ssTop())->type)) == SSConstant)) {
		return UnimplementedOperation;
	}
	classObj = ((ssValue(1))->constant);
	assert(isNonImmediate(classObj));
	assert(objCouldBeClassObj(classObj));
	assert(!((isFixedSizePointerFormat(instSpecOfClassFormat(formatOfClass(classObj))))));
	classTagForClass(classObj);
	resultReg = allocateRegNotConflictingWith(0);
	classFormat = instSpecOfClassFormat(formatOfClass(classObj));
	argInt = ((((ssTop())->constant)) >> 3);
	ssPop(2);
	ssPushRegister(resultReg);
	if ((classFormat == (arrayFormat()))
	 || (classFormat == (indexablePointersFormat()))) {
		return genGetInstanceOfPointerClassintoinitializingIfnumVariableSlots(classObj, resultReg, primIndex == 48, argInt);
	}
	if (classFormat == (firstByteFormat())) {
		return genGetInstanceOfByteClassintoinitializingIfnumBytes(classObj, resultReg, primIndex == 48, argInt);
	}
	return UnimplementedOperation;
}


/*	2019	bitShiftLeft:
	Smi greater or equal to 0, Smi greater or equal to 0 => Smi (no overflow,
	optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genBinarySmiBitShiftLeftInlinePrimitive */
static sqInt
genBinarySmiBitShiftLeftInlinePrimitive(void)
{
    sqInt quickConstant;
    sqInt ra;
    sqInt rr;

	rr = allocateRegForStackEntryAtnotConflictingWith(1, 0);
	popToReg(ssValue(1), rr);
	genRemoveSmallIntegerTagsInScratchReg(rr);
	if ((((ssTop())->type)) == SSConstant) {
		/* begin LogicalShiftLeftCq:R: */
		quickConstant = ((((ssTop())->constant)) >> 3);
		genoperandoperand(LogicalShiftLeftCqR, quickConstant, rr);
	}
	else {
		ra = allocateRegForStackEntryAtnotConflictingWith(0, ((rr < 0) ? (((usqInt)(1)) >> (-rr)) : (1ULL << rr)));
		popToReg(ssTop(), ra);
		genConvertSmallIntegerToIntegerInReg(ra);
		/* begin LogicalShiftLeftR:R: */
		genoperandoperand(LogicalShiftLeftRR, ra, rr);
	}
	genAddSmallIntegerTagsTo(rr);
	ssPop(2);
	return ssPushRegister(rr);
}


/*	2019	bitShiftLeft:
	Smi greater or equal to 0, Smi greater or equal to 0 => Smi (no overflow,
	optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genBinarySmiBitShiftRightInlinePrimitive */
static sqInt
genBinarySmiBitShiftRightInlinePrimitive(void)
{
    sqInt quickConstant;
    sqInt ra;
    sqInt rr;

	rr = allocateRegForStackEntryAtnotConflictingWith(1, 0);
	popToReg(ssValue(1), rr);
	if ((((ssTop())->type)) == SSConstant) {
		/* begin ArithmeticShiftRightCq:R: */
		quickConstant = ((((ssTop())->constant)) >> 3);
		genoperandoperand(ArithmeticShiftRightCqR, quickConstant, rr);
	}
	else {
		ra = allocateRegForStackEntryAtnotConflictingWith(0, ((rr < 0) ? (((usqInt)(1)) >> (-rr)) : (1ULL << rr)));
		popToReg(ssTop(), ra);
		genConvertSmallIntegerToIntegerInReg(ra);
		/* begin ArithmeticShiftRightR:R: */
		genoperandoperand(ArithmeticShiftRightRR, ra, rr);
	}
	genClearAndSetSmallIntegerTagsIn(rr);
	ssPop(2);
	return ssPushRegister(rr);
}

	/* SistaCogit>>#genBinaryVarOpConstSmiInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryVarOpConstSmiInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    sqInt rr;
    sqInt untaggedVal;
    sqInt val;

	assert(primIndex <= 18);
	val = ((ssTop())->constant);
	ssPop(1);
	rr = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), rr);
	ssPop(1);
	untaggedVal = val - (smallIntegerTag());
	switch (primIndex) {
	case 0:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, untaggedVal, rr);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(untaggedVal, BytesPerOop));
		}
		break;
	case 1:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(SubCqR, untaggedVal, rr);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(untaggedVal, BytesPerOop));
		}
		break;
	case 2:
		flag("could use MulCq:R");
		genShiftAwaySmallIntegerTagsInScratchReg(rr);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(MoveCqR, untaggedVal, TempReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(untaggedVal, BytesPerOop));
		}
		/* begin MulR:R: */
		genMulRR(backEnd, TempReg, rr);
		genSetSmallIntegerTagsIn(rr);
		break;
	case 16:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(AndCqR, val, rr);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(val, BytesPerOop));
		}
		break;
	case 17:
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(OrCqR, val, rr);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(val, BytesPerOop));
		}
		break;
	case 18:
		flag("could use XorCq:");
		/* begin XorCw:R: */
		checkLiteralforInstruction(untaggedVal, genoperandoperand(XorCwR, untaggedVal, rr));
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	ssPushRegister(rr);
	return 0;
}

	/* SistaCogit>>#genBinaryVarOpVarSmiInlinePrimitive: */
static sqInt NoDbgRegParms
genBinaryVarOpVarSmiInlinePrimitive(sqInt primIndex)
{
    sqInt ra;
    sqInt reg;
    sqInt rNext1;
    sqInt rr;
    sqInt rTop1;
    sqInt topRegistersMask;

	/* begin allocateRegForStackTopTwoEntriesInto: */
	topRegistersMask = 0;
	rTop1 = (rNext1 = NoReg);
	if ((registerOrNone(ssTop())) != NoReg) {
		rTop1 = registerOrNone(ssTop());
	}
	if ((registerOrNone(ssValue(1))) != NoReg) {
		/* begin registerMaskFor: */
		reg = (rNext1 = registerOrNone(ssValue(1)));
		topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	}
	if (rTop1 == NoReg) {
		rTop1 = allocateRegNotConflictingWith(topRegistersMask);
	}
	if (rNext1 == NoReg) {
		rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
	}
	assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
	ra = rTop1;
	rr = rNext1;
	popToReg(ssTop(), ra);
	ssPop(1);
	popToReg(ssTop(), rr);
	ssPop(1);
	switch (primIndex) {
	case 0:
		genRemoveSmallIntegerTagsInScratchReg(ra);
		/* begin AddR:R: */
		genoperandoperand(AddRR, ra, rr);
		break;
	case 1:
		/* begin SubR:R: */
		genoperandoperand(SubRR, ra, rr);
		genAddSmallIntegerTagsTo(rr);
		break;
	case 2:
		genShiftAwaySmallIntegerTagsInScratchReg(rr);
		genRemoveSmallIntegerTagsInScratchReg(ra);
		/* begin MulR:R: */
		genMulRR(backEnd, ra, rr);
		genSetSmallIntegerTagsIn(rr);
		break;
	case 16:
		/* begin AndR:R: */
		genoperandoperand(AndRR, ra, rr);
		break;
	case 17:
		/* begin OrR:R: */
		genoperandoperand(OrRR, ra, rr);
		break;
	case 18:
		genRemoveSmallIntegerTagsInScratchReg(ra);
		/* begin XorR:R: */
		genoperandoperand(XorRR, ra, rr);
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	ssPushRegister(rr);
	return 0;
}

	/* SistaCogit>>#genByteAtPut */
static sqInt
genByteAtPut(void)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    sqInt ra1;
    sqInt ra2;
    sqInt reg;
    sqInt rNext1;
    sqInt rr;
    sqInt rThird1;
    sqInt rTop1;
    sqInt topRegistersMask;

	/* begin allocateRegForStackTopThreeEntriesInto:thirdIsReceiver: */
	topRegistersMask = 0;
	rTop1 = (rNext1 = (rThird1 = NoReg));
	if (((registerOrNone(ssTop())) != NoReg)) {
		/* begin registerMaskFor: */
		reg = (rTop1 = registerOrNone(ssTop()));
		topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	}
	if (((registerOrNone(ssValue(1))) != NoReg)) {
		topRegistersMask = topRegistersMask | (registerMaskFor((rNext1 = registerOrNone(ssValue(1)))));
	}
	if (((registerOrNone(ssValue(2))) != NoReg)) {
		topRegistersMask = topRegistersMask | (registerMaskFor((rThird1 = registerOrNone(ssValue(2)))));
	}
	if (rThird1 == NoReg) {
		rThird1 = allocateRegNotConflictingWith(topRegistersMask);
		topRegistersMask = topRegistersMask | (((rThird1 < 0) ? (((usqInt)(1)) >> (-rThird1)) : (1ULL << rThird1)));
	}
	if (rTop1 == NoReg) {
		rTop1 = allocateRegNotConflictingWith(topRegistersMask);
		topRegistersMask = topRegistersMask | (((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
	}
	if (rNext1 == NoReg) {
		rNext1 = allocateRegNotConflictingWith(topRegistersMask);
	}
	assert(!(((rTop1 == NoReg)
 || ((rNext1 == NoReg)
 || (rThird1 == NoReg)))));
	ra2 = rTop1;
	ra1 = rNext1;
	rr = rThird1;
	assert((rr != ra1)
	 && ((rr != ra2)
	 && (ra1 != ra2)));
	popToReg(ssTop(), ra2);
	ssPop(1);
	popToReg(ssTop(), ra1);
	ssPop(1);
	popToReg(ssTop(), rr);
	ssPop(1);

	/* shift by baseHeaderSize and then move from 1 relative to zero relative */
	adjust = BaseHeaderSize - 1;
	genConvertSmallIntegerToIntegerInReg(ra1);
	genConvertSmallIntegerToIntegerInReg(ra2);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(AddCqR, adjust, ra1);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(adjust, BytesPerOop));
	}
	/* begin MoveR:Xbr:R: */
	genoperandoperandoperand(MoveRXbrR, ra2, ra1, rr);
	genConvertIntegerToSmallIntegerInReg(ra2);
	return ssPushRegister(ra2);
}

	/* SistaCogit>>#genByteAtPutImmutabilityCheck */
static sqInt
genByteAtPutImmutabilityCheck(void)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt i;
    AbstractInstruction *immutabilityFailure;
    sqInt index;
    int indexIsCst;
    AbstractInstruction *mutableJump;
    sqInt quickConstant;
    sqInt ra1;
    sqInt ra2;
    sqInt rr;


	/* Assumes rr is not a context and no store check is needed */
	indexIsCst = (((ssValue(1))->type)) == SSConstant;
	rr = ReceiverResultReg;
	ra1 = TempReg;
	ra2 = ClassReg;
	voidReceiverResultRegContainsSelf();
	/* begin ssFlushTo: */
	index = simStackPtr - 1;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	popToReg(ssTop(), ra2);

	/* shift by baseHeaderSize and then move from 1 relative to zero relative */
	adjust = BaseHeaderSize - 1;
	if (indexIsCst) {
		/* begin MoveCq:R: */
		quickConstant = (((((ssValue(1))->constant)) >> 3)) + adjust;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, quickConstant, ra1);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
	}
	else {
		popToReg(ssValue(1), ra1);
		genConvertSmallIntegerToIntegerInReg(ra1);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(AddCqR, adjust, ra1);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(adjust, BytesPerOop));
		}
	}
	popToReg(ssValue(2), rr);
	ssPop(3);
	ssPushRegister(ra2);
	mutableJump = genJumpMutablescratchReg(rr, Arg0Reg);
	/* begin PushR: */
	genoperand(PushR, ra2);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(SubCqR, 1 + adjust, ra1);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(1 + adjust, BytesPerOop));
	}
	genVarIndexCallStoreTrampoline();
	/* begin PopR: */
	genoperand(PopR, ra2);
	/* begin Jump: */
	immutabilityFailure = genoperand(Jump, ((sqInt)0));
	jmpTarget(mutableJump, genoperandoperand(MoveRR, ra2, Arg0Reg));
	genConvertSmallIntegerToIntegerInReg(Arg0Reg);
	/* begin MoveR:Xbr:R: */
	genoperandoperandoperand(MoveRXbrR, Arg0Reg, ra1, rr);
	jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Byte equal is falling through if the result is true, or jumping using jmp
	if the result is false.
	The method is required to set the jump target of jmp.
	We look ahead for a branch and pipeline the jumps if possible..
	ReturnReg is used only if not followed immediately by a branch. */

	/* SistaCogit>>#genByteEqualsInlinePrimitiveResult:returnReg: */
static sqInt NoDbgRegParms
genByteEqualsInlinePrimitiveResultreturnReg(AbstractInstruction *jmp, sqInt reg)
{
    AbstractInstruction *anInstruction;
    sqInt bcpc;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt canElide;
    sqInt constant;
    BytecodeDescriptor *descriptor;
    sqInt eA;
    sqInt eB;
    void *jumpTarget;
    void *jumpTarget1;
    AbstractInstruction *localJump;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    sqInt savedB0;
    sqInt savedB1;
    sqInt savedB2;
    sqInt savedB3;
    sqInt savedEA;
    sqInt savedEB;
    sqInt savedNEB;
    sqInt targetBytecodePC;
    sqInt targetBytecodePC1;

	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC1 = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetBytecodePC = targetBytecodePC1;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		/* begin genMoveTrueR: */
		constant = trueObject();
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(gMoveCwR(constant, reg), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(MoveCqR, constant, reg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
		/* begin Jump: */
		localJump = genoperand(Jump, ((sqInt)0));
		jmpTarget(jmp, genMoveFalseR(reg));
		jmpTarget(localJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		ssPushRegister(reg);
		return 0;
	}
	if ((((fixupAt(nextPC))->targetInstruction)) == 0) {

		/* The next instruction is dead.  we can skip it. */
		deadCode = 1;
		ensureFixupAt(targetBytecodePC);
		ensureFixupAt(postBranchPC);
	}
	else {
		ssPushConstant(trueObject());
	}
	/* begin nextDescriptorExtensionsAndNextPCInto: */
	descriptor = generatorAt(byte0);
	savedB0 = byte0;
	savedB1 = byte1;
	savedB2 = byte2;
	savedB3 = byte3;
	savedEA = extA;
	savedEB = extB;
	savedNEB = numExtB;
	bcpc = bytecodePC + ((descriptor->numBytes));
	do {
		if (bcpc > endPC) {
			nextPC = 0;
			goto l5;
		}
		byte0 = (fetchByteofObject(bcpc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		loadSubsequentBytesForDescriptorat(descriptor, bcpc);
		if (!((descriptor->isExtension))) {
			eA = extA;
			eB = extB;
			extA = savedEA;
			extB = savedEB;
			numExtB = savedNEB;
			byte0 = savedB0;
			byte1 = savedB1;
			byte2 = savedB2;
			byte3 = savedB3;
			nextPC = bcpc;
			goto l5;
		}
		((descriptor->generator))();
		bcpc += (descriptor->numBytes);
	} while(1);
	l5:	/* end nextDescriptorExtensionsAndNextPCInto: */;
	canElide = deadCode
	 && (nextPC == postBranchPC);
	if ((branchDescriptor->isBranchTrue)) {
		/* begin Jump: */
		jumpTarget = ensureNonMergeFixupAt(targetBytecodePC);
		genoperand(Jump, ((sqInt)jumpTarget));
		if (canElide) {
			jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		else {
			jmpTarget(jmp, ensureNonMergeFixupAt(postBranchPC));
		}
	}
	else {
		if (!canElide) {
			/* begin Jump: */
			jumpTarget1 = ensureNonMergeFixupAt(postBranchPC);
			genoperand(Jump, ((sqInt)jumpTarget1));
			jmpTarget(jmp, ensureNonMergeFixupAt(targetBytecodePC));
		}
	}
	return 0;
}


/*	3021	Byte Object >> equals:length:	
	The receiver and the arguments are both byte objects and have both the
	same size (length in bytes). 
	The length argument is a smallinteger. 
	Answers true if all fields are equal, false if not. 
	Comparison is bulked to word comparison.
 */
/*	Overview: 
	1.	The primitive is called like that: [byteObj1 equals: byteObj2 length:
	length]. In the worst case we use 5 registers including TempReg 
	and we produce a loop bulk comparing words.
	2.	The common case is a comparison against a cst: [byteString = 'foo'].
	which produces in Scorch [byteString equals: 'foo' length: 3].
	We try to generate fast code for this case with 3 heuristics:
	- specific fast code if len is a constant
	- unroll the loop if len < 2 * wordSize
	- compile-time reads if str1 or str2 is a constant and loop is unrolled.
	We use 3 registers including TempReg in the common case. 
	We could use 1 less reg if the loop is unrolled, the instr is followed by
	a branch
	AND one operand is a constant, but this is complicated enough.
	3.	We ignore the case where all operands are constants 
	(We assume Scorch simplifies it, it works but it is not optimised) */

	/* SistaCogit>>#genByteEqualsInlinePrimitive: */
static sqInt NoDbgRegParms
genByteEqualsInlinePrimitive(sqInt prim)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction22;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction31;
    AbstractInstruction *anInstruction32;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    sqInt extraReg;
    AbstractInstruction *instr;
    AbstractInstruction *jmp;
    AbstractInstruction *jmp2;
    AbstractInstruction *jmpZeroSize;
    sqInt lenCst;
    sqInt lenReg;
    sqInt mask;
    sqInt needjmpZeroSize;
    int needLoop;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant11;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt shift;
    sqInt shift1;
    sqInt str1Reg;
    sqInt str2Reg;
    sqInt unroll;


	/* --- quick path for empty string--- */
	/* This path does not allocate registers and right shift on negative int later in the code.
	   Normally this is resolved by Scorch but we keep it for correctness and consistency */
	jmp2 = ((AbstractInstruction *) 0);
	jmpZeroSize = ((AbstractInstruction *) 0);
	lenCst = 0;
	lenReg = 0;
	str1Reg = 0;
	str2Reg = 0;
	if ((((ssTop())->type)) == SSConstant) {
		lenCst = ((((ssTop())->constant)) >> 3);
		if (lenCst == 0) {
			ssPop(3);
			ssPushConstant(trueObject());
			return 0;
		}
	}
	needLoop = !(((((ssTop())->type)) == SSConstant)
	 && (lenCst <= (BytesPerWord * 2)));
	unroll = (!needLoop)
	 && (lenCst > BytesPerWord);
	if (needLoop) {
		assert(!(((ssTop())->spilled)));
		str1Reg = allocateRegForStackEntryAtnotConflictingWith(1, 0 /* emptyRegisterMask */);
		str2Reg = allocateRegForStackEntryAtnotConflictingWith(2, ((str1Reg < 0) ? (((usqInt)(1)) >> (-str1Reg)) : (1ULL << str1Reg)));
		lenReg = allocateRegForStackEntryAtnotConflictingWith(0, (1ULL << str1Reg) | (1ULL << str2Reg));
		popToReg(ssValue(1), str1Reg);
		popToReg(ssValue(2), str2Reg);
		extraReg = allocateRegNotConflictingWith(((1ULL << str1Reg) | (1ULL << str2Reg)) | (1ULL << lenReg));
	}
	else {
		/* begin emptyRegisterMask */
		mask = 0;
		if (!((((ssValue(1))->type)) == SSConstant)) {
			str1Reg = allocateRegForStackEntryAtnotConflictingWith(1, mask);
			popToReg(ssValue(1), str1Reg);
			mask = mask | (((str1Reg < 0) ? (((usqInt)(1)) >> (-str1Reg)) : (1ULL << str1Reg)));
		}
		if (!((((ssValue(2))->type)) == SSConstant)) {
			str2Reg = allocateRegForStackEntryAtnotConflictingWith(2, mask);
			popToReg(ssValue(2), str2Reg);
			mask = mask | (((str2Reg < 0) ? (((usqInt)(1)) >> (-str2Reg)) : (1ULL << str2Reg)));
		}
		extraReg = allocateRegNotConflictingWith(mask);
	}
	if ((((ssTop())->type)) == SSConstant) {

		/* common case, str = 'foo'. We can precompute lenReg. */
		lenCst = ((usqInt)(((lenCst + BaseHeaderSize) - 1))) >> (shiftForWord());
		if (needLoop) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(MoveCqR, lenCst, lenReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(lenCst, BytesPerOop));
			}
		}
		needjmpZeroSize = 0;
	}
	else {

		/* uncommon case, str = str2. lenReg in word computed at runtime. */
		popToReg(ssTop(), lenReg);
		genConvertSmallIntegerToIntegerInReg(lenReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, 0, lenReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		jmpZeroSize = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		needjmpZeroSize = 1;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(AddCqR, BaseHeaderSize - 1, lenReg);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(BaseHeaderSize - 1, BytesPerOop));
		}
		/* begin ArithmeticShiftRightCq:R: */
		genoperandoperand(ArithmeticShiftRightCqR, shiftForWord(), lenReg);
	}
	if (needLoop) {
		/* begin MoveXwr:R:R: */
		instr = genoperandoperandoperand(MoveXwrRR, lenReg, str1Reg, extraReg);
		/* begin MoveXwr:R:R: */
		genoperandoperandoperand(MoveXwrRR, lenReg, str2Reg, TempReg);
		/* begin CmpR:R: */
		assert(!((extraReg == SPReg)));
		genoperandoperand(CmpRR, extraReg, TempReg);
		/* begin JumpNonZero: */
		jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(AddCqR, -1, lenReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(-1, BytesPerOop));
		}
		/* begin CmpCq:R: */
		quickConstant = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(CmpCqR, quickConstant, lenReg);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		/* begin JumpNonZero: */
		genConditionalBranchoperand(JumpNonZero, ((sqInt)instr));
	}
	else {

		/* Common case, only 1 or 2 word to check: no lenReg allocation, cst micro optimisations */
		/* begin genByteEqualsInlinePrimitiveCmp:with:scratch1:scratch2:field: */
		shift1 = BaseHeaderSize + (0 * BytesPerWord);
		if ((((ssValue(1))->type)) == SSConstant) {
			/* begin MoveCq:R: */
			quickConstant3 = fetchPointerofObject(0, ((ssValue(1))->constant));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction6 = genoperandoperand(MoveCqR, quickConstant3, extraReg);
			if (usesOutOfLineLiteral(anInstruction6)) {
				(anInstruction6->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
			}
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction12 = genoperandoperandoperand(MoveMwrR, shift1, str1Reg, extraReg);
			if (usesOutOfLineLiteral(anInstruction12)) {
				(anInstruction12->dependent = locateLiteralsize(shift1, BytesPerOop));
			}
		}
		if ((((ssValue(2))->type)) == SSConstant) {
			/* begin MoveCq:R: */
			quickConstant11 = fetchPointerofObject(0, ((ssValue(2))->constant));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction22 = genoperandoperand(MoveCqR, quickConstant11, TempReg);
			if (usesOutOfLineLiteral(anInstruction22)) {
				(anInstruction22->dependent = locateLiteralsize(quickConstant11, BytesPerOop));
			}
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction32 = genoperandoperandoperand(MoveMwrR, shift1, str2Reg, TempReg);
			if (usesOutOfLineLiteral(anInstruction32)) {
				(anInstruction32->dependent = locateLiteralsize(shift1, BytesPerOop));
			}
		}
		/* begin CmpR:R: */
		assert(!((extraReg == SPReg)));
		genoperandoperand(CmpRR, extraReg, TempReg);
		/* begin JumpNonZero: */
		jmp = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		if (unroll) {

			/* unrolling more than twice generate more instructions than the loop so we don't do it */
			/* begin genByteEqualsInlinePrimitiveCmp:with:scratch1:scratch2:field: */
			shift = BaseHeaderSize + (1 * BytesPerWord);
			if ((((ssValue(1))->type)) == SSConstant) {
				/* begin MoveCq:R: */
				quickConstant2 = fetchPointerofObject(1, ((ssValue(1))->constant));
				/* begin checkQuickConstant:forInstruction: */
				anInstruction5 = genoperandoperand(MoveCqR, quickConstant2, extraReg);
				if (usesOutOfLineLiteral(anInstruction5)) {
					(anInstruction5->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
				}
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction11 = genoperandoperandoperand(MoveMwrR, shift, str1Reg, extraReg);
				if (usesOutOfLineLiteral(anInstruction11)) {
					(anInstruction11->dependent = locateLiteralsize(shift, BytesPerOop));
				}
			}
			if ((((ssValue(2))->type)) == SSConstant) {
				/* begin MoveCq:R: */
				quickConstant1 = fetchPointerofObject(1, ((ssValue(2))->constant));
				/* begin checkQuickConstant:forInstruction: */
				anInstruction21 = genoperandoperand(MoveCqR, quickConstant1, TempReg);
				if (usesOutOfLineLiteral(anInstruction21)) {
					(anInstruction21->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
				}
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction31 = genoperandoperandoperand(MoveMwrR, shift, str2Reg, TempReg);
				if (usesOutOfLineLiteral(anInstruction31)) {
					(anInstruction31->dependent = locateLiteralsize(shift, BytesPerOop));
				}
			}
			/* begin CmpR:R: */
			assert(!((extraReg == SPReg)));
			genoperandoperand(CmpRR, extraReg, TempReg);
			/* begin JumpNonZero: */
			jmp2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
		}
	}
	if (needjmpZeroSize) {
		jmpTarget(jmpZeroSize, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	ssPop(3);
	genByteEqualsInlinePrimitiveResultreturnReg(jmp, extraReg);
	if (unroll) {
		jmpTarget(jmp2, ((AbstractInstruction *) (((jmp->operands))[0])));
	}
	return 0;
}


/*	SistaV1:	236		11101100	iiiiiiii		callMappedInlinedPrimitive */

	/* SistaCogit>>#genCallMappedInlinedPrimitive */
static sqInt
genCallMappedInlinedPrimitive(void)
{
	return genMappedInlinePrimitive(byte1);
}


/*	SistaV1: 248		11111000 iiiiiiii		mjjjjjjj		Call Primitive #iiiiiiii +
	(jjjjjjj * 256) m=1 means inlined primitive, no hard return after
	execution. See EncoderForSistaV1's class comment and
	StackInterpreter>>#inlinePrimitiveBytecode: 
 */

	/* SistaCogit>>#genCallPrimitiveBytecode */
static sqInt
genCallPrimitiveBytecode(void)
{
    sqInt prim;
    sqInt primSet;

	if (byte2 < 128) {
		return (bytecodePC == initialPC
			? 0
			: EncounteredUnknownBytecode);
	}
	prim = (((sqInt)((usqInt)((byte2 - 128)) << 8))) + byte1;
	primSet = (((usqInt)(prim)) >> 13) & 3;
	prim = prim & 0x1FFF;
	assert(primSet == 0);
	return genSistaInlinePrimitive(prim);
}


/*	Specific version if the branch is only reached while falling through if
	the counter trips after an inlined #== branch. We do not regenerate the
	counter logic in this case to avoid 24 bytes instructions.
 */

	/* SistaCogit>>#genCounterTripOnlyJumpIf:to: */
static sqInt NoDbgRegParms
genCounterTripOnlyJumpIfto(sqInt boolean, sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    sqInt i;
    sqInt index;
    void *jumpTarget;
    AbstractInstruction *mustBeBooleanTrampoline;
    AbstractInstruction *ok;
    sqInt quickConstant;

	extA = 0;
	/* begin ssFlushTo: */
	index = simStackPtr - 1;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	popToReg(ssTop(), TempReg);
	ssPop(1);

	/* counters are increased / decreased in the inlined branch */
	/* We need SendNumArgsReg because of the mustBeBooleanTrampoline */
	counterIndex += 1;
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << SendNumArgsReg), simStackPtr, simNativeStackPtr);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(MoveCqR, 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	mustBeBooleanTrampoline = genCallMustBeBooleanFor(boolean);
	/* begin annotateBytecode: */
	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction->annotation = HasBytecodePC);
	assert((objectAfter(falseObject())) == (trueObject()));
	/* begin genSubConstant:R: */
	if (shouldAnnotateObjectReference(boolean)) {
		annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(SubCqR, boolean, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(boolean, BytesPerOop));
		}
	}
	/* begin JumpZero: */
	jumpTarget = ensureFixupAt(targetBytecodePC);
	genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
	/* begin CmpCq:R: */
	quickConstant = (boolean == (falseObject())
		? (trueObject()) - (falseObject())
		: (falseObject()) - (trueObject()));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(CmpCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpZero: */
	ok = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin Jump: */
	genoperand(Jump, ((sqInt)mustBeBooleanTrampoline));
	jmpTarget(ok, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	250	directCall
	literal index of the method to call on top of stack => (variable number of
	parameters) 
 */

	/* SistaCogit>>#genDirectCall */
static sqInt
genDirectCall(void)
{
    sqInt annotation;
    sqInt litIndex;
    sqInt newMethod;
    sqInt newMethodArgCount;
    sqInt newMethodHeader;
    sqInt sendTable;

	assert(((((ssTop())->type)) == SSConstant)
	 && (isCompiledMethod(((ssTop())->constant))));
	litIndex = ((((ssTop())->constant)) >> 3);
	newMethod = getLiteral(litIndex);
	ssPop(1);
	flag("TODO");

	/* directCallSendTable */
	sendTable = 1;

	/* directCallAnnotation */
	annotation = 1;
	newMethodHeader = rawHeaderOf(newMethod);

	/* The receiver cannot be a forwader */
	/* numArgs >= (NumSendTrampolines - 1) ifTrue:
	   [self MoveCq: newMethodArgCount R: SendNumArgsReg]. */
	/* Load inline cache with method index */
	/* self MoveUniqueC32: litIndex R: ClassReg.
	   (self Call: (sendTable at: (newMethodArgCount min: NumSendTrampolines - 1))) annotation: annotation.
	   self voidReceiverOptStatus.
	   self ssPushRegister: ReceiverResultReg. */
	newMethodArgCount = argumentCountOfMethodHeader(methodHeader);
	return EncounteredUnknownBytecode;
}


/*	2003	/
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant) */
/*	2004	//
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant) */
/*	2005	\\
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant) */
/*	2006	quo:
	Smi, Smi => Smi (no overflow, optimised if one operand is a constant) */
/*	We don't deal with constants here. Too complex and does nto bring much */

	/* SistaCogit>>#genDivInlinePrimitive: */
static sqInt NoDbgRegParms
genDivInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *jumpExact;
    AbstractInstruction *jumpSameSign;
    sqInt ra;
    sqInt reg;
    sqInt rNext1;
    sqInt rr;
    sqInt rTop1;
    sqInt topRegistersMask;

	/* begin allocateRegForStackTopTwoEntriesInto: */
	topRegistersMask = 0;
	rTop1 = (rNext1 = NoReg);
	if ((registerOrNone(ssTop())) != NoReg) {
		rTop1 = registerOrNone(ssTop());
	}
	if ((registerOrNone(ssValue(1))) != NoReg) {
		/* begin registerMaskFor: */
		reg = (rNext1 = registerOrNone(ssValue(1)));
		topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	}
	if (rTop1 == NoReg) {
		rTop1 = allocateRegNotConflictingWith(topRegistersMask);
	}
	if (rNext1 == NoReg) {
		rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
	}
	assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
	ra = rTop1;
	rr = rNext1;
	popToReg(ssTop(), ra);
	ssPop(1);
	popToReg(ssTop(), rr);
	ssPop(1);
	assert(canDivQuoRem(backEnd));
	switch (primIndex) {
	case 4:
		genShiftAwaySmallIntegerTagsInScratchReg(ra);
		genShiftAwaySmallIntegerTagsInScratchReg(rr);
		gDivRRQuoRem(ra, rr, rr, TempReg);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, 0, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		genConvertIntegerToSmallIntegerInReg(ra);
		/* begin XorR:R: */
		genoperandoperand(XorRR, TempReg, ra);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, 0, ra);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpGreaterOrEqual: */
		jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction2 = genoperandoperand(SubCqR, 1, rr);
		if (usesOutOfLineLiteral(anInstruction2)) {
			(anInstruction2->dependent = locateLiteralsize(1, BytesPerOop));
		}
		jmpTarget(jumpSameSign, jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
		genConvertIntegerToSmallIntegerInReg(rr);
		break;
	case 5:
		genRemoveSmallIntegerTagsInScratchReg(ra);
		genRemoveSmallIntegerTagsInScratchReg(rr);
		gDivRRQuoRem(ra, rr, TempReg, rr);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction4 = genoperandoperand(CmpCqR, 0, rr);
		if (usesOutOfLineLiteral(anInstruction4)) {
			(anInstruction4->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpZero: */
		jumpExact = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		/* begin XorR:R: */
		genoperandoperand(XorRR, rr, ra);
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(CmpCqR, 0, ra);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin JumpGreaterOrEqual: */
		jumpSameSign = genConditionalBranchoperand(JumpGreaterOrEqual, ((sqInt)0));
		/* begin XorR:R: */
		genoperandoperand(XorRR, rr, ra);
		/* begin AddR:R: */
		genoperandoperand(AddRR, ra, rr);
		jmpTarget(jumpSameSign, jmpTarget(jumpExact, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
		genSetSmallIntegerTagsIn(rr);
		break;
	case 6:
		genShiftAwaySmallIntegerTagsInScratchReg(ra);
		genShiftAwaySmallIntegerTagsInScratchReg(rr);
		gDivRRQuoRem(ra, rr, rr, ra);
		genConvertIntegerToSmallIntegerInReg(rr);
		break;
	default:
		return EncounteredUnknownBytecode;

	}
	ssPushRegister(rr);
	return 0;
}


/*	50	EnsureEnoughWords
	literal which is a Smi => ret value is receiver */

	/* SistaCogit>>#genEnsureEnoughSlots */
static sqInt
genEnsureEnoughSlots(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt address;
    sqInt address1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt i;
    sqInt index;
    sqInt quickConstant;
    AbstractInstruction *skip;
    sqInt slots;

	assert(((((ssTop())->type)) == SSConstant)
	 && (((((((ssTop())->constant))) & 7) == 1)));
	slots = ((((ssTop())->constant)) >> 3);
	/* begin ssFlushTo: */
	index = simStackPtr - 1;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	/* begin MoveAw:R: */
	address = freeStartAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, TempReg));
	/* begin CmpCq:R: */
	quickConstant = (getScavengeThreshold()) - (BytesPerOop * slots);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, quickConstant, TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpBelow: */
	skip = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin genSetGCNeeded */
	anInstruction1 = genoperandoperand(MoveCqR, 1, TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin MoveR:Aw: */
	address1 = needGCFlagAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address1, genoperandoperand(MoveRAw, TempReg, address1));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceCheckForInterruptTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	jmpTarget(skip, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin annotateBytecode: */
	abstractInstruction1 = ((AbstractInstruction *) (((skip->operands))[0]));
	(abstractInstruction1->annotation = HasBytecodePC);
	return 0;
}


/*	Trap sends Sista trap message to context with top of stack, so we don't
	need any arguments...
 */

	/* SistaCogit>>#generateSistaRuntime */
static void
generateSistaRuntime(void)
{
	/* begin genTrampolineFor:called: */
	ceTrapTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSistaTrap, "ceSistaTrapTrampoline", 0, null, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 0);
}


/*	Override to count inlined branches if followed by a conditional branch.
	We borrow the following conditional branch's counter and when about to
	inline the comparison we decrement the counter (without writing it back)
	and if it trips simply abort the inlining, falling back to the normal send
	which will then continue to the conditional branch which will trip and
	enter the abort. */

	/* SistaCogit>>#genForwardersInlinedIdenticalOrNotIf: */
static sqInt NoDbgRegParms
genForwardersInlinedIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt argConstant;
    sqInt argConstant1;
    sqInt argNeedsReg;
    sqInt argNeedsReg1;
    sqInt argReg;
    sqInt argReg1;
    sqInt argReg2;
    sqInt argReg3;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    BytecodeDescriptor *branchDescriptor2;
    BytecodeDescriptor *branchDescriptor3;
    sqInt constant;
    sqInt constant1;
    sqInt constant11;
    sqInt constant12;
    sqInt constant2;
    sqInt constant3;
    sqInt constant4;
    sqInt counterAddress;
    sqInt counterAddress1;
    sqInt counterReg;
    AbstractInstruction *countTripped;
    AbstractInstruction *countTripped1;
    BytecodeFixup *fixup;
    BytecodeFixup *fixup1;
    sqInt i;
    sqInt i1;
    sqInt index;
    sqInt index1;
    AbstractInstruction *instruction;
    AbstractInstruction *instruction1;
    AbstractInstruction *jumpEqual;
    AbstractInstruction *jumpNotEqual;
    void *jumpTarget;
    void *jumpTarget1;
    void *jumpTarget2;
    void *jumpTarget3;
    AbstractInstruction *label;
    sqInt nExts;
    sqInt nExts1;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt nextPC2;
    sqInt nextPC3;
    sqInt opcode;
    sqInt opcode1;
    sqInt opcode2;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    sqInt postBranchPC2;
    sqInt postBranchPC3;
    BytecodeDescriptor *primDescriptor;
    BytecodeDescriptor *primDescriptor1;
    sqInt rcvrConstant;
    sqInt rcvrConstant1;
    sqInt rcvrNeedsReg;
    sqInt rcvrNeedsReg1;
    sqInt rcvrReg;
    sqInt rcvrReg1;
    sqInt rcvrReg2;
    sqInt rcvrReg3;
    sqInt reg;
    sqInt reg1;
    sqInt regMask;
    sqInt rNext1;
    sqInt rNext11;
    sqInt rTop1;
    sqInt rTop11;
    CogSimStackEntry *simStackEntry;
    CogSimStackEntry *simStackEntry1;
    CogSimStackEntry *simStackEntry2;
    CogSimStackEntry *simStackEntry3;
    sqInt targetBytecodePC;
    sqInt targetBytecodePC1;
    sqInt targetBytecodePC2;
    sqInt targetBytecodePC3;
    sqInt topRegistersMask;
    sqInt topRegistersMask1;
    sqInt unforwardArg;
    sqInt unforwardArg1;
    sqInt unforwardRcvr;
    sqInt unforwardRcvr1;

	if ((isOptimizedMethod(methodObj))
	 || (!needsFrame)) {
		unforwardRcvr1 = mayBeAForwarder(ssValue(1));
		unforwardArg1 = mayBeAForwarder(ssTop());
		if ((!unforwardRcvr1)
		 && (!unforwardArg1)) {
			return genVanillaInlinedIdenticalOrNotIf(orNot);
		}
		assert(unforwardArg1
		 || (unforwardRcvr1));
		/* begin isUnannotatableConstant: */
		simStackEntry = ssValue(1);
		rcvrConstant1 = (((simStackEntry->type)) == SSConstant)
		 && ((isImmediate((simStackEntry->constant)))
		 || (!(shouldAnnotateObjectReference((simStackEntry->constant)))));
		/* begin isUnannotatableConstant: */
		simStackEntry1 = ssTop();
		argConstant1 = (((simStackEntry1->type)) == SSConstant)
		 && ((isImmediate((simStackEntry1->constant)))
		 || (!(shouldAnnotateObjectReference((simStackEntry1->constant)))));
		/* begin extractMaybeBranchDescriptorInto: */
		primDescriptor = generatorAt(byte0);
		nextPC2 = bytecodePC + ((primDescriptor->numBytes));
		nExts = 0;
		while (1) {
			while (1) {
				/* begin generatorForPC: */
				branchDescriptor2 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC2, methodObj)));
				if (!((branchDescriptor2->isExtension))) break;
				nExts += 1;
				nextPC2 += (branchDescriptor2->numBytes);
			}
			/* begin isUnconditionalBranch */
			if (!((isBranch(branchDescriptor2))
			 && (!(((branchDescriptor2->isBranchTrue))
			 || ((branchDescriptor2->isBranchFalse)))))) break;
			nextPC2 = eventualTargetOf((nextPC2 + ((branchDescriptor2->numBytes))) + (((branchDescriptor2->spanFunction))(branchDescriptor2, nextPC2, nExts, methodObj)));
		}
		targetBytecodePC2 = (postBranchPC2 = 0);
		if (((branchDescriptor2->isBranchTrue))
		 || ((branchDescriptor2->isBranchFalse))) {
			targetBytecodePC2 = eventualTargetOf((nextPC2 + ((branchDescriptor2->numBytes))) + (((branchDescriptor2->spanFunction))(branchDescriptor2, nextPC2, nExts, methodObj)));
			postBranchPC2 = eventualTargetOf(nextPC2 + ((branchDescriptor2->numBytes)));
		}
		else {
			nextPC2 = bytecodePC + ((primDescriptor->numBytes));
		}
		branchDescriptor1 = branchDescriptor2;
		nextPC1 = nextPC2;
		postBranchPC1 = postBranchPC2;
		targetBytecodePC1 = targetBytecodePC2;
		/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
		argNeedsReg = !argConstant1;
		rcvrNeedsReg = !rcvrConstant1;
		assert(argNeedsReg
		 || (rcvrNeedsReg));
		argReg2 = (rcvrReg2 = NoReg);
		if (argNeedsReg) {
			if (rcvrNeedsReg) {
				/* begin allocateRegForStackTopTwoEntriesInto: */
				topRegistersMask = 0;
				rTop1 = (rNext1 = NoReg);
				if ((registerOrNone(ssTop())) != NoReg) {
					rTop1 = registerOrNone(ssTop());
				}
				if ((registerOrNone(ssValue(1))) != NoReg) {
					/* begin registerMaskFor: */
					reg = (rNext1 = registerOrNone(ssValue(1)));
					topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
				}
				if (rTop1 == NoReg) {
					rTop1 = allocateRegNotConflictingWith(topRegistersMask);
				}
				if (rNext1 == NoReg) {
					rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
				}
				assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
				argReg2 = rTop1;
				rcvrReg2 = rNext1;
				popToReg(ssTop(), argReg2);
				popToReg(ssValue(1), rcvrReg2);
			}
			else {
				argReg2 = allocateRegForStackEntryAtnotConflictingWith(0, 0);
				popToReg(ssTop(), argReg2);
				if (((ssValue(1))->spilled)) {
					/* begin checkQuickConstant:forInstruction: */
					anInstruction5 = genoperandoperand(AddCqR, BytesPerWord, SPReg);
					if (usesOutOfLineLiteral(anInstruction5)) {
						(anInstruction5->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
					}
				}
			}
		}
		else {
			assert(rcvrNeedsReg);
			assert(!((((ssTop())->spilled))));
			rcvrReg2 = allocateRegForStackEntryAtnotConflictingWith(1, 0);
			popToReg(ssValue(1), rcvrReg2);
		}
		assert(!((argNeedsReg
 && (argReg2 == NoReg))));
		assert(!((rcvrNeedsReg
 && (rcvrReg2 == NoReg))));
		rcvrReg1 = rcvrReg2;
		argReg1 = argReg2;
		if (!(((branchDescriptor1->isBranchTrue))
			 || ((branchDescriptor1->isBranchFalse)))) {
			return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argConstant1, rcvrConstant1, argReg1, rcvrReg1, orNot);
		}
		/* begin ssFlushTo: */
		index = simStackPtr - 2;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		/* begin Label */
		label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
		assert((argReg1 != NoReg)
		 || (rcvrReg1 != NoReg));
		if (argConstant1) {
			/* begin genCmpConstant:R: */
			constant2 = ((ssTop())->constant);
			if (shouldAnnotateObjectReference(constant2)) {
				annotateobjRef(checkLiteralforInstruction(constant2, genoperandoperand(CmpCwR, constant2, rcvrReg1)), constant2);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction3 = genoperandoperand(CmpCqR, constant2, rcvrReg1);
				if (usesOutOfLineLiteral(anInstruction3)) {
					(anInstruction3->dependent = locateLiteralsize(constant2, BytesPerOop));
				}
			}
		}
		else {
			if (rcvrConstant1) {
				/* begin genCmpConstant:R: */
				constant1 = ((ssValue(1))->constant);
				if (shouldAnnotateObjectReference(constant1)) {
					annotateobjRef(checkLiteralforInstruction(constant1, genoperandoperand(CmpCwR, constant1, argReg1)), constant1);
				}
				else {
					/* begin checkQuickConstant:forInstruction: */
					anInstruction4 = genoperandoperand(CmpCqR, constant1, argReg1);
					if (usesOutOfLineLiteral(anInstruction4)) {
						(anInstruction4->dependent = locateLiteralsize(constant1, BytesPerOop));
					}
				}
			}
			else {
				/* begin CmpR:R: */
				assert(!((argReg1 == SPReg)));
				genoperandoperand(CmpRR, argReg1, rcvrReg1);
			}
		}
		ssPop(2);
		if ((((fixupAt(nextPC1))->targetInstruction)) == 0) {

			/* The next instruction is dead.  we can skip it. */
			deadCode = 1;
			ensureFixupAt(targetBytecodePC1);
			ensureFixupAt(postBranchPC1);
		}
		else {
			assert(!(deadCode));
		}
		if (orNot == ((branchDescriptor1->isBranchTrue))) {

			/* a == b ifFalse: ... or a ~~ b ifTrue: ... jump on equal to post-branch pc */
			fixup1 = ensureNonMergeFixupAt(targetBytecodePC1);
			/* begin JumpZero: */
			jumpTarget = ensureNonMergeFixupAt(postBranchPC1);
			genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		}
		else {

			/* orNot is true for ~~ */
			/* a == b ifTrue: ... or a ~~ b ifFalse: ... jump on equal to target pc */
			fixup1 = ensureNonMergeFixupAt(postBranchPC1);
			/* begin JumpZero: */
			jumpTarget1 = ensureNonMergeFixupAt(targetBytecodePC1);
			genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget1));
		}
		if (unforwardArg1
		 && (unforwardRcvr1)) {
			/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
			genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(argReg1, TempReg, label, 0);
		}
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder((unforwardRcvr1
			? rcvrReg1
			: argReg1), TempReg, label, fixup1);
		if (!deadCode) {
			ssPushConstant(trueObject());
		}
		return 0;
	}
	regMask = 0;
	unforwardRcvr = mayBeAForwarder(ssValue(1));
	unforwardArg = mayBeAForwarder(ssTop());
	if ((!unforwardRcvr)
	 && (!unforwardArg)) {

		/* TODO: use genVanilla with profiling counters (not implemented).
		   ^self genVanillaInlinedIdenticalOrNotIf: orNot */
		unforwardRcvr = 1;
	}
	assert(unforwardArg
	 || (unforwardRcvr));
	/* begin isUnannotatableConstant: */
	simStackEntry2 = ssValue(1);
	rcvrConstant = (((simStackEntry2->type)) == SSConstant)
	 && ((isImmediate((simStackEntry2->constant)))
	 || (!(shouldAnnotateObjectReference((simStackEntry2->constant)))));
	/* begin isUnannotatableConstant: */
	simStackEntry3 = ssTop();
	argConstant = (((simStackEntry3->type)) == SSConstant)
	 && ((isImmediate((simStackEntry3->constant)))
	 || (!(shouldAnnotateObjectReference((simStackEntry3->constant)))));
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor1 = generatorAt(byte0);
	nextPC3 = bytecodePC + ((primDescriptor1->numBytes));
	nExts1 = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor3 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC3, methodObj)));
			if (!((branchDescriptor3->isExtension))) break;
			nExts1 += 1;
			nextPC3 += (branchDescriptor3->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor3))
		 && (!(((branchDescriptor3->isBranchTrue))
		 || ((branchDescriptor3->isBranchFalse)))))) break;
		nextPC3 = eventualTargetOf((nextPC3 + ((branchDescriptor3->numBytes))) + (((branchDescriptor3->spanFunction))(branchDescriptor3, nextPC3, nExts1, methodObj)));
	}
	targetBytecodePC3 = (postBranchPC3 = 0);
	if (((branchDescriptor3->isBranchTrue))
	 || ((branchDescriptor3->isBranchFalse))) {
		targetBytecodePC3 = eventualTargetOf((nextPC3 + ((branchDescriptor3->numBytes))) + (((branchDescriptor3->spanFunction))(branchDescriptor3, nextPC3, nExts1, methodObj)));
		postBranchPC3 = eventualTargetOf(nextPC3 + ((branchDescriptor3->numBytes)));
	}
	else {
		nextPC3 = bytecodePC + ((primDescriptor1->numBytes));
	}
	branchDescriptor = branchDescriptor3;
	nextPC = nextPC3;
	postBranchPC = postBranchPC3;
	targetBytecodePC = targetBytecodePC3;
	/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
	argNeedsReg1 = !argConstant;
	rcvrNeedsReg1 = !rcvrConstant;
	assert(argNeedsReg1
	 || (rcvrNeedsReg1));
	argReg3 = (rcvrReg3 = NoReg);
	if (argNeedsReg1) {
		if (rcvrNeedsReg1) {
			/* begin allocateRegForStackTopTwoEntriesInto: */
			topRegistersMask1 = 0;
			rTop11 = (rNext11 = NoReg);
			if ((registerOrNone(ssTop())) != NoReg) {
				rTop11 = registerOrNone(ssTop());
			}
			if ((registerOrNone(ssValue(1))) != NoReg) {
				/* begin registerMaskFor: */
				reg1 = (rNext11 = registerOrNone(ssValue(1)));
				topRegistersMask1 = ((reg1 < 0) ? (((usqInt)(1)) >> (-reg1)) : (1ULL << reg1));
			}
			if (rTop11 == NoReg) {
				rTop11 = allocateRegNotConflictingWith(topRegistersMask1);
			}
			if (rNext11 == NoReg) {
				rNext11 = allocateRegNotConflictingWith(((rTop11 < 0) ? (((usqInt)(1)) >> (-rTop11)) : (1ULL << rTop11)));
			}
			assert(!(((rTop11 == NoReg)
 || (rNext11 == NoReg))));
			argReg3 = rTop11;
			rcvrReg3 = rNext11;
			popToReg(ssTop(), argReg3);
			popToReg(ssValue(1), rcvrReg3);
		}
		else {
			argReg3 = allocateRegForStackEntryAtnotConflictingWith(0, 0);
			popToReg(ssTop(), argReg3);
			if (((ssValue(1))->spilled)) {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction10 = genoperandoperand(AddCqR, BytesPerWord, SPReg);
				if (usesOutOfLineLiteral(anInstruction10)) {
					(anInstruction10->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
				}
			}
		}
	}
	else {
		assert(rcvrNeedsReg1);
		assert(!((((ssTop())->spilled))));
		rcvrReg3 = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), rcvrReg3);
	}
	assert(!((argNeedsReg1
 && (argReg3 == NoReg))));
	assert(!((rcvrNeedsReg1
 && (rcvrReg3 == NoReg))));
	rcvrReg = rcvrReg3;
	argReg = argReg3;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argConstant, rcvrConstant, argReg, rcvrReg, orNot);
	}
	/* begin ssFlushTo: */
	index1 = simStackPtr - 2;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index1) {
		for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i1 <= index1; i1 += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
		}
		simSpillBase = index1 + 1;
	}
	if (unforwardArg) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		instruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(argReg, TempReg, instruction, 0);
	}
	if (unforwardRcvr) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		instruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(rcvrReg, TempReg, instruction1, 0);
	}
	if (argReg == NoReg) {
		/* begin registerMaskFor: */
		regMask = ((rcvrReg < 0) ? (((usqInt)(1)) >> (-rcvrReg)) : (1ULL << rcvrReg));
	}
	else {
		if (rcvrReg == NoReg) {
			/* begin registerMaskFor: */
			regMask = ((argReg < 0) ? (((usqInt)(1)) >> (-argReg)) : (1ULL << argReg));
		}
		else {
			/* begin registerMaskFor:and: */
			regMask = (1ULL << rcvrReg) | (1ULL << argReg);
		}
	}
	counterReg = allocateRegNotConflictingWith(regMask);
	/* begin genExecutionCountLogicInto:counterReg: */
	counterAddress1 = counters + (CounterBytes * counterIndex);
	/* begin MoveA32:R: */
	opcode = MoveA32R;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode, counterAddress1, counterReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(SubCqR, 0x10000, counterReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0x10000, BytesPerOop));
	}
	/* begin JumpCarry: */
	countTripped1 = genConditionalBranchoperand(JumpCarry, ((sqInt)0));
	/* begin MoveR:A32: */
	opcode1 = MoveRA32;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode1, counterReg, counterAddress1));
	counterAddress = counterAddress1;
	countTripped = countTripped1;
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (argConstant) {
		/* begin genCmpConstant:R: */
		constant3 = ((ssTop())->constant);
		if (shouldAnnotateObjectReference(constant3)) {
			annotateobjRef(checkLiteralforInstruction(constant3, genoperandoperand(CmpCwR, constant3, rcvrReg)), constant3);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction6 = genoperandoperand(CmpCqR, constant3, rcvrReg);
			if (usesOutOfLineLiteral(anInstruction6)) {
				(anInstruction6->dependent = locateLiteralsize(constant3, BytesPerOop));
			}
		}
	}
	else {
		if (rcvrConstant) {
			/* begin genCmpConstant:R: */
			constant11 = ((ssValue(1))->constant);
			if (shouldAnnotateObjectReference(constant11)) {
				annotateobjRef(checkLiteralforInstruction(constant11, genoperandoperand(CmpCwR, constant11, argReg)), constant11);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction7 = genoperandoperand(CmpCqR, constant11, argReg);
				if (usesOutOfLineLiteral(anInstruction7)) {
					(anInstruction7->dependent = locateLiteralsize(constant11, BytesPerOop));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);
	if (orNot == ((branchDescriptor->isBranchTrue))) {
		fixup = ensureNonMergeFixupAt(targetBytecodePC);
		/* begin JumpZero: */
		jumpTarget2 = ensureNonMergeFixupAt(postBranchPC);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget2));
	}
	else {

		/* orNot is true for ~~ */
		fixup = ensureNonMergeFixupAt(postBranchPC);
		/* begin JumpZero: */
		jumpTarget3 = ensureNonMergeFixupAt(targetBytecodePC);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget3));
	}
	/* begin genFallsThroughCountLogicCounterReg:counterAddress: */
	anInstruction2 = genoperandoperand(SubCqR, 1, counterReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin MoveR:A32: */
	opcode2 = MoveRA32;
	checkLiteralforInstruction(counterAddress, genoperandoperand(opcode2, counterReg, counterAddress));
	/* begin Jump: */
	genoperand(Jump, ((sqInt)fixup));
	jmpTarget(countTripped, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPop(-2);
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (!unforwardArg) {
		/* begin genCmpConstant:R: */
		constant4 = ((ssTop())->constant);
		if (shouldAnnotateObjectReference(constant4)) {
			annotateobjRef(checkLiteralforInstruction(constant4, genoperandoperand(CmpCwR, constant4, rcvrReg)), constant4);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction8 = genoperandoperand(CmpCqR, constant4, rcvrReg);
			if (usesOutOfLineLiteral(anInstruction8)) {
				(anInstruction8->dependent = locateLiteralsize(constant4, BytesPerOop));
			}
		}
	}
	else {
		if (!unforwardRcvr) {
			/* begin genCmpConstant:R: */
			constant12 = ((ssValue(1))->constant);
			if (shouldAnnotateObjectReference(constant12)) {
				annotateobjRef(checkLiteralforInstruction(constant12, genoperandoperand(CmpCwR, constant12, argReg)), constant12);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction9 = genoperandoperand(CmpCqR, constant12, argReg);
				if (usesOutOfLineLiteral(anInstruction9)) {
					(anInstruction9->dependent = locateLiteralsize(constant12, BytesPerOop));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);
	if (orNot) {
		/* begin JumpNonZero: */
		jumpEqual = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	}
	else {
		/* begin JumpZero: */
		jumpEqual = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	}
	/* begin genMoveFalseR: */
	constant = falseObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, TempReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, constant, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	/* begin Jump: */
	jumpNotEqual = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpEqual, genMoveTrueR(TempReg));
	jmpTarget(jumpNotEqual, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushRegister(TempReg);
	if ((((fixupAt(nextPC))->targetInstruction)) == 0) {
		branchReachedOnlyForCounterTrip = 1;
	}
	return 0;
}

	/* SistaCogit>>#genJumpBinaryInlinePrimitive: */
static sqInt NoDbgRegParms
genJumpBinaryInlinePrimitive(sqInt primIndex)
{
    void *jumpTarget;
    void *jumpTarget1;
    void *jumpTarget2;
    sqInt target;
    sqInt testReg;
    sqInt wordConstant;
    sqInt wordConstant1;

	assert((((ssTop())->type)) == SSConstant);
	target = eventualTargetOf(((((((ssTop())->constant)) >> 3)) + 3) + bytecodePC);
	testReg = allocateRegForStackEntryAtnotConflictingWith(1, 0);
	popToReg(ssValue(1), testReg);
	ssPop(2);
	switch (primIndex) {
	case 16:
		
#    if IMMUTABILITY
		jmpTarget(genJumpMutablescratchReg(testReg, TempReg), ensureFixupAt(target));
#    else
		/* begin Jump: */
		jumpTarget = ensureFixupAt(target);
		genoperand(Jump, ((sqInt)jumpTarget));
#    endif
		return 0;

	case 17:
		
#    if IMMUTABILITY
		jmpTarget(genJumpImmutablescratchReg(testReg, TempReg), ensureFixupAt(target));
#    else

		/* Do nothing - fall through */
#    endif
		return 0;

	case 18:
		/* begin CmpCw:R: */
		wordConstant = storeCheckBoundary();
		checkLiteralforInstruction(wordConstant, genoperandoperand(CmpCwR, wordConstant, testReg));
		/* begin JumpBelow: */
		jumpTarget1 = ensureFixupAt(target);
		genConditionalBranchoperand(JumpBelow, ((sqInt)jumpTarget1));
		return 0;

	case 19:
		/* begin CmpCw:R: */
		wordConstant1 = storeCheckBoundary();
		checkLiteralforInstruction(wordConstant1, genoperandoperand(CmpCwR, wordConstant1, testReg));
		/* begin JumpAboveOrEqual: */
		jumpTarget2 = ensureFixupAt(target);
		genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)jumpTarget2));
		return 0;

	default:
		error("Case not found and no otherwise clause");
	}
	return EncounteredUnknownBytecode;
}


/*	The heart of performance counting in Sista. Conditional branches are 6
	times less
	frequent than sends and can provide basic block frequencies (send counters
	can't). Each conditional has a 32-bit counter split into an upper 16 bits
	counting executions
	and a lower half counting untaken executions of the branch. Executing the
	branch decrements the upper half, tripping if the count goes negative. Not
	taking the branch
	decrements the lower half. N.B. We *do not* eliminate dead branches (true
	ifTrue:/true ifFalse:)
	so that scanning for send and branch data is simplified and that branch
	data is correct. */

	/* SistaCogit>>#genJumpIf:to: */
static sqInt NoDbgRegParms
genJumpIfto(sqInt boolean, sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *abstractInstruction4;
    AbstractInstruction *abstractInstruction5;
    AbstractInstruction *abstractInstruction6;
    AbstractInstruction *abstractInstruction7;
    AbstractInstruction *abstractInstruction8;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    AbstractInstruction *anInstruction8;
    AbstractInstruction *anInstruction9;
    sqInt counterAddress;
    sqInt counterAddress1;
    AbstractInstruction *countTripped;
    AbstractInstruction *countTripped1;
    CogSimStackEntry *desc;
    CogSimStackEntry *desc1;
    CogSimStackEntry *desc2;
    CogSimStackEntry *desc3;
    CogSimStackEntry *desc4;
    sqInt eventualTarget;
    sqInt eventualTarget1;
    sqInt eventualTarget2;
    sqInt eventualTarget3;
    sqInt eventualTarget4;
    BytecodeFixup *fixup;
    BytecodeFixup *fixup1;
    BytecodeFixup *fixup2;
    BytecodeFixup *fixup3;
    sqInt i;
    sqInt i1;
    sqInt i2;
    sqInt i3;
    sqInt i4;
    sqInt index;
    sqInt index1;
    sqInt index2;
    sqInt index3;
    sqInt index4;
    void *jumpTarget;
    void *jumpTarget1;
    void *jumpTarget2;
    void *jumpTarget3;
    void *jumpTarget4;
    BytecodeDescriptor *nextDescriptor;
    sqInt nextPC;
    AbstractInstruction *ok;
    AbstractInstruction *ok1;
    AbstractInstruction *ok2;
    AbstractInstruction *ok3;
    AbstractInstruction *ok4;
    sqInt opcode;
    sqInt opcode1;
    sqInt opcode2;
    sqInt quickConstant;
    sqInt quickConstant1;
    sqInt quickConstant2;
    sqInt quickConstant3;
    sqInt quickConstant4;
    AbstractInstruction *retry;


	/* In optimized code we don't generate counters to improve performance */
	if (isOptimizedMethod(methodObj)) {
		eventualTarget1 = eventualTargetOf(targetBytecodePC);
		/* begin ssFlushTo: */
		index = simStackPtr - 1;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		desc1 = ssTop();
		ssPop(1);
		if ((((desc1->type)) == SSConstant)
		 && ((((desc1->constant)) == (trueObject()))
		 || (((desc1->constant)) == (falseObject())))) {

			/* Must arrange there's a fixup at the target whether it is jumped to or
			   not so that the simStackPtr can be kept correct. */

			/* Must annotate the bytecode for correct pc mapping. */
			fixup = ensureFixupAt(eventualTarget1);
			/* begin annotateBytecode: */
			if (((desc1->constant)) == boolean) {
				/* begin Jump: */
				abstractInstruction = genoperand(Jump, ((sqInt)fixup));
			}
			else {
				if (prevInstIsPCAnnotated()) {
					/* begin Nop */
					abstractInstruction = gen(Nop);
				}
				else {
					/* begin Label */
					abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
				}
			}
			(abstractInstruction->annotation = HasBytecodePC);
			extA = 0;
			return 0;
		}
		popToReg(desc1, TempReg);
		assert((objectAfter(falseObject())) == (trueObject()));
		/* begin genSubConstant:R: */
		if (shouldAnnotateObjectReference(boolean)) {
			annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(SubCqR, boolean, TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(boolean, BytesPerOop));
			}
		}
		/* begin JumpZero: */
		jumpTarget = ensureFixupAt(eventualTarget1);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		if (((extA & 1) != 0)) {
			extA = 0;
			/* begin annotateBytecode: */
			abstractInstruction1 = lastOpcode();
			(abstractInstruction1->annotation = HasBytecodePC);
			return 0;
		}
		extA = 0;
		/* begin CmpCq:R: */
		quickConstant = (boolean == (falseObject())
			? (trueObject()) - (falseObject())
			: (falseObject()) - (trueObject()));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(CmpCqR, quickConstant, TempReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
		/* begin JumpZero: */
		ok1 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		genCallMustBeBooleanFor(boolean);
		jmpTarget(ok1, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
		return 0;
	}
	if (branchReachedOnlyForCounterTrip) {
		branchReachedOnlyForCounterTrip = 0;
		return genCounterTripOnlyJumpIfto(boolean, targetBytecodePC);
	}
	if (boolean == (falseObject())) {
		nextPC = bytecodePC + (((generatorAt(byte0))->numBytes));
		/* begin generatorForPC: */
		nextDescriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
		if (((nextDescriptor->generator)) == genPushConstantTrueBytecode) {
			eventualTarget2 = eventualTargetOf(targetBytecodePC);
			/* begin ssFlushTo: */
			index1 = simStackPtr - 1;
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index1) {
				for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i1 <= index1; i1 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
				}
				simSpillBase = index1 + 1;
			}
			desc2 = ssTop();
			ssPop(1);
			if ((((desc2->type)) == SSConstant)
			 && ((((desc2->constant)) == (trueObject()))
			 || (((desc2->constant)) == (falseObject())))) {

				/* Must arrange there's a fixup at the target whether it is jumped to or
				   not so that the simStackPtr can be kept correct. */

				/* Must annotate the bytecode for correct pc mapping. */
				fixup1 = ensureFixupAt(eventualTarget2);
				/* begin annotateBytecode: */
				if (((desc2->constant)) == boolean) {
					/* begin Jump: */
					abstractInstruction2 = genoperand(Jump, ((sqInt)fixup1));
				}
				else {
					if (prevInstIsPCAnnotated()) {
						/* begin Nop */
						abstractInstruction2 = gen(Nop);
					}
					else {
						/* begin Label */
						abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
					}
				}
				(abstractInstruction2->annotation = HasBytecodePC);
				extA = 0;
				return 0;
			}
			popToReg(desc2, TempReg);
			assert((objectAfter(falseObject())) == (trueObject()));
			/* begin genSubConstant:R: */
			if (shouldAnnotateObjectReference(boolean)) {
				annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction2 = genoperandoperand(SubCqR, boolean, TempReg);
				if (usesOutOfLineLiteral(anInstruction2)) {
					(anInstruction2->dependent = locateLiteralsize(boolean, BytesPerOop));
				}
			}
			/* begin JumpZero: */
			jumpTarget1 = ensureFixupAt(eventualTarget2);
			genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget1));
			if (((extA & 1) != 0)) {
				extA = 0;
				/* begin annotateBytecode: */
				abstractInstruction3 = lastOpcode();
				(abstractInstruction3->annotation = HasBytecodePC);
				return 0;
			}
			extA = 0;
			/* begin CmpCq:R: */
			quickConstant1 = (boolean == (falseObject())
				? (trueObject()) - (falseObject())
				: (falseObject()) - (trueObject()));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction3 = genoperandoperand(CmpCqR, quickConstant1, TempReg);
			if (usesOutOfLineLiteral(anInstruction3)) {
				(anInstruction3->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
			}
			/* begin JumpZero: */
			ok2 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
			genCallMustBeBooleanFor(boolean);
			jmpTarget(ok2, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
			return 0;
		}
		/* begin generatorForPC: */
		nextDescriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(targetBytecodePC, methodObj)));
		if (((nextDescriptor->generator)) == genPushConstantFalseBytecode) {
			eventualTarget3 = eventualTargetOf(targetBytecodePC);
			/* begin ssFlushTo: */
			index2 = simStackPtr - 1;
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index2) {
				for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index2) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index2)); i2 <= index2; i2 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
				}
				simSpillBase = index2 + 1;
			}
			desc3 = ssTop();
			ssPop(1);
			if ((((desc3->type)) == SSConstant)
			 && ((((desc3->constant)) == (trueObject()))
			 || (((desc3->constant)) == (falseObject())))) {

				/* Must arrange there's a fixup at the target whether it is jumped to or
				   not so that the simStackPtr can be kept correct. */

				/* Must annotate the bytecode for correct pc mapping. */
				fixup2 = ensureFixupAt(eventualTarget3);
				/* begin annotateBytecode: */
				if (((desc3->constant)) == boolean) {
					/* begin Jump: */
					abstractInstruction4 = genoperand(Jump, ((sqInt)fixup2));
				}
				else {
					if (prevInstIsPCAnnotated()) {
						/* begin Nop */
						abstractInstruction4 = gen(Nop);
					}
					else {
						/* begin Label */
						abstractInstruction4 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
					}
				}
				(abstractInstruction4->annotation = HasBytecodePC);
				extA = 0;
				return 0;
			}
			popToReg(desc3, TempReg);
			assert((objectAfter(falseObject())) == (trueObject()));
			/* begin genSubConstant:R: */
			if (shouldAnnotateObjectReference(boolean)) {
				annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction4 = genoperandoperand(SubCqR, boolean, TempReg);
				if (usesOutOfLineLiteral(anInstruction4)) {
					(anInstruction4->dependent = locateLiteralsize(boolean, BytesPerOop));
				}
			}
			/* begin JumpZero: */
			jumpTarget2 = ensureFixupAt(eventualTarget3);
			genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget2));
			if (((extA & 1) != 0)) {
				extA = 0;
				/* begin annotateBytecode: */
				abstractInstruction5 = lastOpcode();
				(abstractInstruction5->annotation = HasBytecodePC);
				return 0;
			}
			extA = 0;
			/* begin CmpCq:R: */
			quickConstant2 = (boolean == (falseObject())
				? (trueObject()) - (falseObject())
				: (falseObject()) - (trueObject()));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction5 = genoperandoperand(CmpCqR, quickConstant2, TempReg);
			if (usesOutOfLineLiteral(anInstruction5)) {
				(anInstruction5->dependent = locateLiteralsize(quickConstant2, BytesPerOop));
			}
			/* begin JumpZero: */
			ok3 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
			genCallMustBeBooleanFor(boolean);
			jmpTarget(ok3, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
			return 0;
		}
	}

	/* We ignore the noMustBeBoolean flag. It should not be present in methods with counters, and if it is we don't care. */
	/* We don't generate counters on branches on true/false, the basicblock usage can be inferred */
	extA = 0;
	desc = ssTop();
	if ((((desc->type)) == SSConstant)
	 && ((((desc->constant)) == (trueObject()))
	 || (((desc->constant)) == (falseObject())))) {
		eventualTarget4 = eventualTargetOf(targetBytecodePC);
		/* begin ssFlushTo: */
		index3 = simStackPtr - 1;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index3) {
			for (i3 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index3) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index3)); i3 <= index3; i3 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i3), frameOffsetOfTemporary(i3 - 1), FPReg);
			}
			simSpillBase = index3 + 1;
		}
		desc4 = ssTop();
		ssPop(1);
		if ((((desc4->type)) == SSConstant)
		 && ((((desc4->constant)) == (trueObject()))
		 || (((desc4->constant)) == (falseObject())))) {

			/* Must arrange there's a fixup at the target whether it is jumped to or
			   not so that the simStackPtr can be kept correct. */

			/* Must annotate the bytecode for correct pc mapping. */
			fixup3 = ensureFixupAt(eventualTarget4);
			/* begin annotateBytecode: */
			if (((desc4->constant)) == boolean) {
				/* begin Jump: */
				abstractInstruction6 = genoperand(Jump, ((sqInt)fixup3));
			}
			else {
				if (prevInstIsPCAnnotated()) {
					/* begin Nop */
					abstractInstruction6 = gen(Nop);
				}
				else {
					/* begin Label */
					abstractInstruction6 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
				}
			}
			(abstractInstruction6->annotation = HasBytecodePC);
			extA = 0;
			return 0;
		}
		popToReg(desc4, TempReg);
		assert((objectAfter(falseObject())) == (trueObject()));
		/* begin genSubConstant:R: */
		if (shouldAnnotateObjectReference(boolean)) {
			annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction6 = genoperandoperand(SubCqR, boolean, TempReg);
			if (usesOutOfLineLiteral(anInstruction6)) {
				(anInstruction6->dependent = locateLiteralsize(boolean, BytesPerOop));
			}
		}
		/* begin JumpZero: */
		jumpTarget3 = ensureFixupAt(eventualTarget4);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget3));
		if (((extA & 1) != 0)) {
			extA = 0;
			/* begin annotateBytecode: */
			abstractInstruction7 = lastOpcode();
			(abstractInstruction7->annotation = HasBytecodePC);
			return 0;
		}
		extA = 0;
		/* begin CmpCq:R: */
		quickConstant3 = (boolean == (falseObject())
			? (trueObject()) - (falseObject())
			: (falseObject()) - (trueObject()));
		/* begin checkQuickConstant:forInstruction: */
		anInstruction7 = genoperandoperand(CmpCqR, quickConstant3, TempReg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(quickConstant3, BytesPerOop));
		}
		/* begin JumpZero: */
		ok4 = genConditionalBranchoperand(JumpZero, ((sqInt)0));
		genCallMustBeBooleanFor(boolean);
		jmpTarget(ok4, annotateBytecode(genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
		return 0;
	}
	eventualTarget = eventualTargetOf(targetBytecodePC);
	/* begin ssFlushTo: */
	index4 = simStackPtr - 1;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index4) {
		for (i4 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index4) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index4)); i4 <= index4; i4 += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i4), frameOffsetOfTemporary(i4 - 1), FPReg);
		}
		simSpillBase = index4 + 1;
	}
	popToReg(desc, TempReg);
	ssPop(1);
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << SendNumArgsReg), simStackPtr, simNativeStackPtr);
	/* begin Label */
	retry = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin genExecutionCountLogicInto:counterReg: */
	counterAddress1 = counters + (CounterBytes * counterIndex);
	/* begin MoveA32:R: */
	opcode = MoveA32R;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode, counterAddress1, SendNumArgsReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction9 = genoperandoperand(SubCqR, 0x10000, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction9)) {
		(anInstruction9->dependent = locateLiteralsize(0x10000, BytesPerOop));
	}
	/* begin JumpCarry: */
	countTripped1 = genConditionalBranchoperand(JumpCarry, ((sqInt)0));
	/* begin MoveR:A32: */
	opcode1 = MoveRA32;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode1, SendNumArgsReg, counterAddress1));
	counterAddress = counterAddress1;
	countTripped = countTripped1;

	/* Cunning trick by LPD.  If true and false are contiguous subtract the smaller.
	   Correct result is either 0 or the distance between them.  If result is not 0 or
	   their distance send mustBeBoolean. */
	counterIndex += 1;
	assert((objectAfter(falseObject())) == (trueObject()));
	/* begin genSubConstant:R: */
	if (shouldAnnotateObjectReference(boolean)) {
		annotateobjRef(gSubCwR(boolean, TempReg), TempReg);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction8 = genoperandoperand(SubCqR, boolean, TempReg);
		if (usesOutOfLineLiteral(anInstruction8)) {
			(anInstruction8->dependent = locateLiteralsize(boolean, BytesPerOop));
		}
	}
	/* begin JumpZero: */
	jumpTarget4 = ensureFixupAt(eventualTarget);
	genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget4));
	/* begin genFallsThroughCountLogicCounterReg:counterAddress: */
	anInstruction10 = genoperandoperand(SubCqR, 1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction10)) {
		(anInstruction10->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin MoveR:A32: */
	opcode2 = MoveRA32;
	checkLiteralforInstruction(counterAddress, genoperandoperand(opcode2, SendNumArgsReg, counterAddress));
	/* begin CmpCq:R: */
	quickConstant4 = (boolean == (falseObject())
		? (trueObject()) - (falseObject())
		: (falseObject()) - (trueObject()));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperand(CmpCqR, quickConstant4, TempReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(quickConstant4, BytesPerOop));
	}
	/* begin JumpZero: */
	ok = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(MoveCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(0, BytesPerOop));
	}
	jmpTarget(countTripped, genCallMustBeBooleanFor(boolean));
	/* begin annotateBytecode: */
	abstractInstruction8 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction8->annotation = HasBytecodePC);
	/* begin Jump: */
	genoperand(Jump, ((sqInt)retry));
	jmpTarget(ok, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	Note: those tests work with forwarders (wrong class index) */

	/* SistaCogit>>#genJumpTrinaryInlinePrimitive: */
static sqInt NoDbgRegParms
genJumpTrinaryInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction21;
    AbstractInstruction *anInstruction3;
    sqInt behavior;
    sqInt classIndex;
    sqInt classIndex1;
    AbstractInstruction *jmp;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpImmediate1;
    sqInt oop;
    sqInt oop1;
    sqInt oop11;
    sqInt oop2;
    sqInt oop21;
    sqInt oop3;
    sqInt target;
    BytecodeFixup *targetFixUp;
    sqInt testReg;

	jmpImmediate = ((AbstractInstruction *) 0);
	jmpImmediate1 = ((AbstractInstruction *) 0);
	assert((((ssTop())->type)) == SSConstant);
	assert((((ssValue(1))->type)) == SSConstant);
	testReg = allocateRegForStackEntryAtnotConflictingWith(2, 0);
	popToReg(ssValue(2), testReg);
	behavior = ((ssValue(1))->constant);
	target = eventualTargetOf(((((((ssTop())->constant)) >> 3)) + 3) + bytecodePC);
	ssPop(3);
	targetFixUp = ((AbstractInstruction *) (ensureFixupAt(target)));
	switch (primIndex) {
	case 0:
		
		/* 8000	jumpIfInstanceOf:distance:
		   Anything, literal which is a Behavior, literal which is a Smi */
		/* begin branchIf:instanceOfBehavior:target: */
		classIndex1 = classTagForClass(behavior);
		if (classIndex1 == (fetchClassTagOf(falseObject()))) {
			/* begin branchIf:isOop:target: */
			oop3 = falseObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction3 = genoperandoperand(CmpCqR, oop3, testReg);
			if (usesOutOfLineLiteral(anInstruction3)) {
				(anInstruction3->dependent = locateLiteralsize(oop3, BytesPerOop));
			}
			/* begin JumpZero: */
			genConditionalBranchoperand(JumpZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (classIndex1 == (fetchClassTagOf(trueObject()))) {
			/* begin branchIf:isOop:target: */
			oop11 = trueObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction11 = genoperandoperand(CmpCqR, oop11, testReg);
			if (usesOutOfLineLiteral(anInstruction11)) {
				(anInstruction11->dependent = locateLiteralsize(oop11, BytesPerOop));
			}
			/* begin JumpZero: */
			genConditionalBranchoperand(JumpZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (classIndex1 == (fetchClassTagOf(nilObject()))) {
			/* begin branchIf:isOop:target: */
			oop21 = nilObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction21 = genoperandoperand(CmpCqR, oop21, testReg);
			if (usesOutOfLineLiteral(anInstruction21)) {
				(anInstruction21->dependent = locateLiteralsize(oop21, BytesPerOop));
			}
			/* begin JumpZero: */
			genConditionalBranchoperand(JumpZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (isImmediateClass(behavior)) {
			/* begin branchIf:hasImmediateTag:target: */
			if (classIndex1 == (smallIntegerTag())) {
				jmpImmediate1 = genJumpSmallInteger(testReg);
			}
			if (classIndex1 == (characterTag())) {
				jmpImmediate1 = genJumpCharacter(testReg);
			}
			if (classIndex1 == (smallFloatTag())) {
				jmpImmediate1 = genJumpSmallFloat(testReg);
			}
			jmpTarget(jmpImmediate1, ((AbstractInstruction *) targetFixUp));
		}
		else {
			jmp = genJumpImmediate(testReg);
			genGetClassIndexOfNonImminto(testReg, TempReg);
			genCmpClassIndexR(classIndex1, TempReg);
			/* begin JumpZero: */
			genConditionalBranchoperand(JumpZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
			jmpTarget(jmp, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
		return 0;

	case 1:
		
		/* 8001	jumpIfNotInstanceOf:distance:
		   Anything, literal which is a Behavior, literal which is a Smi */
		/* begin branchIf:notInstanceOfBehavior:target: */
		classIndex = classTagForClass(behavior);
		if (classIndex == (fetchClassTagOf(falseObject()))) {
			/* begin branchIf:isNotOop:target: */
			oop = falseObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(CmpCqR, oop, testReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(oop, BytesPerOop));
			}
			/* begin JumpNonZero: */
			genConditionalBranchoperand(JumpNonZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (classIndex == (fetchClassTagOf(trueObject()))) {
			/* begin branchIf:isNotOop:target: */
			oop1 = trueObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(CmpCqR, oop1, testReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(oop1, BytesPerOop));
			}
			/* begin JumpNonZero: */
			genConditionalBranchoperand(JumpNonZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (classIndex == (fetchClassTagOf(nilObject()))) {
			/* begin branchIf:isNotOop:target: */
			oop2 = nilObject();
			/* begin checkQuickConstant:forInstruction: */
			anInstruction2 = genoperandoperand(CmpCqR, oop2, testReg);
			if (usesOutOfLineLiteral(anInstruction2)) {
				(anInstruction2->dependent = locateLiteralsize(oop2, BytesPerOop));
			}
			/* begin JumpNonZero: */
			genConditionalBranchoperand(JumpNonZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		if (isImmediateClass(behavior)) {
			/* begin branchIf:hasNotImmediateTag:target: */
			if (classIndex == (smallIntegerTag())) {
				jmpImmediate = genJumpNotSmallInteger(testReg);
			}
			if (classIndex == (characterTag())) {
				jmpImmediate = genJumpNotCharacter(testReg);
			}
			if (classIndex == (smallFloatTag())) {
				jmpImmediate = genJumpNotSmallFloat(testReg);
			}
			jmpTarget(jmpImmediate, ((AbstractInstruction *) targetFixUp));
		}
		else {
			jmpTarget(genJumpImmediate(testReg), ((AbstractInstruction *) targetFixUp));
			genGetClassIndexOfNonImminto(testReg, TempReg);
			genCmpClassIndexR(classIndex, TempReg);
			/* begin JumpNonZero: */
			genConditionalBranchoperand(JumpNonZero, ((sqInt)(((AbstractInstruction *) targetFixUp))));
		}
		return 0;

	case 2:
		
		/* 8002	jumpIfInstanceOfOneOf:distance:
		   Anything, Array of behaviors, literal which is a Smi */
		branchIfinstanceOfBehaviorstarget(testReg, behavior, targetFixUp);
		return 0;

	case 3:
		
		/* 8003	jumpIfNotInstanceOfOneOf:distance:
		   Anything, Array of behaviors, literal which is a Smi */
		branchIfnotInstanceOfBehaviorstarget(testReg, behavior, targetFixUp);
		return 0;

	default:
		error("Case not found and no otherwise clause");
	}
	return EncounteredUnknownBytecode;
}


/*	6000	backjumpNoInterrupt
	literal which is a Smi */

	/* SistaCogit>>#genJumpUnaryInlinePrimitive: */
static sqInt NoDbgRegParms
genJumpUnaryInlinePrimitive(sqInt primIndex)
{
    sqInt i;
    sqInt index;
    void *jumpTarget;
    sqInt targetBytecodePC;

	if (primIndex == 0) {
		assert((((ssTop())->type)) == SSConstant);
		targetBytecodePC = ((((((ssTop())->constant)) >> 3)) + 3) + bytecodePC;
		ssPop(1);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}

		/* can't fall through */
		deadCode = 1;
		/* begin Jump: */
		jumpTarget = fixupAtIndex(targetBytecodePC - initialPC);
		genoperand(Jump, ((sqInt)jumpTarget));
		return 0;
	}
	return EncounteredUnknownBytecode;
}


/*	SistaV1:	236		11101100	iiiiiiii		callMappedInlinedPrimitive */
/*	Number of arguments:
	0-49 nullary
	50-99 unary
	100-149 binary
	150-199 trinary
	200-255 variable */
/*	Specification:
	50	EnsureEnoughWords
	literal which is a Smi => ret value is receiver
	150	immCheckPointerAt:put:
	pointer object (Fixed sized or not) and not a context, Smi, Anything =>
	arg2 (1-based, optimised if arg1 is a constant)
	151	immCheckStoreCheckPointerAt:put:
	pointer object (Fixed sized or not) and not a context, Smi, Anything =>
	arg2 (1-based, optimised if arg1 is a constant)
	152	immCheckMaybeContextPointerAt:put:
	pointer object (Fixed sized or not), Smi, Anything => arg2 (1-based,
	optimised if arg1 is a constant)
	153	immCheckMaybeContextStoreCheckPointerAt:put:
	pointer object (Fixed sized or not), Smi, Anything => arg2 (1-based,
	optimised if arg1 is a constant)
	154	immCheckByteAt:put:
	byte object, Smi, 8 bits unsigned Smi => arg2 (1-based, optimised if arg1
	is a constant)
	155	immCheckShortAt:put:
	short object, Smi, 16 bits unsigned Smi => arg2 (1-based, optimised if
	arg1 is a constant)
	156	immCheckWordAt:put:
	word object, Smi, 32 bits unsigned Smi => arg2 (1-based, optimised if arg1
	is a constant)
	157	immCheckDoubleWordAt:put:
	double word object, Smi, 64 bits unsigned Smi or LargePositiveInteger =>
	arg2 (1-based, optimised if arg1 is a constant)
	250	directCall
	method to call on top of stack => (variable number of parameters)
 */

	/* SistaCogit>>#genMappedInlinePrimitive: */
static sqInt NoDbgRegParms
genMappedInlinePrimitive(sqInt primIndex)
{
    AbstractInstruction *abstractInstruction;

	switch (primIndex) {
	case 50:
		return genEnsureEnoughSlots();

	case 150:
		return genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(0, 0, 1);

	case 151:
		return genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(0, 1, 1);

	case 152:
		return genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(1, 0, 1);

	case 153:
		return genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(1, 1, 1);

	case 154:
		
#    if IMMUTABILITY
		return genByteAtPutImmutabilityCheck();
#    else
		genByteAtPut();
		/* begin annotateBytecode: */
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		(abstractInstruction->annotation = HasBytecodePC);
		return 0;
#    endif // IMMUTABILITY

	case 155:
	case 156:
	case 157:
		return EncounteredUnknownBytecode;

	case 250:
		return genDirectCall();

	default:
		return EncounteredUnknownBytecode;

	}
	return 0;
}


/*	This can be entered in one of two states, depending on SendNumArgsReg. See
	e.g. genJumpIf:to:. If SendNumArgsReg is non-zero then this has been
	entered via
	the initial test of the counter in the jump executed count (i.e. the
	counter has
	tripped). In this case TempReg contains the boolean to be tested and
	should not
	be offset, and ceCounterTripped should be invoked with the unoffset
	TempReg. If SendNumArgsReg is zero then this has been entered for
	must-be-boolean processing. TempReg has been offset by boolean and must be
	corrected and
	ceSendMustBeBoolean: invoked with the corrected value. */

	/* SistaCogit>>#genMustBeBooleanTrampolineFor:called: */
static usqInt NoDbgRegParms
genMustBeBooleanTrampolineForcalled(sqInt boolean, char *trampolineName)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpMBB;

	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, 0, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpZero: */
	jumpMBB = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	genSmalltalkToCStackSwitch(1);
	compileCallFornumArgsargargargargresultRegregsToSave(ceCounterTripped, 1, TempReg, null, null, null, TempReg, 0 /* emptyRegisterMask */);
	genLoadStackPointers(backEnd);
	/* begin PopR: */
	genoperand(PopR, LinkReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, FoxMFReceiver, FPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(FoxMFReceiver, BytesPerOop));
	}
	/* begin RetN: */
	genoperand(RetN, 0);
	assert(!(shouldAnnotateObjectReference(boolean)));
	jmpTarget(jumpMBB, checkQuickConstantforInstruction(boolean, genoperandoperand(AddCqR, boolean, TempReg)));
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSendMustBeBoolean, trampolineName, 1, TempReg, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 1);
}

	/* SistaCogit>>#genPointerAtPutConstantMaybeContext:storeCheck:immutabilityCheck: */
static sqInt NoDbgRegParms
genPointerAtPutConstantMaybeContextstoreCheckimmutabilityCheck(sqInt maybeContext, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt i;
    sqInt i1;
    AbstractInstruction *immutabilityFailure;
    sqInt index;
    sqInt index1;
    sqInt indexCst;
    AbstractInstruction *mutableJump;
    sqInt rcvrReg;
    sqInt stackPtr;
    sqInt stackPtr1;
    sqInt stackPtr2;
    sqInt topReg;
    sqInt valReg;

	immutabilityFailure = ((AbstractInstruction *) 0);

	/* we want to have on top of stack the value to write descriptor. */
	indexCst = (((((ssValue(1))->constant)) >> 3)) - 1;
	if (maybeContext
	 || (needsImmCheck)) {
		valReg = ClassReg;
		voidReceiverResultRegContainsSelf();
		rcvrReg = ReceiverResultReg;
	}
	else {
		valReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		if (needsStoreCheck) {
			voidReceiverResultRegContainsSelf();
			rcvrReg = ReceiverResultReg;
		}
		else {
			rcvrReg = allocateRegForStackEntryAtnotConflictingWith(2, ((valReg < 0) ? (((usqInt)(1)) >> (-valReg)) : (1ULL << valReg)));
		}
	}
	/* begin ssAllocateRequiredReg:upThrough: */
	stackPtr = simStackPtr - 2;
	ssAllocateRequiredRegMaskupThroughupThroughNative(((valReg < 0) ? (((usqInt)(1)) >> (-valReg)) : (1ULL << valReg)), stackPtr, simNativeStackPtr);
	/* begin ssAllocateRequiredReg:upThrough: */
	stackPtr1 = simStackPtr - 3;
	ssAllocateRequiredRegMaskupThroughupThroughNative(((rcvrReg < 0) ? (((usqInt)(1)) >> (-rcvrReg)) : (1ULL << rcvrReg)), stackPtr1, simNativeStackPtr);
	popToReg(ssTop(), valReg);
	ssPop(1);
	if (((ssTop())->spilled)) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
		}
	}
	ssPop(1);
	popToReg(ssTop(), rcvrReg);
	ssPop(1);
	ssPushRegister(valReg);
	if (maybeContext) {
		/* begin genGenericStorePop:MaybeContextSlotIndex:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
		assert(needsFrame);
#    if IMMUTABILITY
		if (needsImmCheck) {
			mutableJump = genJumpMutablescratchReg(ReceiverResultReg, TempReg);
			/* begin genStoreTrampolineCall: */
			assert(IMMUTABILITY);
			if (indexCst >= (NumStoreTrampolines - 1)) {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction2 = genoperandoperand(MoveCqR, indexCst, TempReg);
				if (usesOutOfLineLiteral(anInstruction2)) {
					(anInstruction2->dependent = locateLiteralsize(indexCst, BytesPerOop));
				}
				/* begin CallRT: */
				abstractInstruction3 = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
				(abstractInstruction3->annotation = IsRelativeCall);
			}
			else {
				/* begin CallRT: */
				abstractInstruction1 = genoperand(Call, ceStoreTrampolines[indexCst]);
				(abstractInstruction1->annotation = IsRelativeCall);
			}
			/* begin annotateBytecode: */
			abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			(abstractInstruction2->annotation = HasBytecodePC);
			/* begin Jump: */
			immutabilityFailure = genoperand(Jump, ((sqInt)0));
			jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
#    endif // IMMUTABILITY
		ssPop(1);
		/* begin ssAllocateCallReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ClassReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
		ssPush(1);
		genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
		ssStoreAndReplacePoptoReg(0, ClassReg);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, indexCst, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(indexCst, BytesPerOop));
		}
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceStoreContextInstVarTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
#    if IMMUTABILITY
		if (needsImmCheck) {
			jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
		}
#    endif // IMMUTABILITY
		return 0;
	}
	else {
		/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#    if IMMUTABILITY
		if (needsImmCheck) {
			/* begin ssAllocateRequiredReg:upThrough: */
			stackPtr2 = simStackPtr - 1;
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), stackPtr2, simNativeStackPtr);
			ssStoreAndReplacePoptoReg(0, ClassReg);
			/* begin ssFlushTo: */
			index1 = simStackPtr;
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index1) {
				for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i1 <= index1; i1 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
				}
				simSpillBase = index1 + 1;
			}
			return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, indexCst, rcvrReg, TempReg, needsStoreCheck, 0);
		}
#    endif // IMMUTABILITY
		topReg = allocateRegForStackEntryAtnotConflictingWith(0, ((rcvrReg < 0) ? (((usqInt)(1)) >> (-rcvrReg)) : (1ULL << rcvrReg)));
		ssStorePoptoReg(0, topReg);
		return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, indexCst, rcvrReg, TempReg, needsFrame, needsStoreCheck);
	}
}

	/* SistaCogit>>#genPointerAtPutImmCheckAndStoreCheck */
static sqInt
genPointerAtPutImmCheckAndStoreCheck(void)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt i;
    AbstractInstruction *immutableJump;
    sqInt index;
    int indexIsCst;
    AbstractInstruction *jmpDestYoung;
    AbstractInstruction *jmpImmediate;
    AbstractInstruction *jmpSourceOld;
    AbstractInstruction *jumpRemembered;
    sqInt quickConstant;
    sqInt ra1;
    sqInt ra2;
    sqInt rr;
    sqInt scratchReg;
    sqInt wordConstant;


	/* Assumes rr is not a context and no store check is needed */
	indexIsCst = (((ssValue(1))->type)) == SSConstant;
	/* begin ssFlushTo: */
	index = simStackPtr - 3;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	rr = ReceiverResultReg;
	ra1 = TempReg;
	ra2 = ClassReg;
	scratchReg = Arg0Reg;

	/* shift by baseHeaderSize and then move from 1 relative to zero relative */
	adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
	popToReg(ssValue(2), rr);
	if (indexIsCst) {
		/* begin MoveCq:R: */
		quickConstant = (((((ssValue(1))->constant)) >> 3)) + adjust;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, quickConstant, ra1);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
	}
	else {
		popToReg(ssValue(1), ra1);
		genConvertSmallIntegerToIntegerInReg(ra1);
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(AddCqR, adjust, ra1);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
		}
	}
	popToReg(ssTop(), ra2);
	ssPop(3);
	ssPushRegister(ra2);
	voidReceiverResultRegContainsSelf();
	immutableJump = genJumpImmutablescratchReg(rr, scratchReg);
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, ra2, ra1, rr);

	/* Get the old/new boundary in scratchReg */
	jmpImmediate = genJumpImmediate(ra2);
	/* begin MoveCw:R: */
	wordConstant = storeCheckBoundary();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, scratchReg));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, rr);
	/* begin JumpBelow: */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	/* begin CmpR:R: */
	assert(!((scratchReg == SPReg)));
	genoperandoperand(CmpRR, scratchReg, ra2);
	/* begin JumpAboveOrEqual: */
	jmpSourceOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));

	/* Set the inst var index for the benefit of the immutability check. The trampoline will
	   repeat the check to choose between the immutbality violation and the store check. */
	jumpRemembered = genIfRequiredCheckRememberedBitOfscratch(rr, scratchReg);
	jmpTarget(immutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin PushR: */
	genoperand(PushR, ra2);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(SubCqR, 1 + adjust, ra1);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(1 + adjust, BytesPerOop));
	}
	genVarIndexCallStoreTrampoline();
	/* begin PopR: */
	genoperand(PopR, ra2);
	jmpTarget(jmpImmediate, jmpTarget(jmpDestYoung, jmpTarget(jmpSourceOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	/* begin setIfRequiredTargetOf:toTargetOf: */
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jumpRemembered, ((AbstractInstruction *) (((jmpImmediate->operands))[0])));
	}
	return 0;
}

	/* SistaCogit>>#genPointerAtPutImmCheckButNoStoreCheck */
static sqInt
genPointerAtPutImmCheckButNoStoreCheck(void)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt i;
    AbstractInstruction *immutabilityFailure;
    sqInt index;
    int indexIsCst;
    AbstractInstruction *mutableJump;
    sqInt quickConstant;
    sqInt ra1;
    sqInt ra2;
    sqInt rr;


	/* Assumes rr is not a context and no store check is needed */
	indexIsCst = (((ssValue(1))->type)) == SSConstant;
	/* begin ssFlushTo: */
	index = simStackPtr - 3;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	rr = ReceiverResultReg;
	ra1 = TempReg;
	ra2 = ClassReg;
	popToReg(ssValue(2), rr);

	/* shift by baseHeaderSize and then move from 1 relative to zero relative */
	adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
	if (indexIsCst) {
		/* begin MoveCq:R: */
		quickConstant = (((((ssValue(1))->constant)) >> 3)) + adjust;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, quickConstant, ra1);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
		}
	}
	else {
		popToReg(ssValue(1), ra1);
		genConvertSmallIntegerToIntegerInReg(ra1);
		if (adjust != 0) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(AddCqR, adjust, ra1);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(adjust, BytesPerOop));
			}
		}
	}
	popToReg(ssTop(), ra2);
	ssPop(3);
	ssPushRegister(ra2);
	voidReceiverResultRegContainsSelf();

	/* simStack is flushed, but result is not */
	mutableJump = genJumpMutablescratchReg(rr, Arg0Reg);
	/* begin PushR: */
	genoperand(PushR, ra2);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(SubCqR, 1 + adjust, ra1);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(1 + adjust, BytesPerOop));
	}
	genVarIndexCallStoreTrampoline();
	/* begin PopR: */
	genoperand(PopR, ra2);
	/* begin Jump: */
	immutabilityFailure = genoperand(Jump, ((sqInt)0));
	jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, ra2, ra1, rr);
	jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}


/*	150	immCheckPointerAt:put:
	pointer object (Fixed sized or not) and not a context, Smi, Anything =>
	arg2 (1-based, optimised if arg1 is a constant)
	151	immCheckStoreCheckPointerAt:put:
	pointer object (Fixed sized or not) and not a context, Smi, Anything =>
	arg2 (1-based, optimised if arg1 is a constant)
	152	immCheckMaybeContextPointerAt:put:
	pointer object (Fixed sized or not), Smi, Anything => arg2 (1-based,
	optimised if arg1 is a constant)
	153	immCheckMaybeContextStoreCheckPointerAt:put:
	pointer object (Fixed sized or not), Smi, Anything => arg2 (1-based,
	optimised if arg1 is a constant)
 */

	/* SistaCogit>>#genPointerAtPutMaybeContext:storeCheck:immutabilityCheck: */
static sqInt NoDbgRegParms
genPointerAtPutMaybeContextstoreCheckimmutabilityCheck(sqInt maybeContext, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *abstractInstruction;
    CogSimStackEntry *index;


	/* Instruction always annoted - uncommon, IMMUTABILITY disabled. We need a Nop since previous instr can be annotated. */
	index = ssValue(1);
#  if IMMUTABILITY
#  else
	if (needsImmCheck) {
		/* begin annotateBytecode: */
		abstractInstruction = gen(Nop);
		(abstractInstruction->annotation = HasBytecodePC);
	}
#  endif // IMMUTABILITY
	if (((index->type)) == SSConstant) {
		return genPointerAtPutConstantMaybeContextstoreCheckimmutabilityCheck(maybeContext, needsStoreCheck, needsImmCheck);
	}
	if (maybeContext) {
		return EncounteredUnknownBytecode;
	}
#  if IMMUTABILITY
	if (needsImmCheck) {
		if (needsStoreCheck) {
			return genPointerAtPutImmCheckAndStoreCheck();
		}
		else {
			return genPointerAtPutImmCheckButNoStoreCheck();
		}
	}
#  endif // IMMUTABILITY
	return genPointerAtPutStoreCheck(needsStoreCheck);
}


/*	Assumes rr is not a context and no immutability check is needed */
/*	The store check requires rr to be ReceiverResultReg */

	/* SistaCogit>>#genPointerAtPutStoreCheck: */
static sqInt NoDbgRegParms
genPointerAtPutStoreCheck(sqInt needsStoreCheck)
{
    sqInt adjust;
    AbstractInstruction *anInstruction;
    sqInt ra1;
    sqInt ra2;
    sqInt reg;
    sqInt rNext1;
    sqInt rr;
    sqInt rThird1;
    sqInt rTop1;
    sqInt topRegistersMask;

	/* begin allocateRegForStackTopThreeEntriesInto:thirdIsReceiver: */
	topRegistersMask = 0;
	rTop1 = (rNext1 = (rThird1 = NoReg));
	if (((registerOrNone(ssTop())) != NoReg)
	 && ((!needsStoreCheck)
	 || ((registerOrNone(ssTop())) != ReceiverResultReg))) {
		/* begin registerMaskFor: */
		reg = (rTop1 = registerOrNone(ssTop()));
		topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	}
	if (((registerOrNone(ssValue(1))) != NoReg)
	 && ((!needsStoreCheck)
	 || ((registerOrNone(ssValue(1))) != ReceiverResultReg))) {
		topRegistersMask = topRegistersMask | (registerMaskFor((rNext1 = registerOrNone(ssValue(1)))));
	}
	if (((registerOrNone(ssValue(2))) != NoReg)
	 && ((!needsStoreCheck)
	 || ((registerOrNone(ssValue(2))) == ReceiverResultReg))) {
		topRegistersMask = topRegistersMask | (registerMaskFor((rThird1 = registerOrNone(ssValue(2)))));
	}
	if (rThird1 == NoReg) {
		if (needsStoreCheck) {

			/* Free ReceiverResultReg if it was not free */
			rThird1 = ReceiverResultReg;
			/* begin ssAllocateRequiredReg: */
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
			voidReceiverResultRegContainsSelf();
		}
		else {
			rThird1 = allocateRegNotConflictingWith(topRegistersMask);
		}
		topRegistersMask = topRegistersMask | (((rThird1 < 0) ? (((usqInt)(1)) >> (-rThird1)) : (1ULL << rThird1)));
	}
	if (rTop1 == NoReg) {
		rTop1 = allocateRegNotConflictingWith(topRegistersMask);
		topRegistersMask = topRegistersMask | (((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
	}
	if (rNext1 == NoReg) {
		rNext1 = allocateRegNotConflictingWith(topRegistersMask);
	}
	assert(!(((rTop1 == NoReg)
 || ((rNext1 == NoReg)
 || (rThird1 == NoReg)))));
	ra2 = rTop1;
	ra1 = rNext1;
	rr = rThird1;
	assert((rr != ra1)
	 && ((rr != ra2)
	 && (ra1 != ra2)));
	popToReg(ssTop(), ra2);
	ssPop(1);
	popToReg(ssTop(), ra1);
	ssPop(1);
	popToReg(ssTop(), rr);
	ssPop(1);
	genConvertSmallIntegerToIntegerInReg(ra1);

	/* shift by baseHeaderSize and then move from 1 relative to zero relative */
	adjust = (((usqInt)(BaseHeaderSize)) >> (shiftForWord())) - 1;
	if (adjust != 0) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, adjust, ra1);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(adjust, BytesPerOop));
		}
	}
	/* begin MoveR:Xwr:R: */
	genoperandoperandoperand(MoveRXwrR, ra2, ra1, rr);
	if (needsStoreCheck) {
		assert(needsFrame);
		genStoreCheckReceiverRegvalueRegscratchReginFrame(rr, ra2, TempReg, 1);
	}
	ssPushRegister(ra2);
	return 0;
}


/*	SistaV1:	248		11111000 iiiiiiii		mjjjjjjj		Call Primitive #iiiiiiii +
	(jjjjjjj * 256) m=1 means inlined primitive, no hard return after
	execution. This is the dispatch for unmapped sista inlined primitives.
 */

	/* SistaCogit>>#genSistaInlinePrimitive: */
static sqInt NoDbgRegParms
genSistaInlinePrimitive(sqInt prim)
{
	if (prim < 1000) {
		/* begin genNullaryInlinePrimitive: */
		return EncounteredUnknownBytecode;
	}
	if (prim < 2000) {
		return genUnaryInlinePrimitive(prim - 1000);
	}
	if (prim < 3000) {
		return genBinaryInlinePrimitive(prim - 2000);
	}
	if (prim < 4000) {
		return genTrinaryInlinePrimitive(prim - 3000);
	}
	if (prim < 5000) {
		/* begin genQuaternaryInlinePrimitive: */
		return EncounteredUnknownBytecode;
	}
	if (prim < 6000) {
		/* begin genQuinaryInlinePrimitive: */
		return EncounteredUnknownBytecode;
	}
	if (prim < 7000) {
		return genJumpUnaryInlinePrimitive(prim - 6000);
	}
	if (prim < 8000) {
		return genJumpBinaryInlinePrimitive(prim - 7000);
	}
	return genJumpTrinaryInlinePrimitive(prim - 8000);
}


/*	Override to count inlined branches if followed by a conditional branch.
	We borrow the following conditional branch's counter and when about to
	inline the comparison we decrement the counter (without writing it back)
	and if it trips simply abort the inlining, falling back to the normal send
	which will then continue to the conditional branch which will trip and
	enter the abort. */

	/* SistaCogit>>#genSpecialSelectorComparison */
static sqInt
genSpecialSelectorComparison(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    sqInt argInt;
    sqInt argIsIntConst;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt counterAddress;
    sqInt counterAddress1;
    sqInt counterReg;
    AbstractInstruction *countTripped;
    AbstractInstruction *countTripped1;
    sqInt i;
    sqInt index;
    sqInt index1;
    sqInt inlineCAB;
    AbstractInstruction *jumpNotSmallInts;
    void *jumpTarget;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt opcode;
    sqInt opcode1;
    sqInt opcode2;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    BytecodeDescriptor *primDescriptor1;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt targetBytecodePC;
    sqInt targetPC;

	if (isOptimizedMethod(methodObj)) {
		return genSpecialSelectorComparisonWithoutCounters();
	}
	/* begin ssFlushTo: */
	index1 = simStackPtr - 2;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index1) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i <= index1; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index1 + 1;
	}
	primDescriptor = generatorAt(byte0);
	argIsIntConst = ((((ssTop())->type)) == SSConstant)
	 && ((((((argInt = ((ssTop())->constant)))) & 7) == 1));

	/* short-cut the jump if operands are SmallInteger constants. */
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
	 && (((((((ssValue(1))->constant))) & 7) == 1)))
	 || ((mclassIsSmallInteger())
	 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsIntConst
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		return genStaticallyResolvedSpecialSelectorComparison();
	}
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor1 = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetPC = targetBytecodePC;

	/* Further, only interested in inlining = and ~= if there's a SmallInteger constant involved.
	   The relational operators successfully statically predict SmallIntegers; the equality operators do not. */
	inlineCAB = ((branchDescriptor->isBranchTrue))
	 || ((branchDescriptor->isBranchFalse));
	if (inlineCAB
	 && ((((primDescriptor->opcode)) == JumpZero)
	 || (((primDescriptor->opcode)) == JumpNonZero))) {
		inlineCAB = argIsIntConst
		 || (rcvrIsInt);
	}
	if (!inlineCAB) {
		return genSpecialSelectorSend();
	}
	if (argIsIntConst) {
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsIntConst))
		? (argIsIntConst
				? genJumpNotSmallInteger(ReceiverResultReg)
				: (rcvrIsInt
						? genJumpNotSmallInteger(Arg0Reg)
						: (/* begin genJumpNotSmallIntegersIn:and:scratch: */
							genoperandoperand(MoveRR, ReceiverResultReg, TempReg),
							/* begin AndR:R: */
							genoperandoperand(AndRR, Arg0Reg, TempReg),
							genJumpNotSmallInteger(TempReg))))
		: 0);
	counterReg = allocateRegNotConflictingWith((1U << ReceiverResultReg) | (1U << Arg0Reg));
	/* begin genExecutionCountLogicInto:counterReg: */
	counterAddress1 = counters + (CounterBytes * counterIndex);
	/* begin MoveA32:R: */
	opcode = MoveA32R;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode, counterAddress1, counterReg));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(SubCqR, 0x10000, counterReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(0x10000, BytesPerOop));
	}
	/* begin JumpCarry: */
	countTripped1 = genConditionalBranchoperand(JumpCarry, ((sqInt)0));
	/* begin MoveR:A32: */
	opcode1 = MoveRA32;
	checkLiteralforInstruction(counterAddress1, genoperandoperand(opcode1, counterReg, counterAddress1));
	counterAddress = counterAddress1;
	countTripped = countTripped1;
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, argInt, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(argInt, BytesPerOop));
		}
	}
	else {
		/* begin CmpR:R: */
		assert(!((Arg0Reg == SPReg)));
		genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	}
	genConditionalBranchoperand(((branchDescriptor->isBranchTrue)
		? (primDescriptor->opcode)
		: inverseBranchFor((primDescriptor->opcode))), ((usqInt)(ensureNonMergeFixupAt(targetPC))));
	/* begin genFallsThroughCountLogicCounterReg:counterAddress: */
	anInstruction3 = genoperandoperand(SubCqR, 1, counterReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(1, BytesPerOop));
	}
	/* begin MoveR:A32: */
	opcode2 = MoveRA32;
	checkLiteralforInstruction(counterAddress, genoperandoperand(opcode2, counterReg, counterAddress));
	/* begin Jump: */
	jumpTarget = ensureNonMergeFixupAt(postBranchPC);
	genoperand(Jump, ((sqInt)jumpTarget));
	jmpTarget(countTripped, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (jumpNotSmallInts == null) {
		if ((((fixupAt(nextPC))->targetInstruction)) == 0) {
			branchReachedOnlyForCounterTrip = 1;
		}
	}
	else {
		jmpTarget(jumpNotSmallInts, ((AbstractInstruction *) (((countTripped->operands))[0])));
	}
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, argInt, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(argInt, BytesPerOop));
		}
	}
	index = byte0 - ((bytecodeSetOffset == 0x100
		? AltFirstSpecialSelector + 0x100
		: FirstSpecialSelector));
	return genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
}


/*	This method is there because if I put directly the super send in
	genSpecialSelectorComparison Slang does not correctly translte the code to
	C, it does not correctly type one of the branchDescriptor to
	BytecodeDescriptor 
 */

	/* SistaCogit>>#genSpecialSelectorComparisonWithoutCounters */
static sqInt
genSpecialSelectorComparisonWithoutCounters(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt argInt;
    sqInt argIsIntConst;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt i;
    sqInt index;
    sqInt index1;
    sqInt inlineCAB;
    AbstractInstruction *jumpNotSmallInts;
    void *jumpTarget;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    BytecodeDescriptor *primDescriptor1;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt targetBytecodePC;
    sqInt targetPC;

	/* begin ssFlushTo: */
	index1 = simStackPtr - 2;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index1) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i <= index1; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index1 + 1;
	}
	primDescriptor = generatorAt(byte0);
	argIsIntConst = ((((ssTop())->type)) == SSConstant)
	 && ((((((argInt = ((ssTop())->constant)))) & 7) == 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
	 && (((((((ssValue(1))->constant))) & 7) == 1)))
	 || ((mclassIsSmallInteger())
	 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsIntConst
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		return genStaticallyResolvedSpecialSelectorComparison();
	}
	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor1 = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor1->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetPC = targetBytecodePC;

	/* Further, only interested in inlining = and ~= if there's a SmallInteger constant involved.
	   The relational operators successfully statically predict SmallIntegers; the equality operators do not. */
	inlineCAB = ((branchDescriptor->isBranchTrue))
	 || ((branchDescriptor->isBranchFalse));
	if (inlineCAB
	 && ((((primDescriptor->opcode)) == JumpZero)
	 || (((primDescriptor->opcode)) == JumpNonZero))) {
		inlineCAB = argIsIntConst
		 || (rcvrIsInt);
	}
	if (!inlineCAB) {
		return genSpecialSelectorSend();
	}
	if (argIsIntConst) {
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsIntConst))
		? (argIsIntConst
				? genJumpNotSmallInteger(ReceiverResultReg)
				: (rcvrIsInt
						? genJumpNotSmallInteger(Arg0Reg)
						: (/* begin genJumpNotSmallIntegersIn:and:scratch: */
							genoperandoperand(MoveRR, ReceiverResultReg, TempReg),
							/* begin AndR:R: */
							genoperandoperand(AndRR, Arg0Reg, TempReg),
							genJumpNotSmallInteger(TempReg))))
		: 0);
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(CmpCqR, argInt, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(argInt, BytesPerOop));
		}
	}
	else {
		/* begin CmpR:R: */
		assert(!((Arg0Reg == SPReg)));
		genoperandoperand(CmpRR, Arg0Reg, ReceiverResultReg);
	}
	genConditionalBranchoperand(((branchDescriptor->isBranchTrue)
		? (primDescriptor->opcode)
		: inverseBranchFor((primDescriptor->opcode))), ((usqInt)(ensureNonMergeFixupAt(targetPC))));
	/* begin Jump: */
	jumpTarget = ensureNonMergeFixupAt(postBranchPC);
	genoperand(Jump, ((sqInt)jumpTarget));
	if (!jumpNotSmallInts) {
		/* begin annotateInstructionForBytecode */
		if (prevInstIsPCAnnotated()) {
			/* begin Nop */
			abstractInstruction = gen(Nop);
		}
		else {
			/* begin Label */
			abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
		(abstractInstruction->annotation = HasBytecodePC);
		ensureFixupAt(postBranchPC);
		ensureFixupAt(targetPC);
		deadCode = 1;
		return 0;
	}
	jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (argIsIntConst) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, argInt, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(argInt, BytesPerOop));
		}
	}
	index = byte0 - ((bytecodeSetOffset == 0x100
		? AltFirstSpecialSelector + 0x100
		: FirstSpecialSelector));
	return genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
}


/*	Bulk comment, each sub method has its own comment
	3000	pointerAt:put:
	Mutable pointer object (Fixed sized or not) and not a context, Smi,
	Anything => arg2 (1-based, optimised if arg1 is a constant)
	3001	storeCheckPointerAt:put:
	Mutable pointer object (Fixed sized or not) and not a context, Smi,
	Anything => arg2 (1-based, optimised if arg1 is a constant)
	3002	maybeContextPointerAt:put:
	Mutable pointer object (Fixed sized or not), Smi, Anything => arg2
	(1-based, optimised if arg1 is a constant)
	3003	maybeContextStoreCheckPointerAt:put:
	Mutable pointer object (Fixed sized or not), Smi, Anything => arg2
	(1-based, optimised if arg1 is a constant)
	3004	byteAt:put:
	Mutable byte object, Smi, 8 bits unsigned Smi => arg2 (1-based, optimised
	if arg1 is a constant)
	3005	shortAt:put:
	Mutable short object, Smi, 16 bits unsigned Smi => arg2 (1-based,
	optimised if arg1 is a constant)
	3006	wordAt:put:
	Mutable word object, Smi, 32 bits unsigned Smi => arg2 (1-based, optimised
	if arg1 is a constant)
	3007	doubleWordAt:put:
	Mutable double word object, Smi, 64 bits unsigned Smi or
	LargePositiveInteger => arg2 (1-based, optimised if arg1 is a constant)
	3021 is deprecated.
 */

	/* SistaCogit>>#genTrinaryInlinePrimitive: */
static sqInt NoDbgRegParms
genTrinaryInlinePrimitive(sqInt prim)
{
	if (prim <= 7) {
		return genAtPutInlinePrimitive(prim);
	}
	if (prim == 21) {
		return genByteEqualsInlinePrimitive(prim);
	}
	return EncounteredUnknownBytecode;
}


/*	1000	rawClass
	not a forwarder => Behavior (Same as class special send, but receiver is
	not a forwarder)
 */
/*	Important performance note:
	In Scorch, typically a value is known as not being a forwarder if there is
	a trap.
	If the trap is due to a monomorphic send, the #class send leads to:
	trapIf: X notInstanceOf: C
	X rawClass
	therefore X rawClass is simplified in Scorch to the cst:C
	The rawClass is therefore used only for PICs. 
	trapIf: X notInstanceOf: C, C', C''
	X rawClass
	This unsafe operation is important to avoid register flush, but the
	performance 
	difference in differencing rawClass for immediate and rawClass for non
	immediate 
	classes is not that relevant */

	/* SistaCogit>>#genUnaryClassPrimitive */
static sqInt
genUnaryClassPrimitive(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    sqInt destReg;
    AbstractInstruction *jumpIsImm;
    sqInt literal;
    sqInt literal1;
    sqInt topReg;

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	destReg = allocateRegNotConflictingWith(((topReg < 0) ? (((usqInt)(1)) >> (-topReg)) : (1ULL << topReg)));
	popToReg(ssTop(), topReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, topReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = tagMask();
	anInstruction = genoperandoperand(AndCqR, tagMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpIsImm = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	flag("endianness");
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, 0, topReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin checkQuickConstant:forInstruction: */
	literal1 = classIndexMask();
	anInstruction2 = genoperandoperand(AndCqR, classIndexMask(), TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal1, BytesPerOop));
	}
	jmpTarget(jumpIsImm, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genGetClassObjectOfClassIndexintoscratchReg(TempReg, destReg, topReg);
	ssPop(1);
	return ssPushRegister(destReg);
}

	/* SistaCogit>>#genUnaryConvertInlinePrimitive: */
static sqInt NoDbgRegParms
genUnaryConvertInlinePrimitive(sqInt primIndex)
{
    sqInt resultReg;

	assert(((primIndex >= 30) && (primIndex <= 32)));
	switch (primIndex) {
	case 30:
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		popToReg(ssTop(), resultReg);
		genConvertCharacterToSmallIntegerInReg(resultReg);
		break;
	case 0x1F:
		return EncounteredUnknownBytecode;

	case 32:
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		popToReg(ssTop(), resultReg);
		assert(processorHasDoublePrecisionFloatingPointSupport());
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, resultReg, TempReg);
		genConvertSmallIntegerToIntegerInReg(TempReg);
		/* begin ConvertR:Rd: */
		genoperandoperand(ConvertRRd, TempReg, DPFPReg0);
		flag("TODO");
		genAllocFloatValueintoscratchRegscratchReg(DPFPReg0, resultReg, TempReg, NoReg);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	ssPop(1);
	return ssPushRegister(resultReg);
}

	/* SistaCogit>>#genUnaryHashInlinePrimitive: */
static sqInt NoDbgRegParms
genUnaryHashInlinePrimitive(sqInt primIndex)
{
    sqInt rcvrReg;
    sqInt resultReg;

	assert(((primIndex >= 20) && (primIndex <= 23)));
	switch (primIndex) {
	case 20:
		rcvrReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		resultReg = allocateRegNotConflictingWith(((rcvrReg < 0) ? (((usqInt)(1)) >> (-rcvrReg)) : (1ULL << rcvrReg)));
		popToReg(ssTop(), rcvrReg);
		genGetIdentityHashresultReg(rcvrReg, resultReg);
		break;
	case 21:
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		popToReg(ssTop(), resultReg);
		break;
	case 22:
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		popToReg(ssTop(), resultReg);
		genConvertCharacterToSmallIntegerInReg(resultReg);
		break;
	case 23:
		resultReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
		genConvertSmallFloatToSmallFloatHashAsIntegerInRegscratch(resultReg, TempReg);
		break;
	case 24:
		return EncounteredUnknownBytecode;

	default:
		error("Case not found and no otherwise clause");
	}
	ssPop(1);
	return ssPushRegister(resultReg);
}


/*	Bulk comments: each sub-method has its own comment with the specific case.
	1000	rawClass
	not a forwarder => Behavior (Same as class special send, but receiver is
	not a forwarder)
	1001	numSlots
	pointer object => Smi between 0 and SmallInteger maxVal // 4 - 1 (Answers
	total size in pointer-sized slots)
	1002	numBytes
	byte object => Smi between 0 and SmallInteger maxVal - 9 (Includes
	compiled code)
	1003	numShorts
	short object => Smi between 0 and SmallInteger maxVal - 9
	1004	numWords
	word object => Smi between 0 and SmallInteger maxVal - 9
	1005	numDoubleWords
	double word object => Smi between 0 and SmallInteger maxVal - 9
	1011	RawNew
	literal which is a fixed-sized behavior => instance of the receiver with
	fields nilled out
	1012	RawNewNoInit
	literal which is a fixed-sized behavior => instance of the receiver
	(Fields of returned value contain undefined data)
	1020	objectIdentityHash
	non-immediate and non-behavior => 22 bits strictly positive Smi
	1021	smiIdentityHash
	Smi => Smi
	1022	charIdentityHash
	Character => 22 bits strictly positive Smi
	1023	smallfloatIdentityHash
	SmallFloat => Smi
	1024	behaviorIdentityHash
	Behavior => 22 bits strictly positive Smi
	1030	characterAsInteger
	Character => 22 bits strictly positive Smi (Unicode)
	1031	smallFloatAsInteger
	SmallFloat => Smi
	1032	smiAsFloat
	Smi => SmallFloat
	1040	unforward
	Anything => Not a forwarder
	1041	possibleRoot
	non-immediate, not a forwarder => receiver is returned (should be
	effect-only) (If old, becomes gray and remembered to allow many unchecked
	stores in a row afterwards)
 */

	/* SistaCogit>>#genUnaryInlinePrimitive: */
static sqInt NoDbgRegParms
genUnaryInlinePrimitive(sqInt primIndex)
{
	if (primIndex == 0) {
		return genUnaryClassPrimitive();
	}
	if (primIndex <= 6) {
		return genUnarySizeInlinePrimitive(primIndex);
	}
	if (primIndex < 11) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 12) {
		return genUnaryNewInlinePrimitive(primIndex);
	}
	if (primIndex < 20) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 24) {
		return genUnaryHashInlinePrimitive(primIndex);
	}
	if (primIndex < 30) {
		return EncounteredUnknownBytecode;
	}
	if (primIndex <= 32) {
		return genUnaryConvertInlinePrimitive(primIndex);
	}
	if (primIndex == 39) {
		return genUnaryUnforwardNonImmediateInlinePrimitive();
	}
	if (primIndex == 40) {
		return genUnaryUnforwardInlinePrimitive();
	}
	if (primIndex == 41) {
		return genUnaryPossibleRootInlinePrimitive();
	}
	return EncounteredUnknownBytecode;
}


/*	1011	RawNew
	literal which is a fixed-sized behavior => instance of the receiver with
	fields nilled out
	1012	RawNewNoInit
	literal which is a fixed-sized behavior => instance of the receiver
	(Fields of returned value contain undefined data)
 */

	/* SistaCogit>>#genUnaryNewInlinePrimitive: */
static sqInt NoDbgRegParms
genUnaryNewInlinePrimitive(sqInt primIndex)
{
    sqInt classObj;
    sqInt resultReg;

	assert((((ssTop())->type)) == SSConstant);
	classObj = ((ssTop())->constant);
	assert(isNonImmediate(classObj));
	assert(objCouldBeClassObj(classObj));
	assert(isFixedSizePointerFormat(instSpecOfClassFormat(formatOfClass(classObj))));
	classTagForClass(classObj);
	resultReg = allocateRegNotConflictingWith(0);
	genGetInstanceOfPointerClassintoinitializingIfnumVariableSlots(classObj, resultReg, primIndex == 11, 0);
	ssPop(1);
	return ssPushRegister(resultReg);
}


/*	1041	possibleRoot
	non-immediate, not a forwarder => receiver is returned (should be
	effect-only) (If old, becomes gray and remembered to allow many unchecked
	stores in a row afterwards)
 */

	/* SistaCogit>>#genUnaryPossibleRootInlinePrimitive */
static sqInt
genUnaryPossibleRootInlinePrimitive(void)
{
    AbstractInstruction *jmpAlreadyRemembered;
    AbstractInstruction *jmpDestYoung;
    sqInt topReg;
    sqInt wordConstant;

	jmpAlreadyRemembered = ((AbstractInstruction *) 0);
	topReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), topReg);
	/* begin MoveCw:R: */
	wordConstant = storeCheckBoundary();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(wordConstant, genoperandoperand(MoveCwR, wordConstant, TempReg));
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, topReg);
	/* begin JumpBelow: */
	jmpDestYoung = genConditionalBranchoperand(JumpBelow, ((sqInt)0));
	if (!CheckRememberedInTrampoline) {
		jmpAlreadyRemembered = genCheckRememberedBitOfscratch(topReg, TempReg);
	}
	callStoreCheckTrampoline();
	jmpTarget(jmpDestYoung, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	if (!CheckRememberedInTrampoline) {
		jmpTarget(jmpAlreadyRemembered, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	ssPop(1);
	return ssPushRegister(topReg);
}

	/* SistaCogit>>#genUnarySizeInlinePrimitive: */
static sqInt NoDbgRegParms
genUnarySizeInlinePrimitive(sqInt primIndex)
{
    sqInt rcvrReg;
    sqInt resultReg;

	assert(((primIndex >= 1) && (primIndex <= 6)));
	rcvrReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	resultReg = allocateRegNotConflictingWith(((rcvrReg < 0) ? (((usqInt)(1)) >> (-rcvrReg)) : (1ULL << rcvrReg)));
	popToReg(ssTop(), rcvrReg);
	ssPop(1);
	ssPushRegister(resultReg);
	switch (primIndex) {
	case 1:
		genGetNumSlotsOfinto(rcvrReg, resultReg);
		genConvertIntegerToSmallIntegerInReg(resultReg);
		break;
	case 2:
		genGetNumBytesOfinto(rcvrReg, resultReg);
		genConvertIntegerToSmallIntegerInReg(resultReg);
		break;
	case 3:
	case 5:
	case 6:
		return EncounteredUnknownBytecode;

	default:
		error("Case not found and no otherwise clause");
	}
	return 0;
}


/*	1040	unforward
	Anything => Not a forwarder */

	/* SistaCogit>>#genUnaryUnforwardInlinePrimitive */
static sqInt
genUnaryUnforwardInlinePrimitive(void)
{
    AbstractInstruction *instruction;
    sqInt topReg;

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), topReg);
	/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
	instruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(topReg, TempReg, instruction, 0);
	ssPop(1);
	return ssPushRegister(topReg);
}


/*	1039	unforwardNonImmediate
	non immediate => Not a forwarder */
/*	unforwardNonImmediate was used exclusively for literal variables,
	which are now never forwarders because we scan the zone in postBecome,
	so this is effectively a noop. */

	/* SistaCogit>>#genUnaryUnforwardNonImmediateInlinePrimitive */
static sqInt
genUnaryUnforwardNonImmediateInlinePrimitive(void)
{
    sqInt topReg;

	topReg = allocateRegForStackEntryAtnotConflictingWith(0, 0);
	popToReg(ssTop(), topReg);
	ssPop(1);
	return ssPushRegister(topReg);
}


/*	SistaV1: *	217		Trap */

	/* SistaCogit>>#genUnconditionalTrapBytecode */
static sqInt
genUnconditionalTrapBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt i;
    sqInt index;

	/* begin ssFlushTo: */
	index = simStackPtr;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceTrapTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin annotateBytecode: */
	abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction1->annotation = HasBytecodePC);
	deadCode = 1;
	return 0;
}

	/* SistaCogit>>#getJumpTargetPCAt: */
usqInt
getJumpTargetPCAt(sqInt pc)
{
	return jumpTargetPCAt(backEnd, pc);
}

	/* SistaCogit>>#initializeCodeZoneFrom:upTo: */
void
initializeCodeZoneFromupTo(sqInt startAddress, sqInt endAddress)
{
    sqInt fixupSize;
    sqInt fixupSize1;
    sqInt numberOfAbstractOpcodes;
    sqInt numberOfAbstractOpcodes1;
    sqInt opcodeSize;
    sqInt opcodeSize1;
    usqInt startAddress1;

	initialCounterValue = MaxCounterValue;
	initializeBackend();
	sqMakeMemoryExecutableFromToCodeToDataDelta(startAddress, endAddress, 
#  if DUAL_MAPPED_CODE_ZONE
		(&codeToDataDelta)
#  else
		null
#  endif
		);
	codeBase = (methodZoneBase = startAddress);
	stopsFromto(backEnd, startAddress, endAddress - 1);
	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	assertValidDualZone();
	/* begin detectFeatures */
#  if __APPLE__
	/* begin detectFeaturesOnMacOS */
	
#  else
#  if __linux__
	detectFeaturesOnLinux(((AbstractInstruction *) backEnd));
#  else
	detectFeaturesOnRawMachine(((AbstractInstruction *) backEnd));
#  endif
#  endif // __APPLE__
	/* begin maybeGenerateCacheFlush */
	if ((numICacheFlushOpcodes(backEnd)) > 0) {
		/* begin allocateOpcodes:bytecodes: */
		numberOfAbstractOpcodes = numICacheFlushOpcodes(backEnd);
		numAbstractOpcodes = numberOfAbstractOpcodes;
		opcodeSize = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
		fixupSize = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
		abstractOpcodes = alloca(opcodeSize + fixupSize);
		bzero(abstractOpcodes, opcodeSize + fixupSize);
		fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize));
		zeroOpcodeIndexForNewOpcodes();
		labelCounter = 0;
		startAddress1 = methodZoneBase;
		generateICacheFlush(backEnd);
		outputInstructionsForGeneratedRuntimeAt(startAddress1);
		recordGeneratedRunTimeaddress("ceFlushICache", startAddress1);
		ceFlushICache = ((void (*)(usqIntptr_t,usqIntptr_t)) startAddress1);
		/* begin initialFlushICacheFrom:to: */
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) startAddress1), (methodZoneBase - startAddress1) + 1);
		sys_icache_invalidate(((void *) startAddress1), (methodZoneBase - startAddress1) + 1);
#    elif __GNUC__
		__clear_cache(((char *) startAddress1), ((char *) methodZoneBase));
#    else
		error("cache flushing method unknown for this platform");
#    endif
	}
	if ((numDCacheFlushOpcodes(backEnd)) > 0) {
		/* begin allocateOpcodes:bytecodes: */
		numberOfAbstractOpcodes1 = numDCacheFlushOpcodes(backEnd);
		numAbstractOpcodes = numberOfAbstractOpcodes1;
		opcodeSize1 = (sizeof(CogAbstractInstruction)) * numAbstractOpcodes;
		fixupSize1 = (sizeof(CogBytecodeFixup)) * numAbstractOpcodes;
		abstractOpcodes = alloca(opcodeSize1 + fixupSize1);
		bzero(abstractOpcodes, opcodeSize1 + fixupSize1);
		fixups = ((void *)((((usqInt)abstractOpcodes)) + opcodeSize1));
		zeroOpcodeIndexForNewOpcodes();
		labelCounter = 0;
		startAddress1 = methodZoneBase;
		generateDCacheFlush(backEnd);
		outputInstructionsForGeneratedRuntimeAt(startAddress1);
		recordGeneratedRunTimeaddress("ceFlushDCache", startAddress1);
		ceFlushDCache = ((void (*)(usqIntptr_t,usqIntptr_t)) startAddress1);
		/* begin initialFlushICacheFrom:to: */
#    if __APPLE__ && __MACH__
		sys_dcache_flush(((void *) startAddress1), (methodZoneBase - startAddress1) + 1);
		sys_icache_invalidate(((void *) startAddress1), (methodZoneBase - startAddress1) + 1);
#    elif __GNUC__
		__clear_cache(((char *) startAddress1), ((char *) methodZoneBase));
#    else
		error("cache flushing method unknown for this platform");
#    endif
	}
	genGetLeafCallStackPointers();
	generateStackPointerCapture();
	generateTrampolines();
	computeEntryOffsets();
	computeFullBlockEntryOffsets();
	generateClosedPICPrototype();
	alignMethodZoneBase();
	/* begin flushICacheFrom:to: */
	
#  if !DUAL_MAPPED_CODE_ZONE
	/* begin makeCodeZoneExecutable */
#  if __APPLE__ && __MACH__
	pthread_jit_write_protect_np(1);
	PJWPNSet = __LINE__;
	if (!PJWPNState) {
		PJWPNChange = __LINE__;
		PJWPNState = 1;
	}
#  endif // __APPLE__ && __MACH__
#  endif // !DUAL_MAPPED_CODE_ZONE
#  if __APPLE__ && __MACH__
	sys_dcache_flush(((void *) startAddress), (((usqInt)methodZoneBase)) - startAddress);
	sys_icache_invalidate(((void *) startAddress), (((usqInt)methodZoneBase)) - startAddress);
#  else // __APPLE__ && __MACH__
	ceFlushICache(startAddress, ((usqInt)methodZoneBase));
#  endif
	/* begin maybeFlushWritableZoneFrom:to: */
#  if DUAL_MAPPED_CODE_ZONE
	if (codeToDataDelta > 0) {
		flushDCacheFromto(backEnd, startAddress, ((usqInt)methodZoneBase));
	}
#  endif
	/* begin manageFrom:to: */
	mzFreeStart = (baseAddress = methodZoneBase);
	youngReferrers = (limitAddress = endAddress);
	openPICList = null;
	methodBytesFreedSinceLastCompaction = 0;
	methodCount = 0;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	generateOpenPICPrototype();
}

	/* SistaCogit>>#maybeAllocAndInitCounters */
static sqInt
maybeAllocAndInitCounters(void)
{
    sqInt objOop;

	assert(counters == 0);
	counterIndex = 0;
	if (numCounters == 0) {
		return 1;
	}
	/* begin allocateCounters: */
	objOop = allocatePinnedSlots((numCounters + 1) / 2);
	counters = (objOop == null
		? 0
		: objOop + BaseHeaderSize);
	return counters != 0;
}


/*	Mapped: 250	backjumpAlwaysInterrupt
	Unmapped: 6000	backjumpNoInterrupt 
	7016-7020 jumpWritable/Young
	8000-8003 type branches
	In all cases the distance is an integer pushed on stack just before with
	pushIntegerLong:  */

	/* SistaCogit>>#maybeDealWithUnsafeJumpForDescriptor:pc:latestContinuation: */
static sqInt NoDbgRegParms
maybeDealWithUnsafeJumpForDescriptorpclatestContinuation(BytecodeDescriptor *descriptor, sqInt pc, sqInt latestContinuation)
{
    sqInt byte01;
    sqInt byte02;
    sqInt distance;
    sqInt distance1;
    BytecodeFixup *fixup;
    BytecodeFixup *fixup1;
    sqInt newContinuation;
    sqInt prim;
    sqInt targetPC;
    sqInt upperByte;

	newContinuation = latestContinuation;
	if ((descriptor->hasUnsafeJump)) {
		byte01 = fetchByteofObject(pc + 1, methodObj);
		byte02 = fetchByteofObject(pc + 2, methodObj);
		/* begin decodePushIntegerLongBefore:in: */
		distance1 = fetchByteofObject(pc - 1, methodObj);
		upperByte = fetchByteofObject(pc - 3, methodObj);
		if (upperByte > 0x7F) {
			upperByte -= 0x100;
		}
		distance = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
		targetPC = (pc + ((descriptor->numBytes))) + distance;
		if ((descriptor->isMapped)) {
			if (byte01 == 250) {

				/* mapped always interrupt backjump */
				/* begin initializeFixupAt: */
				fixup = fixupAtIndex(targetPC - initialPC);
				/* begin initializeFixup: */
				(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
				/* begin setIsBackwardBranchFixup */
				(fixup->isTargetOfBackwardBranch) = 1;
			}
		}
		else {
			if ((((usqInt)(byte02)) >> 5) == 4) {

				/* inlined sista primitive */
				prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
				if (prim >= 7000) {

					/* branch forward */
					newContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
				}
				else {
					if (prim >= 6000) {

						/* no interrupt back jump */
						/* begin initializeFixupAt: */
						fixup1 = fixupAtIndex(targetPC - initialPC);
						/* begin initializeFixup: */
						(fixup1->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
						/* begin setIsBackwardBranchFixup */
						(fixup1->isTargetOfBackwardBranch) = 1;
					}
				}
			}
		}
	}
	return newContinuation;
}


/*	Collect the branch and send data for the block method starting at
	blockEntryMcpc, storing it into picData.
 */

	/* SistaCogit>>#picDataForBlockEntry:Method: */
static usqInt NoDbgRegParms
picDataForBlockEntryMethod(sqInt blockEntryMcpc, sqInt cogMethod)
{
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    CogBlockMethod *cogBlockMethod;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt prim;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	cogBlockMethod = ((CogBlockMethod *) (blockEntryMcpc - (sizeof(CogBlockMethod))));
	if (((cogBlockMethod->stackCheckOffset)) == 0) {
		return 0;
	}
	/* begin mapFor:bcpc:performUntil:arg: */
	startbcpc = (cogBlockMethod->startpc);
	assert(((cogBlockMethod->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)cogBlockMethod)) + ((cogBlockMethod->stackCheckOffset));
	result = picDataForAnnotationMcpcBcpcMethod(null, (0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))), (((char *) mcpc)), startbcpc, (((void *)cogMethod)));
	if (result != 0) {
		return result;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if (((cogBlockMethod->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = (cogBlockMethod->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) cogBlockMethod);
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == ((cogBlockMethod->startpc)));
		homeMethod = cmHomeMethod(cogBlockMethod);
		map = findMapLocationForMcpcinMethod((((usqInt)cogBlockMethod)) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj))
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							return 0;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							return 0;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc - 1, methodObj);
							upperByte = fetchByteofObject(bcpc - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = picDataForAnnotationMcpcBcpcMethod(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc)), ((isBackwardBranch
	? bcpc - (2 * nExts)
	: bcpc)), (((void *)cogMethod)));
				if (result != 0) {
					return result;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	return 0;
}


/*	N.B. Counters are always 32-bits, having two 16-bit halves for the reached
	and taken counts.
 */

	/* SistaCogit>>#picDataForCounter:at: */
static sqInt NoDbgRegParms
picDataForCounterat(unsigned int counter, sqInt bcpc)
{
    sqInt executedCount;
    sqInt tuple;
    sqInt untakenCount;

	tuple = eeInstantiateClassIndexformatnumSlots(ClassArrayCompactIndex, arrayFormat(), 3);
	if (tuple == 0) {
		return 0;
	}
	assert(CounterBytes == 4);
	executedCount = initialCounterValue - (((usqInt)(counter)) >> 16);
	untakenCount = initialCounterValue - (counter & 0xFFFF);
	storePointerUncheckedofObjectwithValue(0, tuple, (((usqInt)bcpc << 3) | 1));
	storePointerUncheckedofObjectwithValue(1, tuple, (((usqInt)executedCount << 3) | 1));
	storePointerUncheckedofObjectwithValue(2, tuple, (((usqInt)untakenCount << 3) | 1));
	return tuple;
}


/*	Answer a tuple with the send data for a linked send to cogMethod.
	If the target is a CogMethod (monomorphic send) answer
	{ bytecode pc, inline cache class, target method }
	If the target is an open PIC (megamorphic send) answer
	{ bytecode pc, nil, send selector }
	If the target is a closed PIC (polymorphic send) answer
	{ bytecode pc, first class, target method, second class, second target
	method, ... } */

	/* SistaCogit>>#picDataForSendTo:methodClassIfSuper:at:bcpc: */
static sqInt NoDbgRegParms
picDataForSendTomethodClassIfSuperatbcpc(CogMethod *cogMethod, sqInt methodClassOrNil, char *sendMcpc, sqInt sendBcpc)
{
    sqInt class;
    sqInt tuple;

	tuple = eeInstantiateClassIndexformatnumSlots(ClassArrayCompactIndex, arrayFormat(), (((cogMethod->cmType)) == CMClosedPIC
		? (2 * ((cogMethod->cPICNumCases))) + 1
		: 3));
	if (tuple == 0) {
		return 0;
	}
	storePointerUncheckedofObjectwithValue(0, tuple, (((usqInt)sendBcpc << 3) | 1));
	if (((cogMethod->cmType)) >= CMMethod) {
		class = (!(methodClassOrNil)
			? classForInlineCacheTag(instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)((((usqInt)sendMcpc)) - 8)))))
			: methodClassOrNil);
		storePointerofObjectwithValue(1, tuple, class);
		storePointerofObjectwithValue(2, tuple, (cogMethod->methodObject));
		return tuple;
	}
	if (((cogMethod->cmType)) == CMClosedPIC) {
		populatewithPICInfoForfirstCacheTag(tuple, cogMethod, instructionAt(backEnd, pcRelativeAddressAt(backEnd, ((usqInt)((((usqInt)sendMcpc)) - 8)))));
		return tuple;
	}
	if (((cogMethod->cmType)) == CMOpenPIC) {
		storePointerUncheckedofObjectwithValue(1, tuple, nilObject());
		storePointerofObjectwithValue(2, tuple, (cogMethod->selector));
		return tuple;
	}
	error("invalid method type");
	return 0;
}


/*	N.B. Counters are always 32-bits, having two 16-bit halves for the reached
	and taken counts.
 */

	/* SistaCogit>>#picDataFor:Annotation:Mcpc:Bcpc:Method: */
static sqInt NoDbgRegParms
picDataForAnnotationMcpcBcpcMethod(BytecodeDescriptor *descriptor, sqInt isBackwardBranchAndAnnotation, char *mcpc, sqInt bcpc, void *cogMethodArg)
{
    sqInt annotation;
    sqInt association;
    unsigned int counter;
    sqInt entryPoint;
    sqInt methodClassIfSuper;
    sqInt offset1;
    sqInt *sendTable1;
    CogMethod *targetMethod1;
    sqInt tuple;

	if (!descriptor) {
		return 0;
	}
	if (isBranch(descriptor)) {

		/* it's a branch; conditional? */
		if (((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse))) {
			counter = (((usqInt *) (((((CogMethod *) cogMethodArg))->counters))))[counterIndex];
			tuple = picDataForCounterat(counter, bcpc + 1);
			if (tuple == 0) {
				return PrimErrNoMemory;
			}
			storePointerofObjectwithValue(introspectionDataIndex, introspectionData, tuple);
			introspectionDataIndex += 1;
			counterIndex += 1;
		}
		return 0;
	}
	annotation = ((usqInt)(isBackwardBranchAndAnnotation)) >> 1;
	if (!((annotation >= IsSendCall)
		 && (((entryPoint = callTargetFromReturnAddress(backEnd, ((usqInt)mcpc))),
		entryPoint > methodZoneBase)))) {

		/* send is not linked, or is not a send */
		return 0;
	}
	/* begin targetMethodAndSendTableFor:annotation:into: */
	if (annotation == IsSendCall) {
		offset1 = cmEntryOffset;
		sendTable1 = ordinarySendTrampolines;
	}
	else {
		if (annotation == IsDirectedSuperSend) {
			offset1 = cmNoCheckEntryOffset;
			sendTable1 = directedSuperSendTrampolines;
		}
		else {
			if (annotation == IsDirectedSuperBindingSend) {
				offset1 = cmNoCheckEntryOffset;
				sendTable1 = directedSuperBindingSendTrampolines;
			}
			else {
				assert(annotation == IsSuperSend);
				offset1 = cmNoCheckEntryOffset;
				sendTable1 = superSendTrampolines;
			}
		}
	}
	targetMethod1 = ((CogMethod *) (entryPoint - offset1));
	
	/* It's a linked send; find which kind. */
	methodClassIfSuper = null;
	if (sendTable1 == superSendTrampolines) {
		methodClassIfSuper = methodClassOf(((((CogMethod *) cogMethodArg))->methodObject));
	}
	if (sendTable1 == directedSuperSendTrampolines) {
		association = literalBeforeInlineCacheTagAt(backEnd, ((usqInt)mcpc));
		methodClassIfSuper = valueOfAssociation(association);
	}
	tuple = picDataForSendTomethodClassIfSuperatbcpc(targetMethod1, methodClassIfSuper, mcpc, bcpc + 1);
	if (tuple == 0) {
		return PrimErrNoMemory;
	}
	storePointerofObjectwithValue(introspectionDataIndex, introspectionData, tuple);
	introspectionDataIndex += 1;
	return 0;
}


/*	Collect the branch and send data for cogMethod, storing it into arrayObj. */

	/* SistaCogit>>#picDataFor:into: */
sqInt
picDataForinto(CogMethod *cogMethod, sqInt arrayObj)
{
    sqInt aMethodHeader;
    sqInt aMethodHeader1;
    sqInt aMethodObj;
    sqInt annotation;
    sqInt bcpc;
    sqInt bsOffset;
    sqInt byte;
    sqInt byte01;
    sqInt byte02;
    BytecodeDescriptor *descriptor;
    sqInt distance;
    sqInt distance1;
    sqInt distance2;
    sqInt endbcpc;
    sqInt errCode;
    CogMethod *homeMethod;
    sqInt isBackwardBranch;
    sqInt isInBlock;
    sqInt latestContinuation;
    usqInt map;
    sqInt mapByte;
    usqInt mcpc;
    sqInt nExts;
    sqInt newContinuation;
    sqInt nextBcpc;
    sqInt prim;
    sqInt result;
    sqInt startbcpc;
    sqInt targetPC;
    sqInt targetPC1;
    sqInt upperByte;

	latestContinuation = 0;
	if (((cogMethod->stackCheckOffset)) == 0) {
		return 0;
	}
	introspectionDataIndex = (counterIndex = 0);
	introspectionData = arrayObj;
	/* begin mapFor:bcpc:performUntil:arg: */
	startbcpc = startPCOfMethodHeader((cogMethod->methodHeader));
	assert((((((CogBlockMethod *) cogMethod))->stackCheckOffset)) > 0);

	/* The stack check maps to the start of the first bytecode,
	   the first bytecode being effectively after frame build. */
	mcpc = (((usqInt)(((CogBlockMethod *) cogMethod)))) + (((((CogBlockMethod *) cogMethod))->stackCheckOffset));
	result = picDataForAnnotationMcpcBcpcMethod(null, (0 + (((sqInt)((usqInt)(HasBytecodePC) << 1)))), (((char *) mcpc)), startbcpc, (((void *)cogMethod)));
	if (result != 0) {
		errCode = result;
		goto l9;
	}

	/* In both CMMethod and CMBlock cases find the start of the map and
	   skip forward to the bytecode pc map entry for the stack check. */
	bcpc = startbcpc;
	if ((((((CogBlockMethod *) cogMethod))->cmType)) >= CMMethod) {
		/* begin cmIsFullBlock */
		isInBlock = ((((CogBlockMethod *) cogMethod))->cpicHasMNUCaseOrCMIsFullBlock);
		homeMethod = ((CogMethod *) (((CogBlockMethod *) cogMethod)));
		assert(startbcpc == (startPCOfMethodHeader((homeMethod->methodHeader))));
		map = ((((usqInt)homeMethod)) + ((homeMethod->blockSize))) - 1;
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert((annotation == IsAbsPCReference)
		 || ((annotation == IsObjectReference)
		 || ((annotation == IsRelativeCall)
		 || (annotation == IsDisplacementX2N))));
		latestContinuation = startbcpc;
		aMethodObj = (homeMethod->methodObject);
		endbcpc = (numBytesOf(aMethodObj)) - 1;
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader)
						? 0x100
						: 0);
		bcpc += deltaToSkipPrimAndErrorStoreInheader(aMethodObj, (homeMethod->methodHeader));
	}
	else {
		isInBlock = 1;
		assert(bcpc == (((((CogBlockMethod *) cogMethod))->startpc)));
		homeMethod = cmHomeMethod(((CogBlockMethod *) cogMethod));
		map = findMapLocationForMcpcinMethod((((usqInt)(((CogBlockMethod *) cogMethod)))) + (sizeof(CogBlockMethod)), homeMethod);
		assert(map != 0);
		annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift;
		assert(((((usqInt)(annotation)) >> AnnotationShift) == HasBytecodePC)
		 || ((((usqInt)(annotation)) >> AnnotationShift) == IsDisplacementX2N));
		while (((annotation = ((usqInt)((byteAt(map)))) >> AnnotationShift)) != HasBytecodePC) {
			map -= 1;
		}

		/* skip fiducial; i.e. the map entry for the pc immediately following the method header. */
		map -= 1;
		aMethodObj = (homeMethod->methodObject);
		bcpc = startbcpc - ((headerIndicatesAlternateBytecodeSet((homeMethod->methodHeader))
		? AltBlockCreationBytecodeSize
		: BlockCreationBytecodeSize));
		/* begin bytecodeSetOffsetForHeader: */
		aMethodHeader1 = (homeMethod->methodHeader);
		bsOffset = (headerIndicatesAlternateBytecodeSet(aMethodHeader1)
						? 0x100
						: 0);
		byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
		descriptor = generatorAt(byte);
		endbcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, -1, aMethodObj))
	: 0));
		bcpc = startbcpc;
	}
	nExts = 0;
	enumeratingCogMethod = homeMethod;
	while ((((usqInt)((byteAt(map)))) >> AnnotationShift) != HasBytecodePC) {
		map -= 1;
	}
	map -= 1;
	while (((mapByte = byteAt(map))) != MapEnd) {

		/* defensive; we exit on bcpc */
		if (mapByte >= FirstAnnotation) {
			annotation = ((usqInt)(mapByte)) >> AnnotationShift;
			mcpc += (mapByte & DisplacementMask) * 4 /* codeGranularity */;
			if (annotation >= HasBytecodePC) {
				if ((annotation == IsSendCall)
				 && ((((usqInt)(((mapByte = byteAt(map - 1))))) >> AnnotationShift) == IsAnnotationExtension)) {
					annotation += mapByte & DisplacementMask;
					map -= 1;
				}
				while (1) {
					byte = (fetchByteofObject(bcpc, aMethodObj)) + bsOffset;
					descriptor = generatorAt(byte);
					if (isInBlock) {
						if (bcpc >= endbcpc) {
							errCode = 0;
							goto l9;
						}
					}
					else {
						if (((descriptor->isReturn))
						 && (bcpc >= latestContinuation)) {
							errCode = 0;
							goto l9;
						}
						if ((isBranch(descriptor))
						 || ((descriptor->isBlockCreation))) {
							/* begin latestContinuationPCFor:at:exts:in: */
							distance = ((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj);
							targetPC = (bcpc + ((descriptor->numBytes))) + (((distance < 0) ? 0 : distance));
							latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
						}
						/* begin maybeUnsafeJumpContinuation:at:for:in: */
						newContinuation = latestContinuation;
						if ((descriptor->hasUnsafeJump)) {
							byte01 = fetchByteofObject(bcpc + 1, aMethodObj);

							/* pushIntegerLong */
							byte02 = fetchByteofObject(bcpc + 2, aMethodObj);
							/* begin decodePushIntegerLongBefore:in: */
							distance1 = fetchByteofObject(bcpc - 1, methodObj);
							upperByte = fetchByteofObject(bcpc - 3, methodObj);
							if (upperByte > 0x7F) {
								upperByte -= 0x100;
							}
							distance2 = (((sqInt)((usqInt)(upperByte) << 8))) + distance1;
							targetPC1 = (bcpc + ((descriptor->numBytes))) + distance2;
							if (!((descriptor->isMapped))) {
								if ((((usqInt)(byte02)) >> 5) == 4) {

									/* inlined sista primitive */
									prim = (((sqInt)((usqInt)((byte02 & 0x1F)) << 8))) + byte01;
									if (prim >= 7000) {

										/* branch forward */
										newContinuation = ((latestContinuation < targetPC1) ? targetPC1 : latestContinuation);
									}
								}
							}
						}
						latestContinuation = newContinuation;
					}
					nextBcpc = (bcpc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? (/* begin spanFor:at:exts:in: */
		((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj))
	: 0));
					if (((descriptor->isMapped))
					 || (isInBlock
					 && ((descriptor->isMappedInBlock)))) break;
					bcpc = nextBcpc;
					nExts = ((descriptor->isExtension)
						? nExts + 1
						: 0);
				}
				isBackwardBranch = (isBranch(descriptor))
				 && ((assert(((descriptor->spanFunction)) != null),
				(((descriptor->spanFunction))(descriptor, bcpc, nExts, aMethodObj)) < 0));
				result = picDataForAnnotationMcpcBcpcMethod(descriptor, ((isBackwardBranch
	? (((sqInt)((usqInt)(annotation) << 1))) + 1
	: ((sqInt)((usqInt)(annotation) << 1)))), (((char *) mcpc)), ((isBackwardBranch
	? bcpc - (2 * nExts)
	: bcpc)), (((void *)cogMethod)));
				if (result != 0) {
					errCode = result;
					goto l9;
				}
				bcpc = nextBcpc;
				nExts = ((descriptor->isExtension)
					? nExts + 1
					: 0);
			}
		}
		else {
			assert(((((usqInt)(mapByte)) >> AnnotationShift) == IsDisplacementX2N)
			 || ((((usqInt)(mapByte)) >> AnnotationShift) == IsAnnotationExtension));
			if (mapByte < (((sqInt)((usqInt)(IsAnnotationExtension) << AnnotationShift)))) {
				mcpc += (((sqInt)((usqInt)((mapByte - DisplacementX2N)) << AnnotationShift))) * 4 /* codeGranularity */;
			}
		}
		map -= 1;
	}
	errCode = 0;
	l9:	/* end mapFor:bcpc:performUntil:arg: */;
	if (errCode != 0) {
		assert(errCode == PrimErrNoMemory);
		return -1;
	}
	if (((cogMethod->blockEntryOffset)) != 0) {
		errCode = blockDispatchTargetsForperformarg(cogMethod, picDataForBlockEntryMethod, ((sqInt)cogMethod));
		if (errCode != 0) {
			assert(errCode == PrimErrNoMemory);
			return -1;
		}
	}
	return introspectionDataIndex;
}


/*	Populate tuple (which must be large enough) with the ClosedPIC's target
	method class pairs.
	The first entry in tuple contains the bytecode pc for the send, so skip
	the tuple's first field.
 */

	/* SistaCogit>>#populate:withPICInfoFor:firstCacheTag: */
static void NoDbgRegParms
populatewithPICInfoForfirstCacheTag(sqInt tuple, CogMethod *cPIC, sqInt firstCacheTag)
{
    sqInt cacheTag;
    sqInt classOop;
    sqInt entryPoint;
    sqInt i;
    sqInt pc;
    CogMethod *targetMethod;
    sqInt value;

	for (i = 1; i <= ((cPIC->cPICNumCases)); i += 1) {
		pc = addressOfEndOfCaseinCPIC(i, cPIC);
		cacheTag = (i == 1
			? firstCacheTag
			: literalBeforeFollowingAddress(backEnd, pc - (jumpLongConditionalByteSize(backEnd))));
		classOop = classForInlineCacheTag(cacheTag);
		storePointerofObjectwithValue((i * 2) - 1, tuple, classOop);
		if (i == 1) {
			entryPoint = jumpLongTargetBeforeFollowingAddress(backEnd, pc);
		}
		else {
			/* begin jumpLongConditionalTargetBeforeFollowingAddress: */
			entryPoint = jumpLongTargetBeforeFollowingAddress(((AbstractInstruction *) backEnd), pc);
		}
		if (((((usqInt)cPIC)) <= (((usqInt)entryPoint)))
		 && (((((usqInt)cPIC)) + ((cPIC->blockSize))) >= (((usqInt)entryPoint)))) {
			value = splObj(SelectorDoesNotUnderstand);
		}
		else {
			targetMethod = ((CogMethod *) (entryPoint - cmNoCheckEntryOffset));
			assert(isCMMethodEtAl(((CogBlockMethod *) targetMethod)));
			value = (targetMethod->methodObject);
		}
		storePointerofObjectwithValue(i * 2, tuple, value);
	}
}

	/* SistaMethodZone>>#getCogCodeZoneThreshold */
double
getCogCodeZoneThreshold(void)
{
	return thresholdRatio;
}

	/* SistaMethodZone>>#setCogCodeZoneThreshold: */
sqInt
setCogCodeZoneThreshold(double ratio)
{
	if (!((ratio >= 0.1)
		 && (ratio <= 1.0))) {
		return PrimErrBadArgument;
	}
	thresholdRatio = ratio;
	/* begin computeAllocationThreshold */
	allocationThreshold = ((((((usqInt)((limitAddress - baseAddress) * thresholdRatio))) + ((zoneAlignment()) - 1)) & ~7)) + baseAddress;
	return 0;
}


/*	Add a blockStart for an embedded block. For a binary tree walk block
	dispatch blocks must be compiled in pc/depth-first order but are scanned
	in breadth-first
	order, so do an insertion sort (which of course is really a bubble sort
	because we
	have to move everything higher to make room). */

	/* StackToRegisterMappingCogit>>#addBlockStartAt:numArgs:numCopied:span: */
static BlockStart * NoDbgRegParms
addBlockStartAtnumArgsnumCopiedspan(sqInt bytecodepc, sqInt numArgs, sqInt numCopied, sqInt span)
{
    BlockStart *blockStart;
    sqInt i;
    sqInt j;


	/* Transcript ensureCr; nextPutAll: 'addBlockStartAt: '; print: bytecodepc; cr; flush. */
	if (blockCount > 0) {
		i = blockCount - 1;
		while (1) {

			/* check for repeat addition during recompilation due to initialNil miscount. */
			blockStart = (&(blockStarts[i]));
			if (((blockStart->startpc)) == bytecodepc) {
				return blockStart;
			}
			if (!((((blockStart->startpc)) > bytecodepc)
			 && (i > 0))) break;
			i -= 1;
		}
		for (j = blockCount; j >= (i + 1); j += -1) {
			blockStarts[j] = (blockStarts[j - 1]);
		}
		blockStart = (&(blockStarts[i + 1]));
	}
	else {
		blockStart = (&(blockStarts[blockCount]));
	}
	blockCount += 1;
	(blockStart->startpc = bytecodepc);
	(blockStart->numArgs = numArgs);
	(blockStart->numCopied = numCopied);
	(blockStart->numInitialNils = 0);
	(blockStart->stackCheckLabel = null);
	(blockStart->hasInstVarRef = 0);
	(blockStart->span = span);
	return blockStart;
}


/*	e.g.	Receiver				Receiver	or	Receiver				Receiver	(RISC)
	Selector/Arg0	=>		Arg1			Selector/Arg0	=>		Arg1
	Arg1					Arg2			Arg1					Arg2
	Arg2					Arg3			Arg2			sp->	Arg3
	Arg3			sp->	retpc	sp->	Arg3
	sp->	retpc */
/*	Generate code to adjust the possibly stacked arguments immediately
	before jumping to a method looked up by a perform primitive. */

	/* StackToRegisterMappingCogit>>#adjustArgumentsForPerform: */
static void NoDbgRegParms
adjustArgumentsForPerform(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction10;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction12;
    AbstractInstruction *anInstruction3;
    sqInt index;

	assert((numRegArgs()) <= 2);
	assert(numArgs >= 1);
	if (numArgs <= (numRegArgs())) {
		if (numArgs == 2) {
			/* begin MoveR:R: */
			genoperandoperand(MoveRR, Arg1Reg, Arg0Reg);
		}
		return;
	}
	if (((numRegArgs()) + 1) == numArgs) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMwrR, 0, SPReg, Arg1Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperandoperand(MoveMwrR, BytesPerWord, SPReg, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction3 = genoperandoperand(AddCqR, (numArgs + 1) * BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction3)) {
			(anInstruction3->dependent = locateLiteralsize((numArgs + 1) * BytesPerWord, BytesPerOop));
		}
		return;
	}
	for (index = (numArgs - 2); index >= 0; index += -1) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction10 = genoperandoperandoperand(MoveMwrR, index * BytesPerWord, SPReg, TempReg);
		if (usesOutOfLineLiteral(anInstruction10)) {
			(anInstruction10->dependent = locateLiteralsize(index * BytesPerWord, BytesPerOop));
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction11 = genoperandoperandoperand(MoveRMwr, TempReg, (index + 1) * BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction11)) {
			(anInstruction11->dependent = locateLiteralsize((index + 1) * BytesPerWord, BytesPerOop));
		}
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction12 = genoperandoperand(AddCqR, BytesPerWord, SPReg);
	if (usesOutOfLineLiteral(anInstruction12)) {
		(anInstruction12->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
}


/*	If the stack entry is already in a register not conflicting with regMask,
	answers it,
	else allocate a new register not conflicting with reg mask
 */

	/* StackToRegisterMappingCogit>>#allocateRegForStackEntryAt:notConflictingWith: */
static sqInt NoDbgRegParms
allocateRegForStackEntryAtnotConflictingWith(sqInt index, sqInt regMask)
{
    sqInt mask;
    CogSimStackEntry *stackEntry;

	stackEntry = ssValue(index);
	mask = registerMaskOrNone(stackEntry);
	if ((mask != 0)
	 && ((!(mask & regMask)))) {
		flag("TODO");
		return registerOrNone(stackEntry);
	}
	return allocateRegNotConflictingWith(regMask);
}


/*	if there's a free register, use it */

	/* StackToRegisterMappingCogit>>#allocateRegNotConflictingWith: */
static sqInt NoDbgRegParms
allocateRegNotConflictingWith(sqInt regMask)
{
    sqInt reg;

	reg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | regMask);
	if (reg == NoReg) {

		/* No free register, choose one that does not conflict with regMask */
		reg = freeAnyRegNotConflictingWith(regMask);
	}
	if (reg == ReceiverResultReg) {

		/* If we've allocated RcvrResultReg, it's not live anymore */
		voidReceiverResultRegContainsSelf();
	}
	return reg;
}

	/* StackToRegisterMappingCogit>>#anyReferencesToRegister:inTopNItems: */
static sqInt NoDbgRegParms
anyReferencesToRegisterinTopNItems(sqInt reg, sqInt n)
{
    sqInt i;
    sqInt regMask;

	/* begin registerMaskFor: */
	regMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((registerMask(simStackAt(i))) & regMask) != 0)) {
			return 1;
		}
	}
	return 0;
}


/*	This is a static version of ceCallCogCodePopReceiverArg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg0Regs */
void
callCogCodePopReceiverArg0Regs(void)
{
	realCECallCogCodePopReceiverArg0Regs();
}


/*	This is a static version of ceCallCogCodePopReceiverArg1Arg0Regs
	for break-pointing when debugging in C. */
/*	This exists only for break-pointing. */

	/* StackToRegisterMappingCogit>>#callCogCodePopReceiverArg1Arg0Regs */
void
callCogCodePopReceiverArg1Arg0Regs(void)
{
	realCECallCogCodePopReceiverArg1Arg0Regs();
}


/*	Loop over bytecodes, dispatching to the generator for each bytecode,
	handling fixups in due course.
 */

	/* StackToRegisterMappingCogit>>#compileAbstractInstructionsFrom:through: */
static sqInt NoDbgRegParms
compileAbstractInstructionsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt nextOpcodeIndex;
    sqInt result;

	traceSimStack();
	bytecodePC = start;
	nExts = (result = 0);
	descriptor = null;
	deadCode = 0;
	while (1) {
		maybeHaltIfDebugPC();
		mergeWithFixupIfRequired((fixup = fixupAt(bytecodePC)));
		descriptor = loadBytesAndGetDescriptor();
		nextOpcodeIndex = opcodeIndex;
		result = (deadCode
			? mapDeadDescriptorIfNeeded(descriptor)
			: ((descriptor->generator))());
		if (result == 0) {
			/* begin assertExtsAreConsumed: */
			if (!((descriptor->isExtension))) {
				assert((extA == 0)
				 && ((extB == 0)
				 && (numExtB == 0)));
			}
		}
		traceDescriptor(descriptor);
		traceSimStack();
		/* begin patchFixupTargetIfNeeded:nextOpcodeIndex: */
		if ((((((usqInt)((fixup->targetInstruction)))) >= NeedsNonMergeFixupFlag) && ((((usqInt)((fixup->targetInstruction)))) <= NeedsMergeFixupFlag))) {

			/* There is a fixup for this bytecode.  It must point to the first generated
			   instruction for this bytecode.  If there isn't one we need to add a label. */
			if (opcodeIndex == nextOpcodeIndex) {
				/* begin Label */
				genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			(fixup->targetInstruction = abstractInstructionAt(nextOpcodeIndex));
		}
		/* begin maybeDumpLiterals: */
		if ((mustDumpLiterals(opcodeIndex))
		 || (((isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse)))))
		 || ((descriptor->isReturn)))) {
			dumpLiterals(!(((isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse)))))
			 || ((descriptor->isReturn))));
		}
		/* begin nextBytecodePCFor:exts: */
		bytecodePC = (bytecodePC + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, bytecodePC, nExts, methodObj)
	: 0));
		if (!((result == 0)
		 && (bytecodePC <= end))) break;
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
	}
	/* begin checkEnoughOpcodes */
	if (opcodeIndex > numAbstractOpcodes) {
		error("Cog JIT internal error. Too many abstract opcodes.  Num opcodes heuristic is too optimistic.");
	}
	return result;
}

	/* StackToRegisterMappingCogit>>#compileBlockBodies */
static sqInt
compileBlockBodies(void)
{
    BlockStart *blockStart;
    sqInt compiledBlocksCount;
    sqInt initialCounterIndex;
    sqInt initialOpcodeIndex;
    sqInt initialStackPtr;
    sqInt (* const pushNilSizeFunction)(sqInt,sqInt) = squeakV3orSistaV1PushNilSizenumInitialNils;
    sqInt result;
    sqInt savedFirstOpcodeIndex;
    sqInt savedLastDumpedLiteralIndex;
    unsigned char savedNeedsFrame;
    sqInt savedNextLiteralIndex;
    sqInt savedNumArgs;
    sqInt savedNumTemps;

	assert(blockCount > 0);
	savedNeedsFrame = needsFrame;
	savedNumArgs = methodOrBlockNumArgs;
	savedNumTemps = methodOrBlockNumTemps;
	inBlock = InVanillaBlock;
	compiledBlocksCount = 0;
	while (compiledBlocksCount < blockCount) {
		compilationPass = 1;
		blockStart = blockStartAt(compiledBlocksCount);
		if (((result = scanBlock(blockStart))) < 0) {
			return result;
		}
		initialOpcodeIndex = opcodeIndex;
		/* begin maybeCounterIndex */
		initialCounterIndex = counterIndex;
		/* begin saveForRecompile */
		savedFirstOpcodeIndex = firstOpcodeIndex;
		savedNextLiteralIndex = nextLiteralIndex;
		savedLastDumpedLiteralIndex = lastDumpedLiteralIndex;
		while (1) {
			compileBlockEntry(blockStart);
			initialStackPtr = simStackPtr;
			if (((result = compileAbstractInstructionsFromthrough(((blockStart->startpc)) + (pushNilSizeFunction(methodObj, ((blockStart->numInitialNils)))), (((blockStart->startpc)) + ((blockStart->span))) - 1))) < 0) {
				return result;
			}
			if (initialStackPtr == simStackPtr) break;
			assert((initialStackPtr > simStackPtr)
			 || (deadCode));

			/* for asserts */
			compilationPass += 1;
			(blockStart->numInitialNils = (((blockStart->numInitialNils)) + simStackPtr) - initialStackPtr);
			(((blockStart->fakeHeader))->dependent = null);
			reinitializeFixupsFromthrough(((blockStart->startpc)) + ((blockStart->numInitialNils)), (((blockStart->startpc)) + ((blockStart->span))) - 1);
			bzero(abstractOpcodes + initialOpcodeIndex,
									(opcodeIndex - initialOpcodeIndex) * sizeof(AbstractInstruction));
			opcodeIndex = initialOpcodeIndex;
			/* begin maybeSetCounterIndex: */
			counterIndex = initialCounterIndex;
			/* begin resetForRecompile */
			firstOpcodeIndex = savedFirstOpcodeIndex;
			nextLiteralIndex = savedNextLiteralIndex;
			lastDumpedLiteralIndex = savedLastDumpedLiteralIndex;
					}
		compiledBlocksCount += 1;
	}
	needsFrame = savedNeedsFrame;
	methodOrBlockNumArgs = savedNumArgs;
	methodOrBlockNumTemps = savedNumTemps;
	return 0;
}


/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. closure (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	Avoid use of SendNumArgsReg which is the flag determining whether
	context switch is allowed on stack-overflow. */
/*	Build a frame for a block activation. See CoInterpreter
	class>>initializeFrameIndices. Override to push the register receiver and
	register arguments, if any, and to correctly
	initialize the explicitly nilled/pushed temp entries (they are /not/ of
	type constant nil). */

	/* StackToRegisterMappingCogit>>#compileBlockFrameBuild: */
static void NoDbgRegParms
compileBlockFrameBuild(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction * cascade0;
    sqInt constant;
    sqInt constant1;
    sqInt constant2;
    sqInt i;
    sqInt ign;

	/* begin annotateBytecode: */
	abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction->annotation = HasBytecodePC);
	/* begin PushR: */
	genoperand(PushR, LinkReg);
	/* begin PushR: */
	genoperand(PushR, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, ReceiverResultReg, ClassReg);
	cascade0 = (blockStart->fakeHeader);
	addDependent(cascade0, annotateAbsolutePCRef(gPushCw(((sqInt)((blockStart->fakeHeader))))));
	/* begin setLabelOffset: */
	((cascade0->operands))[1] = MFMethodFlagIsBlockFlag;
	/* begin genPushConstant: */
	constant2 = nilObject();
	if (shouldAnnotateObjectReference(constant2)) {
		annotateobjRef(gPushCw(constant2), constant2);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperand(PushCq, constant2);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(constant2, BytesPerOop));
		}
	}
	if ((blockStart->hasInstVarRef)) {

		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);
		genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, ReceiverIndex, ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ClassReg, Arg0Reg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, Arg0Reg, ReceiverResultReg);
	}
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = 0; i < ((blockStart->numCopied)); i += 1) {
		genLoadSlotsourceRegdestReg(i + ClosureFirstCopiedValueIndex, ClassReg, TempReg);
		/* begin PushR: */
		genoperand(PushR, TempReg);
	}
	(blockStart->stackCheckLabel = compileStackOverflowCheck(1));
	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramefulMethod((blockStart->startpc));
	if (((blockStart->numInitialNils)) > 0) {
		if (((blockStart->numInitialNils)) > 1) {
			/* begin genMoveNilR: */
			constant = nilObject();
			if (shouldAnnotateObjectReference(constant)) {
				annotateobjRef(gMoveCwR(constant, TempReg), constant);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction1 = genoperandoperand(MoveCqR, constant, TempReg);
				if (usesOutOfLineLiteral(anInstruction1)) {
					(anInstruction1->dependent = locateLiteralsize(constant, BytesPerOop));
				}
			}
			for (ign = 1; ign <= ((blockStart->numInitialNils)); ign += 1) {
				/* begin PushR: */
				genoperand(PushR, TempReg);
			}
		}
		else {
			/* begin genPushConstant: */
			constant1 = nilObject();
			if (shouldAnnotateObjectReference(constant1)) {
				annotateobjRef(gPushCw(constant1), constant1);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction2 = genoperand(PushCq, constant1);
				if (usesOutOfLineLiteral(anInstruction2)) {
					(anInstruction2->dependent = locateLiteralsize(constant1, BytesPerOop));
				}
			}
		}
	}
}


/*	Make sure ReceiverResultReg holds the receiver, loaded from the closure,
	which is what is initially in ReceiverResultReg. We must annotate the
	first instruction in vanilla blocks so that
	findMethodForStartBcpc:inHomeMethod: can function. We need two annotations
	because the first is a fiducial. */
/*	Make sure ReceiverResultReg holds the receiver, loaded from
	the closure, which is what is initially in ReceiverResultReg */

	/* StackToRegisterMappingCogit>>#compileBlockFramelessEntry: */
static void NoDbgRegParms
compileBlockFramelessEntry(BlockStart *blockStart)
{
    AbstractInstruction *abstractInstruction;

	methodOrBlockNumTemps = (((blockStart->numArgs)) + ((blockStart->numCopied))) + ((blockStart->numInitialNils));
	initSimStackForFramelessBlock((blockStart->startpc));
	if (!(((blockStart->entryLabel)) == null)) {
		/* begin annotateBytecode: */
		abstractInstruction = (blockStart->entryLabel);
		(abstractInstruction->annotation = HasBytecodePC);
	}
	if ((blockStart->hasInstVarRef)) {

		/* Use ReceiverResultReg for Context to agree with store check trampoline */
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ReceiverResultReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, ReceiverResultReg, Arg0Reg);
		genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, ReceiverIndex, ReceiverResultReg);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
	}
	else {
		genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, TempReg);
		genLoadSlotsourceRegdestReg(ReceiverIndex, TempReg, ReceiverResultReg);
	}
}


/*	Compile the abstract instructions for the entire method, including blocks. */
/*	Compile the abstract instructions for the entire method, including blocks. */

	/* StackToRegisterMappingCogit>>#compileEntireMethod */
static sqInt
compileEntireMethod(void)
{
    sqInt result;

	regArgsHaveBeenPushed = 0;
	/* begin preenMethodLabel */
	(((((AbstractInstruction *) methodLabel))->operands))[1] = 0;
	compileAbort();
	compileEntry();
	if (((result = compilePrimitive())) < 0) {
		return result;
	}
	compileFrameBuild();
	if (((result = compileMethodBody())) < 0) {
		return result;
	}
	if (blockCount == 0) {
		return 0;
	}
	if (((result = compileBlockBodies())) < 0) {
		return result;
	}
	return compileBlockDispatch();
}


/*	Make sure ReceiverResultReg holds the receiver, loaded from the closure,
	which is what is initially in ReceiverResultReg.  */
/*	Make sure ReceiverResultReg holds the receiver, loaded from
	the closure, which is what is initially in ReceiverResultReg */

	/* StackToRegisterMappingCogit>>#compileFullBlockFramelessEntry: */
static void NoDbgRegParms
compileFullBlockFramelessEntry(sqInt numCopied)
{
	initSimStackForFramelessBlock(initialPC);
	flag("TODO");
	genLoadSlotsourceRegdestReg(FullClosureReceiverIndex, ReceiverResultReg, Arg0Reg);
	genEnsureOopInRegNotForwardedscratchRegupdatingSlotin(Arg0Reg, TempReg, FullClosureReceiverIndex, ReceiverResultReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, Arg0Reg, ReceiverResultReg);
}


/*	Build a frame for a CogMethod activation. See CoInterpreter
	class>>initializeFrameIndices. receiver (in ReceiverResultReg)
	arg0
	...
	argN
	caller's saved ip/this stackPage (for a base frame)
	fp->	saved fp
	method
	context (uninitialized?)
	receiver
	first temp
	...
	sp->	Nth temp
	If there is a primitive and an error code the Nth temp is the error code.
	Ensure SendNumArgsReg is set early on (incidentally to nilObj) because
	it is the flag determining whether context switch is allowed on
	stack-overflow.  */
/*	We are in a method where the frame is needed *only* for instance variable
	store, typically a setter method.
	This case has 20% overhead with Immutability compared to setter without
	immutability because of the stack
	frame creation. We compile two path, one where the object is immutable,
	one where it isn't. At the beginning 
	of the frame build, we take one path or the other depending on the
	receiver mutability.
	
	Note: this specific case happens only where there are only instance
	variabel stores. We could do something
	similar for literal variable stores, but we don't as it's too uncommon.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFrameBuild */
#if IMMUTABILITY
static void
compileTwoPathFrameBuild(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt constant;
    sqInt i;
    sqInt iLimiT;
    AbstractInstruction *jumpImmutable;
    AbstractInstruction *jumpOld;
    sqInt literal;

	assert(useTwoPaths);
	assert(blockCount == 0);
	jumpImmutable = genJumpImmutablescratchReg(ReceiverResultReg, TempReg);

	/* first path. The receiver is mutable */
	
	/* N.B. FLAGS := destReg - scratchReg */
	/* begin checkQuickConstant:forInstruction: */
	literal = storeCheckBoundary();
	anInstruction = genoperandoperand(CmpCqR, storeCheckBoundary(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	assert(!needsFrame);
	initSimStackForFramelessMethod(initialPC);
	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l4;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
	l4:	/* end compileMethodBody */;

	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	needsFrame = 1;
	jmpTarget(jumpOld, jmpTarget(jumpImmutable, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	genPushRegisterArgs();
	if (!needsFrame) {
		return;
	}
	/* begin PushR: */
	genoperand(PushR, LinkReg);
	/* begin PushR: */
	genoperand(PushR, FPReg);
	/* begin MoveR:R: */
	genoperandoperand(MoveRR, SPReg, FPReg);
	addDependent(methodLabel, annotateAbsolutePCRef(gPushCw(((sqInt)methodLabel))));
	/* begin genMoveNilR: */
	constant = nilObject();
	if (shouldAnnotateObjectReference(constant)) {
		annotateobjRef(gMoveCwR(constant, SendNumArgsReg), constant);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction1 = genoperandoperand(MoveCqR, constant, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction1)) {
			(anInstruction1->dependent = locateLiteralsize(constant, BytesPerOop));
		}
	}
	/* begin PushR: */
	genoperand(PushR, SendNumArgsReg);
	/* begin PushR: */
	genoperand(PushR, ReceiverResultReg);
	for (i = (methodOrBlockNumArgs + 1), iLimiT = (temporaryCountOfMethodHeader(methodHeader)); i <= iLimiT; i += 1) {
		/* begin PushR: */
		genoperand(PushR, SendNumArgsReg);
	}
	if (((primitiveIndexOfMethodheader(methodObj, methodHeader)) > 0)
	 && ((longStoreBytecodeForHeader(methodHeader)) == (fetchByteofObject((startPCOfMethodHeader(methodHeader)) + (sizeOfCallPrimitiveBytecode(methodHeader)), methodObj)))) {
		compileGetErrorCode();
	}
	stackCheckLabel = compileStackOverflowCheck(canContextSwitchIfActivatingheader(methodObj, methodHeader));
	initSimStackForFramefulMethod(initialPC);
}
#endif /* IMMUTABILITY */


/*	We are in a frameless method with at least two inst var stores. We compile
	two paths,
	one where the object is in new space, and one where it isn't. At the
	beginning 
	of the method, we take one path or the other depending on the receiver
	being in newSpace.
 */

	/* StackToRegisterMappingCogit>>#compileTwoPathFramelessInit */
static void
compileTwoPathFramelessInit(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *jumpOld;
    sqInt literal;

	assert(!(IMMUTABILITY));
	assert(!(needsFrame));
	assert(useTwoPaths);

	/* first path. The receiver is young */
	
	/* N.B. FLAGS := destReg - scratchReg */
	/* begin checkQuickConstant:forInstruction: */
	literal = storeCheckBoundary();
	anInstruction = genoperandoperand(CmpCqR, storeCheckBoundary(), ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpAboveOrEqual: */
	jumpOld = genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)0));
	initSimStackForFramelessMethod(initialPC);
	/* begin compileMethodBody */
	if (endPC < initialPC) {
		goto l1;
	}
	compileAbstractInstructionsFromthrough(initialPC + (deltaToSkipPrimAndErrorStoreInheader(methodObj, methodHeader)), endPC);
	l1:	/* end compileMethodBody */;

	/* reset because it impacts inst var store compilation */
	useTwoPaths = 0;
	jmpTarget(jumpOld, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
}

	/* StackToRegisterMappingCogit>>#cPICMissTrampolineFor: */
static sqInt NoDbgRegParms
cPICMissTrampolineFor(sqInt numArgs)
{
	return picMissTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}


/*	Replaces the Blue Book double-extended send [132], in which the first byte
	was wasted on 8 bits of argument count. 
	Here we use 3 bits for the operation sub-type (opType), and the remaining
	5 bits for argument count where needed. 
	The last byte give access to 256 instVars or literals. 
	See also secondExtendedSendBytecode
 */

	/* StackToRegisterMappingCogit>>#doubleExtendedDoAnythingBytecode */
static sqInt
doubleExtendedDoAnythingBytecode(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    sqInt opType;

	opType = ((usqInt)(byte1)) >> 5;
	if (opType == 0) {
		return genSendnumArgs(byte2, byte1 & 0x1F);
	}
	if (opType == 1) {
		return genSendSupernumArgs(byte2, byte1 & 0x1F);
	}
	switch (opType) {
	case 2:
		if (isReadMediatedContextInstVarIndex(byte2)) {
			genPushMaybeContextReceiverVariable(byte2);
		}
		else {
			genPushReceiverVariable(byte2);
			/* begin annotateInstructionForBytecode */
			if (prevInstIsPCAnnotated()) {
				/* begin Nop */
				abstractInstruction = gen(Nop);
			}
			else {
				/* begin Label */
				abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			(abstractInstruction->annotation = HasBytecodePC);
			return 0;
		}
		break;
	case 3:
		genPushLiteralIndex(byte2);
		/* begin annotateInstructionForBytecode */
		if (prevInstIsPCAnnotated()) {
			/* begin Nop */
			abstractInstruction1 = gen(Nop);
		}
		else {
			/* begin Label */
			abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		}
		(abstractInstruction1->annotation = HasBytecodePC);
		return 0;

	case 4:
		genPushLiteralVariable(byte2);
		break;
	case 7:
		genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(0, byte2, ((((ssTop())->type)) != SSConstant)
		 || ((isNonImmediate(((ssTop())->constant)))
		 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
#    if IMMUTABILITY

		/* genStorePop:LiteralVariable: annotates; don't annotate twice */
		return 0;
#    endif
		break;
	default:
		
		/* 5 & 6 */
		if (isWriteMediatedContextInstVarIndex(byte2)) {
			genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
		}
		else {
			genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(opType == 6, byte2, ((((ssTop())->type)) != SSConstant)
			 || ((isNonImmediate(((ssTop())->constant)))
			 && (shouldAnnotateObjectReference(((ssTop())->constant)))), 1);
		}
#    if IMMUTABILITY

		/* genStorePop:...ReceiverVariable: annotate; don't annotate twice */
		return 0;
#    endif
;
	}
	assert(needsFrame);
	assert(!(prevInstIsPCAnnotated()));
	/* begin annotateBytecode: */
	abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction2->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#duplicateTopBytecode */
static sqInt
duplicateTopBytecode(void)
{
    SimStackEntry desc;

	/* begin ssTopDescriptor */
	desc = simStack[simStackPtr];
	return ssPushDesc(desc);
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureFixupAt: */
static BytecodeFixup * NoDbgRegParms
ensureFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;

	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if ((((usqInt)((fixup->targetInstruction)))) <= NeedsNonMergeFixupFlag) {

		/* convert a non-merge into a merge */
		/* begin becomeMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
		(fixup->simStackPtr = simStackPtr);
			}
	else {
		if ((fixup->isTargetOfBackwardBranch)) {

			/* this is the target of a backward branch and
			   so doesn't have a simStackPtr assigned yet. */
			(fixup->simStackPtr = simStackPtr);
					}
		else {
			assert(((fixup->simStackPtr)) == simStackPtr);
					}
	}
	return fixup;
}


/*	Make sure there's a flagged fixup at the target pc in fixups.
	Initially a fixup's target is just a flag. Later on it is replaced with a
	proper instruction. */

	/* StackToRegisterMappingCogit>>#ensureNonMergeFixupAt: */
static BytecodeFixup * NoDbgRegParms
ensureNonMergeFixupAt(sqInt targetPC)
{
    BytecodeFixup *fixup;

	/* begin fixupAt: */
	fixup = fixupAtIndex(targetPC - initialPC);
	traceFixupmerge(fixup, 1);
	if (((fixup->targetInstruction)) == 0) {
		/* begin becomeNonMergeFixup */
		(fixup->targetInstruction) = ((AbstractInstruction *) NeedsNonMergeFixupFlag);
	}
	return fixup;
}

	/* StackToRegisterMappingCogit>>#ensureReceiverResultRegContainsSelf */
static void
ensureReceiverResultRegContainsSelf(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin ssAllocateRequiredReg: */
			ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
			/* begin putSelfInReceiverResultReg */
			storeToReg(simSelf(), ReceiverResultReg);
			((simSelf())->liveRegister = ReceiverResultReg);
		}
	}
	else {
		assert(((((simSelf())->type)) == SSRegister)
		 && (((((simSelf())->registerr)) == ReceiverResultReg)
		 && (receiverIsInReceiverResultReg())));
	}
}

	/* StackToRegisterMappingCogit>>#evaluate:at: */
static void NoDbgRegParms
evaluateat(BytecodeDescriptor *descriptor, sqInt pc)
{
	byte0 = fetchByteofObject(pc, methodObj);
	assert(descriptor == (generatorAt(bytecodeSetOffset + byte0)));
	loadSubsequentBytesForDescriptorat(descriptor, pc);
	((descriptor->generator))();
}


/*	Attempt to follow a branch to a pc. Handle branches to unconditional jumps
	and branches to push: aBoolean; conditional branch pairs. If the branch
	cannot be
	followed answer targetBytecodePC. It is not possible to follow jumps to
	conditional branches because the stack changes depth. That following is
	left to the genJumpIf:to:
	clients. */

	/* StackToRegisterMappingCogit>>#eventualTargetOf: */
static sqInt NoDbgRegParms
eventualTargetOf(sqInt targetBytecodePC)
{
    sqInt cond;
    sqInt currentTarget;
    BytecodeDescriptor *descriptor;
    sqInt nExts;
    sqInt nextPC;
    sqInt span;

	cond = 0;
	nextPC = (currentTarget = targetBytecodePC);
	while (1) {
		nExts = 0;
		while (1) {
			/* begin generatorForPC: */
			descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
			if ((descriptor->isReturn)) {
				return currentTarget;
			}
			if (!((descriptor->isExtension))) break;
			nExts += 1;
			nextPC += (descriptor->numBytes);
		}
		if ((isBranch(descriptor))
		 && (!(((descriptor->isBranchTrue))
		 || ((descriptor->isBranchFalse))))) {
			/* begin spanFor:at:exts:in: */
			span = ((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj);
			if (span < 0) {

				/* Do *not* follow backward branches; these are interrupt points and should not be elided. */
				return currentTarget;
			}
			nextPC = (nextPC + ((descriptor->numBytes))) + span;
		}
		else {
			if (((descriptor->generator)) == genPushConstantTrueBytecode) {
				cond = 1;
			}
			else {
				if (((descriptor->generator)) == genPushConstantFalseBytecode) {
					cond = 0;
				}
				else {
					return currentTarget;
				}
			}
			if (((fixupAt(nextPC))->isTargetOfBackwardBranch)) {
				return currentTarget;
			}
			nextPC = eventualTargetOf(nextPC + ((descriptor->numBytes)));
			nExts = 0;
			while (1) {
				/* begin generatorForPC: */
				descriptor = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC, methodObj)));
				if ((descriptor->isReturn)) {
					return currentTarget;
				}
				if (!((descriptor->isExtension))) break;
				nExts += 1;
				nextPC += (descriptor->numBytes);
			}
			if (!(isBranch(descriptor))) {
				return currentTarget;
			}
			if ((isBranch(descriptor))
			 && (!(((descriptor->isBranchTrue))
			 || ((descriptor->isBranchFalse))))) {
				return currentTarget;
			}
			nextPC = (cond == ((descriptor->isBranchTrue))
				? (nextPC + ((descriptor->numBytes))) + (((descriptor->spanFunction))(descriptor, nextPC, nExts, methodObj))
				: nextPC + ((descriptor->numBytes)));
		}
		currentTarget = nextPC;
	}
	return 0;
}


/*	Spill the closest register on stack not conflicting with regMask. 
	Assertion Failure if regMask has already all the registers */

	/* StackToRegisterMappingCogit>>#freeAnyRegNotConflictingWith: */
static sqInt NoDbgRegParms
freeAnyRegNotConflictingWith(sqInt regMask)
{
    CogSimStackEntry *desc;
    sqInt index;
    sqInt reg;

	assert(needsFrame);
	reg = NoReg;
	index = ((simSpillBase < 0) ? 0 : simSpillBase);
	while ((reg == NoReg)
	 && (index < simStackPtr)) {
		desc = simStackAt(index);
		if (((desc->type)) == SSRegister) {
			if (!(((regMask & (((((desc->registerr)) < 0) ? (((usqInt)(1)) >> (-((desc->registerr)))) : (1ULL << ((desc->registerr)))))) != 0))) {
				reg = (desc->registerr);
			}
		}
		index += 1;
	}
	assert(!((reg == NoReg)));
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg)), simStackPtr, simNativeStackPtr);
	return reg;
}


/*	Return from block, assuming result already loaded into ReceiverResultReg. */
/*	Return from block, assuming result already loaded into ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#genBlockReturn */
static sqInt
genBlockReturn(void)
{
	if (needsFrame) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);
		/* begin PopR: */
		genoperand(PopR, FPReg);
		/* begin PopR: */
		genoperand(PopR, LinkReg);
	}
	/* begin RetN: */
	genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);

	/* can't fall through */
	deadCode = 1;
	return 0;
}


/*	Generate special versions of the ceCallCogCodePopReceiverAndClassRegs
	enilopmart that also pop register args from the stack to undo the pushing
	of register args in the abort/miss trampolines. */

	/* StackToRegisterMappingCogit>>#genCallPICEnilopmartNumArgs: */
static void (*genCallPICEnilopmartNumArgs(sqInt numArgs))(void)
{
    AbstractInstruction *anInstruction;
    sqInt endAddress;
    usqInt enilopmart;
    sqInt quickConstant;
    sqInt reg;
    sqInt size;

	zeroOpcodeIndex();
	/* begin MoveCq:R: */
	quickConstant = varBaseAddress();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(MoveCqR, quickConstant, VarBaseReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	genLoadStackPointers(backEnd);
	/* begin PopR: */
	genoperand(PopR, ClassReg);
	/* begin PopR: */
	genoperand(PopR, TempReg);
	/* begin PopR: */
	reg = LinkReg;
	genoperand(PopR, reg);
	if (numArgs > 0) {
		if (numArgs > 1) {
			/* begin PopR: */
			genoperand(PopR, Arg1Reg);
			assert((numRegArgs()) == 2);
		}
		/* begin PopR: */
		genoperand(PopR, Arg0Reg);
	}
	/* begin PopR: */
	genoperand(PopR, ReceiverResultReg);
	/* begin JumpR: */
	genoperand(JumpR, TempReg);
	computeMaximumSizes();
	size = generateInstructionsAt(methodZoneBase);
	endAddress = outputInstructionsAt(methodZoneBase);
	assert((methodZoneBase + size) == endAddress);
	enilopmart = methodZoneBase;
	methodZoneBase = alignUptoRoutineBoundary(endAddress);
	stopsFromto(backEnd, endAddress, methodZoneBase - 1);
	recordGeneratedRunTimeaddress(trampolineNamenumRegArgs("ceCallPIC", numArgs), enilopmart);
	return ((void (*)(void)) enilopmart);
}


/*	Override to push the register receiver and register arguments, if any. */

	/* StackToRegisterMappingCogit>>#genExternalizePointersForPrimitiveCall */
static sqInt
genExternalizePointersForPrimitiveCall(void)
{
    sqInt address;

	genPushRegisterArgs();
	/* begin MoveR:Aw: */
	address = instructionPointerAddress();
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address, genoperandoperand(MoveRAw, LinkReg, address));
	return genSaveStackPointers(backEnd);
}


/*	Override to push the register receiver and register arguments, if any. */

	/* StackToRegisterMappingCogit>>#genExternalizeStackPointerForFastPrimitiveCall */
static AbstractInstruction *
genExternalizeStackPointerForFastPrimitiveCall(void)
{
    sqInt address;

	genPushRegisterArgs();
	return (/* begin MoveR:Aw: */
	(address = stackPointerAddress()),
	/* begin gen:operand:literal: */
	checkLiteralforInstruction(address, genoperandoperand(MoveRAw, SPReg, address)));
}


/*	Block compilation. At this point in the method create the block. Note its
	start and defer generating code for it until after the method and any
	other preceding
	blocks. The block's actual code will be compiled later. */
/*	253		11111101 eei i i kkk	jjjjjjjj		Push Closure Num Copied iii (+ Ext A
	// 16 * 8) Num Args kkk (+ Ext A \\ 16 * 8) BlockSize jjjjjjjj (+ Ext B *
	256). ee = num extensions
 */

	/* StackToRegisterMappingCogit>>#genExtPushClosureBytecode */
static sqInt
genExtPushClosureBytecode(void)
{
    sqInt i;
    sqInt numArgs;
    sqInt numCopied;
    sqInt reg;
    sqInt startpc;

	assert(needsFrame);
	startpc = bytecodePC + (((generatorAt(byte0))->numBytes));
	addBlockStartAtnumArgsnumCopiedspan(startpc, (numArgs = (byte1 & 7) + ((extA % 16) * 8)), (numCopied = ((((usqInt)(byte1)) >> 3) & 7) + ((extA / 16) * 8)), byte2 + (((sqInt)((usqInt)(extB) << 8))));
	extA = (numExtB = (extB = 0));
	/* begin genInlineClosure:numArgs:numCopied: */
	assert(getActiveContextAllocatesInMachineCode());
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(startpc + 1, numArgs, numCopied, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (ClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	Full Block creation compilation. The block's actual code will be compiled
	separatedly. 
 */
/*	*	255		11111111	xxxxxxxx	siyyyyyy	push Closure Compiled block literal
	index xxxxxxxx (+ Extend A * 256) numCopied yyyyyy receiverOnStack: s = 1
	ignoreOuterContext: i = 1
 */

	/* StackToRegisterMappingCogit>>#genExtPushFullClosureBytecode */
static sqInt
genExtPushFullClosureBytecode(void)
{
    sqInt compiledBlock;
    sqInt i;
    int ignoreContext;
    sqInt numCopied;
    int receiverIsOnStack;
    sqInt reg;

	assert(needsFrame);
	compiledBlock = getLiteral(byte1 + (((sqInt)((usqInt)(extA) << 8))));
	extA = 0;
	numCopied = byte2 & (0x3F);
	receiverIsOnStack = ((byte2 & (128)) != 0);
	ignoreContext = ((byte2 & (64)) != 0);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genCreateFullClosurenumArgsnumCopiedignoreContextcontextNumArgslargeinBlock(compiledBlock, argumentCountOf(compiledBlock), numCopied, ignoreContext, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (FullClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	if (receiverIsOnStack) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
	}
	else {
		storeToReg(simSelf(), (reg = TempReg));
	}
	genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, FullClosureReceiverIndex, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). 
 */
/*	Enilopmarts transfer control from C into machine code (backwards
	trampolines). Override to add version for generic and PIC-specific entry
	with reg args. */

	/* StackToRegisterMappingCogit>>#generateEnilopmarts */
static void
generateEnilopmarts(void)
{

#  if Debug
	/* begin genEnilopmartFor:forCall:called: */
	realCEEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "realCEEnterCogCodePopReceiverReg");
	ceEnterCogCodePopReceiverReg = enterCogCodePopReceiver;
	/* begin genEnilopmartFor:forCall:called: */
	realCECallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "realCECallCogCodePopReceiverReg");
	ceCallCogCodePopReceiverReg = callCogCodePopReceiver;
	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "realCECallCogCodePopReceiverAndClassRegs");
	ceCallCogCodePopReceiverAndClassRegs = callCogCodePopReceiverAndClassRegs;
#  else // Debug
	/* begin genEnilopmartFor:forCall:called: */
	ceEnterCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 0, "ceEnterCogCodePopReceiverReg");
	/* begin genEnilopmartFor:forCall:called: */
	ceCallCogCodePopReceiverReg = genEnilopmartForandandforCallcalled(ReceiverResultReg, NoReg, NoReg, 1, "ceCallCogCodePopReceiverReg");
	/* begin genEnilopmartFor:and:forCall:called: */
	ceCallCogCodePopReceiverAndClassRegs = genEnilopmartForandandforCallcalled(ReceiverResultReg, ClassReg, NoReg, 1, "ceCallCogCodePopReceiverAndClassRegs");
#  endif // Debug
	genPrimReturnEnterCogCodeEnilopmart(0);
	cePrimReturnEnterCogCode = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCode);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCode", cePrimReturnEnterCogCode);
	genPrimReturnEnterCogCodeEnilopmart(1);
	cePrimReturnEnterCogCodeProfiling = methodZoneBase;
	outputInstructionsForGeneratedRuntimeAt(cePrimReturnEnterCogCodeProfiling);
	recordGeneratedRunTimeaddress("cePrimReturnEnterCogCodeProfiling", cePrimReturnEnterCogCodeProfiling);
#  if Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	realCECallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "realCECallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg0Regs = callCogCodePopReceiverArg0Regs;
	realCECallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "realCECallCogCodePopReceiverArg1Arg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = callCogCodePopReceiverArg1Arg0Regs;
#  else // Debug
	/* begin genEnilopmartFor:and:forCall:called: */
	ceCallCogCodePopReceiverArg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, NoReg, 1, "ceCallCogCodePopReceiverArg0Regs");
	ceCallCogCodePopReceiverArg1Arg0Regs = genEnilopmartForandandforCallcalled(ReceiverResultReg, Arg0Reg, Arg1Reg, 1, "ceCallCogCodePopReceiverArg1Arg0Regs");
#  endif // Debug
	ceCall0ArgsPIC = genCallPICEnilopmartNumArgs(0);
	ceCall1ArgsPIC = genCallPICEnilopmartNumArgs(1);
	ceCall2ArgsPIC = genCallPICEnilopmartNumArgs(2);
	assert((numRegArgs()) == 2);
}


/*	Size pc-dependent instructions and assign eventual addresses to all
	instructions. Answer the size of the code.
	Compute forward branches based on virtual address (abstract code starts at
	0), assuming that any branches branched over are long.
	Compute backward branches based on actual address.
	Reuse the fixups array to record the pc-dependent instructions that need
	to have
	their code generation postponed until after the others.
	
	Override to add handling for null branches (branches to the immediately
	following instruction) occasioned by StackToRegisterMapping's following of
	jumps.  */

	/* StackToRegisterMappingCogit>>#generateInstructionsAt: */
static sqInt NoDbgRegParms
generateInstructionsAt(sqInt eventualAbsoluteAddress)
{
    sqInt absoluteAddress;
    AbstractInstruction *abstractInstruction;
    BytecodeFixup *fixup;
    sqInt i;
    sqInt j;
    sqInt pcDependentIndex;

	absoluteAddress = eventualAbsoluteAddress;
	pcDependentIndex = 0;
	for (i = 0; i < opcodeIndex; i += 1) {

		/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
		   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
		abstractInstruction = abstractInstructionAt(i);
		maybeBreakGeneratingFromto(absoluteAddress, absoluteAddress + ((abstractInstruction->maxSize)));
		if (isPCDependent(abstractInstruction)) {
			sizePCDependentInstructionAt(abstractInstruction, absoluteAddress);
			if ((isJump(abstractInstruction))
			 && ((((i + 1) < opcodeIndex)
			 && ((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 1))))
			 || (((i + 2) < opcodeIndex)
			 && (((((AbstractInstruction *) (((abstractInstruction->operands))[0]))) == (abstractInstructionAt(i + 2)))
			 && ((((abstractInstructionAt(i + 1))->opcode)) == Nop))))) {
				(abstractInstruction->opcode = Nop);
				concretizeAt(abstractInstruction, absoluteAddress);
			}
			else {
				fixup = fixupAtIndex(pcDependentIndex);
				pcDependentIndex += 1;
				(fixup->instructionIndex = i);
			}
			absoluteAddress += (abstractInstruction->machineCodeSize);
		}
		else {

			/* N.B. if you want to break in resizing, break here, note the instruction index, back up to the
			   sender, restart, and step into computeMaximumSizes, breaking at this instruction's index. */
			absoluteAddress = concretizeAt(abstractInstruction, absoluteAddress);
			assert(((abstractInstruction->machineCodeSize)) == ((abstractInstruction->maxSize)));
		}
	}
	for (j = 0; j < pcDependentIndex; j += 1) {
		fixup = fixupAtIndex(j);
		abstractInstruction = abstractInstructionAt((fixup->instructionIndex));
		maybeBreakGeneratingFromto((abstractInstruction->address), (((abstractInstruction->address)) + ((abstractInstruction->maxSize))) - 1);
		concretizeAt(abstractInstruction, (abstractInstruction->address));
	}
	return absoluteAddress - eventualAbsoluteAddress;
}


/*	Generate the run-time entries for the various method and PIC entry misses
	and aborts.
	Read the class-side method trampolines for documentation on the various
	trampolines 
 */

	/* StackToRegisterMappingCogit>>#generateMissAbortTrampolines */
static void
generateMissAbortTrampolines(void)
{
    sqInt numArgs;
    sqInt numArgsLimiT;

	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		methodAbortTrampolines[numArgs] = (genMethodAbortTrampolineFor(numArgs));
	}
	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		picAbortTrampolines[numArgs] = (genPICAbortTrampolineFor(numArgs));
	}
	for (numArgs = 0, numArgsLimiT = ((numRegArgs()) + 1); numArgs <= numArgsLimiT; numArgs += 1) {
		picMissTrampolines[numArgs] = (genPICMissTrampolineFor(numArgs));
	}
	/* begin genTrampolineFor:called:arg: */
	ceReapAndResetErrorCodeTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceReapAndResetErrorCodeFor, "ceReapAndResetErrorCodeTrampoline", 1, ClassReg, null, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 0);
}


/*	Override to generate code to push the register arg(s) for <= numRegArg
	arity sends.
 */

	/* StackToRegisterMappingCogit>>#generateSendTrampolines */
static void
generateSendTrampolines(void)
{
    sqInt numArgs;

	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		ordinarySendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendsupertonumArgs, numArgs, trampolineNamenumArgs("ceSend", numArgs), ClassReg, (assert(0 >= 0),
-2 - 0), ReceiverResultReg, (numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
	}
	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		directedSuperSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendabovetonumArgs, numArgs, trampolineNamenumArgs("ceDirectedSuperSend", numArgs), ClassReg, TempReg, ReceiverResultReg, (numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
		directedSuperBindingSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendaboveClassBindingtonumArgs, numArgs, trampolineNamenumArgs("ceDirectedSuperBindingSend", numArgs), ClassReg, TempReg, ReceiverResultReg, (numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
	}
	for (numArgs = 0; numArgs < NumSendTrampolines; numArgs += 1) {
		superSendTrampolines[numArgs] = (genSendTrampolineFornumArgscalledargargargarg(ceSendsupertonumArgs, numArgs, trampolineNamenumArgs("ceSuperSend", numArgs), ClassReg, (assert(1 >= 0),
-2 - 1), ReceiverResultReg, (numArgs <= (NumSendTrampolines - 2)
	? (assert(numArgs >= 0),
		-2 - numArgs)
	: SendNumArgsReg)));
	}
	firstSend = ordinarySendTrampolines[0];
	lastSend = superSendTrampolines[NumSendTrampolines - 1];
}


/*	Generate trampolines for tracing. In the simulator we can save a lot of
	time and avoid noise instructions in the lastNInstructions log by
	short-cutting these
	trampolines, but we need them in the real vm. */

	/* StackToRegisterMappingCogit>>#generateTracingTrampolines */
static void
generateTracingTrampolines(void)
{
	/* begin genTrampolineFor:called:arg:regsToSave: */
	ceTraceLinkedSendTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceLinkedSend, "ceTraceLinkedSendTrampoline", 1, ReceiverResultReg, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	/* begin genTrampolineFor:called:regsToSave: */
	ceTraceBlockActivationTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceBlockActivation, "ceTraceBlockActivationTrampoline", 0, null, null, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
	/* begin genTrampolineFor:called:arg:arg:regsToSave: */
	ceTraceStoreTrampoline = genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceTraceStoreOfinto, "ceTraceStoreTrampoline", 2, TempReg, ReceiverResultReg, null, null, CallerSavedRegisterMask, 1, NoReg, 0);
}


/*	Generates the machine code for #== in the case where the instruction is
	not followed by a branch
 */

	/* StackToRegisterMappingCogit>>#genIdenticalNoBranchArgIsConstant:rcvrIsConstant:argReg:rcvrReg:orNotIf: */
static sqInt NoDbgRegParms
genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(sqInt argIsConstant, sqInt rcvrIsConstant, sqInt argReg, sqInt rcvrRegOrNone, sqInt orNot)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    sqInt constant;
    sqInt constant1;
    sqInt constant11;
    sqInt constant2;
    sqInt constant3;
    sqInt constant4;
    AbstractInstruction *jumpEqual;
    AbstractInstruction *jumpNotEqual;
    AbstractInstruction *label;
    sqInt resultReg;

	/* begin Label */
	label = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrRegOrNone != NoReg));
	if (argIsConstant) {
		/* begin genCmpConstant:R: */
		constant4 = ((ssTop())->constant);
		if (shouldAnnotateObjectReference(constant4)) {
			annotateobjRef(checkLiteralforInstruction(constant4, genoperandoperand(CmpCwR, constant4, rcvrRegOrNone)), constant4);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(CmpCqR, constant4, rcvrRegOrNone);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(constant4, BytesPerOop));
			}
		}
	}
	else {
		if (rcvrIsConstant) {
			/* begin genCmpConstant:R: */
			constant11 = ((ssValue(1))->constant);
			if (shouldAnnotateObjectReference(constant11)) {
				annotateobjRef(checkLiteralforInstruction(constant11, genoperandoperand(CmpCwR, constant11, argReg)), constant11);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction1 = genoperandoperand(CmpCqR, constant11, argReg);
				if (usesOutOfLineLiteral(anInstruction1)) {
					(anInstruction1->dependent = locateLiteralsize(constant11, BytesPerOop));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrRegOrNone);
		}
	}
	ssPop(2);
	resultReg = (rcvrRegOrNone == NoReg
		? argReg
		: rcvrRegOrNone);
	/* begin JumpZero: */
	jumpEqual = genConditionalBranchoperand(JumpZero, ((sqInt)0));
	if (!argIsConstant) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(argReg, TempReg, label, 0);
	}
	if (!rcvrIsConstant) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(rcvrRegOrNone, TempReg, label, 0);
	}
	if (orNot) {
		/* begin genMoveTrueR: */
		constant = trueObject();
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(gMoveCwR(constant, resultReg), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction2 = genoperandoperand(MoveCqR, constant, resultReg);
			if (usesOutOfLineLiteral(anInstruction2)) {
				(anInstruction2->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
	}
	else {
		/* begin genMoveFalseR: */
		constant1 = falseObject();
		if (shouldAnnotateObjectReference(constant1)) {
			annotateobjRef(gMoveCwR(constant1, resultReg), constant1);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction3 = genoperandoperand(MoveCqR, constant1, resultReg);
			if (usesOutOfLineLiteral(anInstruction3)) {
				(anInstruction3->dependent = locateLiteralsize(constant1, BytesPerOop));
			}
		}
	}
	/* begin Jump: */
	jumpNotEqual = genoperand(Jump, ((sqInt)0));
	jmpTarget(jumpEqual, (orNot
		? (/* begin genMoveFalseR: */
			(constant2 = falseObject()),
			(shouldAnnotateObjectReference(constant2)
					? annotateobjRef(gMoveCwR(constant2, resultReg), constant2)
					: (/* begin checkQuickConstant:forInstruction: */
						(anInstruction4 = genoperandoperand(MoveCqR, constant2, resultReg)),
						(usesOutOfLineLiteral(anInstruction4)
								? (anInstruction4->dependent = locateLiteralsize(constant2, BytesPerOop))
								: 0),
						anInstruction4)))
		: (/* begin genMoveTrueR: */
			(constant3 = trueObject()),
			(shouldAnnotateObjectReference(constant3)
					? annotateobjRef(gMoveCwR(constant3, resultReg), constant3)
					: (/* begin checkQuickConstant:forInstruction: */
						(anInstruction5 = genoperandoperand(MoveCqR, constant3, resultReg)),
						(usesOutOfLineLiteral(anInstruction5)
								? (anInstruction5->dependent = locateLiteralsize(constant3, BytesPerOop))
								: 0),
						anInstruction5)))));
	jmpTarget(jumpNotEqual, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	ssPushRegister(resultReg);
	return 0;
}


/*	Decompose code generation for #== into a common constant-folding version,
	followed by a double dispatch through the objectRepresentation to a
	version that doesn't deal with forwarders and a version that does. */

	/* StackToRegisterMappingCogit>>#genInlinedIdenticalOrNotIf: */
static sqInt NoDbgRegParms
genInlinedIdenticalOrNotIf(sqInt orNot)
{
    BytecodeDescriptor *primDescriptor;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	if ((isUnannotatableConstant(ssTop()))
	 && (isUnannotatableConstant(ssValue(1)))) {
		assert(!((primDescriptor->isMapped)));
		result = ((orNot
			? (((ssTop())->constant)) != (((ssValue(1))->constant))
			: (((ssTop())->constant)) == (((ssValue(1))->constant)))
			? trueObject()
			: falseObject());
		ssPop(2);
		return ssPushConstant(result);
	}
	/* begin genInlinedIdenticalOrNotIfGuts: */
	return genForwardersInlinedIdenticalOrNotIf(orNot);
}

	/* StackToRegisterMappingCogit>>#genJumpBackTo: */
static sqInt NoDbgRegParms
genJumpBackTo(sqInt targetBytecodePC)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt address;
    sqInt i;
    sqInt index;
    void *jumpTarget;
    void *jumpTarget1;

	/* begin ssFlushTo: */
	index = simStackPtr;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}

	/* can't fall through */
	deadCode = 1;
	/* begin MoveAw:R: */
	address = stackLimitAddress();
	/* begin gen:literal:operand: */
	checkLiteralforInstruction(address, genoperandoperand(MoveAwR, address, TempReg));
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SPReg);
	/* begin JumpAboveOrEqual: */
	jumpTarget = fixupAtIndex(targetBytecodePC - initialPC);
	genConditionalBranchoperand(JumpAboveOrEqual, ((sqInt)jumpTarget));
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceCheckForInterruptTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
	/* begin annotateBytecode: */
	abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	(abstractInstruction1->annotation = HasBytecodePC);
	/* begin Jump: */
	jumpTarget1 = fixupAtIndex(targetBytecodePC - initialPC);
	genoperand(Jump, ((sqInt)jumpTarget1));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genJumpTo: */
static sqInt NoDbgRegParms
genJumpTo(sqInt targetBytecodePC)
{
    sqInt eventualTarget;
    BytecodeFixup *fixup;
    BytecodeDescriptor *generator;
    sqInt i;
    sqInt i1;
    sqInt index;
    sqInt index1;

	eventualTarget = eventualTargetOf(targetBytecodePC);
	if ((eventualTarget > bytecodePC)
	 && (((simStackPtr >= methodOrBlockNumArgs)
	 && (stackEntryIsBoolean(ssTop())))
	 && ((((generator = generatorForPC(eventualTarget))->isBranchTrue))
	 || (((generator = generatorForPC(eventualTarget))->isBranchFalse))))) {
		eventualTarget = (eventualTarget + ((generator->numBytes))) + ((((generator->isBranchTrue)) == ((((ssTop())->constant)) == (trueObject()))
	? (/* begin spanFor:at:exts:in: */
		((generator->spanFunction))(generator, eventualTarget, 0, methodObj))
	: 0));
		ssPop(1);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
		ssPop(-1);
	}
	else {
		/* begin ssFlushTo: */
		index1 = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index1) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i1 <= index1; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = index1 + 1;
		}
		fixup = ensureFixupAt(eventualTarget);
	}

	/* can't fall through */
	deadCode = 1;
	/* begin Jump: */
	genoperand(Jump, ((sqInt)fixup));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genMarshalledSend:numArgs:sendTable: */
static sqInt NoDbgRegParms
genMarshalledSendnumArgssendTable(sqInt selectorIndex, sqInt numArgs, sqInt *sendTable)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt annotation;
    AbstractInstruction *instruction;

	assert(needsFrame);
	/* begin annotationForSendTable: */
	if (sendTable == ordinarySendTrampolines) {
		annotation = IsSendCall;
		goto l2;
	}
	if (sendTable == directedSuperSendTrampolines) {
		annotation = IsDirectedSuperSend;
		goto l2;
	}
	if (sendTable == directedSuperBindingSendTrampolines) {
		annotation = IsDirectedSuperBindingSend;
		goto l2;
	}
	assert(sendTable == superSendTrampolines);
	annotation = IsSuperSend;
	l2:	/* end annotationForSendTable: */;
	if ((annotation == IsSuperSend)
	 || (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend)))) {
		/* begin genEnsureOopInRegNotForwarded:scratchReg:jumpBackTo: */
		instruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		genEnsureOopInRegNotForwardedscratchRegifForwarderifNotForwarder(ReceiverResultReg, TempReg, instruction, 0);
	}
	if (numArgs >= (NumSendTrampolines - 1)) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, numArgs, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(numArgs, BytesPerOop));
		}
	}
	if (((annotation >= IsDirectedSuperSend) && (annotation <= IsDirectedSuperBindingSend))) {
		/* begin genMoveConstant:R: */
		if (shouldAnnotateObjectReference(tempOop)) {
			annotateobjRef(gMoveCwR(tempOop, TempReg), tempOop);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(MoveCqR, tempOop, TempReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(tempOop, BytesPerOop));
			}
		}
	}
	genLoadInlineCacheWithSelector(selectorIndex);
	((genoperand(Call, sendTable[((numArgs < (NumSendTrampolines - 1)) ? numArgs : (NumSendTrampolines - 1))]))->annotation = annotation);
	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	return ssPushRegister(ReceiverResultReg);
}


/*	Generate the abort for a method. This abort performs either a call of
	ceSICMiss: to handle a single-in-line cache miss or a call of
	ceStackOverflow: to handle a
	stack overflow. It distinguishes the two by testing ResultReceiverReg. If
	the register is zero then this is a stack-overflow because a) the receiver
	has already
	been pushed and so can be set to zero before calling the abort, and b) the
	receiver must always contain an object (and hence be non-zero) on SIC
	miss.  */

	/* StackToRegisterMappingCogit>>#genMethodAbortTrampolineFor: */
static usqInt NoDbgRegParms
genMethodAbortTrampolineFor(sqInt numArgs)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jumpSICMiss;

	zeroOpcodeIndex();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, 0, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(0, BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpSICMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveR:Mw:r: */
	anInstruction = genoperandoperandoperand(MoveRMwr, LinkReg, 0, SPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(0, BytesPerOop));
	}
	compileTrampolineFornumArgsargargargargregsToSavepushLinkRegresultReg(ceStackOverflow, 1, SendNumArgsReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg);
	jmpTarget(jumpSICMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceSICMiss, trampolineNamenumRegArgs("ceMethodAbort", numArgs), 1, ReceiverResultReg, null, null, null, 0 /* emptyRegisterMask */, 0, NoReg, 1);
}


/*	Generate the abort for a PIC. This abort performs either a call of
	ceInterpretMethodFromPIC:receiver: to handle invoking an uncogged
	target or a call of ceMNUFromPICMNUMethod:receiver: to handle an
	MNU dispatch in a closed PIC. It distinguishes the two by testing
	ClassReg. If the register is zero then this is an MNU. */

	/* StackToRegisterMappingCogit>>#genPICAbortTrampolineFor: */
static usqInt NoDbgRegParms
genPICAbortTrampolineFor(sqInt numArgs)
{
	zeroOpcodeIndex();
	genPushRegisterArgsForAbortMissNumArgs(backEnd, numArgs);
	return genInnerPICAbortTrampoline(trampolineNamenumRegArgs("cePICAbort", numArgs));
}

	/* StackToRegisterMappingCogit>>#genPICMissTrampolineFor: */
static usqInt NoDbgRegParms
genPICMissTrampolineFor(sqInt numArgs)
{
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(ceCPICMissreceiver, trampolineNamenumRegArgs("cePICMiss", numArgs), 2, ClassReg, ReceiverResultReg, null, null, 0 /* emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genPopStackBytecode */
static sqInt
genPopStackBytecode(void)
{
    AbstractInstruction *anInstruction;

	if (((ssTop())->spilled)) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(AddCqR, BytesPerWord, SPReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
		}
	}
	ssPop(1);
	return 0;
}


/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive. */
/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive.
	Override to push the register args first. */

	/* StackToRegisterMappingCogit>>#genPrimitiveClosureValue */
static sqInt
genPrimitiveClosureValue(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpFail1;
    AbstractInstruction *jumpFail2;
    AbstractInstruction *jumpFail3;
    AbstractInstruction *jumpFail4;
    AbstractInstruction *jumpFailNArgs;
    sqInt literal;
    sqInt offset;
    void (*primitiveRoutine)(void);
    sqInt result;

	genPushRegisterArgs();
	genLoadSlotsourceRegdestReg(ClosureNumArgsIndex, ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(CmpCqR, (((usqInt)methodOrBlockNumArgs << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize((((usqInt)methodOrBlockNumArgs << 3) | 1), BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpFailNArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(ClosureOuterContextIndex, ReceiverResultReg, ClassReg);
	jumpFail1 = genJumpImmediate(ClassReg);
	genGetCompactClassIndexNonImmOfinto(ClassReg, TempReg);
	genCmpClassMethodContextCompactIndexR(TempReg);
	/* begin JumpNonZero: */
	jumpFail2 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(MethodIndex, ClassReg, SendNumArgsReg);
	jumpFail3 = genJumpImmediate(SendNumArgsReg);
	genGetFormatOfinto(SendNumArgsReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstCompiledMethodFormat();
	anInstruction2 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpFail4 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	/* begin MoveM16:r:R: */
	offset = offsetof(CogMethod, blockEntryOffset);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperandoperand(MoveM16rR, offset, ClassReg, TempReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin AddR:R: */
	genoperandoperand(AddRR, ClassReg, TempReg);
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, null);
	if (primitiveRoutine == primitiveClosureValueNoContextSwitch) {
		if (blockNoContextSwitchOffset == null) {
			return NotFullyInitialized;
		}
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(SubCqR, blockNoContextSwitchOffset, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(blockNoContextSwitchOffset, BytesPerOop));
		}
	}
	/* begin JumpR: */
	genoperand(JumpR, TempReg);
	jmpTarget(jumpBCMethod, jmpTarget(jumpFail1, jmpTarget(jumpFail2, jmpTarget(jumpFail3, jmpTarget(jumpFail4, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	if (((result = compileInterpreterPrimitiveflags(primitiveRoutine, primitivePropertyFlagsnumArgs(primitiveIndex, methodOrBlockNumArgs)))) < 0) {
		return result;
	}
	jmpTarget(jumpFailNArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Check the argument count. Fail if wrong.
	Get the method from the outerContext and see if it is cogged. If so, jump
	to the
	block entry or the no-context-switch entry, as appropriate, and we're
	done. If not,
	invoke the interpreter primitive. */
/*	Override to push the register args first. */

	/* StackToRegisterMappingCogit>>#genPrimitiveFullClosureValue */
static sqInt
genPrimitiveFullClosureValue(void)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *jumpBCMethod;
    AbstractInstruction *jumpFail4;
    AbstractInstruction *jumpFailImmediateMethod;
    AbstractInstruction *jumpFailNArgs;
    sqInt literal;
    void (*primitiveRoutine)(void);
    sqInt quickConstant;
    sqInt result;

	genPushRegisterArgs();
	genLoadSlotsourceRegdestReg(ClosureNumArgsIndex, ReceiverResultReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperand(CmpCqR, (((usqInt)methodOrBlockNumArgs << 3) | 1), TempReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize((((usqInt)methodOrBlockNumArgs << 3) | 1), BytesPerOop));
	}
	/* begin JumpNonZero: */
	jumpFailNArgs = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	genLoadSlotsourceRegdestReg(FullClosureCompiledBlockIndex, ReceiverResultReg, SendNumArgsReg);
	jumpFailImmediateMethod = genJumpImmediate(SendNumArgsReg);
	genGetFormatOfinto(SendNumArgsReg, TempReg);
	/* begin checkQuickConstant:forInstruction: */
	literal = firstCompiledMethodFormat();
	anInstruction1 = genoperandoperand(CmpCqR, firstCompiledMethodFormat(), TempReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(literal, BytesPerOop));
	}
	/* begin JumpLess: */
	jumpFail4 = genConditionalBranchoperand(JumpLess, ((sqInt)0));
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);
	jumpBCMethod = genJumpImmediate(ClassReg);
	primitiveRoutine = functionPointerForCompiledMethodprimitiveIndexprimitivePropertyFlagsInto(methodObj, primitiveIndex, null);
	/* begin AddCq:R: */
	quickConstant = (primitiveRoutine == primitiveFullClosureValueNoContextSwitch
		? fullBlockNoContextSwitchEntryOffset()
		: fullBlockEntryOffset());
	/* begin checkQuickConstant:forInstruction: */
	anInstruction2 = genoperandoperand(AddCqR, quickConstant, ClassReg);
	if (usesOutOfLineLiteral(anInstruction2)) {
		(anInstruction2->dependent = locateLiteralsize(quickConstant, BytesPerOop));
	}
	/* begin JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpBCMethod, jmpTarget(jumpFailImmediateMethod, jmpTarget(jumpFail4, genoperandoperand(Label, (labelCounter += 1), bytecodePC))));
	if (((result = compileInterpreterPrimitiveflags(primitiveRoutine, primitivePropertyFlagsnumArgs(primitiveIndex, methodOrBlockNumArgs)))) < 0) {
		return result;
	}
	jmpTarget(jumpFailNArgs, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return CompletePrimitive;
}


/*	Generate an in-line perform primitive. The lookup code requires the
	selector to be in Arg0Reg.
	adjustArgumentsForPerform: adjusts the arguments once
	genLookupForPerformNumArgs: has generated the code for the lookup. */

	/* StackToRegisterMappingCogit>>#genPrimitivePerform */
static sqInt
genPrimitivePerform(void)
{
    AbstractInstruction *anInstruction;
    sqInt offset;

	if (methodOrBlockNumArgs > (numRegArgs())) {
		/* begin MoveMw:r:R: */
		offset = (methodOrBlockNumArgs - 1) * BytesPerWord;
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperandoperand(MoveMwrR, offset, SPReg, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
		}
	}
	return genLookupForPerformNumArgs(methodOrBlockNumArgs);
}


/*	Generate an in-line perform:withArguments: primitive. The lookup code
	requires the selector to be in Arg0Reg
	and the array to be in Arg1Reg. The primitive will only handle cases 0 to
	numRegArgs. Is it worth it you ask?
	Here are arguemnt count requencies for a short run of Croquet/Virtend
	which show that even for V3, with only
	one rtegister arg, it very much is (but we wimp out on V3 cuz of the
	complexity of checking the Array):
	[0] = 253743		52.5%
	[1] = 116117		24%/76.5%
	[2] = 99876		20.7%/97.2%
	[3] = 12837		2.7%/99.9%
	[4] = 209
	[5] = 84
	[6] = 2
	[7] = 313
	[8] = 0
	[9] = 0
	[10] = 0
	[11] = 1
	[12] = 0
	[13] = 0
	[14] = 0
	[15] = 0 */

	/* StackToRegisterMappingCogit>>#genPrimitivePerformWithArguments */
static sqInt
genPrimitivePerformWithArguments(void)
{
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction11;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    sqInt cacheBaseReg;
    AbstractInstruction *itsAHit;
    AbstractInstruction *jumpBadNumArgs1;
    AbstractInstruction *jumpBadNumArgs2;
    AbstractInstruction *jumpClassMiss;
    AbstractInstruction *jumpImmArray;
    AbstractInstruction *jumpInterpret;
    AbstractInstruction *jumpSelectorMiss;
    sqInt offset;
    sqInt quickConstant1;

	/* begin genLookupForPerformWithArguments */
	jumpImmArray = genJumpImmediate(Arg1Reg);
	genGetInlineCacheClassTagFromintoforEntry(ReceiverResultReg, SendNumArgsReg, 0);
	flag("lookupInMethodCacheSel:classTag:");
	cacheBaseReg = NoReg;
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 0, cacheBaseReg);
	/* begin JumpNonZero: */
	jumpClassMiss = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin MoveMw:r:R: */
	offset = (cacheBaseReg == NoReg
		? (methodCacheAddress()) + (((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))))
		: ((sqInt)((usqInt)(MethodCacheMethod) << (shiftForWord()))));
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperandoperand(MoveMwrR, offset, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	itsAHit = anInstruction1;
	genLoadSlotsourceRegdestReg(HeaderIndex, SendNumArgsReg, ClassReg);

	/* check the argument count; if it's wrong fall back on the interpreter primitive. */
	jumpInterpret = genJumpImmediate(ClassReg);
	/* begin genLoadcmNumArgsOf:into: */
	/* begin checkQuickConstant:forInstruction: */
	anInstruction11 = genoperandoperandoperand(MoveMbrR, BytesPerWord, ClassReg, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction11)) {
		(anInstruction11->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
	}
	genGetRawSlotSizeOfNonImminto(Arg1Reg, TempReg);
	/* begin CmpR:R: */
	assert(!((TempReg == SPReg)));
	genoperandoperand(CmpRR, TempReg, SendNumArgsReg);
	/* begin JumpNonZero: */
	jumpBadNumArgs1 = genConditionalBranchoperand(JumpNonZero, ((sqInt)0));
	/* begin CmpCq:R: */
	quickConstant1 = numRegArgs();
	/* begin checkQuickConstant:forInstruction: */
	anInstruction3 = genoperandoperand(CmpCqR, quickConstant1, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction3)) {
		(anInstruction3->dependent = locateLiteralsize(quickConstant1, BytesPerOop));
	}
	/* begin JumpGreater: */
	jumpBadNumArgs2 = genConditionalBranchoperand(JumpGreater, ((sqInt)0));
	genFetchRegArgsForPerformWithArguments(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction4 = genoperandoperand(AddCqR, cmNoCheckEntryOffset, ClassReg);
	if (usesOutOfLineLiteral(anInstruction4)) {
		(anInstruction4->dependent = locateLiteralsize(cmNoCheckEntryOffset, BytesPerOop));
	}
	/* begin JumpR: */
	genoperand(JumpR, ClassReg);
	jmpTarget(jumpSelectorMiss, jmpTarget(jumpClassMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC)));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 1, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpSelectorMiss, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	jumpSelectorMiss = compilePerformMethodCacheProbeForwithShiftbaseRegOrNone(Arg0Reg, 2, cacheBaseReg);
	/* begin JumpZero: */
	genConditionalBranchoperand(JumpZero, ((sqInt)itsAHit));
	jmpTarget(jumpImmArray, jmpTarget(jumpSelectorMiss, jmpTarget(jumpInterpret, jmpTarget(jumpBadNumArgs1, jmpTarget(jumpBadNumArgs2, genoperandoperand(Label, (labelCounter += 1), bytecodePC))))));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genPushActiveContextBytecode */
static sqInt
genPushActiveContextBytecode(void)
{
	assert(needsFrame);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genGetActiveContextNumArgslargeinBlock(methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	return ssPushRegister(ReceiverResultReg);
}


/*	Block compilation. At this point in the method create the block. Note its
	start and defer generating code for it until after the method and any
	other preceding
	blocks. The block's actual code will be compiled later. */
/*	143 10001111 llllkkkk jjjjjjjj iiiiiiii	Push Closure Num Copied llll Num
	Args kkkk BlockSize jjjjjjjjiiiiiiii */

	/* StackToRegisterMappingCogit>>#genPushClosureCopyCopiedValuesBytecode */
static sqInt
genPushClosureCopyCopiedValuesBytecode(void)
{
    sqInt i;
    sqInt numArgs;
    sqInt numCopied;
    sqInt reg;
    sqInt startpc;

	assert(needsFrame);
	startpc = bytecodePC + (((generatorAt(byte0))->numBytes));
	addBlockStartAtnumArgsnumCopiedspan(startpc, (numArgs = byte1 & 15), (numCopied = ((usqInt)(byte1)) >> 4), (((sqInt)((usqInt)(byte2) << 8))) + byte3);
	/* begin genInlineClosure:numArgs:numCopied: */
	assert(getActiveContextAllocatesInMachineCode());
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateCallReg:and:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg) | (1U << SendNumArgsReg)) | (1U << ClassReg)), simStackPtr, simNativeStackPtr);
	genNoPopCreateClosureAtnumArgsnumCopiedcontextNumArgslargeinBlock(startpc + 1, numArgs, numCopied, methodOrBlockNumArgs, methodNeedsLargeContext(methodObj), inBlock);
	for (i = 1; i <= numCopied; i += 1) {
		reg = ssStorePoptoPreferredReg(1, TempReg);
		genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, (ClosureFirstCopiedValueIndex + numCopied) - i, ReceiverResultReg);
	}
	ssPushRegister(ReceiverResultReg);
	return 0;
}


/*	<SmallInteger> */
/*	Override to avoid the BytecodeSetHasDirectedSuperSend check, which is
	unnecessary here given the simulation stack. */

	/* StackToRegisterMappingCogit>>#genPushLiteralIndex: */
static sqInt NoDbgRegParms
genPushLiteralIndex(sqInt literalIndex)
{
    sqInt literal;

	literal = getLiteral(literalIndex);
	return ssPushConstant(literal);
}

	/* StackToRegisterMappingCogit>>#genPushLiteralVariable: */
static sqInt NoDbgRegParms
genPushLiteralVariable(sqInt literalIndex)
{
    AbstractInstruction *anInstruction;
    sqInt association;
    sqInt bcpc;
    BytecodeDescriptor *descriptor1;
    sqInt eA;
    sqInt eB;
    sqInt freeReg;
    sqInt savedB0;
    sqInt savedB1;
    sqInt savedB2;
    sqInt savedB3;
    sqInt savedEA;
    sqInt savedEB;
    sqInt savedNEB;


	/* If followed by a directed super send bytecode, avoid generating any code yet.
	   The association will be passed to the directed send trampoline in a register
	   and fully dereferenced only when first linked.  It will be ignored in later sends. */
	association = getLiteral(literalIndex);
	assert(!(directedSendUsesBinding));
	/* begin nextDescriptorExtensionsAndNextPCInto: */
	descriptor1 = generatorAt(byte0);
	savedB0 = byte0;
	savedB1 = byte1;
	savedB2 = byte2;
	savedB3 = byte3;
	savedEA = extA;
	savedEB = extB;
	savedNEB = numExtB;
	bcpc = bytecodePC + ((descriptor1->numBytes));
	do {
		if (bcpc > endPC) {
			goto l1;
		}
		byte0 = (fetchByteofObject(bcpc, methodObj)) + bytecodeSetOffset;
		descriptor1 = generatorAt(byte0);
		loadSubsequentBytesForDescriptorat(descriptor1, bcpc);
		if (!((descriptor1->isExtension))) {
			eA = extA;
			eB = extB;
			extA = savedEA;
			extB = savedEB;
			numExtB = savedNEB;
			byte0 = savedB0;
			byte1 = savedB1;
			byte2 = savedB2;
			byte3 = savedB3;
			if ((descriptor1 != null)
			 && ((((descriptor1->generator)) == genExtSendSuperBytecode)
			 && (eB >= 64))) {
				ssPushConstant(association);
				directedSendUsesBinding = 1;
				return 0;
			}
			goto l1;
		}
		((descriptor1->generator))();
		bcpc += (descriptor1->numBytes);
	} while(1);
	l1:	/* end nextDescriptorExtensionsAndNextPCInto: */;

	/* N.B. Do _not_ use ReceiverResultReg to avoid overwriting receiver in assignment in frameless methods. */
	/* So far descriptors are not rich enough to describe the entire dereference so generate the register
	   load but don't push the result.  There is an order-of-evaluation issue if we defer the dereference. */
	freeReg = allocateRegNotConflictingWith(0);
	/* begin genMoveConstant:R: */
	if (shouldAnnotateObjectReference(association)) {
		annotateobjRef(gMoveCwR(association, TempReg), association);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, association, TempReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(association, BytesPerOop));
		}
	}
	genLoadSlotsourceRegdestReg(ValueIndex, TempReg, freeReg);
	ssPushRegister(freeReg);
	return 0;
}

	/* StackToRegisterMappingCogit>>#genPushMaybeContextReceiverVariable: */
static sqInt NoDbgRegParms
genPushMaybeContextReceiverVariable(sqInt slotIndex)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *jmpDone;
    AbstractInstruction *jmpSingle;

	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ReceiverResultReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ensureReceiverResultRegContainsSelf();
	/* begin genPushMaybeContextSlotIndex: */
	assert(needsFrame);
	if (((CallerSavedRegisterMask & ((1U << ReceiverResultReg))) != 0)) {

		/* We have no way of reloading ReceiverResultReg since we need the inst var value as the result. */
		voidReceiverResultRegContainsSelf();
	}
	if (slotIndex == InstructionPointerIndex) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(slotIndex, BytesPerOop));
		}
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceFetchContextInstVarTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		return ssPushRegister(SendNumArgsReg);
	}
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	/* begin genJumpNotSmallIntegerInScratchReg: */
	jmpSingle = genJumpNotSmallInteger(TempReg);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(slotIndex, BytesPerOop));
	}
	/* begin CallRT: */
	abstractInstruction1 = genoperand(Call, ceFetchContextInstVarTrampoline);
	(abstractInstruction1->annotation = IsRelativeCall);
	/* begin Jump: */
	jmpDone = genoperand(Jump, ((sqInt)0));
	jmpTarget(jmpSingle, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	genLoadSlotsourceRegdestReg(slotIndex, ReceiverResultReg, SendNumArgsReg);
	jmpTarget(jmpDone, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return ssPushRegister(SendNumArgsReg);
}

	/* StackToRegisterMappingCogit>>#genPushNewArrayBytecode */
static sqInt
genPushNewArrayBytecode(void)
{
    sqInt i;
    sqInt i1;
    sqInt index;
    int popValues;
    sqInt size;

	assert(needsFrame);
	voidReceiverResultRegContainsSelf();
	if ((popValues = byte1 > 0x7F)) {
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i1 <= index; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
	}
	else {
		/* begin ssAllocateCallReg:and: */
		ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << SendNumArgsReg)) | ((1U << ReceiverResultReg))), simStackPtr, simNativeStackPtr);
	}
	size = byte1 & 0x7F;
	if (!popValues) {
		if (tryCollapseTempVectorInitializationOfSize(size)) {
			return 0;
		}
	}
	genNewArrayOfSizeinitialized(size, !popValues);
	if (popValues) {
		for (i = (size - 1); i >= 0; i += -1) {
			/* begin PopR: */
			genoperand(PopR, TempReg);
			genStoreSourceRegslotIndexintoNewObjectInDestReg(TempReg, i, ReceiverResultReg);
		}
		ssPop(size);
	}
	return ssPushRegister(ReceiverResultReg);
}

	/* StackToRegisterMappingCogit>>#genPushReceiverBytecode */
static sqInt
genPushReceiverBytecode(void)
{
	if ((((simSelf())->liveRegister)) == ReceiverResultReg) {
		return ssPushRegister(ReceiverResultReg);
	}
	return ssPushDesc(ssSelfDescriptor());
}

	/* StackToRegisterMappingCogit>>#genPushReceiverVariable: */
static sqInt NoDbgRegParms
genPushReceiverVariable(sqInt index)
{
	ensureReceiverResultRegContainsSelf();
	return ssPushBaseoffset(ReceiverResultReg, slotOffsetOfInstVarIndex(index));
}


/*	Ensure that the register args are pushed before the retpc for methods with
	arity <= self numRegArgs.
 */
/*	This isn't as clumsy on a RISC. But putting the receiver and
	args above the return address means the CoInterpreter has a
	single machine-code frame format which saves us a lot of work. */

	/* StackToRegisterMappingCogit>>#genPushRegisterArgs */
static void
genPushRegisterArgs(void)
{
	if (!(regArgsHaveBeenPushed
		 || (methodOrBlockNumArgs > (numRegArgs())))) {
		genPushRegisterArgsForNumArgsscratchReg(backEnd, methodOrBlockNumArgs, SendNumArgsReg);
		regArgsHaveBeenPushed = 1;
	}
}

	/* StackToRegisterMappingCogit>>#genPushRemoteTempLongBytecode */
static sqInt
genPushRemoteTempLongBytecode(void)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt regMask;
    sqInt remoteTempReg;
    sqInt tempVectReg;

	tempVectReg = allocateRegNotConflictingWith(0);
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfTemporary(byte2);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, FPReg, tempVectReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin availableRegOrNoneNotConflictingWith: */
	regMask = ((tempVectReg < 0) ? (((usqInt)(1)) >> (-tempVectReg)) : (1ULL << tempVectReg));
	remoteTempReg = availableRegisterOrNoneFor(backEnd, (liveRegisters()) | regMask);
	if (remoteTempReg == NoReg) {
		remoteTempReg = tempVectReg;
	}
	genLoadSlotsourceRegdestReg(byte1, tempVectReg, remoteTempReg);
	return ssPushRegister(remoteTempReg);
}


/*	If a frameless method (not a block), only argument temps can be accessed.
	This is assured by the use of needsFrameIfMod16GENumArgs: in pushTemp. */

	/* StackToRegisterMappingCogit>>#genPushTemporaryVariable: */
static sqInt NoDbgRegParms
genPushTemporaryVariable(sqInt index)
{
	assert((inBlock > 0)
	 || (needsFrame
	 || (index < methodOrBlockNumArgs)));
	return ssPushDesc(simStack[index + 1]);
}


/*	In a frameless method ReceiverResultReg already contains self.
	In a frameful method, ReceiverResultReg /may/ contain self. */

	/* StackToRegisterMappingCogit>>#genReturnReceiver */
static sqInt
genReturnReceiver(void)
{
	if (needsFrame) {
		if (!((((simSelf())->liveRegister)) == ReceiverResultReg)) {
			/* begin putSelfInReceiverResultReg */
			storeToReg(simSelf(), ReceiverResultReg);
		}
	}
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromBlock */
static sqInt
genReturnTopFromBlock(void)
{
	assert(inBlock > 0);
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genBlockReturn();
}

	/* StackToRegisterMappingCogit>>#genReturnTopFromMethod */
static sqInt
genReturnTopFromMethod(void)
{
	popToReg(ssTop(), ReceiverResultReg);
	ssPop(1);
	return genUpArrowReturn();
}

	/* StackToRegisterMappingCogit>>#genSendDirectedSuper:numArgs: */
static sqInt NoDbgRegParms
genSendDirectedSupernumArgs(sqInt selectorIndex, sqInt numArgs)
{
    sqInt result;

	assert((((ssTop())->type)) == SSConstant);
	tempOop = ((ssTop())->constant);
	ssPop(1);
	marshallSendArguments(numArgs);
	result = genMarshalledSendnumArgssendTable(selectorIndex, numArgs, (directedSendUsesBinding
		? directedSuperBindingSendTrampolines
		: directedSuperSendTrampolines));
	directedSendUsesBinding = 0;
	return result;
}

	/* StackToRegisterMappingCogit>>#genSendSuper:numArgs: */
static sqInt NoDbgRegParms
genSendSupernumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, superSendTrampolines);
}


/*	Generate a trampoline with four arguments.
	Hack: a negative value indicates an abstract register, a non-negative
	value indicates a constant. */

	/* StackToRegisterMappingCogit>>#genSendTrampolineFor:numArgs:called:arg:arg:arg:arg: */
static usqInt NoDbgRegParms
genSendTrampolineFornumArgscalledargargargarg(void *aRoutine, sqInt numArgs, char *aString, sqInt regOrConst0, sqInt regOrConst1, sqInt regOrConst2, sqInt regOrConst3)
{
    sqInt routine;
    usqInt startAddress;

	startAddress = methodZoneBase;
	zeroOpcodeIndex();
	genPushRegisterArgsForNumArgsscratchReg(backEnd, numArgs, SendNumArgsReg);
	/* begin selectorIndexDereferenceRoutine */
	routine = ceDereferenceSelectorIndex;
	if (!(routine == null)) {

		/* Explicitly save LinkReg via ExtraReg2; it's presumably faster than pushing/popping */
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, LinkReg, Extra2Reg);
		/* begin Call: */
		genoperand(Call, routine);
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, Extra2Reg, LinkReg);
	}
	genTrampolineForcallednumArgsargargargargregsToSavepushLinkRegresultRegappendOpcodes(aRoutine, aString, 4, regOrConst0, regOrConst1, regOrConst2, regOrConst3, 0 /* emptyRegisterMask */, 1, NoReg, 1);
	return startAddress;
}

	/* StackToRegisterMappingCogit>>#genSend:numArgs: */
static sqInt NoDbgRegParms
genSendnumArgs(sqInt selectorIndex, sqInt numArgs)
{
	marshallSendArguments(numArgs);
	return genMarshalledSendnumArgssendTable(selectorIndex, numArgs, ordinarySendTrampolines);
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorArithmetic */
static sqInt
genSpecialSelectorArithmetic(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    AbstractInstruction *anInstruction3;
    AbstractInstruction *anInstruction4;
    AbstractInstruction *anInstruction5;
    AbstractInstruction *anInstruction6;
    AbstractInstruction *anInstruction7;
    sqInt argInt;
    int argIsConst;
    sqInt argIsInt;
    sqInt i;
    sqInt index;
    sqInt index1;
    AbstractInstruction *jumpContinue;
    AbstractInstruction *jumpNotSmallInts;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int rcvrIsConst;
    sqInt rcvrIsInt;
    sqInt result;

	primDescriptor = generatorAt(byte0);
	argIsInt = ((argIsConst = (((ssTop())->type)) == SSConstant))
	 && ((((((argInt = ((ssTop())->constant)))) & 7) == 1));
	rcvrIsInt = (((rcvrIsConst = (((ssValue(1))->type)) == SSConstant))
	 && ((((((rcvrInt = ((ssValue(1))->constant)))) & 7) == 1)))
	 || ((mclassIsSmallInteger())
	 && (isSameEntryAs(ssValue(1), simSelf())));
	if (argIsInt
	 && (rcvrIsInt
	 && (rcvrIsConst))) {
		rcvrInt = (rcvrInt >> 3);
		argInt = (argInt >> 3);
		switch ((primDescriptor->opcode)) {
		case AddRR:
			result = rcvrInt + argInt;
			break;
		case SubRR:
			result = rcvrInt - argInt;
			break;
		case AndRR:
			result = rcvrInt & argInt;
			break;
		case OrRR:
			result = rcvrInt | argInt;
			break;
		default:
			error("Case not found and no otherwise clause");
		}
		if (isIntegerValue(result)) {

			/* Must annotate the bytecode for correct pc mapping. */
			return (ssPop(2),
				ssPushAnnotatedConstant((((usqInt)result << 3) | 1)));
		}
		return genSpecialSelectorSend();
	}
	if ((rcvrIsConst
	 && (!rcvrIsInt))
	 || (argIsConst
	 && (!argIsInt))) {
		return genSpecialSelectorSend();
	}
	if (!(argIsInt
		 || (rcvrIsInt))) {
		return genSpecialSelectorSend();
	}
	if (argIsInt) {
		/* begin ssFlushTo: */
		index1 = simStackPtr - 2;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index1) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i <= index1; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index1 + 1;
		}
		popToReg(ssValue(1), ReceiverResultReg);
		ssPop(2);
	}
	else {
		marshallSendArguments(1);
	}
	jumpNotSmallInts = (!(rcvrIsInt
 && (argIsInt))
		? (argIsInt
				? genJumpNotSmallInteger(ReceiverResultReg)
				: (rcvrIsInt
						? genJumpNotSmallInteger(Arg0Reg)
						: (/* begin genJumpNotSmallIntegersIn:and:scratch: */
							genoperandoperand(MoveRR, ReceiverResultReg, TempReg),
							/* begin AndR:R: */
							genoperandoperand(AndRR, Arg0Reg, TempReg),
							genJumpNotSmallInteger(TempReg))))
		: 0);
	switch ((primDescriptor->opcode)) {
	case AddRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(argInt - ConstZero, BytesPerOop));
			}
			/* begin JumpNoOverflow: */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction1 = genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction1)) {
				(anInstruction1->dependent = locateLiteralsize(argInt - ConstZero, BytesPerOop));
			}
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(ReceiverResultReg);
			/* begin AddR:R: */
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
			/* begin JumpNoOverflow: */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			if (rcvrIsInt
			 && (rcvrIsConst)) {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction2 = genoperandoperand(MoveCqR, rcvrInt, ReceiverResultReg);
				if (usesOutOfLineLiteral(anInstruction2)) {
					(anInstruction2->dependent = locateLiteralsize(rcvrInt, BytesPerOop));
				}
			}
			else {
				/* begin SubR:R: */
				genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);
				genSetSmallIntegerTagsIn(ReceiverResultReg);
			}
		}
		break;
	case SubRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction3 = genoperandoperand(SubCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction3)) {
				(anInstruction3->dependent = locateLiteralsize(argInt - ConstZero, BytesPerOop));
			}
			/* begin JumpNoOverflow: */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			/* begin checkQuickConstant:forInstruction: */
			anInstruction4 = genoperandoperand(AddCqR, argInt - ConstZero, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction4)) {
				(anInstruction4->dependent = locateLiteralsize(argInt - ConstZero, BytesPerOop));
			}
		}
		else {
			genRemoveSmallIntegerTagsInScratchReg(Arg0Reg);
			/* begin SubR:R: */
			genoperandoperand(SubRR, Arg0Reg, ReceiverResultReg);
			/* begin JumpNoOverflow: */
			jumpContinue = genConditionalBranchoperand(JumpNoOverflow, ((sqInt)0));
			/* begin AddR:R: */
			genoperandoperand(AddRR, Arg0Reg, ReceiverResultReg);
			genSetSmallIntegerTagsIn(Arg0Reg);
		}
		break;
	case AndRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction5 = genoperandoperand(AndCqR, argInt, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction5)) {
				(anInstruction5->dependent = locateLiteralsize(argInt, BytesPerOop));
			}
		}
		else {
			/* begin AndR:R: */
			genoperandoperand(AndRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (!(jumpNotSmallInts == null)
			? (/* begin Jump: */
				genoperand(Jump, ((sqInt)0)))
			: 0);
		break;
	case OrRR:
		if (argIsInt) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction6 = genoperandoperand(OrCqR, argInt, ReceiverResultReg);
			if (usesOutOfLineLiteral(anInstruction6)) {
				(anInstruction6->dependent = locateLiteralsize(argInt, BytesPerOop));
			}
		}
		else {
			/* begin OrR:R: */
			genoperandoperand(OrRR, Arg0Reg, ReceiverResultReg);
		}
		jumpContinue = (!(jumpNotSmallInts == null)
			? (/* begin Jump: */
				genoperand(Jump, ((sqInt)0)))
			: 0);
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	if (jumpNotSmallInts == null) {
		if (!jumpContinue) {

			/* overflow cannot happen */
			/* begin annotateInstructionForBytecode */
			if (prevInstIsPCAnnotated()) {
				/* begin Nop */
				abstractInstruction = gen(Nop);
			}
			else {
				/* begin Label */
				abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
			}
			(abstractInstruction->annotation = HasBytecodePC);
			ssPushRegister(ReceiverResultReg);
			return 0;
		}
	}
	else {
		jmpTarget(jumpNotSmallInts, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
	if (argIsInt) {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction7 = genoperandoperand(MoveCqR, argInt, Arg0Reg);
		if (usesOutOfLineLiteral(anInstruction7)) {
			(anInstruction7->dependent = locateLiteralsize(argInt, BytesPerOop));
		}
	}
	index = byte0 - ((bytecodeSetOffset == 0x100
		? AltFirstSpecialSelector + 0x100
		: FirstSpecialSelector));
	genMarshalledSendnumArgssendTable((-index) - 1, 1, ordinarySendTrampolines);
	jmpTarget(jumpContinue, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	return 0;
}

	/* StackToRegisterMappingCogit>>#genSpecialSelectorClass */
static sqInt
genSpecialSelectorClass(void)
{
    sqInt requiredReg1;
    sqInt topReg;

	topReg = registerOrNone(ssTop());
	ssPop(1);
	if ((topReg == NoReg)
	 || (topReg == ClassReg)) {
		/* begin ssAllocateRequiredReg:and: */
		requiredReg1 = (topReg = SendNumArgsReg);
		ssAllocateRequiredRegMaskupThroughupThroughNative((((requiredReg1 < 0) ? (((usqInt)(1)) >> (-requiredReg1)) : (1ULL << requiredReg1))) | ((1U << ClassReg)), simStackPtr, simNativeStackPtr);
	}
	else {
		/* begin ssAllocateRequiredReg: */
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), simStackPtr, simNativeStackPtr);
	}
	ssPush(1);
	popToReg(ssTop(), topReg);
	genGetClassObjectOfintoscratchRegmayBeAForwarder(topReg, ClassReg, TempReg, mayBeAForwarder(ssTop()));
	return (ssPop(1),
		ssPushRegister(ClassReg));
}


/*	Assumes both operands are ints */

	/* StackToRegisterMappingCogit>>#genStaticallyResolvedSpecialSelectorComparison */
static sqInt
genStaticallyResolvedSpecialSelectorComparison(void)
{
    sqInt argInt;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrInt;
    int result;

	primDescriptor = generatorAt(byte0);
	argInt = ((ssTop())->constant);
	rcvrInt = ((ssValue(1))->constant);
	switch ((primDescriptor->opcode)) {
	case JumpLess:
		result = rcvrInt < argInt;
		break;
	case JumpLessOrEqual:
		result = rcvrInt <= argInt;
		break;
	case JumpGreater:
		result = rcvrInt > argInt;
		break;
	case JumpGreaterOrEqual:
		result = rcvrInt >= argInt;
		break;
	case JumpZero:
		result = rcvrInt == argInt;
		break;
	case JumpNonZero:
		result = rcvrInt != argInt;
		break;
	default:
		error("Case not found and no otherwise clause");
	}
	ssPop(2);
	return ssPushAnnotatedConstant((result
		? trueObject()
		: falseObject()));
}


/*	We need a frame because the association has to be in ReceiverResultReg for
	the various trampolines
	and ReceiverResultReg holds only the receiver in frameless methods.
 */

	/* StackToRegisterMappingCogit>>#genStorePop:LiteralVariable:needsStoreCheck:needsImmutabilityCheck: */
static sqInt NoDbgRegParms
genStorePopLiteralVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt litVarIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *anInstruction;
    sqInt association;
    sqInt i;
    sqInt index;
    sqInt stackPtr;
    sqInt topReg;

	assert(needsFrame);
	/* begin genLoadLiteralVariable:in: */
	association = getLiteral(litVarIndex);
	voidReceiverResultRegContainsSelf();
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	/* begin genMoveConstant:R: */
	if (shouldAnnotateObjectReference(association)) {
		annotateobjRef(gMoveCwR(association, ReceiverResultReg), association);
	}
	else {
		/* begin checkQuickConstant:forInstruction: */
		anInstruction = genoperandoperand(MoveCqR, association, ReceiverResultReg);
		if (usesOutOfLineLiteral(anInstruction)) {
			(anInstruction->dependent = locateLiteralsize(association, BytesPerOop));
		}
	}
	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
#  if IMMUTABILITY
	if (needsImmCheck) {
		/* begin ssAllocateRequiredReg:upThrough: */
		stackPtr = simStackPtr - 1;
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), stackPtr, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, ValueIndex, ReceiverResultReg, TempReg, needsStoreCheck, 0);
	}
#  endif // IMMUTABILITY
	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, ValueIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}


/*	The reason we need a frame here is that assigning to an inst var of a
	context may
	involve wholesale reorganization of stack pages, and the only way to
	preserve the
	execution state of an activation in that case is if it has a frame. */

	/* StackToRegisterMappingCogit>>#genStorePop:MaybeContextReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static sqInt NoDbgRegParms
genStorePopMaybeContextReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    AbstractInstruction *abstractInstruction2;
    AbstractInstruction *abstractInstruction3;
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    sqInt i;
    AbstractInstruction *immutabilityFailure;
    sqInt index;
    AbstractInstruction *mutableJump;

	immutabilityFailure = ((AbstractInstruction *) 0);
	assert(needsFrame);
	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();
	/* begin genGenericStorePop:MaybeContextSlotIndex:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
	assert(needsFrame);
#  if IMMUTABILITY
	if (needsImmCheck) {
		mutableJump = genJumpMutablescratchReg(ReceiverResultReg, TempReg);
		/* begin genStoreTrampolineCall: */
		assert(IMMUTABILITY);
		if (slotIndex >= (NumStoreTrampolines - 1)) {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(MoveCqR, slotIndex, TempReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(slotIndex, BytesPerOop));
			}
			/* begin CallRT: */
			abstractInstruction3 = genoperand(Call, ceStoreTrampolines[NumStoreTrampolines - 1]);
			(abstractInstruction3->annotation = IsRelativeCall);
		}
		else {
			/* begin CallRT: */
			abstractInstruction1 = genoperand(Call, ceStoreTrampolines[slotIndex]);
			(abstractInstruction1->annotation = IsRelativeCall);
		}
		/* begin annotateBytecode: */
		abstractInstruction2 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		(abstractInstruction2->annotation = HasBytecodePC);
		/* begin putSelfInReceiverResultReg */
		storeToReg(simSelf(), ReceiverResultReg);
		/* begin Jump: */
		immutabilityFailure = genoperand(Jump, ((sqInt)0));
		jmpTarget(mutableJump, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif // IMMUTABILITY
	ssPop(1);
	/* begin ssAllocateCallReg:and: */
	ssAllocateRequiredRegMaskupThroughupThroughNative(CallerSavedRegisterMask | (((1U << ClassReg)) | ((1U << SendNumArgsReg))), simStackPtr, simNativeStackPtr);
	ssPush(1);
	genLoadSlotsourceRegdestReg(SenderIndex, ReceiverResultReg, TempReg);
	ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
	/* begin ssFlushTo: */
	index = simStackPtr;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	/* begin checkQuickConstant:forInstruction: */
	anInstruction1 = genoperandoperand(MoveCqR, slotIndex, SendNumArgsReg);
	if (usesOutOfLineLiteral(anInstruction1)) {
		(anInstruction1->dependent = locateLiteralsize(slotIndex, BytesPerOop));
	}
	/* begin CallRT: */
	abstractInstruction = genoperand(Call, ceStoreContextInstVarTrampoline);
	(abstractInstruction->annotation = IsRelativeCall);
#  if IMMUTABILITY
	if (needsImmCheck) {
		jmpTarget(immutabilityFailure, genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	}
#  endif
	return 0;
}

	/* StackToRegisterMappingCogit>>#genStorePop:ReceiverVariable:needsStoreCheck:needsImmutabilityCheck: */
static sqInt NoDbgRegParms
genStorePopReceiverVariableneedsStoreCheckneedsImmutabilityCheck(sqInt popBoolean, sqInt slotIndex, sqInt needsStoreCheck, sqInt needsImmCheck)
{
    sqInt i;
    sqInt index;
    sqInt needsImmCheck1;
    sqInt needsStoreCheck1;
    sqInt stackPtr;
    sqInt topReg;

	ssFlushUpThroughReceiverVariable(slotIndex);
	ensureReceiverResultRegContainsSelf();
	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
	needsStoreCheck1 = (!useTwoPaths)
	 && (needsStoreCheck);
	needsImmCheck1 = needsImmCheck
	 && (!useTwoPaths);
#  if IMMUTABILITY
	if (needsImmCheck1) {
		/* begin ssAllocateRequiredReg:upThrough: */
		stackPtr = simStackPtr - 1;
		ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ClassReg), stackPtr, simNativeStackPtr);
		ssStoreAndReplacePoptoReg(popBoolean, ClassReg);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		return genStoreWithImmutabilityCheckSourceRegslotIndexdestRegscratchRegneedsStoreCheckneedRestoreRcvr(ClassReg, slotIndex, ReceiverResultReg, TempReg, needsStoreCheck1, 1);
	}
#  endif // IMMUTABILITY
	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck1);
}


/*	The only reason we assert needsFrame here is that in a frameless method
	ReceiverResultReg must and does contain only self, but the ceStoreCheck
	trampoline expects the target of the store to be in ReceiverResultReg. So
	in a frameless method we would have a conflict between the receiver and
	the temote temp store, unless we we smart enough to realise that
	ReceiverResultReg was unused after the literal variable store, unlikely
	given that methods return self by default. */

	/* StackToRegisterMappingCogit>>#genStorePop:RemoteTemp:At:needsStoreCheck: */
static sqInt NoDbgRegParms
genStorePopRemoteTempAtneedsStoreCheck(sqInt popBoolean, sqInt slotIndex, sqInt remoteTempIndex, sqInt needsStoreCheck)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt topReg;

	assert(needsFrame);
	/* begin ssAllocateRequiredReg: */
	ssAllocateRequiredRegMaskupThroughupThroughNative((1U << ReceiverResultReg), simStackPtr, simNativeStackPtr);
	voidReceiverResultRegContainsSelf();
	/* begin MoveMw:r:R: */
	offset = frameOffsetOfTemporary(remoteTempIndex);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveMwrR, offset, FPReg, ReceiverResultReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	/* begin genGenericStorePop:slotIndex:destReg:needsStoreCheck:needsRestoreRcvr:needsImmutabilityCheck: */
	topReg = allocateRegForStackEntryAtnotConflictingWith(0, (1U << ReceiverResultReg));
	ssStorePoptoReg(popBoolean, topReg);
	return genStoreSourceRegslotIndexdestRegscratchReginFrameneedsStoreCheck(topReg, slotIndex, ReceiverResultReg, TempReg, needsFrame, needsStoreCheck);
}

	/* StackToRegisterMappingCogit>>#genStorePop:TemporaryVariable: */
static sqInt NoDbgRegParms
genStorePopTemporaryVariable(sqInt popBoolean, sqInt tempIndex)
{
    AbstractInstruction *anInstruction;
    sqInt offset;
    sqInt reg;

	ssFlushUpThroughTemporaryVariable(tempIndex);
	reg = ssStorePoptoPreferredReg(popBoolean, TempReg);
	/* begin MoveR:Mw:r: */
	offset = frameOffsetOfTemporary(tempIndex);
	/* begin checkQuickConstant:forInstruction: */
	anInstruction = genoperandoperandoperand(MoveRMwr, reg, offset, FPReg);
	if (usesOutOfLineLiteral(anInstruction)) {
		(anInstruction->dependent = locateLiteralsize(offset, BytesPerOop));
	}
	((simStackAt(tempIndex + 1))->bcptr = bytecodePC);
	return 0;
}


/*	Generate a method return from within a method or a block.
	Frameless method activation looks like
	CISCs (x86):
	receiver
	args
	sp->	ret pc.
	RISCs (ARM):
	receiver
	args
	ret pc in LR.
	A fully framed activation is described in CoInterpreter
	class>initializeFrameIndices. Return pops receiver and arguments off the
	stack. Callee pushes the result. */

	/* StackToRegisterMappingCogit>>#genUpArrowReturn */
static sqInt
genUpArrowReturn(void)
{
    AbstractInstruction *abstractInstruction;
    AbstractInstruction *abstractInstruction1;
    sqInt i;
    sqInt index;
    sqInt offset;


	/* can't fall through */
	deadCode = 1;
	if (inBlock > 0) {
		assert(needsFrame);
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
		/* begin CallRT: */
		abstractInstruction = genoperand(Call, ceNonLocalReturnTrampoline);
		(abstractInstruction->annotation = IsRelativeCall);
		/* begin annotateBytecode: */
		abstractInstruction1 = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
		(abstractInstruction1->annotation = HasBytecodePC);
		return 0;
	}
	if (
#  if IMMUTABILITY
		needsFrame
			 && (!useTwoPaths)
#  else
		needsFrame
#  endif
		) {
		/* begin MoveR:R: */
		genoperandoperand(MoveRR, FPReg, SPReg);
		/* begin PopR: */
		genoperand(PopR, FPReg);
		/* begin PopR: */
		genoperand(PopR, LinkReg);
		/* begin RetN: */
		genoperand(RetN, (methodOrBlockNumArgs + 1) * BytesPerWord);
	}
	else {
		/* begin RetN: */
		offset = ((methodOrBlockNumArgs > (numRegArgs()))
		 || (regArgsHaveBeenPushed)
			? (methodOrBlockNumArgs + 1) * BytesPerWord
			: 0);
		genoperand(RetN, offset);
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#genVanillaInlinedIdenticalOrNotIf: */
static sqInt NoDbgRegParms
genVanillaInlinedIdenticalOrNotIf(sqInt orNot)
{
    AbstractInstruction *anInstruction;
    AbstractInstruction *anInstruction1;
    AbstractInstruction *anInstruction2;
    int argIsConstant;
    sqInt argNeedsReg;
    sqInt argReg;
    sqInt argReg1;
    BytecodeDescriptor *branchDescriptor;
    BytecodeDescriptor *branchDescriptor1;
    sqInt constant;
    sqInt constant1;
    sqInt i;
    sqInt index;
    void *jumpTarget;
    void *jumpTarget1;
    void *jumpTarget2;
    void *jumpTarget3;
    sqInt nExts;
    sqInt nextPC;
    sqInt nextPC1;
    sqInt postBranchPC;
    sqInt postBranchPC1;
    BytecodeDescriptor *primDescriptor;
    sqInt rcvrIsConstant;
    sqInt rcvrNeedsReg;
    sqInt rcvrReg;
    sqInt rcvrReg1;
    sqInt reg;
    sqInt rNext1;
    sqInt rTop1;
    sqInt targetBytecodePC;
    sqInt targetPC;
    sqInt topRegistersMask;

	/* begin extractMaybeBranchDescriptorInto: */
	primDescriptor = generatorAt(byte0);
	nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	nExts = 0;
	while (1) {
		while (1) {
			/* begin generatorForPC: */
			branchDescriptor1 = generatorAt(bytecodeSetOffset + (fetchByteofObject(nextPC1, methodObj)));
			if (!((branchDescriptor1->isExtension))) break;
			nExts += 1;
			nextPC1 += (branchDescriptor1->numBytes);
		}
		/* begin isUnconditionalBranch */
		if (!((isBranch(branchDescriptor1))
		 && (!(((branchDescriptor1->isBranchTrue))
		 || ((branchDescriptor1->isBranchFalse)))))) break;
		nextPC1 = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
	}
	targetBytecodePC = (postBranchPC1 = 0);
	if (((branchDescriptor1->isBranchTrue))
	 || ((branchDescriptor1->isBranchFalse))) {
		targetBytecodePC = eventualTargetOf((nextPC1 + ((branchDescriptor1->numBytes))) + (((branchDescriptor1->spanFunction))(branchDescriptor1, nextPC1, nExts, methodObj)));
		postBranchPC1 = eventualTargetOf(nextPC1 + ((branchDescriptor1->numBytes)));
	}
	else {
		nextPC1 = bytecodePC + ((primDescriptor->numBytes));
	}
	branchDescriptor = branchDescriptor1;
	nextPC = nextPC1;
	postBranchPC = postBranchPC1;
	targetPC = targetBytecodePC;

	/* They can't be both constants to use correct machine opcodes.
	   However annotable constants can't be resolved statically, hence we need to careful. */
	argIsConstant = (((ssTop())->type)) == SSConstant;
	rcvrIsConstant = (!argIsConstant)
	 && ((((ssValue(1))->type)) == SSConstant);
	/* begin allocateEqualsEqualsRegistersArgNeedsReg:rcvrNeedsReg:into: */
	argNeedsReg = !argIsConstant;
	rcvrNeedsReg = !rcvrIsConstant;
	assert(argNeedsReg
	 || (rcvrNeedsReg));
	argReg1 = (rcvrReg1 = NoReg);
	if (argNeedsReg) {
		if (rcvrNeedsReg) {
			/* begin allocateRegForStackTopTwoEntriesInto: */
			topRegistersMask = 0;
			rTop1 = (rNext1 = NoReg);
			if ((registerOrNone(ssTop())) != NoReg) {
				rTop1 = registerOrNone(ssTop());
			}
			if ((registerOrNone(ssValue(1))) != NoReg) {
				/* begin registerMaskFor: */
				reg = (rNext1 = registerOrNone(ssValue(1)));
				topRegistersMask = ((reg < 0) ? (((usqInt)(1)) >> (-reg)) : (1ULL << reg));
			}
			if (rTop1 == NoReg) {
				rTop1 = allocateRegNotConflictingWith(topRegistersMask);
			}
			if (rNext1 == NoReg) {
				rNext1 = allocateRegNotConflictingWith(((rTop1 < 0) ? (((usqInt)(1)) >> (-rTop1)) : (1ULL << rTop1)));
			}
			assert(!(((rTop1 == NoReg)
 || (rNext1 == NoReg))));
			argReg1 = rTop1;
			rcvrReg1 = rNext1;
			popToReg(ssTop(), argReg1);
			popToReg(ssValue(1), rcvrReg1);
		}
		else {
			argReg1 = allocateRegForStackEntryAtnotConflictingWith(0, 0);
			popToReg(ssTop(), argReg1);
			if (((ssValue(1))->spilled)) {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction2 = genoperandoperand(AddCqR, BytesPerWord, SPReg);
				if (usesOutOfLineLiteral(anInstruction2)) {
					(anInstruction2->dependent = locateLiteralsize(BytesPerWord, BytesPerOop));
				}
			}
		}
	}
	else {
		assert(rcvrNeedsReg);
		assert(!((((ssTop())->spilled))));
		rcvrReg1 = allocateRegForStackEntryAtnotConflictingWith(1, 0);
		popToReg(ssValue(1), rcvrReg1);
	}
	assert(!((argNeedsReg
 && (argReg1 == NoReg))));
	assert(!((rcvrNeedsReg
 && (rcvrReg1 == NoReg))));
	rcvrReg = rcvrReg1;
	argReg = argReg1;
	if (!(((branchDescriptor->isBranchTrue))
		 || ((branchDescriptor->isBranchFalse)))) {
		return genIdenticalNoBranchArgIsConstantrcvrIsConstantargRegrcvrRegorNotIf(argIsConstant, rcvrIsConstant, argReg, rcvrReg, orNot);
	}
	/* begin ssFlushTo: */
	index = simStackPtr - 2;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index) {
		for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
		}
		simSpillBase = index + 1;
	}
	/* begin genCmpArgIsConstant:rcvrIsConstant:argReg:rcvrReg: */
	assert((argReg != NoReg)
	 || (rcvrReg != NoReg));
	if (argIsConstant) {
		/* begin genCmpConstant:R: */
		constant = ((ssTop())->constant);
		if (shouldAnnotateObjectReference(constant)) {
			annotateobjRef(checkLiteralforInstruction(constant, genoperandoperand(CmpCwR, constant, rcvrReg)), constant);
		}
		else {
			/* begin checkQuickConstant:forInstruction: */
			anInstruction = genoperandoperand(CmpCqR, constant, rcvrReg);
			if (usesOutOfLineLiteral(anInstruction)) {
				(anInstruction->dependent = locateLiteralsize(constant, BytesPerOop));
			}
		}
	}
	else {
		if (rcvrIsConstant) {
			/* begin genCmpConstant:R: */
			constant1 = ((ssValue(1))->constant);
			if (shouldAnnotateObjectReference(constant1)) {
				annotateobjRef(checkLiteralforInstruction(constant1, genoperandoperand(CmpCwR, constant1, argReg)), constant1);
			}
			else {
				/* begin checkQuickConstant:forInstruction: */
				anInstruction1 = genoperandoperand(CmpCqR, constant1, argReg);
				if (usesOutOfLineLiteral(anInstruction1)) {
					(anInstruction1->dependent = locateLiteralsize(constant1, BytesPerOop));
				}
			}
		}
		else {
			/* begin CmpR:R: */
			assert(!((argReg == SPReg)));
			genoperandoperand(CmpRR, argReg, rcvrReg);
		}
	}
	ssPop(2);
	if ((((fixupAt(nextPC))->targetInstruction)) == 0) {

		/* The next instruction is dead.  we can skip it. */
		deadCode = 1;
		ensureFixupAt(targetPC);
		ensureFixupAt(postBranchPC);
	}
	else {
		assert(!(deadCode));
	}
	if (orNot == ((branchDescriptor->isBranchTrue))) {

		/* a == b ifFalse: ... or a ~~ b ifTrue: ... jump on equal to post-branch pc */
		ensureNonMergeFixupAt(targetPC);
		/* begin JumpZero: */
		jumpTarget = ensureNonMergeFixupAt(postBranchPC);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget));
		/* begin Jump: */
		jumpTarget1 = ensureNonMergeFixupAt(targetPC);
		genoperand(Jump, ((sqInt)jumpTarget1));
	}
	else {

		/* orNot is true for ~~ */
		/* a == b ifTrue: ... or a ~~ b ifFalse: ... jump on equal to target pc */
		/* begin JumpZero: */
		jumpTarget2 = ensureNonMergeFixupAt(targetPC);
		genConditionalBranchoperand(JumpZero, ((sqInt)jumpTarget2));
		/* begin Jump: */
		jumpTarget3 = ensureNonMergeFixupAt(postBranchPC);
		genoperand(Jump, ((sqInt)jumpTarget3));
	}
	if (!deadCode) {
		ssPushConstant(trueObject());
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#initSimStackForFramefulMethod: */
static void NoDbgRegParms
initSimStackForFramefulMethod(sqInt startpc)
{
    CogSimStackEntry * cascade0;
    CogSimStackEntry *desc;
    sqInt i;


	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
	cascade0 = simSelf();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 1);
	(cascade0->registerr = FPReg);
	(cascade0->offset = FoxMFReceiver);
	(cascade0->liveRegister = NoReg);
	for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxCallerSavedIP + (((methodOrBlockNumArgs - i) + 1) * BytesPerWord));
		(desc->bcptr = startpc);
	}
	for (i = (methodOrBlockNumArgs + 1); i <= simStackPtr; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = FPReg);
		(desc->offset = FoxMFReceiver - ((i - methodOrBlockNumArgs) * BytesPerWord));
		(desc->bcptr = startpc);
	}
}


/*	The register receiver (the closure itself) and args are pushed by the
	closure value primitive(s)
	and hence a frameless block has all arguments and copied values pushed to
	the stack. However,
	the method receiver (self) is put in the ReceiverResultReg by the block
	entry. 
 */

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessBlock: */
static void NoDbgRegParms
initSimStackForFramelessBlock(sqInt startpc)
{
    CogSimStackEntry * cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps >= methodOrBlockNumArgs);
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		desc = simStackAt(i);
		(desc->type = SSBaseOffset);
		(desc->spilled = 1);
		(desc->registerr = SPReg);
		(desc->offset = (methodOrBlockNumArgs - i) * BytesPerWord);
		(desc->bcptr = startpc);
	}

	/* N.B. Includes num args */
	simStackPtr = methodOrBlockNumTemps;
	simSpillBase = methodOrBlockNumTemps + 1;
	}

	/* StackToRegisterMappingCogit>>#initSimStackForFramelessMethod: */
static void NoDbgRegParms
initSimStackForFramelessMethod(sqInt startpc)
{
    CogSimStackEntry * cascade0;
    CogSimStackEntry *desc;
    sqInt i;

	cascade0 = simSelf();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = ReceiverResultReg);
	(cascade0->liveRegister = ReceiverResultReg);
	assert(methodOrBlockNumTemps == methodOrBlockNumArgs);
	assert((numRegArgs()) <= 2);
	if (((methodOrBlockNumArgs >= 1) && (methodOrBlockNumArgs <= (numRegArgs())))) {
		desc = simStackAt(1);
		(desc->type = SSRegister);
		(desc->spilled = 0);
		(desc->registerr = Arg0Reg);
		(desc->bcptr = startpc);
		if (methodOrBlockNumArgs > 1) {
			desc = simStackAt(2);
			(desc->type = SSRegister);
			(desc->spilled = 0);
			(desc->registerr = Arg1Reg);
			(desc->bcptr = startpc);
		}
	}
	else {
		for (i = 1; i <= methodOrBlockNumArgs; i += 1) {
			desc = simStackAt(i);
			(desc->type = SSBaseOffset);
			(desc->registerr = SPReg);
			(desc->spilled = 1);
			(desc->offset = (methodOrBlockNumArgs - i) * BytesPerWord);
			(desc->bcptr = startpc);
		}
	}
	simStackPtr = methodOrBlockNumArgs;
	simSpillBase = methodOrBlockNumArgs + 1;
	}


/*	Do not inline (inBlock access) */

	/* StackToRegisterMappingCogit>>#isNonForwarderReceiver: */
static sqInt NoDbgRegParms
isNonForwarderReceiver(sqInt reg)
{
	return ((((simSelf())->liveRegister)) == ReceiverResultReg)
	 && ((inBlock == 0)
	 && (reg == ReceiverResultReg));
}

	/* StackToRegisterMappingCogit>>#liveRegisters */
static sqInt
liveRegisters(void)
{
    sqInt i;
    sqInt regsSet;

	if (needsFrame) {
		regsSet = 0;
	}
	else {
		/* begin registerMaskFor: */
		regsSet = (1U << ReceiverResultReg);
		if ((methodOrBlockNumArgs <= (numRegArgs()))
		 && (methodOrBlockNumArgs > 0)) {
			regsSet = regsSet | ((1U << Arg0Reg));
			if (methodOrBlockNumArgs > 1) {
				regsSet = regsSet | ((1U << Arg1Reg));
			}
		}
	}
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= simStackPtr; i += 1) {
		regsSet = regsSet | (registerMask(simStackAt(i)));
	}
	return regsSet;
}


/*	insert nops for dead code that is mapped so that bc 
	to mc mapping is not many to one */

	/* StackToRegisterMappingCogit>>#mapDeadDescriptorIfNeeded: */
static sqInt NoDbgRegParms
mapDeadDescriptorIfNeeded(BytecodeDescriptor *descriptor)
{
    AbstractInstruction *abstractInstruction;

	flag("annotateInstruction");
	if (((descriptor->isMapped))
	 || ((inBlock > 0)
	 && ((descriptor->isMappedInBlock)))) {
		/* begin annotateBytecode: */
		abstractInstruction = gen(Nop);
		(abstractInstruction->annotation = HasBytecodePC);
	}
	return 0;
}


/*	Spill everything on the simulated stack that needs spilling (that below
	receiver and arguments).
	Marshall receiver and arguments to stack and/or registers depending on arg
	count. If the args don't fit in registers push receiver and args (spill
	everything), but still assign
	the receiver to ReceiverResultReg. */

	/* StackToRegisterMappingCogit>>#marshallSendArguments: */
static void NoDbgRegParms
marshallSendArguments(sqInt numArgs)
{
    sqInt anyRefs;
    CogSimStackEntry * cascade0;
    sqInt i;
    sqInt i1;
    sqInt i2;
    sqInt index;
    sqInt index1;
    sqInt index2;
    sqInt numSpilled;
    sqInt stackPtr;
    sqInt stackPtr1;
    sqInt stackPtr2;

	/* begin ssFlushTo: */
	index2 = (simStackPtr - numArgs) - 1;
	assert(tempsValidAndVolatileEntriesSpilled());
	if (simSpillBase <= index2) {
		for (i2 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index2) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index2)); i2 <= index2; i2 += 1) {
			assert(needsFrame);
			ensureSpilledAtfrom(simStackAt(i2), frameOffsetOfTemporary(i2 - 1), FPReg);
		}
		simSpillBase = index2 + 1;
	}
	if (numArgs > (numRegArgs())) {

		/* If there are no spills and no references to ReceiverResultReg
		   the fetch of ReceiverResultReg from the stack can be avoided
		   by assigning directly to ReceiverResultReg and pushing it. */
		numSpilled = numberOfSpillsInTopNItems(numArgs + 1);
		anyRefs = anyReferencesToRegisterinTopNItems(ReceiverResultReg, numArgs + 1);
		if ((numSpilled > 0)
		 || (anyRefs)) {
			/* begin ssFlushTo: */
			index = simStackPtr;
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			storeToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
		}
		else {
			cascade0 = simStackAt(simStackPtr - numArgs);
			storeToReg(cascade0, ReceiverResultReg);
			(cascade0->type = SSRegister);
			(cascade0->registerr = ReceiverResultReg);
			/* begin ssFlushTo: */
			index1 = simStackPtr;
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index1) {
				for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index1) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index1)); i1 <= index1; i1 += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
				}
				simSpillBase = index1 + 1;
			}
		}
	}
	else {

		/* Move the args to the register arguments, being careful to do
		   so last to first so e.g. previous contents don't get overwritten.
		   Also check for any arg registers in use by other args. */
		if (numArgs > 0) {
			if (numArgs > 1) {
				/* begin ssAllocateRequiredReg:upThrough: */
				stackPtr = simStackPtr - 2;
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg0Reg), stackPtr, simNativeStackPtr);
				/* begin ssAllocateRequiredReg:upThrough: */
				stackPtr1 = simStackPtr - 1;
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg1Reg), stackPtr1, simNativeStackPtr);
			}
			else {
				/* begin ssAllocateRequiredReg:upThrough: */
				stackPtr2 = simStackPtr - 1;
				ssAllocateRequiredRegMaskupThroughupThroughNative((1U << Arg0Reg), stackPtr2, simNativeStackPtr);
			}
		}
		if (numArgs > 1) {
			popToReg(simStackAt(simStackPtr), Arg1Reg);
		}
		if (numArgs > 0) {
			popToReg(simStackAt((simStackPtr - numArgs) + 1), Arg0Reg);
		}
		popToReg(simStackAt(simStackPtr - numArgs), ReceiverResultReg);
	}
	ssPop(numArgs + 1);
}


/*	For assert checking; or rather for avoiding assert fails when dealing with
	the hack for block temps in the SqueakV3PlusClosures bytecode set.
 */

	/* StackToRegisterMappingCogit>>#maybeCompilingFirstPassOfBlockWithInitialPushNil */
static sqInt
maybeCompilingFirstPassOfBlockWithInitialPushNil(void)
{
	return (inBlock == InVanillaBlock)
	 && ((methodOrBlockNumTemps > methodOrBlockNumArgs)
	 && (compilationPass == 1));
}


/*	If this bytecode has a fixup, some kind of merge needs to be done. There
	are 4 cases:
	1) the bytecode has no fixup (fixup isNotAFixup)
	do nothing
	2) the bytecode has a non merge fixup
	the fixup has needsNonMergeFixup.
	The code generating non merge fixup (currently only special selector code)
	is responsible
	for the merge so no need to do it.
	We set deadCode to false as the instruction can be reached from jumps.
	3) the bytecode has a merge fixup, but execution flow *cannot* fall
	through to the merge point.
	the fixup has needsMergeFixup and deadCode = true.
	ignores the current simStack as it does not mean anything 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	4) the bytecode has a merge fixup and execution flow *can* fall through to
	the merge point.
	the fixup has needsMergeFixup and deadCode = false.
	flushes the stack to the stack pointer so the fall through execution path
	simStack is 
	in the state the merge point expects it to be. 
	restores the simStack to the state the jumps to the merge point expects it
	to be.
	
	In addition, if this is a backjump merge point, we patch the fixup to hold
	the current simStackPtr 
	for later assertions. */

	/* StackToRegisterMappingCogit>>#mergeWithFixupIfRequired: */
static sqInt NoDbgRegParms
mergeWithFixupIfRequired(BytecodeFixup *fixup)
{
    CogSimStackEntry * cascade0;
    sqInt i;
    sqInt i1;
    sqInt index;

	/* begin assertCorrectSimStackPtr */
	assert((simSpillBase >= methodOrBlockNumTemps)
	 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
	 && (simSpillBase > methodOrBlockNumArgs)));
	if (needsFrame
	 && (simSpillBase > 0)) {
		assert(((((simStackAt(simSpillBase - 1))->spilled)) == 1)
		 || ((maybeCompilingFirstPassOfBlockWithInitialPushNil())
		 && (simSpillBase > methodOrBlockNumArgs)));
		assert((simSpillBase > simStackPtr)
		 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	}
	if (((fixup->targetInstruction)) == 0) {
		return 0;
	}
	if ((((usqInt)((fixup->targetInstruction)))) == NeedsNonMergeFixupFlag) {
		deadCode = 0;
		return 0;
	}
	assert(isMergeFixup(fixup));
	traceMerge(fixup);
	if (deadCode) {

		/* case 3 */
		/* Would like to assert fixup simStackPtr >= methodOrBlockNumTemps
		   but can't because of the initialNils hack. */
		assert((((fixup->simStackPtr)) >= methodOrBlockNumTemps)
		 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
		simStackPtr = (fixup->simStackPtr);
			}
	else {

		/* case 4 */
		/* begin ssFlushTo: */
		index = simStackPtr;
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= index) {
			for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
			}
			simSpillBase = index + 1;
		}
	}
	deadCode = 0;
	if ((fixup->isTargetOfBackwardBranch)) {
		(fixup->simStackPtr = simStackPtr);
			}
	(fixup->targetInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC));
	assert(simStackPtr == ((fixup->simStackPtr)));
	/* begin restoreSimStackAtMergePoint: */
	((simSelf())->liveRegister = NoReg);
	for (i1 = (methodOrBlockNumTemps + 1); i1 <= simStackPtr; i1 += 1) {
		cascade0 = simStackAt(i1);
		(cascade0->type = SSSpill);
		(cascade0->offset = FoxMFReceiver - ((i1 - methodOrBlockNumArgs) * BytesPerOop));
		(cascade0->registerr = FPReg);
		(cascade0->spilled = 1);
	}
	simSpillBase = simStackPtr + 1;
	return 0;
}

	/* StackToRegisterMappingCogit>>#methodAbortTrampolineFor: */
static sqInt NoDbgRegParms
methodAbortTrampolineFor(sqInt numArgs)
{
	return methodAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}


/*	This is a hook for subclasses to filter out methods they can't deal with. */
/*	Frameless methods with local temporaries cause problems,
	mostly in asserts, and yet they matter not at all for performance.
	Shun them. */

	/* StackToRegisterMappingCogit>>#methodFoundInvalidPostScan */
static sqInt
methodFoundInvalidPostScan(void)
{
	if (!needsFrame) {
		return methodOrBlockNumTemps > methodOrBlockNumArgs;
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#needsFrameIfMod16GENumArgs: */
static sqInt NoDbgRegParms
needsFrameIfMod16GENumArgs(sqInt stackDelta)
{
	return (byte0 % 16) >= methodOrBlockNumArgs;
}


/*	As of August 2013, the code generator can't deal with spills in frameless
	methods (the
	issue is to do with the stack offset to get at an argument, which is
	changed when there's a spill).
	In e.g. TextColor>>#dominates: other ^other class == self class the second
	send of class
	needs also rto allocate a register that the first one used, but the first
	one's register can't be
	spilled. So avoid this by only allowing class to be sent if the stack
	contains a single element. */

	/* StackToRegisterMappingCogit>>#needsFrameIfStackGreaterThanOne: */
static sqInt NoDbgRegParms
needsFrameIfStackGreaterThanOne(sqInt stackDelta)
{
	return stackDelta > 1;
}

	/* StackToRegisterMappingCogit>>#numberOfSpillsInTopNItems: */
static sqInt NoDbgRegParms
numberOfSpillsInTopNItems(sqInt n)
{
    sqInt i;

	for (i = simStackPtr; i >= ((simStackPtr - n) + 1); i += -1) {
		if ((((simStackAt(i))->type)) == SSSpill) {
			return n - (simStackPtr - i);
		}
	}
	return 0;
}

	/* StackToRegisterMappingCogit>>#picAbortTrampolineFor: */
static sqInt NoDbgRegParms
picAbortTrampolineFor(sqInt numArgs)
{
	return picAbortTrampolines[((numArgs < ((numRegArgs()) + 1)) ? numArgs : ((numRegArgs()) + 1))];
}

	/* StackToRegisterMappingCogit>>#prevInstIsPCAnnotated */
static sqInt
prevInstIsPCAnnotated(void)
{
    sqInt prevIndex;
    AbstractInstruction *prevInst;

	if (!(opcodeIndex > 0)) {
		return 0;
	}
	prevIndex = opcodeIndex - 1;
	while (1) {
		if (prevIndex <= 0) {
			return 0;
		}
		prevInst = abstractInstructionAt(prevIndex);
		if (isPCMappedAnnotation((!((prevInst->annotation))
			? 0
			: (prevInst->annotation)))) {
			return 1;
		}
		if (!(((prevInst->opcode)) == Label)) break;
		prevIndex -= 1;
	}
	return 0;
}


/*	Used to mark ReceiverResultReg as dead or not containing simSelf.
	Used when the simStack has already been flushed, e.g. for sends. */

	/* StackToRegisterMappingCogit>>#receiverIsInReceiverResultReg */
static sqInt
receiverIsInReceiverResultReg(void)
{
	return (((simSelf())->liveRegister)) == ReceiverResultReg;
}


/*	When a block must be recompiled due to overestimating the
	numInitialNils fixups must be restored, which means rescannning
	since backward branches need their targets initialized. */

	/* StackToRegisterMappingCogit>>#reinitializeFixupsFrom:through: */
static void NoDbgRegParms
reinitializeFixupsFromthrough(sqInt start, sqInt end)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt nExts;
    sqInt pc;
    BytecodeFixup * self_in_reinitialize;
    sqInt targetPC;

	pc = start;
	nExts = 0;
	while (pc <= end) {
		/* begin reinitialize */
		self_in_reinitialize = fixupAtIndex(pc - initialPC);
		(self_in_reinitialize->targetInstruction) = 0;
		(self_in_reinitialize->simStackPtr) = 0;
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((isBranch(descriptor))
		 && ((assert(((descriptor->spanFunction)) != null),
		(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0))) {
			/* begin spanFor:at:exts:in: */
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			/* begin initializeFixupAt: */
			fixup = fixupAtIndex(targetPC - initialPC);
			/* begin initializeFixup: */
			(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
			/* begin setIsBackwardBranchFixup */
			(fixup->isTargetOfBackwardBranch) = 1;
		}
		if ((descriptor->isBlockCreation)) {
			/* begin spanFor:at:exts:in: */
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			pc = (pc + ((descriptor->numBytes))) + distance;
		}
		else {
			pc += (descriptor->numBytes);
		}
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: 0);
	}
}


/*	Scan the block to determine if the block needs a frame or not */

	/* StackToRegisterMappingCogit>>#scanBlock: */
static sqInt NoDbgRegParms
scanBlock(BlockStart *blockStart)
{
    BytecodeDescriptor *descriptor;
    sqInt end;
    sqInt framelessStackDelta;
    sqInt nExts;
    sqInt numPushNils;
    sqInt (* const numPushNilsFunction)(struct _BytecodeDescriptor *,sqInt,sqInt,sqInt) = squeakV3orSistaV1NumPushNils;
    sqInt pc;
    sqInt pushingNils;

	needsFrame = 0;
	prevBCDescriptor = null;
	methodOrBlockNumArgs = (blockStart->numArgs);
	inBlock = InVanillaBlock;
	pc = (blockStart->startpc);
	end = ((blockStart->startpc)) + ((blockStart->span));
	framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0))));
	pushingNils = 1;
	while (pc < end) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (!needsFrame) {
			if ((((descriptor->needsFrameFunction)) == null)
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {
				needsFrame = 1;
			}
			else {
				framelessStackDelta += (descriptor->stackDelta);
			}
		}
		/* begin maybeNoteDescriptor:blockStart: */
		if ((descriptor->isInstVarRef)) {
			(blockStart->hasInstVarRef = 1);
		}
		if (pushingNils
		 && (!((descriptor->isExtension)))) {

			/* Count the initial number of pushed nils acting as temp initializers.  We can't tell
			   whether an initial pushNil is an operand reference or a temp initializer, except
			   when the pushNil is a jump target (has a fixup), which never happens:
			   self systemNavigation browseAllSelect:
			   [:m| | ebc |
			   (ebc := m embeddedBlockClosures
			   select: [:ea| ea decompile statements first isMessage]
			   thenCollect: [:ea| ea decompile statements first selector]) notEmpty
			   and: [(#(whileTrue whileFalse whileTrue: whileFalse:) intersection: ebc) notEmpty]]
			   or if the bytecode set has a push multiple nils bytecode.  We simply count initial nils.
			   Rarely we may end up over-estimating.  We will correct by checking the stack depth
			   at the end of the block in compileBlockBodies. */
			if (((numPushNils = numPushNilsFunction(descriptor, pc, nExts, methodObj))) > 0) {
				assert(((descriptor->numBytes)) == 1);
				(blockStart->numInitialNils = ((blockStart->numInitialNils)) + numPushNils);
			}
			else {
				pushingNils = 0;
			}
		}
		/* begin nextBytecodePCFor:at:exts:in: */
		pc = (pc + ((descriptor->numBytes))) + (((descriptor->isBlockCreation)
	? ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)
	: 0));
		if ((descriptor->isExtension)) {
			nExts += 1;
		}
		else {
			nExts = (extA = (numExtB = (extB = 0)));
		}
		prevBCDescriptor = descriptor;
	}
	if (!needsFrame) {
		assert((framelessStackDelta >= 0)
		 && (((blockStart->numInitialNils)) >= framelessStackDelta));
		(blockStart->numInitialNils = ((blockStart->numInitialNils)) - framelessStackDelta);
	}
	return 0;
}


/*	Scan the method (and all embedded blocks) to determine
	- what the last bytecode is; extra bytes at the end of a method are used
	to encode things like source pointers or temp names
	- if the method needs a frame or not
	- what are the targets of any backward branches.
	- how many blocks it creates
	Answer the block count or on error a negative error code */

	/* StackToRegisterMappingCogit>>#scanMethod */
static sqInt
scanMethod(void)
{
    BytecodeDescriptor *descriptor;
    sqInt distance;
    BytecodeFixup *fixup;
    sqInt framelessStackDelta;
    sqInt latestContinuation;
    sqInt nExts;
    sqInt numBlocks;
    sqInt pc;
    sqInt seenInstVarStore;
    sqInt targetPC;

	needsFrame = (useTwoPaths = (seenInstVarStore = 0));
	/* begin maybeInitNumCounters */
	numCounters = 0;
	prevBCDescriptor = null;
	if ((primitiveIndex > 0)
	 && (isQuickPrimitiveIndex(primitiveIndex))) {
		return 0;
	}
	pc = (latestContinuation = initialPC);
	numBlocks = (framelessStackDelta = (nExts = (extA = (numExtB = (extB = 0)))));
	while (pc <= endPC) {
		byte0 = (fetchByteofObject(pc, methodObj)) + bytecodeSetOffset;
		descriptor = generatorAt(byte0);
		if ((descriptor->isExtension)) {
			if (((descriptor->opcode)) == Nop) {

				/* unknown bytecode tag; see Cogit class>>#generatorTableFrom: */
				return EncounteredUnknownBytecode;
			}
			loadSubsequentBytesForDescriptorat(descriptor, pc);
			((descriptor->generator))();
		}
		if (((descriptor->isReturn))
		 && (pc >= latestContinuation)) {
			endPC = pc;
		}
		if (!needsFrame) {
			if ((((descriptor->needsFrameFunction)) == null)
			 || (((descriptor->needsFrameFunction))(framelessStackDelta))) {

				/* With immutability we win simply by avoiding a frame build if the receiver is young and not immutable. */
#        if IMMUTABILITY
				if ((descriptor->is1ByteInstVarStore)) {
					useTwoPaths = 1;
				}
				else {
					needsFrame = 1;
					useTwoPaths = 0;
				}
#        else // IMMUTABILITY
				needsFrame = 1;
				useTwoPaths = 0;
#        endif
			}
			else {

				/* Without immutability we win if there are two or more stores and the receiver is new. */
				framelessStackDelta += (descriptor->stackDelta);
#        if IMMUTABILITY
#        else
				if ((descriptor->is1ByteInstVarStore)) {
					if (seenInstVarStore) {
						useTwoPaths = 1;
					}
					else {
						seenInstVarStore = 1;
					}
				}
#        endif // IMMUTABILITY
			}
		}
		if (isBranch(descriptor)) {
			/* begin spanFor:at:exts:in: */
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			if ((assert(((descriptor->spanFunction)) != null),
			(((descriptor->spanFunction))(descriptor, pc, nExts, methodObj)) < 0)) {
				/* begin initializeFixupAt: */
				fixup = fixupAtIndex(targetPC - initialPC);
				/* begin initializeFixup: */
				(fixup->targetInstruction) = ((AbstractInstruction *) NeedsMergeFixupFlag);
				/* begin setIsBackwardBranchFixup */
				(fixup->isTargetOfBackwardBranch) = 1;
			}
			else {
				latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
				/* begin maybeCountCounter */
				numCounters += 1;
			}
		}
		latestContinuation = maybeDealWithUnsafeJumpForDescriptorpclatestContinuation(descriptor, pc, latestContinuation);
		if ((descriptor->isBlockCreation)) {
			numBlocks += 1;
			/* begin spanFor:at:exts:in: */
			distance = ((descriptor->spanFunction))(descriptor, pc, nExts, methodObj);
			targetPC = (pc + ((descriptor->numBytes))) + distance;
			latestContinuation = ((latestContinuation < targetPC) ? targetPC : latestContinuation);
		}
		pc += (descriptor->numBytes);
		nExts = ((descriptor->isExtension)
			? nExts + 1
			: (extA = (numExtB = (extB = 0))));
		prevBCDescriptor = descriptor;
	}
	return numBlocks;
}

	/* StackToRegisterMappingCogit>>#squeakV3orSistaV1PushNilSize:numInitialNils: */
static sqInt NoDbgRegParms
squeakV3orSistaV1PushNilSizenumInitialNils(sqInt aMethodObj, sqInt numInitialNils)
{
	return (methodUsesAlternateBytecodeSet(aMethodObj)
		? /* begin sistaV1PushNilSize:numInitialNils: */ numInitialNils
		: numInitialNils);
}

	/* StackToRegisterMappingCogit>>#squeakV3orSistaV1:Num:Push:Nils: */
static sqInt NoDbgRegParms
squeakV3orSistaV1NumPushNils(BytecodeDescriptor *descriptor, sqInt pc, sqInt nExts, sqInt aMethodObj)
{
	return (bytecodeSetOffset == 0
		? (((descriptor->generator)) == genPushConstantNilBytecode
				? 1
				: 0)
		: (/* begin sistaV1:Num:Push:Nils: */
			(((descriptor->generator)) == genPushConstantNilBytecode
					? 1
					: 0)));
}

	/* StackToRegisterMappingCogit>>#ssAllocateRequiredRegMask:upThrough:upThroughNative: */
static void NoDbgRegParms
ssAllocateRequiredRegMaskupThroughupThroughNative(sqInt requiredRegsMask, sqInt stackPtr, sqInt nativeStackPtr)
{
    sqInt i;
    sqInt i1;
    sqInt lastRequired;
    sqInt lastRequiredNative;
    sqInt liveRegs;

	lastRequired = -1;

	/* compute live regs while noting the last occurrence of required regs.
	   If these are not free we must spill from simSpillBase to last occurrence.
	   Note we are conservative here; we could allocate FPReg in frameless methods. */
	lastRequiredNative = -1;
	/* begin registerMaskFor:and: */
	liveRegs = (1U << FPReg) | (1U << SPReg);
	for (i = (((simSpillBase < 0) ? 0 : simSpillBase)); i <= stackPtr; i += 1) {
		liveRegs = liveRegs | (registerMask(simStackAt(i)));
		if ((((registerMask(simStackAt(i))) & requiredRegsMask) != 0)) {
			lastRequired = i;
		}
	}
	if (((liveRegs & requiredRegsMask) != 0)) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= lastRequired) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < lastRequired) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : lastRequired)); i1 <= lastRequired; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = lastRequired + 1;
		}
		assert(!(((((liveRegisters()) & requiredRegsMask) != 0))));
	}
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughReceiverVariable: */
static void NoDbgRegParms
ssFlushUpThroughReceiverVariable(sqInt slotIndex)
{
    sqInt i;
    sqInt index;

	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((((simStackAt(index))->type)) == SSBaseOffset)
		 && (((((simStackAt(index))->registerr)) == ReceiverResultReg)
		 && ((((simStackAt(index))->offset)) == (slotOffsetOfInstVarIndex(slotIndex))))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
	l1:	/* end ssFlushUpThrough: */;
}


/*	Any occurrences on the stack of the value being stored (which is the top
	of stack)
	must be flushed, and hence any values colder than them stack. */

	/* StackToRegisterMappingCogit>>#ssFlushUpThroughTemporaryVariable: */
static void NoDbgRegParms
ssFlushUpThroughTemporaryVariable(sqInt tempIndex)
{
    sqInt i;
    sqInt index;
    sqInt offset;

	offset = ((simStackAt(tempIndex + 1))->offset);
	assert(offset == (frameOffsetOfTemporary(tempIndex)));
	/* begin ssFlushUpThrough: */
	assert(simSpillBase >= 0);
	for (index = (simStackPtr - 1); index >= simSpillBase; index += -1) {
		if (((((simStackAt(index))->type)) == SSBaseOffset)
		 && (((((simStackAt(index))->registerr)) == FPReg)
		 && ((((simStackAt(index))->offset)) == offset))) {
			/* begin ssFlushTo: */
			assert(tempsValidAndVolatileEntriesSpilled());
			if (simSpillBase <= index) {
				for (i = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < index) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : index)); i <= index; i += 1) {
					assert(needsFrame);
					ensureSpilledAtfrom(simStackAt(i), frameOffsetOfTemporary(i - 1), FPReg);
				}
				simSpillBase = index + 1;
			}
			goto l1;
		}
	}
	l1:	/* end ssFlushUpThrough: */;
}

	/* StackToRegisterMappingCogit>>#ssPop: */
static void NoDbgRegParms
ssPop(sqInt n)
{
    sqInt i;

	assert(((simStackPtr - n) >= methodOrBlockNumTemps)
	 || (((!needsFrame)
	 && ((simStackPtr - n) >= 0))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil())));
	simStackPtr -= n;
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
}

	/* StackToRegisterMappingCogit>>#ssPushAnnotatedConstant: */
static sqInt NoDbgRegParms
ssPushAnnotatedConstant(sqInt literal)
{
    AbstractInstruction *abstractInstruction;

	ssPushConstant(literal);
	/* begin annotateInstructionForBytecode */
	if (prevInstIsPCAnnotated()) {
		/* begin Nop */
		abstractInstruction = gen(Nop);
	}
	else {
		/* begin Label */
		abstractInstruction = genoperandoperand(Label, (labelCounter += 1), bytecodePC);
	}
	(abstractInstruction->annotation = HasBytecodePC);
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushBase:offset: */
static sqInt NoDbgRegParms
ssPushBaseoffset(sqInt reg, sqInt offset)
{
    CogSimStackEntry * cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSBaseOffset);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->offset = offset);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushConstant: */
static sqInt NoDbgRegParms
ssPushConstant(sqInt literal)
{
    CogSimStackEntry * cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSConstant);
	(cascade0->spilled = 0);
	(cascade0->constant = literal);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushDesc: */
static sqInt NoDbgRegParms
ssPushDesc(SimStackEntry simStackEntry)
{
    sqInt i;

	if (((simStackEntry.type)) == SSSpill) {
		(simStackEntry.type = SSBaseOffset);
	}
	(simStackEntry.spilled = 0);
	(simStackEntry.bcptr = bytecodePC);
	simStack[(simStackPtr += 1)] = simStackEntry;
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPushRegister: */
static sqInt NoDbgRegParms
ssPushRegister(sqInt reg)
{
    CogSimStackEntry * cascade0;
    sqInt i;

	ssPush(1);
	cascade0 = ssTop();
	(cascade0->type = SSRegister);
	(cascade0->spilled = 0);
	(cascade0->registerr = reg);
	(cascade0->bcptr = bytecodePC);
	/* begin updateSimSpillBase */
	assert(((simSpillBase > methodOrBlockNumTemps)
	 && (simStackPtr >= methodOrBlockNumTemps))
	 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()));
	if (simSpillBase > simStackPtr) {
		simSpillBase = simStackPtr + 1;
		while (((simSpillBase - 1) > methodOrBlockNumTemps)
		 && (!(((simStackAt(simSpillBase - 1))->spilled)))) {
			simSpillBase -= 1;
		}
	}
	else {
		while ((((simStackAt(simSpillBase))->spilled))
		 && (simSpillBase <= simStackPtr)) {
			simSpillBase += 1;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i <= ((((simSpillBase - 1) < simStackPtr) ? (simSpillBase - 1) : simStackPtr)); i += 1) {
		assert((((simStackAt(i))->spilled)) == 1);
	}
	assert((simSpillBase > simStackPtr)
	 || ((((simStackAt(simSpillBase))->spilled)) == 0));
	return 0;
}

	/* StackToRegisterMappingCogit>>#ssPush: */
static void NoDbgRegParms
ssPush(sqInt n)
{
	simStackPtr += n;
}

	/* StackToRegisterMappingCogit>>#ssSelfDescriptor */
static SimStackEntry
ssSelfDescriptor(void)
{
	return simStack[0];
}


/*	In addition to ssStorePop:toReg:, if this is a store and not
	a popInto I change the simulated stack to use the register 
	for the top value */

	/* StackToRegisterMappingCogit>>#ssStoreAndReplacePop:toReg: */
static void NoDbgRegParms
ssStoreAndReplacePoptoReg(sqInt popBoolean, sqInt reg)
{
    char topSpilled;

	topSpilled = ((ssTop())->spilled);
	ssStorePoptoReg(popBoolean
	 || (topSpilled), reg);
	if (!popBoolean) {
		if (!topSpilled) {
			ssPop(1);
		}
		ssPushRegister(reg);
	}
}


/*	Store or pop the top simulated stack entry to a register.
	Use preferredReg if the entry is not itself a register.
	Answer the actual register the result ends up in. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toPreferredReg: */
static sqInt NoDbgRegParms
ssStorePoptoPreferredReg(sqInt popBoolean, sqInt preferredReg)
{
    sqInt actualReg;

	actualReg = preferredReg;
	if ((((ssTop())->type)) == SSRegister) {
		assert(!(((ssTop())->spilled)));
		actualReg = ((ssTop())->registerr);
	}
	ssStorePoptoReg(popBoolean, actualReg);
	return actualReg;
}


/*	Store or pop the top simulated stack entry to a register.
	N.B.: popToReg: and storeToReg: does not generate anything if 
	it moves a register to the same register. */

	/* StackToRegisterMappingCogit>>#ssStorePop:toReg: */
static void NoDbgRegParms
ssStorePoptoReg(sqInt popBoolean, sqInt reg)
{
	if (popBoolean) {
		popToReg(ssTop(), reg);
		ssPop(1);
	}
	else {
		storeToReg(ssTop(), reg);
	}
}

	/* StackToRegisterMappingCogit>>#ssTop */
static CogSimStackEntry *
ssTop(void)
{
	return simStackAt(simStackPtr);
}

	/* StackToRegisterMappingCogit>>#ssValue: */
static CogSimStackEntry * NoDbgRegParms
ssValue(sqInt n)
{
	return simStackAt(simStackPtr - n);
}

	/* StackToRegisterMappingCogit>>#stackEntryIsBoolean: */
static sqInt NoDbgRegParms
stackEntryIsBoolean(CogSimStackEntry *simStackEntry)
{
	return (((simStackEntry->type)) == SSConstant)
	 && ((((simStackEntry->constant)) == (trueObject()))
	 || (((simStackEntry->constant)) == (falseObject())));
}


/*	Answer if the stack is valid up to, but not including, simSpillBase. */

	/* StackToRegisterMappingCogit>>#tempsValidAndVolatileEntriesSpilled */
static sqInt
tempsValidAndVolatileEntriesSpilled(void)
{
    sqInt culprit;
    sqInt i;

	culprit = 0;
	for (i = 1; i <= methodOrBlockNumTemps; i += 1) {
		if (!(((((simStackAt(i))->type)) == SSBaseOffset)
			 || (maybeCompilingFirstPassOfBlockWithInitialPushNil()))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	for (i = (methodOrBlockNumTemps + 1); i < simSpillBase; i += 1) {
		if (!(((simStackAt(i))->spilled))) {
			if (!culprit) {
				culprit = i;
			}
			return 0;
		}
	}
	return 1;
}


/*	If the sequence of bytecodes is
	push: (Array new: 1)
	popIntoTemp: tempIndex
	pushConstant: const or pushTemp: n
	popIntoTemp: 0 inVectorAt: tempIndex
	collapse this into
	tempAt: tempIndex put: {const or temp}
	and answer true, otherwise answer false.
	One might think that we should look for a sequence of more than
	one pushes and pops but this is extremely rare.
	Exclude pushRcvr: n to avoid potential complications with context inst
	vars.  */

	/* StackToRegisterMappingCogit>>#tryCollapseTempVectorInitializationOfSize: */
static sqInt NoDbgRegParms
tryCollapseTempVectorInitializationOfSize(sqInt slots)
{
    sqInt pc;
    sqInt pc1;
    sqInt pc2;
    BytecodeDescriptor *pushArrayDesc;
    BytecodeDescriptor *pushValueDesc;
    sqInt reg;
    sqInt remoteTempIndex;
    BytecodeDescriptor *storeArrayDesc;
    BytecodeDescriptor *storeValueDesc;
    sqInt tempIndex;

	if (slots != 1) {
		return 0;
	}
	/* begin generatorForPC: */
	pushArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(bytecodePC, methodObj)));
	assert(((pushArrayDesc->generator)) == genPushNewArrayBytecode);
	/* begin generatorForPC: */
	pc = bytecodePC + ((pushArrayDesc->numBytes));
	storeArrayDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc, methodObj)));
	if (((storeArrayDesc->generator)) == genStoreAndPopTemporaryVariableBytecode) {
		tempIndex = (fetchByteofObject(bytecodePC + ((pushArrayDesc->numBytes)), methodObj)) & 7;
	}
	else {
		if (!(((storeArrayDesc->generator)) == genLongStoreAndPopTemporaryVariableBytecode)) {
			return 0;
		}
		tempIndex = fetchByteofObject((bytecodePC + ((pushArrayDesc->numBytes))) + 1, methodObj);
	}
	/* begin generatorForPC: */
	pc1 = (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes));
	pushValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc1, methodObj)));
	if (!((((pushValueDesc->generator)) == genPushLiteralConstantBytecode)
		 || ((((pushValueDesc->generator)) == genPushQuickIntegerConstantBytecode)
		 || (((pushValueDesc->generator)) == genPushTemporaryVariableBytecode)))) {
		return 0;
	}
	/* begin generatorForPC: */
	pc2 = ((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes));
	storeValueDesc = generatorAt(bytecodeSetOffset + (fetchByteofObject(pc2, methodObj)));
	remoteTempIndex = fetchByteofObject((((bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + 2, methodObj);
	if (!((((storeValueDesc->generator)) == genStoreAndPopRemoteTempLongBytecode)
		 && (tempIndex == remoteTempIndex))) {
		return 0;
	}
	genNewArrayOfSizeinitialized(1, 0);
	evaluateat(pushValueDesc, (bytecodePC + ((pushArrayDesc->numBytes))) + ((storeArrayDesc->numBytes)));
	reg = ssStorePoptoPreferredReg(1, TempReg);
	genStoreSourceRegslotIndexintoNewObjectInDestReg(reg, 0, ReceiverResultReg);
	ssPushRegister(ReceiverResultReg);
	evaluateat(storeArrayDesc, bytecodePC + ((pushArrayDesc->numBytes)));

	/* + pushArrayDesc numBytes this gets added by nextBytecodePCFor:at:exts:in: */
	bytecodePC = ((bytecodePC + ((storeArrayDesc->numBytes))) + ((pushValueDesc->numBytes))) + ((storeValueDesc->numBytes));
	return 1;
}

	/* StackToRegisterMappingCogit>>#violatesEnsureSpilledSpillAssert */
static sqInt
violatesEnsureSpilledSpillAssert(void)
{
	return 1;
}


/*	Used when ReceiverResultReg is allocated for other than simSelf, and
	there may be references to ReceiverResultReg which need to be spilled. */

	/* StackToRegisterMappingCogit>>#voidReceiverResultRegContainsSelf */
static void
voidReceiverResultRegContainsSelf(void)
{
    sqInt i;
    sqInt i1;
    sqInt spillIndex;

	/* begin voidReceiverOptStatus */
	((simSelf())->liveRegister = NoReg);
	spillIndex = 0;
	for (i = ((((methodOrBlockNumTemps + 1) < simSpillBase) ? simSpillBase : (methodOrBlockNumTemps + 1))); i <= simStackPtr; i += 1) {
		if ((registerOrNone(simStackAt(i))) == ReceiverResultReg) {
			spillIndex = i;
		}
	}
	if (spillIndex > 0) {
		/* begin ssFlushTo: */
		assert(tempsValidAndVolatileEntriesSpilled());
		if (simSpillBase <= spillIndex) {
			for (i1 = (((((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) < spillIndex) ? ((((((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) < simStackPtr) ? (((simSpillBase < (methodOrBlockNumTemps + 1)) ? (methodOrBlockNumTemps + 1) : simSpillBase)) : simStackPtr)) : spillIndex)); i1 <= spillIndex; i1 += 1) {
				assert(needsFrame);
				ensureSpilledAtfrom(simStackAt(i1), frameOffsetOfTemporary(i1 - 1), FPReg);
			}
			simSpillBase = spillIndex + 1;
		}
	}
}
